<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Wed, 18 Jun 2025 16:39:48 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2MzA3MTQzOX0.SxRBIud_XE2YWQFaIJD9BPB1w-3JsFhiRkJIIe9Yq-g/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Guatemalan Engineer Ascends From Poverty to Ph.D.</title><link>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&width=1200&height=800&coordinates=0%2C242%2C0%2C243"/><br/><br/><p>Although she is just now starting her career as a tech professional,<a href="https://fr.linkedin.com/in/mayra-yucely-beb-caal-b1a1b969" rel="noopener noreferrer" target="_blank"> Mayra Yucely Beb Caal</a> has already overcome towering obstacles. The IEEE member sees her life as an example for other young people, demonstrating that they can succeed despite disadvantages they face due to their gender, ethnicity, language, or economic background.</p><p>Born in Cobán, the capital of Alta Verapaz in northern Guatemala, she grew up in a community far removed from the world of technology. But she attributes her success to having been steeped in the region’s cultural richness and her people’s unshakable resilience. The daughter of a single mother who was a schoolteacher, Caal says she spent her early years living with her aunts while her mother worked in distant towns for weeks at a time to provide for the family. In her community—mostly descendants of the indigenous<a href="https://en.wikipedia.org/wiki/Q%CA%BCeqchi%CA%BC" rel="noopener noreferrer" target="_blank"> Maya-Kekchi</a> people—technology was rarely discussed. Pursuing a degree meant studying to become a physician, the most prestigious occupation anyone there was aware of.</p><p>No one imagined that a girl from Cobán would one day hold a doctorate in engineering or conduct cancer research in France.</p><p>On the path to her ambitious goals, Caal got a big assist from IEEE. She received a <a href="https://ieeesystemscouncil.org/awards/james-o-gray-scholarship" rel="noopener noreferrer" target="_blank">Gray scholarship</a>, awarded by the IEEE Systems Council to students pursuing graduate studies in process control systems engineering, plant automation, or instrumentation measurement. The US $5,000 award supplemented other scholarships which helped her to study for her Ph.D. </p><h2>Discovering robotics and mechatronics in high school</h2><p>Caal was introduced to technology when, at age 14, she received a government scholarship to attend the<a href="https://au.linkedin.com/school/intecapoficial/" rel="noopener noreferrer" target="_blank"> Instituto Técnico de Capacitación y Productividad</a>, a high school in Guatemala City. It was her first exposure to electronics, <a href="https://spectrum.ieee.org/topic/robotics/" target="_self">robotics</a>, and <a href="https://spectrum.ieee.org/houston-mechatronics-raises-20m-to-bring-nasa-expertise-to-transforming-robot-submersibles" target="_self">mechatronics</a> (an interdisciplinary field that combines mechanical engineering, electronics, computer science, and control systems)—subjects that weren’t taught in her local school. Caal was fascinated by the ability to study the fields, though her family couldn’t afford the tuition to the private universities where she could earn a degree. But that didn’t dissuade her.</p><h2>Pursuing a mechatronics career despite gender barriers</h2><p>She applied for a scholarship from<a href="https://fundacionjbg.org/" rel="noopener noreferrer" target="_blank"> the Gutiérrez Foundation</a>, named for the founder of <a href="https://somoscmi.com/en/" rel="noopener noreferrer" target="_blank">CMI</a>, a Guatemala-based multinational company. The foundation’s scholarship covers full tuition, fees, and the cost of books for the duration of a recipient’s undergraduate studies.</p><p>In 2016 Caal earned a bachelor’s degree in mechatronics engineering at the<a href="https://www.uvg.edu.gt/" rel="noopener noreferrer" target="_blank"> Universidad del Valle de Guatemala</a>, also in Guatemala City. There were few women in her class.</p><p>The job market was unwelcoming, however, she says. Despite her credentials, employers often required five years of experience for entry-level positions, and they expressed a preference for male employees, she says. It took six months to land her first job as a mechanical maintenance supervisor near her hometown.</p><p>She held that job for six months before moving back to Guatemala City in search of better opportunities. She took a position as head of mechanical maintenance at <a href="https://mayaprin.com/" rel="noopener noreferrer" target="_blank">Mayaprin</a>, a company specializing in commercial printing services, but she wasn’t satisfied with her career trajectory.</p><h2>Earning an engineering education abroad</h2><p>Caal decided to return to school in 2018 to pursue a master’s degree in mechatronics and micromechatronics engineering. She received a scholarship from the<a href="https://erasmus-plus.ec.europa.eu/opportunities/opportunities-for-individuals/students/erasmus-mundus-joint-masters" rel="noopener noreferrer" target="_blank"> Mundus Joint Master</a> program, part of a European Commission–sponsored initiative that provides funding for education, training, and youth in sports. Because the Mundus scholarship requires recipients to study at several universities, she took classes at schools in Europe and Africa, including <a href="https://www.supmicrotech.fr/en" rel="noopener noreferrer" target="_blank">École Nationale Supérieure de Mécanique et des Microtechniques</a>, <a href="https://nu.edu.eg/" rel="noopener noreferrer" target="_blank">Nile University</a>, and <a href="https://www.uniovi.es/en/" rel="noopener noreferrer" target="_blank">Universidad de Oviedo</a>. Her studies focused on mechatronics and microelectronics, and the courses were taught in French, English, and Spanish.</p><p>The multilingual challenge was immense, she says. She recently had learned English, and French was completely new to her. Yet she persevered, driven by her goal of working on technology that could serve humanity.</p><p>She received a master’s degree from Universidad de Oviedo in 2020 and was accepted into a Ph.D. program at <a href="https://www.univ-fcomte.fr/universite-bourgogne-franche-comte" rel="noopener noreferrer" target="_blank">Université de Bourgogne Franche-Comté</a>, in Besançon, France. Her doctoral studies were aided by the Gray scholarship.</p><p>Her research led to a full-time job last year as an R&D engineer focused on mechatronics and robotics at<a href="https://www.hyprview.com/" rel="noopener noreferrer" target="_blank"> HyprView</a> in Caen, France. The startup, founded in 2021, develops software to assist with medical data analysis and boost the performance of imaging tools.</p><p>Caal says she is part of a team that uses AI and automated systems to improve cancer detection. Although she has held the position for less than a year, she says she already feels she is contributing to public health through applied technology.</p><h2>IEEE support and STEM mentorship</h2><p>Through much of Caal’s journey, IEEE has played a critical role. As an undergraduate, she was vice president and then president of her university’s <a href="https://www.facebook.com/IEEEUVG/?locale=pt_PT" rel="noopener noreferrer" target="_blank">IEEE student branch</a>. Her first international conference experience came from attending<a href="https://r9.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Region 9</a> conferences, which she says opened her eyes to the world of research, publishing, and the global engineering community.</p><p>She organized outreach efforts to local schools, conducting simple experiments to encourage girls to consider STEM careers. Her efforts were in direct opposition to longstanding gender norms in Guatemala. Caal was also an active member of the IEEE <a href="https://www.femto-st.fr/en/femto-st-student-chapter-0" rel="noopener noreferrer" target="_blank">student branch at FEMTO-ST /Université de Bourgogne Franche-Comté</a>.</p><p>Today, Caal continues to advise these student branches while advancing her career in France.</p><p>Language issues and gender bias remain obstacles: “As a young woman leading male engineers, I have repeatedly had to prove my competence in ways my male peers haven’t,” she says. But the challenges have only strengthened her resolve, she adds.</p><p>Eventually, she says, she hopes to return to Guatemala to help build a stronger research infrastructure there with sufficient career opportunities for tech professionals in industry and academia. She says she also wants to ensure that children in even the most rural, poverty-stricken schools have access to food, electricity, and the Internet.</p><p>Her mission is clear: “To use technology to serve a purpose, always aimed at improving lives.”</p><p>“I don’t want to create technology just for the sake of it,” she says. “I want it to mean something—to help solve real problems in society, like the ones I faced early on.” </p>]]></description><pubDate>Tue, 17 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</guid><category>Cancer detection</category><category>Guatemala</category><category>Ieee member news</category><category>Ieee systems council</category><category>James o. gray scholarship</category><category>Mechatronics</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&amp;width=980"></media:content></item><item><title>Why JPEGs Still Rule the Web</title><link>https://spectrum.ieee.org/jpeg-image-format-history</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&width=1200&height=800&coordinates=0%2C250%2C0%2C250"/><br/><br/><p><span>For roughly three decades, the JPEG has been the World Wide Web’s primary image format. But it wasn’t the one the Web started with. In fact, the first mainstream graphical browser, NCSA Mosaic, didn’t initially support inline JPEG files—</span><a href="https://ftp.jurassic.nl/pub/irix/mosaic/Mac/FAQ/FAQ.HTML" target="_blank">just inline GIFs</a><span>, along with a couple of other </span><a href="https://spectrum.ieee.org/carnegie-mellon-is-saving-old-software-from-oblivion" target="_blank">formats forgotten to history</a><span>. However, the JPEG had many advantages over the format it quickly usurped.</span></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://tedium.co/" target="_blank"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://tedium.co/" target="_blank"><img alt='Tedium logo, a red rectangle with the word Tedium in white, above the text "This post originally appeared on Tedium."' class="rm-shortcode" data-rm-shortcode-id="c603546dab9e1dd1612b1364d3107471" data-rm-shortcode-name="rebelmouse-image" id="6aeb5" loading="lazy" src="https://spectrum.ieee.org/media-library/tedium-logo-a-red-rectangle-with-the-word-tedium-in-white-above-the-text-this-post-originally-appeared-on-tedium.png?id=60568211&width=980"/></a><small class="image-media media-photo-credit" placeholder="add photo credit..."><a href="https://spectrum.ieee.org/media-library/eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjpbwfnzsi6imh0dhbzoi8vyxnzzxrzlnjibc5tcy82mdu2odixms9vcmlnaw4ucg5niiwizxhwaxjlc19hdci6mtc1nzmynzi1mn0._gbglxpbsmfwoobs84_whxbl_vnslwx1geovlhgvwku/image.png?width=980" target="_blank"> </a></small></p><p>Despite not appearing together right away—the JPEG first appeared in Netscape in 1995, three years after the image standard was officially published—the JPEG and Web browser fit together naturally. JPEG files degraded more gracefully than GIFs, retaining more of the picture’s initial form—and that allowed the format to scale to greater levels of success. While it wasn’t capable of animation, it progressively expanded from something a modem could pokily render to a format that was good enough for high-end professional photography.</p><p>For the Internet’s purposes, the degradation was the important part. But it wasn’t the only thing that made the JPEG immensely valuable to the digital world. An essential part was that it was a documented standard built by numerous stakeholders.</p><h2>The GIF was a de facto standard. The JPEG was an actual one</h2><p>How important is it that JPEG was a standard? Let me tell you a story.</p><p>During <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2013/05/21/an-honor-for-the-creator-of-the-gif/?smid=tw-nytimes" target="_blank">a 2013 <em><em>New York Times</em></em> interview</a> conducted just before he received an award honoring his creation, GIF creator Steve Wilhite stepped into a debate he unwittingly created. <span>Simply put, nobody knew how to pronounce the acronym for the image format he had fostered, the Graphics Interchange Format. He used the moment to attempt to set the record straight—it was pronounced like the peanut butter brand: “It is a soft ‘G,’ pronounced ‘jif.’ End of story,” he said.</span></p><p>I <a href="https://shortformblog.com/post/51026114908/steve-wilhite-gif-award" target="_blank">posted a quote from Wilhite</a> on my popular Tumblr around that time, a period when the social media site was the center of the GIF universe. And soon afterward, my post got thousands of reblogs—nearly all of them disagreeing with Wilhite. Soon, <a href="https://knowyourmeme.com/memes/gif-vs-jif-pronunciation-debate/" target="_blank">Wilhite’s quote became a meme</a>.</p><p>The situation paints how Wilhite, who died in 2022, did not develop his format by committee. He could say it sounded like “JIF” because he built it himself. He was handed the project as a CompuServe employee in 1987; he produced the object, and that was that. The initial document describing how it works? <a href="https://www.w3.org/Graphics/GIF/spec-gif87.txt" target="_blank">Dead simple</a>. Thirty-eight years later, we’re still using the GIF—but it never rose to the same prevalence of JPEG.</p><p>The JPEG, which formally emerged about five years later, was very much <em><em>not</em></em> that situation. Far from it, in fact—it’s the difference between a de facto standard and an actual one. And that proved essential to its eventual ubiquity.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Full-resolution photo of a sunlit pine forest with a narrow trail winding through the trees and grassy undergrowth." class="rm-shortcode" data-rm-shortcode-id="d5f40fd52d60e8ec21c06940db3febc7" data-rm-shortcode-name="rebelmouse-image" id="2999a" loading="lazy" src="https://spectrum.ieee.org/media-library/full-resolution-photo-of-a-sunlit-pine-forest-with-a-narrow-trail-winding-through-the-trees-and-grassy-undergrowth.jpg?id=61013768&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">We’re going to degrade the quality of this image throughout this article. At its full image size, it’s 13.7 megabytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Irina Iriser</small></p><h2>How the JPEG format came to life</h2><p>Built with input from dozens of stakeholders, the Joint Photographic Experts Group ultimately aimed to create a format that fit everyone’s needs. (Reflecting its committee-led roots, there would be no confusion about the format’s name—an acronym of the organization that designed it.) And when the format was finally unleashed on the world, it was the subject of a book that was more than 600 pages.</p><p><em><em>JPEG: Still Image Data Compression Standard</em></em>, written by IBM employees and JPEG organization stakeholders William B. Pennebaker and Joan L. Mitchell, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA1&printsec=frontcover" target="_blank">describes</a> a landscape of multimedia imagery, held back without a way to balance the need for photorealistic images and immediacy. Standardization, they believed, could fix this.</p><p>“The problem was not so much the lack of algorithms for image compression (as there is a long history of technical work in this area),” the authors wrote, “but, rather, the lack of a standard algorithm—one which would allow an interchange of images between diverse applications.”</p><p>And they were absolutely right. For more than 30 years, JPEG has made high-quality, high-resolution photography accessible in operating systems far and wide. Although we no longer need to compress JPEGs to within an inch of their life, having that capability helped enable the modern Internet.</p><p><a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&dq=ibm+jpeg&pg=PA278&printsec=frontcover" target="_blank">As the book notes</a>, Mitchell and Pennebaker were given IBM’s support to follow through this research and work with the JPEG committee, and that support led them to develop many of the JPEG format’s foundational patents. Described in <a href="https://patents.google.com/patent/US4905297" target="_blank">patents</a> filed by Mitchell and Pennebaker in 1988, IBM and other members of the JPEG standards committee, such as AT&T and Canon, were developing ways to use compression to make high-quality images easier to deliver in confined settings.</p><p>Each member brought their own needs to the process. Canon, obviously, was more focused on printers and photography, while AT&T’s interests were tied to data transmission. Together, the companies left behind a standard that has stood the test of time.</p><p>All this means, funnily enough, that the first place that a program capable of using JPEG compression appeared was not MacOS or Windows, but OS/2—a fascinating-but-failed graphical operating system created by Pennebaker and Mitchell’s employer, IBM. As early as 1990, OS/2 supported the format through the <a href="https://www.edm2.com/index.php/OS/2_Image_Support" target="_blank">OS/2 Image Support</a> application.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Nearly identical photo of a sunlit pine forest." class="rm-shortcode" data-rm-shortcode-id="b810c6423ebd0b3e07f4d42c4c7162ac" data-rm-shortcode-name="rebelmouse-image" id="ef951" loading="lazy" src="https://spectrum.ieee.org/media-library/nearly-identical-photo-of-a-sunlit-pine-forest.jpg?id=61015732&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">At 50 percent of its initial quality, the image is down to about 2.6 MB. By dropping half of the image’s quality, we brought it down to one-fifth of the original file size. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><h2>What a JPEG does when you heavily compress it</h2><p>The thing that differentiates a JPEG file from a PNG or a GIF is how the data degrades as you compress it. The goal for a JPEG image is to still look like a photo when all is said and done, even if some compression is necessary to make it all work at a reasonable size. That way, you can display something that looks close to the original image in fewer bytes.</p><p>Or, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA4&printsec=frontcover" target="_blank">as Pennebaker and Mitchell put it</a>, “the most effective compression is achieved by approximating the original image (rather than reproducing it exactly).”</p><p>Central to this is a compression process called <a href="https://spectrum.ieee.org/compression-algorithms" target="_blank">discrete cosine transform</a> (DCT), a lossy form of compression encoding heavily used in all sorts of compressed formats, most notably in digital audio and signal processing. Essentially, it delivers a lower-quality product by removing details, while still keeping the heart of the original product through approximation. The stronger the cosine transformation, the more compressed the final result.</p><p>The algorithm, <a href="https://ieeexplore.ieee.org/abstract/document/1672377" target="_blank">developed by researchers</a> in the 1970s, essentially takes a grid of data and treats it as if you’re controlling its frequency with a knob. The data rate is controlled like water from a faucet: The more data you want, the higher the setting. DCT allows a trickle of data to still come out in highly compressed situations, even if it means a slightly compromised result. In other words, you may not keep all the data when you compress it, but DCT allows you to keep the heart of it.</p><p>(See <a href="https://www.youtube.com/watch?v=Q2aEzeMDHMA" target="_blank">this video</a> for a more technical but still somewhat easy-to-follow description of DCT.)</p><p>DCT is everywhere. If you <a href="https://ottverse.com/discrete-cosine-transform-dct-video-compression/" target="_blank">have ever seen a streaming video</a> or an online radio stream that degraded in quality because your bandwidth suddenly declined, you’ve witnessed DCT being utilized in real time.</p><p>A JPEG file doesn’t have to leverage the DCT with just one method, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA81&printsec=frontcover" target="_blank">as <em><em>JPEG: Still Image Data Compression Standard</em></em> explains</a>:</p><p>The JPEG standard describes a family of large image compression techniques, rather than a single compression technique. It provides a “tool kit” of compression techniques from which applications can select elements that satisfy their particular requirements.</p><p>The toolkit has four modes:</p><ul><li><strong>Sequential DCT,</strong> which displays the compressed image in order, like a window shade slowly being rolled down</li><li><strong>Progressive DCT,</strong> which displays the full image in the lowest-resolution format, then adds detail as more information rolls in</li><li><strong>Sequential lossless,</strong> which uses the window-shade format but doesn’t compress the image</li><li><strong>Hierarchical mode,</strong> which combines the prior three modes—so maybe it starts with a progressive mode, then loads DCT compression slowly, but then reaches a lossless final result</li></ul><p>At the time the JPEG was being created, modems were extremely common. That meant images loaded slowly, making Progressive DCT the most fitting format for the early Internet. Over time, the progressive DCT mode has become less common, as many computers can simply load the sequential DCT in one fell swoop.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with very slight degradation visible." class="rm-shortcode" data-rm-shortcode-id="44d7f73d86c46d8ae4653970ebbffbdd" data-rm-shortcode-name="rebelmouse-image" id="fd0cd" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-very-slight-degradation-visible.jpg?id=61029700&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">That same forest, saved at 5 percent quality, now down to about 419 kilobytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>When an image is compressed with DCT, the change tends to be less noticeable in busier, more textured areas of the picture, like hair or foliage. Those areas are harder to compress, which means they keep their integrity longer. It tends to be more noticeable, however, with solid colors or in areas where the image sharply changes from one color to another—like text on a page. Ever screenshot a social media post, only for it to look noisy? Congratulations, you just made a JPEG file.</p><p>Other formats, like PNG, do better with text, because their compression format is intended to be non-lossy. (Side note: PNG’s compression format, DEFLATE, <a href="https://www.ietf.org/rfc/rfc1951" target="_blank">was designed</a> by Phil Katz, who also created the ZIP format. The PNG format uses it in part because it was a license-free compression format. So it turns out the brilliant coder with the <a href="https://www.wsj.com/articles/SB961363319756539141" target="_blank">sad life story</a> improved the Internet in multiple ways before his <a href="https://tedium.co/2015/02/17/early-internet-history-tales/" target="_blank">untimely passing</a>.)</p><p>In many ways, the JPEG is one tool in our image-making toolkit. Despite its age and maturity, it remains one of our best options for sharing photos on the Internet. But it is not a tool for every setting—despite the fact that, like a wrench sometimes used as a hammer, we often leverage it that way.</p><h2>Forgent Networks claimed to own the JPEG’s defining algorithm</h2><p>The JPEG format gained popularity in the ’90s for reasons beyond the quality of the format. Patents also played a role: Starting in 1994, the tech company Unisys <a href="https://www.theregister.com/1999/09/01/unisys_demands_5k_licence_fee/" target="_blank">attempted to bill individual users</a> who relied on GIF files, which used a patent the company owned. This made the free-to-use JPEG more popular. (This situation also led to the creation of the patent-free PNG format.)</p><p>While the JPEG was standards-based, it could still have faced the same fate as the GIF, thanks to the quirks of the patent system. A few years before the file format came to life, a pair of Compression Labs employees <a href="https://patents.google.com/patent/US4698672A/en" target="_blank">filed a patent application</a> that dealt with the compression of motion graphics. By the time anyone noticed its similarity to JPEG compression, the format was ubiquitous.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with more noticeable color degradation visible. Areas with previously subtle color gradients now appear more like blocks of color." class="rm-shortcode" data-rm-shortcode-id="5fb227e9168811101372ad575e42dc89" data-rm-shortcode-name="rebelmouse-image" id="e1296" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-more-noticeable-color-degradation-visible-areas-with-previously-subtle-color-gradie.jpg?id=61016218&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Our forest, saved at 1 percent quality. This image is only about 239 KB in size, yet it’s still easily recognizable as the same photo. That’s the power of the JPEG.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Then in 1997, a company named Forgent Networks acquired Compression Labs. The company eventually spotted the patent and began filing lawsuits over it, a series of events it saw as a stroke of good luck.</p><p>“The patent, in some respects, is a lottery ticket,” Forgent CEO Jay Peterson <a href="https://www.cnet.com/tech/tech-industry/staking-a-claim-in-the-patent-gold-mine/" target="_blank">told <em><em>CNET</em></em> in 2005</a>. “If you told me five years ago that ‘You have the patent for JPEG,’ I wouldn’t have believed it.”</p><p>While Forgent’s claim of ownership of the JPEG compression algorithm was tenuous, it ultimately saw more success with its legal battles than Unisys did. The company earned more than US $100 million from digital-camera makers before the patent finally ran out of steam around 2007. The company also attempted to extract licensing fees from the PC industry. Eventually, Forgent agreed <a href="https://www.cnet.com/tech/tech-industry/forgent-settles-jpeg-patent-cases/" target="_blank">to a modest $8 million</a> settlement.</p><p>As the company took an increasingly aggressive approach to its acquired patent, it began to lose battles both in the court of public opinion and in actual courtrooms. <a href="https://arstechnica.com/uncategorized/2006/05/6930-2/" target="_blank">Critics pounced on examples of prior art</a>, while courts limited the patent’s use to motion-based uses like video.</p><p>By 2007, Forgent’s compression patent expired—and its litigation-heavy approach to business went away. That year, the company became <a href="https://www.asuresoftware.com" target="_blank">Asure Software</a>, which now specializes in payroll and HR solutions. Talk about a reboot.</p><h2>Why the JPEG won’t die</h2><p>The JPEG file format has served us well. It’s been difficult to remove the format from its perch. The JPEG 2000 format, for example, was intended to supplant it by offering more lossless options and better performance. The format is <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000143.shtml" target="_blank">widely used by the Library of Congress</a> and specialized sites like the Internet Archive; however, it is less popular as an end-user format.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Animated GIF of the forest images, starting at full resolution and progressing through increasingly degraded version of the iamge." class="rm-shortcode" data-rm-shortcode-id="ea7ce7f87d0b7e0afed75e4a9a57e2a7" data-rm-shortcode-name="rebelmouse-image" id="171a2" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-gif-of-the-forest-images-starting-at-full-resolution-and-progressing-through-increasingly-degraded-version-of-the-iamg.gif?id=61016209&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">See the forest JPEG degrade from its full resolution to 1 percent quality in this GIF. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Other image technologies have had somewhat more luck getting past the JPEG format. The Google-supported <a href="https://developers.google.com/speed/webp" target="_blank">WebP</a> is popular with website developers (<a href="https://www.pcgamer.com/heres-why-you-have-to-deal-with-so-many-annoying-webps-now/" target="_blank">and controversial</a> with end users). Meanwhile, the formats <a href="https://aomediacodec.github.io/av1-avif/" target="_blank">AVIF</a> and <a href="https://www.iso.org/standard/83650.html" target="_blank">HEIC</a>, each developed by standards bodies, have largely outpaced both JPEG and JPEG 2000.</p><p>Still, the JPEG will be difficult to kill at this juncture. These days, the format is similar to MP3 or ZIP files—two legacy formats too popular and widely used to kill. Other formats that compress the files better and do the same things more efficiently are out there, but it’s difficult to topple a format with a 30-year head start.</p><p>Shaking off the JPEG is easier said than done. I think most people will be fine to keep it around.</p><p><em><em>Ernie Smith is the editor of </em></em><a href="https://tedium.co/" target="_blank"><em><em>Tedium</em></em></a><em><em>, a long-running newsletter that hunts for the end of the long tail.</em></em></p><em><em><br/></em></em>]]></description><pubDate>Tue, 17 Jun 2025 14:45:46 +0000</pubDate><guid>https://spectrum.ieee.org/jpeg-image-format-history</guid><category>Compression algorithms</category><category>Digital photography</category><category>Discrete cosine transform</category><category>Image compression</category><category>Jpeg</category><dc:creator>Ernie Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&amp;width=980"></media:content></item><item><title>Why Pilots Will Matter in the Age of Autonomous Planes</title><link>https://spectrum.ieee.org/autonomous-planes-certification</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-electric-vertical-takeoff-aircraft-lifts-off-into-a-cloudy-sky-with-an-office-building-in-the-background.jpg?id=60489393&width=1200&height=800&coordinates=118%2C0%2C119%2C0"/><br/><br/><p><strong> In August 2001,</strong> an anonymous guest <a href="https://www.airliners.net/forum/viewtopic.php?t=101783" rel="noopener noreferrer" target="_blank">posted on the forum</a> at Airliners.net, a popular aviation website. “How Long Will Pilots Be Needed?” they wondered, observing that “20 years or so down the road” technology could be so advanced that planes would fly themselves. “So would it really be useful for a person to go to college now and be an airline pilot if a few years down the road they will be phased out by technology?”</p><p>Twenty-four years later, the basic technology required to make aircraft fly themselves exists, as evidenced by the fact that most commercial flights are flown largely on autopilot. Yet, the fundamental model of flying commercial <a href="https://spectrum.ieee.org/tag/aircraft" target="_self">aircraft</a> hasn’t really changed. Passengers are still flown on large jetliners by two or more highly trained human pilots functioning as a team.</p><p>The main reason why airlines are still decades away from pilotless planes boils down to the strict regulatory framework for aviation. At the heart of this regulation is certification—the process by which governmental authorities determine that an aircraft design is safe for flight. Even for conventional aircraft based on proven technologies, taking a concept from design through certification can require hundreds of millions of dollars and the better part of a decade. Tack on any novel technologies, such as the <a href="https://spectrum.ieee.org/tag/autonomy" target="_self">autonomy</a> necessary to remove the pilot from the cockpit, and that process just gets longer and more expensive, with no guarantee of success.</p><p>Nevertheless, and despite the daunting odds against them, a new generation of startups is making a run at certifying autonomous passenger and cargo aircraft, in the process laying the groundwork for the next chapter of aviation. Instead of airliners, these companies are starting with small aircraft: electric air taxis and single-engine planes that typically seat fewer than a dozen people. Not only are the associated capital costs more manageable on a startup’s budget, there’s also a persuasive safety case to be made: Small aircraft are still prone to the types of accidents that have been largely eliminated from commercial airline operations. According to <a href="https://www.aopa.org/training-and-safety/air-safety-institute/accident-analysis/richard-g-mcspadden-report/mcspadden-report-figure-view" rel="noopener noreferrer" target="_blank">statistics</a> compiled by the Aircraft Owners and Pilots Association, around 300 people die each year in small plane and helicopter crashes in the United States alone.</p><p>“Loss of control—mishandling the plane, usually as a result of disorientation or excessive workload—and controlled flight into terrain, [those] are the leading causes of accidents in small aircraft,” says <a href="https://reliable.co/company" rel="noopener noreferrer" target="_blank">Robert Rose</a>, cofounder and CEO of <a href="https://reliable.co/" rel="noopener noreferrer" target="_blank">Reliable Robotics</a>, one of a few startups now working on retrofits that could enable Cessna Caravan planes to fly autonomously. A veteran of SpaceX and Tesla, Rose is adamant that “we, as a nation, possess the technology to prevent these accidents. If we can [autonomously] land a rocket on a small barge in the middle of the ocean, clearly we can find the centerline at an airport.”</p><h2>The economic case for autonomy in aviation</h2><p>While the safety argument for making small aircraft autonomous is a compelling one, the move is fundamentally rooted in economics. California-based Reliable Robotics and Massachusetts-based Merlin Labs are developing the commercial versions of their <a href="https://www.businesswire.com/news/home/20231206413888/en/Reliable-Robotics-Flies-Large-Cargo-Aircraft-with-No-One-On-Board" rel="noopener noreferrer" target="_blank">autonomous Caravans</a> initially for the cargo feeder industry, which uses small airplanes to move packages to and from rural markets on behalf of carriers like FedEx and UPS. (Both companies also have <a href="https://www.businesswire.com/news/home/20240916015386/en/U.S.-Air-Force-Awards-Reliable-Robotics-Multi-Year-Contract-for-Dual-Use-Advanced-Aircraft-Automation" rel="noopener noreferrer" target="_blank">military funding</a> to develop autonomous aircraft.) Pilots for these feeder networks are typically flying alone, often at night and in bad weather, and <a href="https://www.ainonline.com/aviation-news/business-aviation/2023-11-01/deeper-look-part-135-cargo-ops-accidents" rel="noopener noreferrer" target="_blank">their safety record is poor</a>. This is a comparatively low-volume segment of the aviation industry, and there’s no money for second pilots and other risk mitigations typical of airline operations.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A white airplane is parked on an airport tarmac close to a hangar. Two men are carrying a large box towards the airplane." class="rm-shortcode" data-rm-shortcode-id="ba50de53ab179841fa15be01da30008d" data-rm-shortcode-name="rebelmouse-image" id="21280" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-airplane-is-parked-on-an-airport-tarmac-close-to-a-hangar-two-men-are-carrying-a-large-box-towards-the-airplane.jpg?id=60489399&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Reliable Robotics is one of a couple of companies that are outfitting Cessna Caravan airplanes with advanced software to provide a high level of autonomy, for applications that include cargo transportation. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Reliable Robotics</small></p><p>The economic argument for autonomy is even more compelling in the emerging <a href="https://spectrum.ieee.org/evtol-aircraft" target="_self">air-taxi industry</a>, where hundreds of hopefuls—including a dozen or so <a href="https://spectrum.ieee.org/evtol-air-taxi-industry" target="_self">serious contenders</a>—are racing to develop electric vertical takeoff and landing aircraft to ferry passengers around crowded urban areas. Most of these <a href="https://spectrum.ieee.org/tag/evtol" target="_self">eVTOLs</a> are the size of helicopters, with space for just four or five passengers, and their proponents envision scores or even hundreds of them in the air over major cities, collectively moving millions of passengers annually. The concept is called <a href="https://spectrum.ieee.org/evtol-policy-strategy" target="_self">urban air mobility</a>, and in the speculative math that underpins it, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale.</p><p>China has already certified a pilotless air taxi: the EH216-S, a two-seat multicopter developed by Guangzhou-based <a href="https://www.ehang.com/" target="_blank">EHang</a> that in March <a href="https://www.ehang.com/news/1198.html" target="_blank">obtained initial approval</a> from the <a href="https://www.caac.gov.cn/English/" target="_blank">Civil Aviation Administration of China</a> for limited commercial sightseeing operations. However, many Western observers doubt that EHang’s design would pass muster by the U.S. <a href="https://www.faa.gov/" target="_blank">Federal Aviation Administration</a> (FAA) or the <a href="https://www.easa.europa.eu/en" rel="noopener noreferrer" target="_blank">European Union Aviation Safety Agency</a> (EASA), both of which have an especially conservative approach to safety. For that reason, most Western eVTOL makers have opted to develop piloted aircraft first and plan to introduce autonomous versions at some later date. They figure that seeking certification of novel electric aircraft designs, even without autonomy, is already a big ask of these regulators.</p><p>A notable exception to this strategy is <a href="https://wisk.aero/" rel="noopener noreferrer" target="_blank">Wisk Aero</a>, which began as a project funded by Google cofounder Larry Page and is now a wholly owned subsidiary of Boeing. In January 2022, the company declared that it would obtain FAA certification for its self-flying air taxi by the end of the decade and be operating close to 14 million flights annually within five years after that—a staggering ambition, given that the entire U.S. air traffic system currently manages around 16 million flights per year. While overheated expectations around urban air mobility have cooled considerably in the three years since that announcement, Wisk continues to forge ahead with its autonomous <a href="https://wisk.aero/aircraft/" rel="noopener noreferrer" target="_blank">Generation 6 eVTOL</a>, the company’s sixth aircraft design and the first it plans to certify for passenger-carrying operations.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="a futuristic, bright yellow aircraft  sits on a large concrete pad with a blue sky in the background. " class="rm-shortcode" data-rm-shortcode-id="5b316b2f31dd3cd77f483ae99f98bd26" data-rm-shortcode-name="rebelmouse-image" id="7ea5a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-futuristic-bright-yellow-aircraft-sits-on-a-large-concrete-pad-with-a-blue-sky-in-the-background.jpg?id=60489618&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">A mockup of Wisk’s sixth generation of electric vertical takeoff and landing aircraft was unveiled in October 2022. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Wisk</small></p><p>Importantly, Wisk, Reliable Robotics, and Merlin Labs aren’t just developing autonomous aircraft—they have already launched formal certification programs with the FAA. That means they’re working closely with the agency to define the rules and standards by which autonomous aircraft will be approved for commercial operations, blazing a trail for others to follow. The task is a daunting one, but the regulators and industry are not starting from scratch. Rather, they’re building on decades of certification experience and best practices that have helped to dramatically improve the safety of the aviation industry over its history.</p><h2>Aviation safety starts with certification</h2><p>Although the fatal January 2025 midair collision of an Army Black Hawk helicopter and an American Eagle CRJ700 near Washington, D.C.’s Reagan National Airport <a href="https://www.ipsos.com/en-us/many-americans-are-losing-faith-safety-air-travel" target="_blank">shook public confidence</a> in the safety of the U.S. air transport system, commercial aviation remains a remarkably safe way to get around. According to researchers at MIT, the risk of a fatality from commercial air travel was just <a href="https://www.sciencedirect.com/science/article/pii/S0969699724001066" target="_blank">one per 13.7 million passenger boardings</a> worldwide between 2018 and 2022. Fifty years earlier, the risk was an order of magnitude higher: one per 350,000 boardings between 1968 and 1977.</p><p>There are many reasons for this great leap in safety, and the certification process is an important one. Today, a majority of aviation accidents are attributed to human error, but that’s not because people are inherently less reliable than aircraft. It’s because a systematic approach to design and testing has over the past several decades eliminated many of the mechanical problems that used to cause accidents routinely. In this context, the argument for enhancing safety through autonomy can be thought of as transferring even more responsibilities from highly variable humans to engineered systems that can be subjected to greater scrutiny.</p><p>The overarching principle of certification is that the equipment and systems on an aircraft must be designed and installed so that they perform as intended during any foreseeable circumstances that they might encounter. “Perform as intended” includes <em><em>not</em></em> performing any <em><em>unintended</em></em> functions. An example of an unintended function is pushing the nose of an aircraft down past the level that a pilot can recover—that was the fatal result of a hidden software flaw that caused two <a href="https://spectrum.ieee.org/how-the-boeing-737-max-disaster-looks-to-a-software-developer" target="_self">crashes of the Boeing 737 Max</a> and led to an extended grounding of the fleet while that oversight was remedied.</p><p>Another key principle of certification is that the probability of a failure condition must be inversely proportional to its consequences. In other words, the more serious the impact of a failure, the more remote its chances of occurrence need to be. Aircraft are complex machines with millions of components that can and do fail, but many of these components can fail with no serious effects. For example, it’s no big deal if a lightbulb in the cabin burns out on a regular basis. Certifying authorities like the FAA generally accept a high probability of failure conditions that have a negligible impact on safety. However, failure conditions that are potentially catastrophic are required to be “extremely improbable.”</p><p>Whether a failure condition is extremely improbable is fundamentally a qualitative evaluation that relies on the best judgments of engineers about how a system is likely to fail, supported by numerical assessments of the likelihood of failure. The critical systems on large commercial airliners are held to a numerical safety level of 10<span><sup>-9</sup></span>, meaning that catastrophic failures are expected no more than once in a billion flight hours (the equivalent of once in about 114,000 years of continuous operation).</p><p>Achieving such vanishingly low probabilities may require expensive, heavy, and redundant systems, so regulators typically relax the safety expectations for small aircraft that carry fewer people. For example, a four-seat airplane like a Cessna 172 may only be held to a numerical safety level of 10<span><sup>-6</sup></span>, meaning that catastrophic failures are expected no more than once in a million flight hours. That said, aircraft manufacturers are free to design to higher standards, and Wisk is targeting the highest numerical safety level, 10<span><sup>-9</sup></span>, for its Gen 6 eVTOL.</p><p>These basic principles of certification apply regardless of whether or not there’s a human pilot sitting in the cockpit, which is why developers of autonomous aircraft are confident they don’t need to completely reinvent the certification framework.</p><p>“Everybody thinks that you need to think about the autonomy a different way than you would think about a piloted aircraft,” says <a href="https://wisk.aero/about/" target="_blank">Cindy Comer</a>, Wisk’s vice president of certification, safety management systems and quality. “But really we just don’t get to pass off these failure conditions to a pilot. We still do our safety assessment the same way. We still may design our aircraft in a very similar way, but it may be to higher levels, it may be with more redundancy, or maybe we add equipment, because we no longer have that person that can sit there and see the things, grab the things, to pull the breakers.</p><p>“So it drives our safety assessments to say, ‘Okay, we can’t put this on the pilot now. So what do we put it on?’”</p><h3>Key Aircraft Autonomy Projects</h3><br/><div class="flourish-embed flourish-table" data-src="visualisation/23472514?820658"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="table visualization" src="https://public.flourish.studio/visualisation/23472514/thumbnail" width="100%"/></noscript></div>
<h2>Making autonomy certifiable presents unique challenges</h2><p>Answering that question—What do we put it on?—for every foreseeable failure condition is where the real work of certifying an autonomous aircraft comes in. Conventionally piloted aircraft may use the same overarching framework for certification, but they have the advantage of decades of certification history and precedent to fill in all of the details, down to requirements for such things as the actuation of the landing gear and the markings of instruments and placards. For the new systems on autonomous aircraft, many of those details must be negotiated with the FAA or some other certifying authority, which must be convinced in each instance that the proposed solution is at least as safe as the approach used on conventional aircraft.</p><p>In the United States, applicants for type certificates have considerable flexibility in proposing how to meet the FAA’s safety goals. For each project incorporating novel technologies, the applicant and the agency agree on a set of requirements and standards, which becomes the “regulatory basis” for that aircraft. Theoretically, each autonomous-aircraft developer could have a very different regulatory basis, although in practice, the FAA looks for common ground. Nevertheless, the flexibility in this approach allows industry to explore a variety of possible ways to comply with a certification requirement before a solution is codified in regulation.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A white single-engine airplane with a high wing is seen flying over scrubby brown terrain." class="rm-shortcode" data-rm-shortcode-id="996780adebb70d2a4a94c3b6f98569c5" data-rm-shortcode-name="rebelmouse-image" id="73873" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-single-engine-airplane-with-a-high-wing-is-seen-flying-over-scrubby-brown-terrain.jpg?id=60489677&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Merlin Labs launched the flight test campaign for its certification-ready autonomy system in June 2024. The Merlin Pilot system is integrated directly onto the aircraft and is intended in the near term to reduce crew workload rather than fully replace pilots.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Merlin Labs</small></p><p>“Once you have the regulatory basis in place, then you need to come to agreement on how you’re going to demonstrate compliance to all of those regulations,” says Rose. “You can pull from existing standards, you can modify existing standards, or you can, in some cases, even just propose your own standards.” After agreeing upon the means of compliance, the applicant and regulator develop a detailed project plan that outlines the tests that will be performed and the reports—known as artifacts—that will be submitted to the regulator to support certification.</p><p>For conventional piloted aircraft with a history of real-world operations, much of how those aircraft will function in the national airspace system is assumed. “Large commercial airplanes operate from airports around the world with relatively known and static equipment that helps them navigate and approach and land,” says <a href="https://www.linkedin.com/in/brian-yutko/" target="_blank">Brian Yutko</a>, until recently Wisk’s CEO (he now heads commercial airplane product development at Boeing). This infrastructure, he adds, has been established over decades and is reflected in the design of aircraft in ways that are often taken for granted.</p><p>The existing system relies heavily on human pilots communicating with air traffic controllers over radio. Autonomous aircraft will require new concepts of operations, or “ConOps,” for how they will function, which could include using ground supervisors to handle radio calls, for example. In turn, the specifics of each ConOps will influence aircraft design requirements. According to Comer, crystallizing the ConOps at the beginning of the certification process “helps drive a common understanding of what you’re actually doing, and that may be different for every applicant with the FAA.”</p><p>Basically, Wisk intends for its autonomous air taxi, which Yutko has likened to “a tram in the sky,” to fly along very specific and limited routes with predetermined emergency landing locations. Such a narrow set of tasks is an easier thing to automate than the varied and flexible operations performed by most small piloted aircraft today (or, for that matter, most self-driving cars). Meanwhile, human supervisors on the ground will monitor flights and communicate with air traffic control as required.</p><p>Reliable Robotics’ automated Cessna Caravans will also have remote operators to handle communications with air traffic control, but they will fly over a much larger and more variable operating area. Because of this added complexity, Reliable has opted to split up the work of certifying its autonomous aircraft into chunks, beginning with certification of an advanced, always-on autopilot. This will assist but not replace the onboard pilot during all phases of flight, including landing as well as taxi and takeoff—which traditional autopilots are not capable of. Taking the pilot out of the cockpit will come as a follow-on certification project.</p><h2>Autonomous aircraft will do what autopilots can’t</h2><p>Proponents of autonomy like to point out that most commercial airline flights today are flown on autopilot from shortly after takeoff until touchdown or just before. It may therefore seem surprising that Europe’s aviation regulator, EASA, <a href="https://www.easa.europa.eu/en/document-library/general-publications/easa-artificial-intelligence-roadmap-20" target="_blank">does not expect</a> to see fully autonomous airliners until after 2050, while other regulators haven’t even speculated on a timeline for the shift.</p><p>There are several reasons why “solving” autonomy in aircraft is not just a matter of expanding the functionality of existing autopilots. Basic flight control—moving flight-control surfaces and power inputs to make an aircraft fly how and where you want—is a relatively simple thing to automate, and most of the time, when everything goes as expected, autopilot works just fine. However, most existing autopilot systems assume there’s a human pilot, and for that reason they aren’t reliable enough to enable full autonomy.</p><p>“There are autopilot actuators that go into aircraft today,” notes Reliable cofounder Rose. “But there’s a person sitting there monitoring them, and if [the actuators] do anything funny, then you click the off switch or actually, in many cases, you can just physically overpower the actuator. That’s not the case with ours—our actuators need to work all the time.”</p><p>More challenging is solving for situations when everything does <em><em>not</em></em> go as expected, such as when another aircraft conflicts with the programmed flight path or a stray vehicle blocks the assigned runway. Autonomous-aircraft developers can’t count on a remote operator to manage these types of urgent, sudden conflicts, because the command-and-control (C2) link between the ground and the aircraft could also fail.</p><p>“The aircraft, without having a [pilot] on board, needs to know where it is, and how to get where it’s going and how to avoid things along the way, over the length of its concept of operations,” says Yutko. Wisk’s Gen 6 flier will have the ability to safely complete a flight even if it loses both its C2 link and GPS signal immediately after takeoff, he says. “It turns out that if you don’t do that, then you start to impose really difficult technical requirements on the C2 link, or on your ability to maintain GPS.”</p><p class="pull-quote">In the speculative math that underpins urban air mobility, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale.</p><p>Neither Wisk nor Reliable Robotics is using machine learning algorithms in its technical solutions, in large part because there’s no consensus on how to assure, to aviation’s exacting standards, the safety of such algorithms. These algorithms are frequently characterized as “nondeterministic,” meaning that their outputs can’t be reliably predicted from their inputs.</p><p>Some autonomous-aircraft developers are incorporating artificial intelligence into their designs. Merlin Labs, for example, is developing natural-language-processing algorithms to communicate with air traffic control. For the most part, however, autonomous-aircraft developers aren’t counting on technology alone to solve the innumerable contingencies that can arise in flight—that’s where the ground operators come in.</p><p>“We basically have taken everything that can be [automated] deterministically, and we’re making it deterministic,” Rose explains. “And all of the things that are…very hard to automate, that a human can do easily, then let the human do it.”</p><p>Which raises the question: If humans are required to supervise autonomous aircraft, does the business case for them still hold up? Their developers say it does, but in ways that aren’t as simple as just striking “pilots” from the balance sheet. For example, those remote supervisors will need training, but that’s likely to be far less extensive and costly than the training required to competently fly an aircraft. For Reliable Robotics and other companies targeting cargo delivery, autonomy also promises to improve the efficiency of the existing cargo feeder network.</p><p>“The reality is, in cargo aircraft, especially small cargo aircraft, pilots are super underutilized,” says Rose. Pilots at the feeder airlines may spend most of their day hanging out in a hotel room between their morning and evening flights. If people were instead managing autonomous cargo aircraft remotely, they could conceivably oversee additional flights across multiple time zones. “Our analysis has shown you can easily double the productivity of a pilot by putting them into our control center, potentially triple or quadruple the productivity [depending] on the mission set,” Rose says.</p><h2>Autonomous tech might eventually trickle up</h2><p>Even if companies like Wisk and Reliable Robotics succeed in certifying and commercializing their autonomous aircraft, human pilots still won’t face imminent extinction. Solving autonomy for one aircraft type and concept of operations doesn’t mean it’s solved for all types and concepts of operations. The technical, regulatory, and social barriers standing in the way of autonomous passenger jets are formidable.</p><p>“I think for as long as we’re all alive, there will be piloted large commercial aircraft,” Yutko says. “If you solve Gen 6, you don’t get uncrewed large airplanes. You just don’t, and I’m not certain that we will in our lifetimes.” However, he does think it likely that some of the technologies now being developed at Wisk—such as navigating in the absence of GPS or techniques for automating emergency checklists—will find their way into conventionally piloted aircraft in ways that enhance safety.</p><p>“I think those will be the types of things that we see in our lifetime benefiting big commercial transport applications, and I think it’s phenomenal,” adds Comer.</p><p>As for whether it makes sense for anyone to embark upon a career as an airline pilot under the looming shadow of autonomy, it probably still does, at least for now. But check back in another 20 years. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Tue, 17 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-planes-certification</guid><category>Autonomous systems</category><category>Aircraft certification</category><category>Autonomous aircraft</category><category>Autonomous passenger jets</category><category>Electric air taxis</category><category>Evtols</category><dc:creator>Elan Head</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-electric-vertical-takeoff-aircraft-lifts-off-into-a-cloudy-sky-with-an-office-building-in-the-background.jpg?id=60489393&amp;width=980"></media:content></item><item><title>Airbnb’s Dying Software Gets a Second Life</title><link>https://spectrum.ieee.org/apache-airflow-3-programmatic-workflows</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/pixel-art-of-four-pinwheels.jpg?id=60930318&width=1200&height=800&coordinates=140%2C0%2C140%2C0"/><br/><br/><p><a href="https://www.linkedin.com/in/vikramkoka/" rel="noopener noreferrer" target="_blank" title="Link: https://www.linkedin.com/in/vikramkoka/">Vikram Koka</a> stumbled upon <a href="https://airflow.apache.org/" rel="noopener noreferrer" target="_blank">Apache Airflow</a> in late 2019. He was working in the Internet of Things industry and searching for a solution to orchestrate sensor data using software. Airflow seemed to be a perfect fit, but Koka noticed the <a href="https://spectrum.ieee.org/tag/open-source" target="_self">open-source</a> project’s stagnant state. Thus began a journey to breathe a second life into this dying software.</p><p>Airflow was the brainchild of <a href="https://spectrum.ieee.org/the-secret-of-airbnbs-pricing-algorithm" target="_self">Airbnb</a>. The company created the system to <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" rel="noopener noreferrer" target="_blank" title="Link: https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">automate and manage its data-related workflows</a>, such as cleaning and organizing datasets in its data warehouse and calculating metrics around host and guest engagement. In 2015, <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" rel="noopener noreferrer" target="_blank" title="Link: https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">Airbnb released the software as open source</a>. Then, four years later, Airflow transitioned into a <a href="https://news.apache.org/foundation/entry/the-apache-software-foundation-announces44" rel="noopener noreferrer" target="_blank">top-level project</a> at the <a href="https://apache.org/" rel="noopener noreferrer" target="_blank">Apache Software Foundation</a>, a leading developer and steward of open-source software.</p><p class="ieee-inbody-related"><strong>RELATED: <a href="https://spectrum.ieee.org/the-secret-of-airbnbs-pricing-algorithm" target="_blank">The Secret of Airbnb’s Pricing Algorithm</a></strong><br/></p><p>What was once a thriving project had stalled, however, with flat downloads and a lack of version updates. Leadership was divided, with some maintainers focusing on other endeavors.</p><p>Yet Koka believed in the software’s potential. Unlike static configuration files, Airflow follows the principle of “configuration as code.” Workflows are represented as <a href="https://networkx.org/nx-guides/content/algorithms/dag/index.html" rel="noopener noreferrer" target="_blank">directed acyclic graphs</a> of tasks—a graph with directed edges and no loops. Developers can code these tasks in the <a href="https://spectrum.ieee.org/tag/python" target="_self">Python</a> <a href="https://spectrum.ieee.org/tag/programming-languages" target="_self">programming language</a>, allowing them to import libraries and other dependencies that can help them better define tasks. Akin to a musical conductor, Airflow orchestrates the symphony of tasks and manages the scheduling, execution, and monitoring of workflows.</p><p>This flexibility is what caught Koka’s eye. “I fell in love with the concept of code-first pipelines—pipelines which could actually be deployed in code,” he says. “The whole notion of programmatic workflows really appealed to me.”</p><p>Koka started work righting the Airflow ship. As an open-source contributor with decades of experience in the data and <a href="https://spectrum.ieee.org/tag/software-engineering" target="_self" title="Link: https://spectrum.ieee.org/tag/software-engineering">software-engineering</a> space, he connected with people in the community to fix bugs around reliability and craft other enhancements. It took a year, but Airflow 2.0 was released in December 2020.</p><h2>Airflow’s Growth and Community Expansion</h2><p>The release served as a crucial turning point for the project. Downloads from its <a href="https://github.com/apache/airflow" rel="noopener noreferrer" target="_blank">GitHub repository</a> increased, and more enterprises adopted the software. Encouraged by this growth, the team envisioned the next generation of Airflow: a modular architecture, a more modern user interface, and a “run anywhere, anytime” feature, enabling it to operate on premises, in the <a href="https://spectrum.ieee.org/tag/cloud-computing" target="_self">cloud</a>, or on <a href="https://spectrum.ieee.org/tag/edge" target="_self">edge</a> devices and handle event-driven and ad hoc scenarios in addition to scheduled tasks. The team delivered on this vision with the launch of Airflow 3.0 last April.</p><p>“It was amazing that we managed to ‘rebuild the plane while flying it’ when we worked on Airflow 3—even if we had some temporary issues and glitches,” says <a href="https://www.linkedin.com/in/jarekpotiuk/" rel="noopener noreferrer" target="_blank">Jarek Potiuk</a>, one of the foremost contributors to Airflow and now a member of its project-management committee. “We had to refactor and move a lot of pieces of the software while keeping Airflow 2 running and providing some bug fixes for it.”</p><p>Compared with Airflow’s second version, which Koka says had only a few hundred to a thousand downloads per month on GitHub, “now we’re averaging somewhere between 35 to 40 million downloads a month,” he says. The project’s community also soared, with more than 3,000 developers of all skill levels from around the world contributing to Airflow.</p><p><a href="https://www.linkedin.com/in/jens-scheffler/" rel="noopener noreferrer" target="_blank">Jens Scheffler</a> is an active part of that community. As a technical architect of digital testing automation at <a href="https://www.bosch.com/" rel="noopener noreferrer" target="_blank">Bosch</a>, his team was one of the early adopters of Airflow, using the software to orchestrate tests for the company’s automated driving systems.</p><p>Scheffler was inspired by the openness and responsiveness of Airflow members to his requests for guidance and support, so he considered “giving back something to the community—a contribution of code.” He submitted a few patches at first, then implemented an idea for a feature that would benefit not only his team but other Airflow users as well. Scheffler also discovered other departments within Bosch employing Airflow, so they’ve formed a small in-house community “so we can exchange knowledge and keep in touch.”</p><p>Koka, who is also a member of Airflow’s project-management committee and a chief strategy officer at the data-operations platform <a href="https://www.astronomer.io/" rel="noopener noreferrer" target="_blank">Astronomer</a>, notes that managing a huge group of contributors is challenging, but nurturing that network is as essential as improving the software. The Airflow team has established a system that enables developers to contribute gradually, starting with documentation and then progressing to small issues and bug fixes before tackling larger features. The team also makes it a point to swiftly respond and provide constructive feedback.</p><p>“For many of us in the community, [Airflow] is an adopted child. None of us were the original creators, but we want more people feeling they’ve also adopted it,” says Koka. “We’re in different organizations, in different countries, speak different languages, but we’re still able to come together toward a certain mission. I love being able to do that.”</p><h2>Future of Airflow in AI and Machine Learning</h2><p>The Airflow team is already planning future features. This includes tools to write tasks in programming languages other than Python, human-in-the-loop capabilities to review and approve tasks at certain checkpoints, and support for <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">artificial intelligence (AI)</a> and <a href="https://spectrum.ieee.org/tag/machine-learning" target="_self">machine learning</a> workflows. According to <a href="https://airflow.apache.org/blog/airflow-survey-2024/" rel="noopener noreferrer" target="_blank">Airflow’s 2024 survey</a>, the software has a rising number of use cases in machine learning operations (MLOps) and <a href="https://spectrum.ieee.org/tag/generative-ai" target="_self">generative AI</a>.</p><p>“We are at a pivotal moment where AI and ML workloads are the most important things in the IT industry, and there is a great need to make all those workloads—from training to inference and <a href="https://spectrum.ieee.org/ai-agents" target="_self">agentic</a> processing—robust, reliable, scalable, and generally have a rock-solid foundation they can run on,” Potiuk says. “I see Airflow as such a foundation.”</p>]]></description><pubDate>Tue, 17 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/apache-airflow-3-programmatic-workflows</guid><category>Software</category><category>Open source</category><category>Ai</category><category>Airbnb</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/pixel-art-of-four-pinwheels.jpg?id=60930318&amp;width=980"></media:content></item><item><title>Why the Semiconductor Industry Can’t Abandon Women</title><link>https://spectrum.ieee.org/women-in-semiconductors-workforce</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-a-gloved-hand-holding-a-computer-chip-against-a-gradient-background.jpg?id=61009400&width=1200&height=800&coordinates=0%2C60%2C0%2C60"/><br/><br/><p>The percentage of women in the semiconductor industry is stubbornly low. According to a report released in April, <a href="https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Women-in-Semiconductor-2024-Insights-and-Trends-from-Accenture-and-GSA.pdf" rel="noopener noreferrer" target="_blank">51 percent of companies report</a> having less than 20 percent of their technical roles filled by women. At the same time, fewer of these companies were publicly committed to equal opportunity measures in 2024 than the year prior, the same report found.</p><p>This lack of support comes at the same time that <a href="https://spectrum.ieee.org/workforce-shortage" target="_blank">major workforce shortages</a> are expected, says <a href="https://www.linkedin.com/in/andreamohamed/" rel="noopener noreferrer" target="_blank">Andrea Mohamed</a><span>, COO and co-founder of </span><a href="https://quantumbloom.com/" target="_blank">QuantumBloom</a><span>, which helps companies attract, retain, and advance early career women in STEM. The company</span><span style="background-color: initial;"> focuses on the transition from higher education to the workforce, a critical point during which many women leave STEM.</span></p><p><span><em><span><em>IEEE Spectrum</em></span></em> spoke to Mohamed about supporting women in semiconductor jobs, and why a retreat from these initiatives is at odds with the needs of the industry. </span></p><p class="rm-anchors" id="top"><span>Andrea Mohamed on: </span></p><ul><li><a href="#perspective">The current state of the semiconductor industry</a></li><li><a href="#support">How a lack of support for women will impact the industry</a></li><li><a href="#regress">Whether the industry is regressing in its hiring practices</a></li><li><a href="#lesson">What the semiconductor industry can learn from other industries</a></li></ul><p><strong>Tell me about your perspective as a returning veteran of the semiconductor industry.</strong></p><p class="rm-anchors" id="perspective"><strong>Andrea Mohamed: </strong>I worked for a semiconductor startup company over 20 years ago, and it was very male dominated. Now, it’s still very male dominated. Seeing the semiconductor industry with fresh eyes, what I see is an industry that hasn’t evolved as quickly as other STEM-intensive industries. I’ve worked for science and research-oriented organizations, and the progress that’s been made in other sectors just hasn’t been made in this particular sector. </p><p><a href="#top">Return to top</a></p><p class="rm-anchors" id="support"><strong>How might the lack of support for women in the U.S. semiconductor industry create additional problems?</strong></p><p><strong>Mohamed: </strong>On a macro scale, you have an industry that is facing a lot of geopolitical and economic forces that are disrupting the whole supply chain ecosystem around semiconductors, and there’s a push to reshore and <a href="https://spectrum.ieee.org/tsmc-arizona" target="_blank">onshore</a>. There are a lot of infrastructure gaps in doing that, one of them being <a href="https://spectrum.ieee.org/chips-act-workforce-development" target="_blank">the workforce component</a>. It’s not just semiconductors that are poised to be reshored and onshored to the United States; it’s also pharmaceuticals and automotive. And all of that is going to continue to put pressure on the supply and demand curve, if you will, around labor.</p><p>There’s been an enormous amount of attention on the STEM <a href="https://spectrum.ieee.org/chip-design-enrollment" target="_blank">education</a> pipeline, and rightfully so. China and India are producing STEM graduates at a rate that we are not keeping pace with. While we’ve had that focus on the STEM education pipeline, there’s been very little focused attention on what industry is doing inside companies to address the workforce challenges. </p><p>There is a lot of additional concern around corporate cultures, burn-and-churn cyclical nature, policies that seem out of date relative to other industries, including as it relates to child care. Industry is very clearly articulating to education what it needs the next generation to have <a href="https://spectrum.ieee.org/ieee-microcredential-program" target="_blank">from a skills perspective</a>. But we don’t see the voice of the next generation worker influencing how industry is attracting them. We’ve got to start to see the industry recognize how it’s in its own way when it comes to workforce development.</p><p><strong>It sounds like the problem goes beyond the “leaky pipeline” that’s often discussed. </strong></p><p><strong>Mohamed: </strong>Right. We keep talking about the leaky pipeline for all these stages of women dropping out. It starts in middle school, when girls’ interest and confidence in STEM start to wane. At every stage there’s a leak. And then you get to this early career stage, which QuantumBloom is focused on, and that bucket is gushing. We’re losing a ton, and we’re all thinking about just putting more water in the bucket, when really, we need to fix the holes. There’s a lot of discussion about what it’s going to take to attract women, people of color, other communities into the semiconductor workforce, and very little on fixing the holes.</p><p><span>Oftentimes the early career experience is pretty much sink or swim for everybody, regardless of gender. We know with women, it’s more likely that they leave.</span></p><p><span><a href="#top">Return to top</a></span></p><p class="rm-anchors" id="regress"><strong>I understand that the semiconductor industry may actually be regressing in these areas. Can you talk about that? </strong></p><p><strong>Mohamed: </strong>The latest report that came out from <a href="https://www.gsaglobal.org/about-gsa/" target="_blank">Global Semiconductor Alliance</a> and Accenture <a href="https://www.accenture.com/us-en/insights/high-tech/women-semiconductor-leadership" target="_blank">on the state of women and semiconductors</a>, to me, is like a canary in a coal mine. We’re seeing a decrease in public commitments for diversity and the progress that we’ve made around programs that support women. It’s counterintuitive that we are decreasing support at exactly the time we need to be attracting this audience into the industry. </p><p>I understand the pressures that companies are facing around anything that’s related to DEI. We need to change the conversation from DEI to talent management. This is retention and avoiding turnover costs. This is about needing every available brilliant mind in the United States that wants to be in semiconductors. We have offshored this industry for so long. Other countries have existing talent bases. We have to build it.</p><p><strong>So the industry should work on these initiatives to build better workplaces, regardless of whether they’re labeled as promoting diversity?</strong></p><p><strong>Mohamed: </strong>I think a lot of DEI activity was performative. A lot of companies were really not committed to creating great workplaces for everybody. I think that’s part of the reason DEI has gotten politicized. There’s this notion that people were given opportunities that weren’t based on merit. What I’m saying is that this is not a merit conversation, right? <span>Women are graduating with bachelor’s degrees <a href="https://www.pewresearch.org/short-reads/2024/11/18/us-women-are-outpacing-men-in-college-completion-including-in-every-major-racial-and-ethnic-group/" target="_blank">at a rate higher than men</a> and increasing. </span><span>Really, this is about human capital development. You have women who are opting out of your industry, </span><span>a</span><span>nd you have to recognize and pay attention to the unique lived experience of women in these environments in order to solve the problem.</span></p><p>So there are semantics in all of this, but it’s not just relabeling. This is about business. You are not going to be able to compete on a global stage in the United States if you are not finding ways to attract and retain new communities of workers, and women are one of those communities. That means understanding what women need from their employer, because if you do not provide it, they will go somewhere else that does. The concern by companies about, if they run a program like QuantumBloom, does that create a risk? It’s the wrong question about risk. Your big risk is that your fab is empty, because you can’t find workers and retain them. </p><p><a href="#top">Return to top</a></p><p class="rm-anchors" id="lesson"><strong>What have you observed in other industries, and what can semiconductor leaders learn from them?</strong></p><p><strong>Mohamed: </strong>Many women whose roots are in engineering end up working potentially in a technical organization, but not in a technical role. You see them also pivot into completely different industries. They go to business school, they become a consultant, they go to law school. </p><p>In other industries, there are organizations that are very intentional about attracting and retaining their youngest talent. They are dedicating resources to investing in them, which is very rare—most organizations invest more the higher up you go. Really, we need to be thinking about flipping that script and investing more sooner. </p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Woman with long blonde hair in a blue blouse against a solid blue background." class="rm-shortcode" data-rm-shortcode-id="05250695ff2c19a019192f1b48040ead" data-rm-shortcode-name="rebelmouse-image" id="ac967" loading="lazy" src="https://spectrum.ieee.org/media-library/woman-with-long-blonde-hair-in-a-blue-blouse-against-a-solid-blue-background.jpg?id=61009402&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Andrea Mohamed is COO and co-founder of QuantumBloom, a professional development company focused on women in STEM.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Andrea Mohamed</small></p><p>When I think about employer-led solutions around early career talent, what comes to mind are apprenticeships, rotational programs, and <a href="https://spectrum.ieee.org/leadership-skills-ieee-courses" target="_blank">leadership skill development</a>—all the things you’re not taught in school but that are really important to your success. <span>These are skills that you take with you for an entire career. </span><span>When you invest in the top, most of the time people say, “I wish I had this in my 20s.” </span><span>I don’t see m</span><span>any</span><span> of th</span><span>ese solutions</span><span> being used</span><span> in this industry. I heard recently one of the big semiconductor giants in this country used to have an engineering rotational program and stopped it five years ago. I was talking to a person who had been in that program and how pivotal it was in their early career experience. </span></p><p><strong>Are there other steps that you think are important for semiconductor leaders to take?</strong></p><p><strong>Mohamed: </strong>The things that QuantumBloom solves are very early career and focused on individuals. At the same time, companies need to be thinking about top-down culture change and industry transformation. Those are longer-term horizon things to fix. </p><p>People join companies and quit bosses. The relationship with your boss is so important. You can be in a relatively terrible organization culturally and have a wonderful boss, and you can have career success. Vice versa, you could be in an awesome corporate culture with a terrible boss and not thrive. If we can improve that primary work relationship, build more empathy for each other’s experiences at a local level, we can improve work outcomes and retention. And then things start to spread. That manager who may be supporting a particular woman in our program, they learn skills and tools to be more inclusive leaders that extends beyond just that woman. </p><p>We’re doing that more at that local level, but man, companies really need to be addressing top-down transformation and culture change. <span>At the end of the day, we need semiconductor leaders to envision becoming a magnet for all talent, and then commit the resources and organizational changes needed to make that vision reality.</span></p><p><span><a href="#top">Return to top</a></span></p>]]></description><pubDate>Mon, 16 Jun 2025 14:56:52 +0000</pubDate><guid>https://spectrum.ieee.org/women-in-semiconductors-workforce</guid><category>Culture</category><category>Semiconductor industry</category><category>Women in engineering</category><category>Workforce development</category><dc:creator>Gwendolyn Rak</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-a-gloved-hand-holding-a-computer-chip-against-a-gradient-background.jpg?id=61009400&amp;width=980"></media:content></item><item><title>Europe’s Plan for Faster Space Travel</title><link>https://spectrum.ieee.org/esa-nuclear-rocket</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/schematic-collage-of-an-alumni-nuclear-thermal-propulsion-system-surrounded-by-a-dark-starry-sky.jpg?id=60934597&width=1200&height=800&coordinates=0%2C250%2C0%2C250"/><br/><br/><p><a data-linked-post="2650251627" href="https://spectrum.ieee.org/rockets-for-the-red-planet" target="_blank">Getting to Mars</a><span> takes a really long time, about nine months using today’s rocket technology. This is because regular rocket engines burn fuel and oxygen together (like a car engine), but they’re not very efficient. The fundamental problem is that spacecraft must carry both fuel and oxidizer since there’s no air in space to support combustion. This creates a vicious circle: The more fuel you carry to go faster, the heavier your spacecraft becomes, requiring even more fuel to accelerate that extra weight. To go faster, you’d need massive amounts of fuel, making the rockets incredibly expensive and heavy. Current chemical propulsion systems are just about at their theoretical limits, with little room for improvement in efficiency.</span></p><p class="ieee-inbody-related"><span><strong>RELATED: <a href="https://spectrum.ieee.org/rockets-for-the-red-planet" target="_blank">Rockets for the Red Planet</a></strong><br/></span></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://www.universetoday.com/"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://www.universetoday.com/" target="_blank"><img alt='Universe Today logo; text reads "This post originally appeared on Universe Today."' class="rm-shortcode" data-rm-shortcode-id="087bc0ad57330681cee5d3354f3d2ac7" data-rm-shortcode-name="rebelmouse-image" id="b9dd2" loading="lazy" src="https://spectrum.ieee.org/media-library/universe-today-logo-text-reads-this-post-originally-appeared-on-universe-today.png?id=60568425&width=980"/></a></p><p>Whilst <a href="https://arstechnica.com/science/2025/06/5-things-in-trumps-budget-that-wont-make-nasa-great-again/" target="_blank">NASA funding has been slashed</a> by the Trump administration with no allocation for <a data-linked-post="2652903460" href="https://spectrum.ieee.org/nuclear-powered-rockets-get-a-second-look-for-travel-to-mars" target="_blank">nuclear thermal propulsion</a> and/or nuclear electric propulsion, scientists at the European Space Agency (ESA) have been studying nuclear propulsion. Here’s how it works: Instead of burning fuel with oxygen, a nuclear reactor heats up a propellant like hydrogen. The super-heated propellant then shoots out of the rocket nozzle, pushing the spacecraft forward. This method is much more efficient than chemical rockets.</p><h2>Revisiting Nuclear Rockets for Mars</h2><p>Nuclear rockets offer several key advantages, such as cutting Mars trip times in half—from nine months to about four to five months. The efficiency gains come from the fact that nuclear reactors produce far more energy per unit of fuel than chemical reactions. Surprisingly, astronauts would actually receive less harmful radiation on shorter trips, even though the engine itself produces radiation. This happens because space travelers are constantly bombarded by cosmic radiation during their journey, and cutting travel time in half significantly reduces their total exposure. These engines work best for big spacecraft that need to speed up and slow down dramatically, perfect for moon and Mars missions where rapid velocity changes of at least 25,000 km/h are required.</p><p>The study, called <a href="https://www.esa.int/ESA_Multimedia/Images/2025/06/Alumni_nuclear_thermal_propulsion_system_schematic" target="_blank">“Alumni,”</a> prioritized safety through careful design. The nuclear reactor only turns on when the spacecraft is far from Earth in a safe orbit. Before activation, the uranium fuel has very low radioactivity and isn’t toxic. Multiple radiation shields protect the crew during the short engine burns that last less than two hours. The reactor is designed never to return to Earth’s atmosphere. The research team spent over a year analyzing this technology and concluded it’s feasible for long-term development. However, there’s still significant work ahead, including laboratory testing of the new ceramic-metal reactor design, building safe testing facilities, and solving technical challenges like fuel sourcing and reactor restart systems.</p><p>Nuclear thermal propulsion could revolutionize space travel, making missions to Mars and the moon faster and more practical. While the technology is promising and appears safe, it will take many years of development before we see nuclear-powered spacecraft heading to the Red Planet. It’s great to see Europe demonstrating that it has the expertise to develop this technology, potentially ushering in a new era of space exploration where distant worlds become more accessible than ever before.</p>]]></description><pubDate>Sat, 14 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/esa-nuclear-rocket</guid><category>Rockets</category><category>Mars</category><category>Esa</category><category>Nasa</category><dc:creator>Mark Thompson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/schematic-collage-of-an-alumni-nuclear-thermal-propulsion-system-surrounded-by-a-dark-starry-sky.jpg?id=60934597&amp;width=980"></media:content></item><item><title>Video Friday: AI Model Gives Neo Robot Autonomy</title><link>https://spectrum.ieee.org/video-friday-neo-humanoid-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/robot-and-person-standing-face-to-face-in-a-wooden-hallway-with-tall-bushy-plants.png?id=60988606&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN, CHINA</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="qnzl5dvtdkk">Introducing Redwood—1X’s breakthrough <a data-linked-post="2671886754" href="https://spectrum.ieee.org/chain-of-thought-prompting" target="_blank">AI model</a> capable of doing chores around the home. For the first time, <a data-linked-post="2671238743" href="https://spectrum.ieee.org/video-friday-good-over-all-terrains" target="_blank">NEO Gamma</a> moves, understands, and interacts autonomously in complex human environments. Built to learn from real-world experiences, Redwood empowers NEO to perform end-to-end mobile manipulation tasks like retrieving objects for users, opening doors, and navigating around the home gracefully, on top of hardware designed for compliance, safety, and resilience.</blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ebbfc0b339e850cd28b7e5f5fac9c43c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/qnzL5dVTDKk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="a2f3f4b5f15fde9ec50d5116b96b764a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Dp6sqx9BGZs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">- YouTube</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://www.youtube.com/watch?v=Dp6sqx9BGZs" target="_blank">www.youtube.com</a></small></p><p>[ <a href="https://www.1x.tech/discover/redwood-ai">1X Technology</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="7gap8k9jz2q"><a data-linked-post="2650251927" href="https://spectrum.ieee.org/therapeutic-robots-paro-and-keepon-are-cute-but-still-costly" target="_blank">Marek Michalowski</a>, who co-created <a data-linked-post="2650276785" href="https://spectrum.ieee.org/keepon-helps-kids-learn-to-argue-better" target="_blank">Keepon</a>, has not posted to his YouTube channel in 17 years—until this week. The new post? It’s about a project from 10 years ago!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f79c76127d0315323179d7d47ac57117" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/7gAp8k9jZ2Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://jonathanproto.com/project-sundial">Project Sundial</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lkc2y0yb89u"><em>Helix can now handle a wider variety of packaging approaching human-level dexterity and speed, bringing us closer to fully autonomous package sorting. This rapid progress underscores the scalability of Helix’s learning-based approach to robotics, translating quickly into real-world applications.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d3d0673b70a96eaa2d77f39e3bf16d02" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lkc2y0yb89U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.figure.ai/news/helix">Figure</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="nzuvdu2q0zo">This is certainly an atypical Video Friday selection, but I saw this Broadway musical called “Maybe Happy Ending” a few months ago because the main characters are deprecated humanoid home-service robots. It was utterly charming, and it just won the Tony award for best new musical among others.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e25d5605adc019d20ed170c527ed4700" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/nZUVDu2q0Zo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ “<a href="https://www.maybehappyending.com/">Maybe Happy Ending</a>” ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ptydwp9utis"><a data-linked-post="2664552687" href="https://spectrum.ieee.org/boston-dynamics-dancing-robots" target="_blank">Boston Dynamics</a> brought a bunch of Spots to “America’s Got Talent,” and kudos to them for recovering so gracefully from an on-stage failure.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7ec3c1744aa63eeb5904fa3ee0779adc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ptYDWP9uTis?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://bostondynamics.com/products/spot/">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="41xnc4mu-hs">I think this is the first time I’ve seen end-effector changers used for either feet or heads.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1a29c3d692365d22f63dade5335a6c2f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/41XNc4Mu-hs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://unit.aist.go.jp/jrl-22022/en/">CNRS-AIST Joint Robotics Laboratory</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="tbdtcwzfeiu"><em>ChatGPT has gone fully Navrim—complete with existential dread and maximum gloom! Watch as the most pessimistic ChatGPT-powered robot yet moves chess pieces across a physical board, deeply contemplating both chess strategy and the futility of existence. Experience firsthand how seamlessly AI blends with robotics, even if Navrim insists there’s absolutely no point.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="08d839d8c7f07846798dcb0748af4c05" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TbDTCwzFeIU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Not bad for $219 all in.</p><p>[ <a href="https://vassarrobotics.com/">Vassar Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="9u0hocl0aj4"><em>We present a single-layer multimodal sensory skin made using only a highly sensitive hydrogel membrane. Using electrical impedance tomography techniques, we access up to 863,040 conductive pathways across the membrane, allowing us to identify at least six distinct types of multimodal stimuli, including human touch, damage, multipoint insulated presses, and local heating. To demonstrate our approach’s versatility, we cast the hydrogel into the shape and size of an adult human hand.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="30c9c1e376a765bd37eb45ea36471e1c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9U0hoCL0aJ4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dshardman.co.uk/publication/scirohand/">Bio-Inspired Robotics Laboratory</a> ] paper published by [ <a href="https://www.science.org/journal/scirobotics" target="_blank">Science Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="pqdqamtjwrw"><em>This paper introduces a novel robot designed to exhibit two distinct modes of mobility: rotational aerial flight and terrestrial locomotion. This versatile robot comprises a sturdy external frame, two motors, and a single wing embodying its fuselage. The robot is capable of vertical takeoff and landing in mono-wing flight mode, with the unique ability to fly in both clockwise and counterclockwise directions, setting it apart from traditional mono-wings.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fea3e49bf322eec7899d20a2236783a4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PQDqAMTjWrw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://journals.sagepub.com/doi/10.1177/02783649251344968">AIR Lab</a> paper ] published in [ <a href="https://journals.sagepub.com/home/ijra" target="_blank">The International journal of Robotics Research</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="hzq2hhoi6la">When TRON 1 goes to work, all he does is steal snacks from hoomans. Apparently.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8d3121ce94715e6d531c92002d4575b7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hZQ2hhoi6lA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.limxdynamics.com/en/tron1">LimX Dynamics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="jb2txzph7xs"><em>The 100,000th robot has just rolled off the line at Pudu Robotics’ Super Factory! This key milestone highlights our cutting-edge manufacturing strength and marks a global shipment volume of over 100,000 units delivered worldwide.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="5bc0b47f42e246bf950808b9fc53d28e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Jb2tXzph7Xs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.pudurobotics.com/en">Pudu Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="cn5whdtrlv0">Now that is a big saw.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e4d5683d9d16f4c931c72693ecd4b188" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/CN5WhDTRlV0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kuka.com/en-se/industries/solutions-database/2025/05/catonator_smartproduction-nordic">Kuka Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="2rih4zintzm"><em>NASA Jet Propulsion Laboratory has developed the Exploration Rover for Navigating Extreme Sloped Terrain or ERNEST. This rover could lead to a new class of low-cost planetary rovers for exploration of previously inaccessible locations on Mars and the moon.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ccaf709dda32b8b5778c7a0f178c877e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2RiH4ZInTZM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.hou.usra.edu/meetings/lpsc2025/pdf/1729.pdf">NASA Jet Propulsion Laboratory</a> paper ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="zobe3aoz5fw"><em>Brett Adcock, founder and CEO of Figure AI, speaks with Bloomberg Television’s Ed Ludlow about how the company is training humanoid robots for logistics, manufacturing, and future roles in the home at Bloomberg Tech in San Francisco.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2467bd8c8c1fc278a40446b7ee5655e9" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/zObe3aOz5fw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.figure.ai/">Figure</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="b6pz2r1uhxw"><em>Peggy Johnson, CEO of Agility Robotics, discusses how humanoid robots like Digit are transforming logistics and manufacturing. She speaks with Bloomberg Businessweek’s Brad Stone about the rapid advances in automation and the next era of robots in the workplace at Bloomberg Tech in San Francisco.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d7a77a584e471d9a057897f5a2a150d7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/B6pz2R1UHXw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="piyyufkvg1e">This ICRA 2025 Plenary is from Allison Okamura, titled “Rewired: The Interplay of Robots and Society.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c721b1931ac1615ffee7f918db1f8861" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PIYyufKvG1E?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://2025.ieee-icra.org/program/plenary-sessions/">ICRA 2025</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 13 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-neo-humanoid-robot</guid><category>Video friday</category><category>Humanoid robots</category><category>Autonomous robots</category><category>Dexterity</category><category>Dancing robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/robot-and-person-standing-face-to-face-in-a-wooden-hallway-with-tall-bushy-plants.png?id=60988606&amp;width=980"></media:content></item><item><title>Telecom Expert Honored By IEEE Standards Association</title><link>https://spectrum.ieee.org/telecom-kevin-lu-ieee-standards</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/headshot-of-kevin-lu-with-a-closed-mouth-smile-he-is-wearing-a-suit-and-tie.jpg?id=60768495&width=1200&height=800&coordinates=0%2C201%2C0%2C202"/><br/><br/><p>Growing up in Taipei, Taiwan, in the 1960s with limited access to television and other forms of entertainment, <a href="https://kevinwlu.github.io/" rel="noopener noreferrer" target="_blank">Kevin Lu</a> amused himself by examining how machines worked. He became fascinated by heavy construction equipment and built miniature versions of the machinery out of scrap materials.</p><p>“We didn’t have a lot at the time,” Lu recalls. “TV was just becoming available to the average household, and there weren’t many toys. So I made my own.”</p><h3>Kevin Lu</h3><br/><p><strong>Employer: </strong></p><p><strong></strong>Stevens Institute of Technology, in Hoboken, N.J.</p><p><strong>Title: </strong></p><p><strong></strong>Teaching Professor and Associate Chair for Undergraduate Studies in the Department of Electrical & Computer Engineering</p><p><strong>Member grade: </strong></p><p><strong></strong>Life senior member</p><p><strong>Alma maters:</strong> </p><p>National Chiao Tung University in Hsinchu; Washington University, in St. Louis<br/></p><p>That boy would grow up to publish pioneering work on optical networks, have a long career in <a href="https://spectrum.ieee.org/topic/telecommunications/" target="_self">telecommunications</a> R&D, and teach students about the emerging <a href="https://spectrum.ieee.org/tag/internet-of-things" target="_self">Internet of Things</a>.</p><p>Lu, an IEEE Life senior member, also has played a significant role in IEEE’s global standards development program. He was honored last year with the<a href="https://standards.ieee.org/about/awards/dsvs/" rel="noopener noreferrer" target="_blank"> IEEE Standards Board Distinguished Service Award</a> for “superior IEEE SA governance leadership as the IEEE SA Standards Board audit committee chair and as the IEEE SA Industry Connections committee chair.”</p><p>Now approaching retirement, Lu reflects on his career, which has gracefully threaded together engineering, teaching, and volunteerism, with no signs of slowing down.</p><h2>Switching from an interest in mechanics to electronics </h2><p>Born in Taipei City, Lu was the youngest of four siblings. He says he was influenced by his family and circumstances. His father, a nontechnical administrative staff member at<a href="https://www.cht.com.tw/en/home/cht" rel="noopener noreferrer" target="_blank"> ChungHwa Telecom</a>, the country’s telephone company, kept the home filled with telecom newsletters. Lu says his brother performed bold chemistry experiments that sometimes ended with singed eyebrows or small explosions. Kevin gravitated toward mechanical projects, such as building scale models of cranes, before eventually embracing electronics.</p><p>“My parents encouraged a career in engineering because they thought it would provide a good living,” he says.</p><p>He earned a bachelor’s degree in control engineering from the<a href="https://en.wikipedia.org/wiki/National_Chiao_Tung_University" rel="noopener noreferrer" target="_blank"> </a><a href="https://www.nycu.edu.tw/nycu/en/index" rel="noopener noreferrer" target="_blank">National Chiao Tung University</a> in Hsinchu in 1979. He then attended <a href="https://washu.edu/" rel="noopener noreferrer" target="_blank">Washington University</a> in St. Louis, earning master’s and doctoral degrees in systems science and mathematics in 1981 and 1984.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Kevin Lu holding an IEEE plaque for Distinguished Service. He is posing in a suit and tie between James E. Matthews and Yatin Trivedi." class="rm-shortcode" data-rm-shortcode-id="2d93d4c59ac43c06bd208e4598fa4bb8" data-rm-shortcode-name="rebelmouse-image" id="bb5fc" loading="lazy" src="https://spectrum.ieee.org/media-library/kevin-lu-holding-an-ieee-plaque-for-distinguished-service-he-is-posing-in-a-suit-and-tie-between-james-e-matthews-and-yatin-tr.jpg?id=60773483&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Kevin Lu [center] shows off the plaque commemorating him being honored with the 2024 IEEE Standards Board Distinguished Service Award. He is flanked by James E. Matthews, president of the IEEE Standards Association, and Yatin Trivedi, a member of the IEEE Standards Association Board of Governors.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Kevin Lu</small></p><h2>A long career with Bellcore</h2><p>A chance meeting at the campus placement office led to a job interview with Bell Communications Research, known as <a href="https://journal.businesstoday.org/bt-online/2017/3/18/the-legacy-of-bellcore" target="_blank">Bellcore</a> (formerly part of<a href="https://spectrum.ieee.org/bell-labs-100-birthday" target="_self"> Bell Labs</a>, now <a href="https://www.nokia.com/bell-labs/" rel="noopener noreferrer" target="_blank">Nokia Bell Labs</a>). He was hired and worked at the company’s facility in Piscataway, N.J.</p><p>The timing couldn’t have been better. In 1984 the U.S. telecommunications industry was undergoing a massive structural change, with<a href="https://spectrum.ieee.org/the-end-of-att" target="_self"> AT&T’s divestiture</a> spawning new entities including Bellcore. His job was “member of the technical staff,” which he took great pride in, he says, noting that “Nobel laureates held that same title at Bell Labs.”</p><p>For the next 28 years, he contributed to projects that shaped the modern communications landscape. In 1990 he wrote the seminal paper “<a href="https://ieeexplore.ieee.org/document/57809" rel="noopener noreferrer" target="_blank">System and Cost Analyses of Broad-Band Fiber Loop Architectures</a>,” which was published in the<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=49" rel="noopener noreferrer" target="_blank"> <em><em>IEEE Journal on Selected Areas in Communications</em></em></a>. It advocated for<a href="https://www.cisco.com/c/en/us/products/switches/what-is-passive-optical-networking.html" rel="noopener noreferrer" target="_blank"> passive optical networks</a>—a concept that is now central to global fiber deployment.</p><p>The road from idea to implementation was long, Lu says.</p><p>“It wasn’t until 2009 that <a href="https://www.verizon.com/plans/unlimited/?customer_id=279-516-8739&cmp=KNC-C-Mobility-NON-R-BPUR-NONE-NONE-2K0VZ0-COE-GAW-586&kpid=go_cmp-12581404974_adg-116456529981_ad-744100437038_kwd-13038366_dev-c_ext-_prd-_sig-Cj0KCQjw0LDBBhCnARIsAMpYlAq9WwyKM15CSLCEQRyF4Vw6_61Zv40sVedCSuoNbs5B2EDj-J7ixo0aAhWEEALw_wcB&gad_source=1&gad_campaignid=12581404974&gbraid=0AAAAAD6-lLtQ9KmXfAIaC7HQNGbpj53de&gclid=Cj0KCQjw0LDBBhCnARIsAMpYlAq9WwyKM15CSLCEQRyF4Vw6_61Zv40sVedCSuoNbs5B2EDj-J7ixo0aAhWEEALw_wcB" rel="noopener noreferrer" target="_blank">Verizon</a> installed a unit in my home,” he says, laughing. “Fiber is expensive, so companies deployed wireless first to build up enough revenue.”</p><p>Bellcore eventually became Telcordia, which Ericsson <a href="https://www.ericsson.com/en/about-us/history/company/the-consequences-of-expansion/ericsson-acquired-telcordia" rel="noopener noreferrer" target="_blank">acquired in 2012</a>. Although Lu had risen through the ranks to become Telcordia’s chief scientist, he left during the Ericsson acquisition and joined<a href="https://www.broadcom.com/" rel="noopener noreferrer" target="_blank"> Broadcom</a>. There he worked on cellphone chips and contributed to mobile standards for the<a href="https://www.3gpp.org/about-us" rel="noopener noreferrer" target="_blank"> 3rd Generation Partnership Project</a> (3GPP), a global consortium of telecommunications standards organizations that creates and maintains specifications for mobile systems.</p><p>After Broadcom exited the cellular baseband chip business, Lu left in 2013, for a job in academia.</p><h2>An academic career at Stevens</h2><p>In 2015 Lu joined the <a href="https://www.stevens.edu/" rel="noopener noreferrer" target="_blank">Stevens Institute of Technology</a>, in Hoboken, N.J., as an adjunct professor in the electrical and computer engineering department. He became a full-time professor in 2018.</p><p>Now, he says, he sees academia as a continuation of—not a departure from—his life’s work.</p><p>“The decades I spent in that world give me insights students won’t get from textbooks,” he says.</p><p class="pull-quote"><span>“When students tell me they’ve discovered their path … that’s the most rewarding thing.”</span></p><p>In May 2019 Stevens honored him with its <a href="https://www.stevens.edu/news/stevens-honors-excellence-research-teaching-during-2018-2019-academic-year" target="_blank">Morton Distinguished Teaching Professor Award</a>.</p><p>He encourages his students to embrace lifelong learning and develop soft skills alongside technical knowledge. He doesn’t just teach engineering, he says; he works “to help students discover who they are and where they might thrive.”</p><p>Although he recently announced his intention to retire, the school has persuaded him to remain, with the offer of a new role, to be formally announced before the next semester.</p><p>“I’ll continue on for at least three more years,” he says.</p><h2>Involvement with standards development</h2><p>Throughout his career, IEEE has remained a constant, he says. He joined in 1980 as a student member, drawn by the affordability of dues and publishing opportunities.</p><p>His early IEEE involvement was rooted in power systems—an echo of his dissertation. His career in the telecom industry led him to become involved with the<a href="https://www.comsoc.org/" target="_blank"> IEEE Communications Society</a> and the <a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a>. He served as the society’s director of standards development in 2012 and 2013. In that role, he chaired its <a href="https://www.comsoc.org/about/boards/standards-development-board" rel="noopener noreferrer" target="_blank">Standards Development Board</a>. He also served on the society’s <a href="https://www.comsoc.org/about/boards/standardization-programs-development-board" rel="noopener noreferrer" target="_blank">Standardization Programs Development Board</a> for several years.</p><p>Lu now chairs the IEEE Standards Board’s I<a href="https://standards.ieee.org/about/bog/iccom/" rel="noopener noreferrer" target="_blank">ndustry Connections committee</a>, which ensures that proposed Industry Connections activities are within the scope and purpose of IEEE. The committee, he says, is “a well-oiled machine.” He has led the group since 2018, and although he has given a lot of thought to turning over the reins to a successor, he has stayed on as chair to ensure its continuity.</p><p>He also has served on the <a href="https://standards.ieee.org/about/sasb/audcom/" rel="noopener noreferrer" target="_blank">audit</a>, <a href="https://standards.ieee.org/about/sasb/patcom/" rel="noopener noreferrer" target="_blank">patent</a>, <a href="https://standards.ieee.org/about/sasb/procom/" rel="noopener noreferrer" target="_blank">procedures</a>, and <a href="https://standards.ieee.org/about/sasb/nescom/" rel="noopener noreferrer" target="_blank">new standards</a> committees.</p><p>Even after decades of professional achievement, he says, he remains focused on learning, mentoring, and building bridges between generations of engineers.</p><p>What excites him most about the direction his career has taken, he says, is “when students tell me they’ve discovered their career path.”</p><p>“That’s the most rewarding thing,” he says. “That’s when I know I’ve done my job. I take pride in seeing them embrace my philosophy of making lifelong learning a daily habit.”</p>]]></description><pubDate>Thu, 12 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/telecom-kevin-lu-ieee-standards</guid><category>Bellcore</category><category>Careers</category><category>Ieee member news</category><category>Ieee standards</category><category>Stevens institute of technology</category><category>Telecommunications</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/headshot-of-kevin-lu-with-a-closed-mouth-smile-he-is-wearing-a-suit-and-tie.jpg?id=60768495&amp;width=980"></media:content></item><item><title>Anti-Distraction Systems Shut Down Smartphone Use</title><link>https://spectrum.ieee.org/distracted-driving-smartphone</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-driving-a-car-touches-a-smartphone-on-his-dashboard-the-image-on-the-phone-reads-safe-mode.jpg?id=60567887&width=1200&height=800&coordinates=187%2C0%2C188%2C0"/><br/><br/><p>As mobile phone use continues to be a leading cause of vehicle accidents, a range of technologies has emerged designed to combat <a href="https://spectrum.ieee.org/texting-while-driving-were-really-bad-at-it-but-we-think-were-good" target="_blank">distracted driving</a>. From mobile apps to hardware-integrated systems, these tools aim to limit phone use behind the wheel. But a closer look reveals significant differences in how effectively they prevent distractions—especially in fleet vehicles.</p><p>While apps like <a href="https://www.att.com/" target="_blank">AT&T</a>’s<a href="https://www.att.com/support/article/wireless/KM1000730/" target="_blank"> DriveMode</a> and <a href="https://www.apple.com/store?afid=p240%7Cgo~cmp-21644513200~adg-172212795549~ad-748928271266_kwd-10778630~dev-c~ext-~prd-~mca-~nt-search&cid=aos-us-kwgo-brand-iphone-tradeinpromo-052225-" target="_blank">Apple</a>’s built-in<a href="https://youtu.be/FXNrstZG5v4?si=eHXtjQxy18S9_T0n" target="_blank"> Do Not Disturb While Driving</a> offer basic protections, they rely heavily on driver cooperation. Many can be bypassed with a swipe or a second phone, limiting their effectiveness when liability and safety are paramount.</p><p>“We think technologies that reduce visual-manual interaction with phones are obviously a good thing,”<a href="https://www.linkedin.com/in/ian-reagan-03b09631" target="_blank"> Ian Reagan</a>, a senior research scientist at the <a href="https://www.iihs.org/" target="_blank">Insurance Institute for Highway Safety</a> told <em><a href="https://spectrum.ieee.org/" target="_blank">IEEE Spectrum</a></em>. “But most are opt-in. We’d like to see them as opt-out by default.”</p><p class="pull-quote">“Mobile use while driving is an addiction. We needed a system that prevents distraction without waiting for the driver to choose safety. That’s what we built.” <strong>Ori Gilboa, SaverOne</strong></p><p>Now, a new generation of anti-distraction technology is shifting from soft nudges to hard enforcement. And for companies managing fleets of drivers, the stakes—and the solutions—are getting more serious.</p><h2>The Need for Enforceable Solutions</h2><p>“There’s a difference between tools that monitor and tools that prevent,” says <a href="https://il.linkedin.com/in/ori-gilboa-6a8a07128" target="_blank">Ori Gilboa</a>, CEO of <a href="https://saver.one/" target="_blank">SaverOne</a>, a Tel Aviv–area startup leading a new wave of hardware-integrated solutions that make driver cooperation a nonissue. “That distinction matters when lives are on the line.”</p><p>SaverOne’s system uses a passive sensor network to scan the vehicle cabin for phones, identify the driver’s device, and place it into “safe mode”—automatically blocking risky apps while allowing essential functions like navigation and preapproved voice calls. Crucially, the system works even if the driver tries to cheat by disabling Bluetooth or by bringing a second phone.</p><h2>Designed to Be Driverproof</h2><p>The system consists of four small hidden sensors and a central receiver—about the size of an <a href="https://www.apple.com/iphone/?afid=p240%7Cgo~cmp-14645102856~adg-126654827229~ad-735242796883_kwd-334361787~dev-c~ext-~prd-~mca-~nt-search&cid=wwa-us-kwgo-iphone-Core-iPhone-Avail-iPhone-Core-Exact-iPhone-Exact-iphone" target="_blank">iPhone</a>—installed inside the vehicle. It can pinpoint mobile devices within centimeters and distinguishes between driver and passenger phones. If the driver’s phone is active and doesn’t connect to SaverOne’s app, a buzzer sounds until the issue is resolved.</p><p>“What sets us apart is our prevention-first approach,” says Gilboa. “Most systems focus on what went wrong after the fact. We stop the distraction before it happens.”</p><p>Gilboa said the system’s design respects driver usability, preserving tools like turn-by-turn navigation and voice calls to approved contacts. “We want drivers to be reachable—but not distracted,” he adds.</p><h2>Global Expansion, Measurable Impact</h2><p>Since launching its second-generation product in 2022, SaverOne has rapidly expanded. After early pilot deployments with Israeli fleet operators such as <a href="https://www.bynet.co.il/en/about/" target="_blank">Bynet Data Communications</a>,<a href="https://iec-global.com/" target="_blank"> Israel Electric Corporation</a>, and ice-cream purveyor <a href="https://www.froneri.com/" target="_blank">Froneri</a>, the company gained traction, securing deals with a broader array of Israeli companies. By mid-2023, <a href="https://www.cemex.com/" target="_blank">Cemex</a> Israel, the global cement giant’s local subsidiary, had agreed to deploy the driver-distraction-prevention system on its 380-vehicle fleet. In January 2024, following a successful trial with 17 trucks, <a href="https://www.strauss-group.com/" target="_blank">Strauss Group</a>, one of Israel’s largest food and beverage companies, decided to install the SaverOne system on its fleet of 80 food-delivery trucks. Though smaller than the Cemex Israel contract, that agreement proved significant because Strauss accumulated data demonstrating a statistically significant reduction in accident rates among the equipped vehicles. That news has helped SaverOne in its bid to go global. CEMEX has since outfitted trucks in fleets across Europe. In the United States, SaverOne is now being adopted by <a href="https://www.fedex.com/global/choose-location.html" target="_blank">FedEx</a> contractors in North Carolina and Philadelphia, says Gilboa.</p><p>Some fleet operators report as much as a 60 percent reduction in accident rates post-installation. While those figures are difficult to verify independently, a more concrete metric is phone interaction. Fleet managers have observed a dramatic drop—from drivers checking their phones 10 times per hour to near zero.</p><p>“The system educates through behavior,” says Gilboa. “It’s not about punishment—it’s about making the right choice automatic.”</p><p>But Reagan cautions that long-term behavioral change remains unproven, comparing it to early<a href="https://en.wikipedia.org/wiki/Intelligent_speed_assistance" target="_blank"> intelligent speed assistance</a> trials in Europe using systems that detected vehicles’ locations, used digital maps to keep track of local speed limits, and reduced engine power to prevent the vehicles from exceeding the legal limit. “When the limiter was on,” Reagan says, “people obeyed the posted speed limits. When it was turned off, they sped again. Whether tech like this [driver-distraction-prevention system] creates lasting change—well, we just don’t know yet.”</p><h2>Could Regulation Be the Tipping Point?</h2><p>Despite promising results, broader adoption—particularly in the consumer market—may hinge on regulation. IIHS’s Reagan notes that although <a href="https://www.nhtsa.gov/risky-driving/distracted-driving" target="_blank">distracted driving</a> officially accounts for about 10 percent of crash fatalities, or roughly 3,500 deaths per year, the real figure is likely far higher. Despite the undercount, the urgency is still hard to ignore. As Reagan put it, “Phones let you mentally escape the car, even when you’re barreling down the highway at 115 kilometers per hour [about 70 miles per hour]. That’s the real danger.”</p><p>He adds that government regulation requiring carmakers to install systems like SaverOne’s could be a game changer. “The tech exists,” Reagan said. “What we need is the political will to mandate it.”</p><p>SaverOne is still focused on fleet customers, but the company is in discussions with insurers exploring offering discounts to young or high-risk drivers who use distraction-prevention systems, Gilboa says.</p><p>“Mobile use while driving is an addiction,” he says. “We needed a system that prevents distraction without waiting for the driver to choose safety. That’s what we built.”</p>]]></description><pubDate>Wed, 11 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/distracted-driving-smartphone</guid><category>Driver distraction</category><category>Smartphone app</category><category>Driving safety</category><category>Distracted driving</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-driving-a-car-touches-a-smartphone-on-his-dashboard-the-image-on-the-phone-reads-safe-mode.jpg?id=60567887&amp;width=980"></media:content></item><item><title>Three Steps to Stopping Killer Asteroids</title><link>https://spectrum.ieee.org/planetary-defense-killer-asteroids</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/spacecraft-approaches-large-asteroid-against-starry-background.png?id=60694025&width=1200&height=800&coordinates=416%2C0%2C417%2C0"/><br/><br/><p><strong>Impact was imminent. </strong>Occasional gasps arose as the asteroid took shape and a jagged, rocky surface filled the view. Then the images abruptly stopped.</p><p>The mission control room at Johns Hopkins University Applied Physics Lab in Laurel, Md., erupted in cheers. “We have impact!” said the lead engineer, who gave a two-handed high five to a nearby colleague. Others waved their hands in the air in victory and slapped each other on the back.</p><p>This had been a test, and humanity had passed it, taking one crucial step closer to protecting Earth from an asteroid impact. The <a href="https://dart.jhuapl.edu/Gallery/media/videos/From%20Impact%20to%20Innovation_%20A%20Year%20of%20Science%20and%20Triumph%20for%20Historic%20DART%20Mission.mp4" target="_blank">test</a> was the culmination of NASA’s <a href="https://science.nasa.gov/mission/dart/" target="_blank">Double Asteroid Redirection Test</a> (DART) mission, for which I was the coordination lead. On 26 September 2022, the DART spacecraft had successfully crashed into Dimorphos, a roughly 150-meter-diameter asteroid that was 11 million kilometers from Earth. The collision nudged the asteroid and modified its trajectory.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Diagram of DART impacting Dimorphos, altering its orbit around asteroid Didymos." class="rm-shortcode" data-rm-shortcode-id="d720ffe9969cc7ec026bc60d50cd9d15" data-rm-shortcode-name="rebelmouse-image" id="3a002" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-dart-impacting-dimorphos-altering-its-orbit-around-asteroid-didymos.png?id=60755072&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">In 2022, NASA’s Double Asteroid Redirection Test slammed a golf-cart-size spacecraft, DART, into the near-Earth asteroid Dimorphos (1). DART—which first deployed a small observer craft, LICIACube, to observe the collision (2)—bumped Dimorphos’s trajectory (3) enough to alter its future course (4).</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">GyGinfographics; Source: NASA</small></p><p>The celebrations in the control room were the culmination of years of effort to prove that the momentum from a golf-cart-size spacecraft can alter an asteroid’s future path.  And DART’s collision with asteroid Dimorphos kicked off a new era in space exploration, in which technologies for planetary defense are now taking shape.</p><p>If one day an asteroid like Dimorphos is discovered to be headed toward Earth, an interceptor craft like DART could collide with the asteroid years in advance to avert disaster. Here’s how that might work.</p><h2>Step 1: Find and Track Near-Earth Asteroids  </h2><p>The first step in averting an asteroid impact with Earth is just to know what near-Earth objects (NEOs) are out there.</p><p>The University of Hawaii’s <a href="https://atlas.fallingstar.com/" target="_blank">Asteroid Terrestrial-impact Last Alert System</a> (ATLAS) station, in Chile plays a critical role in these observations of NEOs, which are asteroids orbiting near Earth’s orbit. In late December, it detected a previously unknown NEO during a routine sweep of the skies. The asteroid was given the name 2024 YR4, following the <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">standard astronomical convention</a> for new objects. “2024 Y” <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">represents the 24th-half-month of the year 2024</a>—that is, 16 to 31 December. The “R4” encodes the <a href="https://sci.esa.int/web/home/-/30244-asteroid-numbers-and-names" target="_blank">sequence of discovery</a>—in this case, that it was <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">the 117th object</a> found during the year’s final couple of weeks.</p><h3>Hera </h3><br/><img alt="Illustration of a yellow satellite with two blue solar panels deployed." class="rm-shortcode" data-rm-shortcode-id="855d19cc951655fcf339a06fa6dde6b4" data-rm-shortcode-name="rebelmouse-image" id="f536f" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-yellow-satellite-with-two-blue-solar-panels-deployed.png?id=60756477&width=980"/><p>This European Space Agency mission will rendezvous with the Didymos–Dimorphos asteroid system and study the aftereffects of NASA’s DART impact close up.</p><p><strong>Launch:</strong></p><p>2024</p><p><strong>Rendezvous: </strong></p><p>2026</p><h3></h3><br/><p>Until that point in the year, <a href="https://cneos.jpl.nasa.gov/stats/totals.html" rel="noopener noreferrer" target="_blank">more than 3,000 NEOs</a> had already been discovered. Nothing about <a href="https://science.nasa.gov/solar-system/asteroids/2024-yr4/" rel="noopener noreferrer" target="_blank">2024 YR4</a> initially stood out as concerning. It was a seemingly run-of-the-mill asteroid. However, further observations soon suggested it wasn’t ordinary at all.</p><p>Throughout the first weeks of 2025, the probability of a 2024 YR4 collision with Earth kept growing. On 29 January, astronomers calculated its odds of eventual impact to be 1.3 percent. And in crossing the 1 percent threshold, 2024 YR4 triggered an alert from the <a href="https://iawn.net/" rel="noopener noreferrer" target="_blank">International Asteroid Warning Network</a> to the United Nations’ <a href="https://www.unoosa.org/" rel="noopener noreferrer" target="_blank">Office for Outer Space Affairs</a> about the potential impact. Such alerts are posted publicly on the IAWN’s <a href="https://iawn.net/" rel="noopener noreferrer" target="_blank">website</a>. The <a href="https://iawn.net/documents/NOTIFICATIONS/2024-YR4_IAWN_Potential-Impact-Notification_20250129.pdf" rel="noopener noreferrer" target="_blank">29 January notice</a> assessed the regions of the planet at highest risk from 2024 YR4 (also known as its risk corridor), as well as the expected damage if the asteroid did crash into Earth.</p><p>On average, an object of 2024 YR4’s size—estimated at <a href="https://webbtelescope.org/contents/media/images/01JQSF5C4CGVMCS3EV9YX6CQAR" rel="noopener noreferrer" target="_blank">60 meters</a> across—slams into our planet once every thousand years. It’s considered a “city-killer” asteroid—not big enough to trigger a mass extinction, like the estimated 10-km one that likely killed the dinosaurs, but still big enough to be deadly up to roughly 50 km from the impact location. Fortunately, by 24 February, <a href="https://www.minorplanetcenter.net/db_search/show_object?object_id=2024+YR4" rel="noopener noreferrer" target="_blank">further observations</a> by telescopes across the globe had refined the asteroid’s trajectory enough to rule out near-term Earth impact.</p><p>Yet when it comes to asteroids and Earth, there won’t always be such an uncomplicated, happy ending. Another asteroid that size or even larger will eventually be on a collision course with the planet [see chart below].</p><h3></h3><br/><img alt="Near-Earth objects threat; size, frequency, damage, energy, discovery percentage comparison." class="rm-shortcode" data-rm-shortcode-id="1a163985e40a1e1b33f1a47a44dfeabd" data-rm-shortcode-name="rebelmouse-image" id="1d275" loading="lazy" src="https://spectrum.ieee.org/media-library/near-earth-objects-threat-size-frequency-damage-energy-discovery-percentage-comparison.png?id=60762777&width=980"/><h3></h3><br/><p>The world’s space agencies track an estimated <a href="https://science.nasa.gov/science-research/planetary-science/planetary-defense/near-earth-asteroids/" target="_blank">95 percent of NEOs greater than 1 km in diameter.</a> The International Asteroid Warning Network and a related <a href="https://www.cosmos.esa.int/web/smpag/smpag_members" target="_blank">Space Mission Planning Advisory Group</a> (SMPAG) are global coordinating bodies that monitor these efforts. And thankfully, none of the giant NEOs tracked by the above pose an impact risk to Earth for at least the next hundred years. (Meanwhile, comet impacts with Earth are even rarer than those of asteroids.)</p><p>But you can only track the NEOs that are known. And plenty of city-killer asteroids remain lurking and undiscovered, potentially still posing a real risk to life on the planet. In the 50-meter range, a meager 7 percent of NEOs have been found. That’s not for lack of trying. It’s just more difficult to find small asteroids because smaller asteroids appear dimmer than larger ones.</p><h3>NASA’s & FEMA’s 2024 Planetary-Defense Exercise</h3><br/><p>Last year, NASA and the Federal Emergency Management Agency sponsored an <a href="https://www.nasa.gov/wp-content/uploads/2024/06/ttx5-quicklook-report-final.pdf" target="_blank"> Interagency Tabletop Exercise</a> around a hypothetical asteroid impact threat. In the fictional scenario, telescope observations detected an NEO, yielding a 72 percent chance of the object colliding with Earth in 2038. I served as a facilitator for this tabletop exercise, which aimed to further discussion and opportunities to stress-test new approaches to a realistic “killer asteroid” scenario.</p><p>One complicating factor we introduced was that the NEO’s size remained alarmingly difficult to pin down: Was it a 60-meter city killer? Or was it an 800-meter object that could devastate a country? If the latter, it would have risked the lives and livelihoods of more than 10 million people.</p><p>To keep the exercise focused, we centered it around the hypothetical NEO’s detection—and the decisions and next actions that would follow. We tracked the unfolding discussions and decisions in the aftermath of detection. Several U.S. agencies and organizations participated, as did the <a href="https://www.unoosa.org/" target="_blank">U.N. Office of Outer Space Affairs</a> and international partners.</p><p>One of the key gaps identified was the limited readiness to quickly deploy space missions for reconnaissance of the asteroid threat and for preventing Earth impact. The scenario’s large uncertainties underscored the need for capabilities to rapidly obtain better information about the asteroid.</p><p>“I know what I would prefer [to do],” said one anonymous participant quoted in the exercise’s quick-look <a href="https://www.nasa.gov/wp-content/uploads/2024/06/ttx5-quicklook-report-final.pdf" target="_blank">report</a>. “But Congress will tell us to wait.” —<em>N.L.C.</em></p><h3></h3><br/><p>New hardware is clearly needed. Sometime soon, the <a href="https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory" rel="noopener noreferrer" target="_blank">Vera C. Rubin Observatory</a>, in Chile, is expected to see first light. The observatory will survey the entire visible sky every few nights, through a 3,200-megapixel camera on an 8.4-meter telescope. No Earth-based telescope in the history of the NEO hunt can match its capabilities. Adding to our NEO search will be NASA’s <a href="https://en.wikipedia.org/wiki/NEO_Surveyor" rel="noopener noreferrer" target="_blank">NEO Surveyor</a>, an infrared space telescope scheduled to launch as soon as 2027. Together, the two new facilities are expected to discover thousands of new-to-us near-Earth asteroids. For objects 140 meters and larger, the two telescopes will locate an anticipated 90 percent of the entire population.</p><p>Once an NEO has been discovered, astronomers routinely track its orbit and extrapolate its trajectory over the coming century. So any NEO already on the books (for example, in NASA’s <a href="https://cneos.jpl.nasa.gov/" rel="noopener noreferrer" target="_blank">database</a> or ESA’s <a href="https://neo.ssa.esa.int/" rel="noopener noreferrer" target="_blank">database</a>) is quite likely to come with decades of warning. Ideally, that should leave ample time to develop and deploy a spacecraft to learn more about it and redirect the wayward space rock if necessary.</p><h2>Step 2: Send an NEO Reconnaissance Mission</h2><p>Imagine that the probability of 2024 YR4 colliding with Earth rose instead of fell, with the estimated impact to take place sometime in 2032. Here’s why that would have been especially worrying.</p><p>Asteroid 2024 YR4’s elongated <a href="https://neo.ssa.esa.int/documents/d/guest/close-approach-fact-sheet-for-asteroid-2024yr4-version-1-0-" rel="noopener noreferrer" target="_blank">orbit</a> made it unobservable from Earth after mid-May of this year. So we wouldn’t have been able to see it with even the most sensitive telescopes until its next swing through our region of the solar system—around June 2028.</p><p>In that alternate universe, we would’ve had to wait three years to launch a reconnaissance mission to study the object up close. Only then would we have known the next steps to take to redirect the asteroid away from Earth before its fated visit four years later.</p><p>As it happens, SMPAG held <a href="https://www.cosmos.esa.int/web/smpag" target="_blank">preliminary discussions</a> about 2024 YR4 in late January and early February. However, because the asteroid’s risk of collision with Earth soon dwindled to zero, the group didn’t develop specific recommendations.</p><h3>Hayabusa2#</h3><br/><img alt="Illustration of a yellow satellite with blue solar panels in space." class="rm-shortcode" data-rm-shortcode-id="1cd9cf1a8a713bccf46db5c0457d0c9e" data-rm-shortcode-name="rebelmouse-image" id="249e4" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-yellow-satellite-with-blue-solar-panels-in-space.png?id=60762281&width=980"/><p>The Japan Aerospace Exploration Agency has extended a previous mission (Hayabusa2) to encounter two more near-Earth asteroids over the next six years.</p><p><strong>Flyby:</strong></p><p>2026</p><p><strong>Rendezvous</strong></p><p>2031</p><h3></h3><br/><p>DART would have provided a foundation for a 2028 reconnaissance mission, as would NASA’s <a href="https://en.wikipedia.org/wiki/Lucy_(spacecraft)" target="_blank">Lucy mission</a>, which flew past the <a href="https://en.wikipedia.org/wiki/152830_Dinkinesh" target="_blank">asteroid Dinkinesh</a> in 2023. Reconnaissance flybys provide as little as a few precious seconds to capture the needed data about the target asteroid. Of course, inserting a reconnaissance craft into orbit around the asteroid would allow more detailed measurements. However, few NEO trajectories offer the opportunity for any maneuver other than a flyby—especially when time is of the essence.</p><p>Whatever the trajectory, the most important question for a reconnaissance mission would be whether the asteroid was in fact on a collision course with Earth in 2032. If so, where on the planet would it hit? That future impact location could potentially be <a href="https://cneos.jpl.nasa.gov/pd/cs/" target="_blank">narrowed down</a> to <a href="https://cneos.jpl.nasa.gov/pd/cs/ttx24/03_TTX_module2.pdf" target="_blank">within a hundred kilometers</a>.</p><p>The mission might also uncover some complications. For starters, we might discover that the asteroid is actually plural. Some 15 percent of NEOs are believed to have secondary objects orbiting them—they’re asteroids with moons. And some asteroids are essentially a flying jumble of rocks.</p><p>Another wrinkle comes in determining the asteroid’s mass. We need to know the mass to calculate the damage it could cause on impact, as well as the oomph required to divert it.</p><p>Unfortunately, the technology to measure the mass of a city-killer asteroid doesn’t exist. The mass of a larger, kilometer-size asteroid is measured by determining the gravitational pull on the reconnaissance spacecraft, but that trick doesn’t work for smaller asteroids. Right now, the best we can do is estimate the mass by measuring the asteroid’s physical size from closeup imaging during a flyby and then inferring the composition.</p><p>These challenges will need to be mastered in time for the reconnaissance mission, as the spacecraft—traveling at up to 90,000 kilometers per hour—flies past the potentially irregularly shaped object or objects half-shrouded in darkness. So it probably makes sense to tackle those challenges now rather than waiting until an actual threat emerges.</p><h2>Step 3: Change NEO’s Course With Interceptor</h2><p>If the reconnaissance mission does conclude that a killer asteroid is on the way and narrows down the date of impact, then what? Returning to 2024 YR4, that might make 22 December 2032 a very bad day for one city-size region of the planet. Even if it fell in the ocean, we’d need to look at geological and oceanic computer models to forecast the tsunami risk. If that risk is small, then world leaders and NEO advisors might opt to let the asteroid proceed.</p><p>On the other hand, if the asteroid is on course to strike a highly populated area, then launching a spacecraft to deflect the asteroid and prevent impact might be warranted.</p><h3>NEO Surveyor </h3><br/><img alt="Diagram of the EM Spectrum Explorer satellite design with shaded components." class="rm-shortcode" data-rm-shortcode-id="a15d11841b516e46c3979de971530464" data-rm-shortcode-name="rebelmouse-image" id="13e5d" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-the-em-spectrum-explorer-satellite-design-with-shaded-components.png?id=60762509&width=980"/><p>NASA’s infrared space telescope has been designed to detect and track near-Earth object (NEO) asteroids that are potentially hazardous to Earth.</p><p><strong>Launch: </strong></p><p>as early as 2027</p><p>Here, lessons from DART are instructive. For one thing, a spacecraft impact can pack only so much punch. It’s unclear whether a deflection spacecraft the size of the DART would be able to nudge a 2024 YR4–like asteroid with enough force to avoid Earth. It’s also possible the impactor’s nudge could inadvertently cause it to land in an even worse spot, inflicting more damage. And if the asteroid is only weakly held together, a DART-like collision might break it into multiple, smaller rubble piles—one or more of which could still reach Earth. So any kind of deflection mission has to be carefully considered.</p><p>Other asteroid defense technologies are also worth considering. These other options are still untested, but we might as well get started, when nothing’s yet at stake.</p><p>If you have decades of lead time, for instance, a rendezvous spacecraft could be dispatched to orbit the killer asteroid and slowly and continually act on it. Researchers have suggested <a href="https://www.planetary.org/articles/asteroid-deflection-techniques-to-save-the-earth" target="_blank">using such a spacecraft’s gravity</a> to tug the asteroid off its path or <a href="https://arc.aiaa.org/doi/10.2514/6.2025-0088" target="_blank">ion-beam engines</a> to gradually push it. The spacecraft could use one or both techniques over the span of years or decades to cause a large enough change in the asteroid’s trajectory to prevent Earth impact.</p><p>But if time is short, there are far fewer options. If the situation is dire enough, with a monster asteroid likely heading for a populated area, then using a nuclear explosive to break up or divert the asteroid could be on the table. That’s the premise of the 1998 blockbuster <a href="https://www.imdb.com/title/tt0120591/" target="_blank"><em>Armageddon</em></a> (as well as the 2021 Netflix satire <a href="https://www.imdb.com/title/tt11286314" target="_blank"><em>Don’t Look Up</em></a>). Absurd, yes, but worth considering if you’re otherwise out of options.</p><p>Of course, the whole idea of planetary defense is to have options and to do as much advance preparation as possible. A number of countries have planetary-defense missions currently in space or planned in the next few years.</p><p>The ESA’s <a href="https://www.heramission.space/" target="_blank">Hera</a> mission launched last year and is on its way to rendezvous late next year with the asteroid system that DART struck, to investigate the aftermath of DART’s 2022 deflection test. The Japanese Aerospace Exploration Agency’s <a href="https://en.wikipedia.org/wiki/Hayabusa2" target="_blank">Hayabusa2</a> is set to fly by an NEO in 2026 and rendezvous with a different asteroid in 2031. It’s the next chapter to JAXA’s original Hayabusa2 mission, which brought back <a href="https://spectrum.ieee.org/japan-prepares-to-welcome-home-asteroid-explorer-hayabusa2" target="_self">samples of the asteroid Ryugu</a> in 2020. China plans to <a href="https://spacenews.com/china-reschedules-planetary-defense-mission-for-2027-launch/" target="_blank">perform a kinetic impactor demonstration</a> similar to DART, with an observer spacecraft to watch, scheduled to launch in 2027.</p><p>And in 2029, a 340-meter asteroid called <a href="https://science.nasa.gov/solar-system/asteroids/apophis/" target="_blank">Apophis</a>—after the Egyptian god of chaos and darkness—will pass within 32,000 km of Earth, which is closer than some geosynchronous satellites. This will happen on 13 April 2029—Friday the 13th, that is. Apophis won’t hit Earth, but its close pass has prompted the U.N. to designate 2029 the <a href="https://www.un.org/en/observances/asteroid-awareness-year" target="_blank">International Year of Asteroid Awareness and Planetary Defense</a>. The asteroid will be bright enough to be seen by the naked eye across parts of Europe, Asia, and Africa. And NASA has redirected its OSIRIS-REx spacecraft (which <a href="https://spectrum.ieee.org/distributed-acoustic-sensing-2671316332" target="_blank">returned samples of the asteroid Bennu</a> to Earth in 2023) to rendezvous with Apophis. The renamed <a href="https://science.nasa.gov/mission/osiris-apex/" target="_blank">OSIRIS-APEX</a> mission will give astronomers an important opportunity to further refine how we measure and characterize NEO asteroids.</p><p>While NEO researchers will continue to collect new data and develop new insights and perspectives, leading toward, we hope, better and stronger planetary defense, one perennial will hold as true in the future as it does today: In this very high-stakes game, you never get to pick the asteroid. The asteroid always picks you. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Wed, 11 Jun 2025 12:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/planetary-defense-killer-asteroids</guid><category>Space exploration</category><category>Asteroid tracking</category><category>Aerospace engineering</category><category>Space technology</category><category>Near-earth objects</category><category>Planetary defense</category><dc:creator>Nancy L. Chabot</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/spacecraft-approaches-large-asteroid-against-starry-background.png?id=60694025&amp;width=980"></media:content></item><item><title>IBM Says It’s Cracked Quantum Error Correction</title><link>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p>IBM has unveiled a new quantum computing architecture it says will slash the number of qubits required for error correction. The advance will underpin its goal of <a href="https://spectrum.ieee.org/ibm-quantum-computer-2668978269" target="_blank">building a large-scale, fault-tolerant quantum computer</a>, called Starling, that will be available to customers by 2029.</p><p>Because of the inherent unreliability of the qubits (the quantum equivalent of bits) that quantum computers are built from, error correction will be crucial for building reliable, large-scale devices. <a data-linked-post="2650272311" href="https://spectrum.ieee.org/google-tests-first-error-correction-in-quantum-computing" target="_blank">Error-correction approaches</a> spread each unit of information across many physical qubits to create “logical qubits.” This provides redundancy against errors in individual physical qubits.</p><p>One of the most popular approaches is known as a surface code, which requires roughly 1,000 physical qubits to make up one logical qubit. This was the approach IBM focused on initially, but the company eventually realized that creating the hardware to support it was an “engineering pipe dream,” <a href="https://research.ibm.com/people/jay-gambetta" target="_blank">Jay Gambetta</a>, the vice president of IBM Quantum, said in a press briefing.</p><p>Around 2019, the company began to investigate alternatives. In a <a href="https://www.nature.com/articles/s41586-024-07107-7" target="_blank">paper</a> published in <em>Nature</em> last year, IBM researchers outlined a new error-correction scheme called quantum low-density parity check (qLDPC) codes that would require roughly one-tenth of the number of qubits that surface codes need. Now, the company <a href="https://newsroom.ibm.com/2025-06-10-IBM-Sets-the-Course-to-Build-Worlds-First-Large-Scale,-Fault-Tolerant-Quantum-Computer-at-New-IBM-Quantum-Data-Center" target="_blank">has unveiled a new quantum-computing architecture</a> that can realize this new approach.</p><p>“We’ve cracked the code to quantum error correction and it’s our plan to build the first large-scale, fault-tolerant quantum computer,” said Gambetta, who is also an IBM Fellow. “We feel confident it is now a question of engineering to build these machines, rather than science.”</p><h2>IBM Unveils New Quantum Roadmap</h2><p>IBM will take the first step towards realizing this architecture later this year with a processor called Loon. This chip will feature couplers that can connect distant qubits on the same chip, which is key for implementing qLDPC codes. These “non-local” interactions are what make the approach more efficient than the surface code, which relies solely on qubits communicating with their neighbors.</p><p>According to <a href="https://www.ibm.com/downloads/documents/us-en/131cf87ab63319bf" rel="noopener noreferrer" target="_blank">a roadmap</a> released alongside details of the new architecture, the company plans to build a follow-on processor called Kookaburra in 2026 that will feature both a logical processing unit and a quantum memory. This will be the first demonstration of the kind of base module that subsequent systems will be built from. The following year IBM plans to link two of these modules together to create a device called Cockatoo.</p><p>The road map doesn’t detail how many modules will be used to create Starling, IBM’s planned commercial offering, but the computer will feature 200 logical qubits and be capable of running 100 million quantum operations. Exactly how many physical qubits will be required is yet to be finalized, said <a href="https://research.ibm.com/people/matthias-steffen" target="_blank">Matthias Steffen</a>, IBM Fellow, who leads the quantum-processor technology team. But the new architecture is likely to require on the order of several hundred physical qubits to create 10 logical qubits, he added.</p><p>IBM plans to build Starling by 2028, before making it available on the cloud the following year. It will be housed in a new quantum data center in Poughkeepsie, N.Y., and will lay the foundations for the final system on IBM’s current road map, a 2,000 logical qubit machine codenamed Blue Jay.</p><p>IBM’s new architecture is a significant advance over its previous technology, says <a href="https://www.gartner.com/en/experts/mark-horvath" target="_blank">Mark Horvath</a>, a vice president analyst at Gartner, who was briefed in advance of the announcement. The new chip’s increased connectivity makes it substantially more powerful and is backed up by significant breakthroughs in 3D fabrication. And if it helps IBM reach 200 logical qubits, that would bring quantum computers into the realm of solving practical problems, Horvath says.</p><p>However, Horvath adds that the modular approach IBM is banking on to get there could prove challenging. “That’s a very complicated task,” he says. “I think it will eventually work. It’s just, it’s a lot further off than people think it is.”</p><p>One of biggest remaining hurdles is improving gate fidelities across the device. To successfully implement this new architecture, error rates need to come down by an order of magnitude, admitted IBM’s Steffen, though the company is confident this is achievable. One of the main paths forward will be to improve the coherence times of the underlying qubits, which refers to how long they can maintain their quantum state. “We do have evidence that this is really one of the main bottlenecks to improving gate errors,” Steffen says.</p><p>In isolated test devices, IBM has managed to push average coherence times to 2 milliseconds but translating that to larger chips is not straightforward. Steffen said the company recently made progress with its Heron chips, going from around 150 to 250 microseconds.</p><p>Significant engineering challenges remain in supporting infrastructure as well, said Steffen, including connectors that link together different parts of the system and amplifiers. But a big advantage of the new architecture is that it requires far fewer components due to the reduced number of physical qubits. “This is one of the reasons why we’re so excited about these qLDPC codes, because it also reduces all of the nonquantum-processor overhead,” he says.<br/></p><p><em>This story was updated on 10 June 2025 to correct some details of IBM’s current roadmap.</em><br/></p>]]></description><pubDate>Tue, 10 Jun 2025 14:00:10 +0000</pubDate><guid>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</guid><category>Quantum computing</category><category>Ibm</category><category>Error correction</category><category>Quantum computers</category><category>Qubits</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&amp;width=980"></media:content></item><item><title>Navigating the Dual-Use Dilemma</title><link>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&width=1200&height=800&coordinates=70%2C0%2C70%2C0"/><br/><br/><p>Open-source technology developed in the civilian sector has the capacity to also be used in military applications or be simply misused. Navigating this <a href="https://link.springer.com/article/10.1007/s11948-009-9159-9" rel="noopener noreferrer" target="_blank">dual-use</a> potential is becoming more important across engineering fields, as innovation goes both ways. While the “openness” of open-source technology is part of what drives innovation and allows everyone access, it also, unfortunately, means it’s just as easily accessible to others, including the military and criminals.</p><p>What happens when a rogue state, a nonstate militia, or a school shooter displays the same creativity and innovation with open-source technology that engineers do? This is the question we are discussing here: How can we uphold our principles of open research and innovation to drive progress while mitigating the inherent risks that come with accessible technology?</p><p>More than just open-ended risk, let’s discuss the specific challenges open-source technology and its dual-use potential have on robotics. Understanding these challenges can help engineers learn what to look for in their own disciplines.</p><h2>The Power and Peril of Openness</h2><p>Open-access publications, software, and educational content are fundamental to advancing robotics. They have democratized access to knowledge, enabled reproducibility, and fostered a vibrant, collaborative international community of scientists. Platforms like arXiv and GitHub and open-source initiatives like the <a href="https://www.ros.org/" rel="noopener noreferrer" target="_blank">Robot Operating System</a> (ROS) and the <a href="https://github.com/open-dynamic-robot-initiative/" rel="noopener noreferrer" target="_blank">Open Dynamic Robot Initiative</a> have been pivotal in accelerating robotics research and innovation, and there is no doubt that they should remain openly accessible. Losing access to these resources would be devastating to the robotics field.</p><p>However, robotics carries inherent dual-use risks since most robotics technology can be repurposed <a href="https://spectrum.ieee.org/autonomous-weapons-challenges" target="_blank">for military use</a> or <a href="https://spectrum.ieee.org/why-you-should-fear-slaughterbots-a-response" target="_blank">harmful purposes</a>. One recent example of custom-made drones in current conflicts is particularly insightful. The resourcefulness displayed by Ukrainian soldiers in repurposing and sometimes <a href="https://www.cnas.org/publications/reports/evolution-not-revolution" rel="noopener noreferrer" target="_blank">augmenting civilian drone technology</a> received worldwide, often admiring, news coverage. Their creativity has been made possible through the affordability of commercial drones, spare parts, 3D printers, and the availability of open-source software and hardware. This allows people with little technological background and money to easily create, control, and repurpose robots for military applications. One can certainly argue that this has had an empowering effect on Ukrainians defending their country. However, these same conditions also present opportunities for a wide range of potential bad actors.</p><p>Openly available knowledge, designs, and software can be misused to enhance existing weapons systems with capabilities like vision-based <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12663" rel="noopener noreferrer" target="_blank">navigation, autonomous targeting, or swarming</a>. Additionally, unless proper security measures are taken, the public nature of open-source code makes it vulnerable to cyberattacks, potentially allowing malicious actors to gain control of robotic systems and cause them to malfunction or be used for <a href="https://www.sciencedirect.com/science/article/pii/S2667305323000625" rel="noopener noreferrer" target="_blank">malevolent purposes</a>. Many ROS users already recognize that they do not invest enough in <a href="https://aliasrobotics.com/files/robot_cybersecurity_review.pdf" rel="noopener noreferrer" target="_blank">cybersecurity</a> for their applications.</p><h2>Guidance Is Necessary</h2><p>Dual-use risks stemming from openness in research and innovation are a concern for many engineering fields. Did you know that engineering was originally a military-only activity? The word “engineer” was coined in the Middle Ages to describe “a designer and constructor of fortifications and weapons.” Some engineering specializations, especially those that include the development of weapons of mass destruction (chemical, biological, radiological, and nuclear), have developed clear guidance, and in some cases, regulations for how research and innovation can be conducted and disseminated. They also have community-driven processes intended to mitigate dual-use risks associated with spreading knowledge. For instance, BioRxiv and MedRxiv—the preprint servers for biology and health sciences—screen submissions for material that poses a biosecurity or health risk before publishing them.</p><p>The field of robotics, in comparison, offers no specific regulation and little guidance as to how roboticists should think of and address the risks associated with openness. Dual-use risk is not taught in most universities, despite it being something that students will likely face in their careers, such as when assessing whether their work is subject to <a href="https://www.sipri.org/publications/2020/policy-reports/responsible-artificial-intelligence-research-and-innovation-international-peace-and-security" rel="noopener noreferrer" target="_blank">export-control regulations on dual-use items</a>.</p><p>As a result, roboticists may not feel they have an incentive or are equipped to evaluate and mitigate the dual-use risks associated with their work. This represents a major problem, as the likelihood of harm associated with the misuse of open robotic research and innovation is likely higher than that of nuclear and biological research, both of which require significantly more resources. Producing “do-it-yourself” robotic weapon systems using open-source design and software and off-the-shelf commercial components is relatively easy and accessible. With this in mind, we think that it’s high time for the robotics community to work toward its own set of sector-specific guidance for how researchers and companies can best navigate the dual-use risks associated with the open diffusion of their work.</p><h2>A Road Map for Responsible Robotics</h2><p>Striking a balance between security and openness is a complex challenge, but one that the robotics community must embrace. We cannot afford to stifle innovation, nor can we ignore the potential for harm. A proactive, multipronged approach is needed to navigate this dual-use dilemma. Drawing lessons from other fields of engineering, we propose a road map focusing on four key areas: education, incentives, moderation, and red lines.</p><h3>Education</h3><p>Integrating responsible research and innovation into robotics education at all levels is paramount. This includes not only dedicated courses but also the <a href="https://journals.uclpress.co.uk/lre/article/id/129/" rel="noopener noreferrer" target="_blank">systematic inclusion</a> of dual-use and cybersecurity considerations within core <a href="https://link.springer.com/article/10.1007/s11948-019-00164-6" rel="noopener noreferrer" target="_blank">robotics curricula</a>. We must foster a culture of responsible innovation so that we can empower roboticists to make informed decisions and proactively address potential risks.</p><p>Educational initiatives could include:</p><ul><li>Developing and disseminating open-source educational materials on responsible robotics for robotics teachers, researchers, and professionals from resources such as the <a href="https://disarmament.unoda.org/responsible-innovation-ai/resources/" rel="noopener noreferrer" target="_blank">United Nations Office for Disarmament Affairs</a> (UNODA) and the <a href="https://airesponsibly.net/education/" rel="noopener noreferrer" target="_blank">Center for Responsible AI</a> at New York University. </li><li>Organizing workshops and seminars on dual-use and ethical considerations at robotics conferences and universities.</li><li>Encouraging universities to offer courses or modules dedicated to <a href="https://journals.sagepub.com/doi/10.1177/20539517231219958" rel="noopener noreferrer" target="_blank">responsible research and innovation in robotics</a>.</li></ul><h3>Incentives</h3><p>Everyone should be encouraged to assess the potential negative consequences of making their work fully or partially open. Funding agencies can mandate risk assessments as a condition for project funding, signaling their importance. Professional organizations, like the <a href="https://www.ieee-ras.org/" rel="noopener noreferrer" target="_blank">IEEE Robotics and Automation Society</a> (RAS), can adopt and promote <a href="https://www.ieee-ras.org/industry-government/standards" rel="noopener noreferrer" target="_blank">best practices</a>, providing tools and frameworks for researchers to identify, assess, and mitigate risks. Such tools could include self-assessment checklists for individual researchers and guidance for how faculties and labs can set up ethical review boards. Academic journals and conferences can make peer-review risk assessments an integral part of the publication process, especially for high-risk applications.</p><p>Additionally, incentives like awards and recognition programs can highlight exemplary contributions to risk assessment and mitigation, fostering a culture of responsibility within the community. Risk assessment can also be encouraged and rewarded in more informal ways. People in leadership positions, such as Ph.D. supervisors and heads of labs, could build ad hoc opportunities for students and researchers to discuss possible risks. They can hold seminars on the topic and provide introductions to external experts and stakeholders like social scientists and experts from NGOs.</p><h3>Moderation</h3><p>The robotics community can implement <a href="https://dl.acm.org/doi/10.1145/3593013.3593981" rel="noopener noreferrer" target="_blank">self-regulation mechanisms</a> to moderate the diffusion of high-risk material. This could involve:</p><ul><li>Screening work prior to publication to prevent the dissemination of content posing serious risks.</li><li>Implementing graduated access controls (“gating”) to certain source code or data on open-source repositories, potentially requiring users to identify themselves and specify their intended use.</li><li>Establishing clear guidelines and community oversight to ensure transparency and prevent misuse of these moderation mechanisms. For example, organizations like RAS could design categories of risk levels for robotics research and applications and create a monitoring committee to track and document real cases of the misuse of robotics research to understand and visualize the scale of the risks and create better mitigation strategies.</li></ul><h3>Red Lines</h3><p>The robotics community should also seek to define and enforce red lines for the development and deployment of robotics technologies. Efforts to define red lines have already been made in that direction, notably in the context of the <a href="https://standards.ieee.org/industry-connections/ec/autonomous-systems/" rel="noopener noreferrer" target="_blank">IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</a>. Companies, including <a href="https://bostondynamics.com/" rel="noopener noreferrer" target="_blank">Boston Dynamics</a>, <a href="https://www.unitree.com/" rel="noopener noreferrer" target="_blank">Unitree</a>, <a href="https://www.agilityrobotics.com/" rel="noopener noreferrer" target="_blank">Agility Robotics</a>, <a href="https://clearpathrobotics.com/" rel="noopener noreferrer" target="_blank">Clearpath Robotics</a>, <a href="https://www.anybotics.com/" rel="noopener noreferrer" target="_blank">ANYbotics</a>, and <a href="https://www.openrobotics.org/" rel="noopener noreferrer" target="_blank">Open Robotics</a> wrote an open letter calling for regulations on the <a href="https://bostondynamics.com/news/general-purpose-robots-should-not-be-weaponized/" rel="noopener noreferrer" target="_blank">weaponization of general-purpose robots</a>. Unfortunately, their efforts were very narrow in scope, and there is a lot of value in further mapping end uses of robotics that should be deemed off-limits or demand extra caution.</p><p>It will absolutely be difficult for the community to agree on standard red lines, because what is considered ethically acceptable or problematic is highly subjective. To support the process, individuals and companies can reflect on what they consider to be unacceptable use of their work. This could result in policies and terms of use that beneficiaries of open research and open-source design software would have to formally agree to (such as specific-use open-source licenses). This would provide a basis for revoking access, denying software updates, and potentially suing or blacklisting people who misuse the technology. Some companies, including Boston Dynamics, have already implemented these measures to some extent. Any person or company conducting open research could replicate this example.</p><p>Openness is the key to innovation and the democratization of many engineering disciplines, including robotics, but it also amplifies the potential for misuse. The engineering community has a responsibility to proactively address the dual-use dilemma. By embracing responsible practices, from education and risk assessment to moderation and red lines, we can foster an ecosystem where openness and security coexist. The challenges are significant, but the stakes are too high to ignore. It is crucial to ensure that research and innovation benefit society globally and do not become a driver of instability in the world. This goal, we believe, aligns with the mission of the IEEE, which is to “advance technology for the benefit of humanity.” The engineering community, especially roboticists, needs to be proactive on these issues to prevent any backlash from society and to preempt potentially counterproductive measures or international regulations that could harm open science.</p>]]></description><pubDate>Tue, 10 Jun 2025 13:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</guid><category>Robotics</category><category>Guest articles</category><category>Dual-use</category><dc:creator>Ludovic Righetti</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&amp;width=980"></media:content></item><item><title>IEEE’s 5 New E-Books Provide On-ramp to Engineering</title><link>https://spectrum.ieee.org/tryengineering-5-new-ebooks</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&width=1200&height=800&coordinates=140%2C0%2C140%2C0"/><br/><br/><p>As the home for IEEE’s preuniversity resources, activities, and hands-on experiences, <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering</a> serves as a hub for educators, parents, and IEEE volunteers to teach school-age children about engineering.</p><p>With support from IEEE partners, TryEngineering has launched a series of <a href="https://tryengineering.org/news/discover-tryengineerings-ebooks/" rel="noopener noreferrer" target="_blank">e-books</a>. Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way. Visually appealing, the books use colorful charts and graphs to grab children’s attention.</p><p>Each of the five English-language publications provides an overview of a technology or topic. The books include stories about <a href="https://spectrum.ieee.org/topic/careers/" target="_self">engineers</a>, technologists, and early pioneers.</p><h2>Engineering disciplines, solutions, and ethics</h2><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2023/" rel="noopener noreferrer" target="_blank"><em><em>Engineers Make the World a Better Place</em></em></a> was created with funding from the <a href="https://www.ieee.org/about/corporate/initiatives/initiatives-committee.html" rel="noopener noreferrer" target="_blank">IEEE New Initiatives Committee</a>. The book introduces students to engineering disciplines and explains how engineers improve society by solving challenging problems, such as<a href="https://tryengineering.org/news/high-school-students-modify-ride-on-cars-for-disabled-children/" rel="noopener noreferrer" target="_blank"> improving access for children with limited physical mobility</a>.</p><p>With support from <a href="https://www.onsemi.com/" rel="noopener noreferrer" target="_blank">Onsemi</a>’s <a href="https://spectrum.ieee.org/ieee-tryengineering-onsemi-grant" target="_self">Giving Now</a> program, IEEE <a href="https://spectrum.ieee.org/topic/semiconductors/" target="_self">semiconductor</a> experts wrote <a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2024/" rel="noopener noreferrer" target="_blank"><em><em>Microchip Adventures: A Journey Into the World of Semiconductors</em></em></a>. It includes an introduction to the field, a list of commonly used terms, an explanation of how chips are made, and an overview of the <a href="https://spectrum.ieee.org/topic/tech-history/" target="_self">technology’s history</a>.</p><p class="pull-quote">Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_signalprocessing_2025/" rel="noopener noreferrer" target="_blank"><em>Wave Wonders: A Signal Processing Journey</em></a> was written with experts from the <a href="https://signalprocessingsociety.org/" rel="noopener noreferrer" target="_blank">IEEE Signal Processing Society</a>. It teaches students how to tell the difference between digital and analog signals. The e-book introduces readers to the inventor of the <a href="https://spectrum.ieee.org/the-first-transatlantic-telegraph-cable-was-a-bold-beautiful-failure" target="_self">telegraph</a>, <a href="https://www.britannica.com/biography/Samuel-F-B-Morse" rel="noopener noreferrer" target="_blank">Samuel Morse</a>. Also included is the <a href="https://tryengineering.org/resource/lesson-plan/electric-messages-then-and-now/" rel="noopener noreferrer" target="_blank">Electric Messages lesson plan</a>, which explains how early telegraphs worked.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_oceanengineering/" target="_blank"><em><em>Ocean Engineering Heroes: Making the Oceans and the World a Better Place</em></em></a> was created in partnership with the <a href="https://ieeeoes.org/" rel="noopener noreferrer" target="_blank">IEEE Oceanic Engineering Society</a>. It includes video interviews with several society leaders about oceans and ways to help keep them clean. It also discusses the impact of pollution including sound pollution from ships and other sources. Included are links to resources from other organizations such as the <a href="https://www.noaa.gov/" rel="noopener noreferrer" target="_blank">National Oceanic and Atmospheric Administration</a>.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_aiadventures/" rel="noopener noreferrer" target="_blank"><em><em>AI Adventures: Exploring the World of Artificial Intelligence</em></em></a> was written with assistance from the <a href="https://www.computer.org/" rel="noopener noreferrer" target="_blank">IEEE Computer Society</a>. The publication describes how <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">AI</a> models work and explains commonly used terms including <em><em>machine learning</em></em> and <em><em>neural networks</em></em>. The book covers the importance of <a href="https://spectrum.ieee.org/ai-ethics-advice" target="_self">ethics when using AI</a>.</p><p>Visit the <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering website</a> for the e-books and many other resources for educators, parents, and volunteers. To help expand the site’s pool of offerings, consider donating to the <a href="https://secure.ieeefoundation.org/site/Donation2?df_id=1720&mfc_pref=T&1720.donation=form1" rel="noopener noreferrer" target="_blank">IEEE TryEngineering Fund</a>.</p>]]></description><pubDate>Mon, 09 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/tryengineering-5-new-ebooks</guid><category>Artificial intelligence</category><category>Education</category><category>Ieee products and services</category><category>Ocean engineering</category><category>Signal processing</category><category>Students</category><category>Type:ti</category><dc:creator>Debra Gulick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&amp;width=980"></media:content></item><item><title>Doctors Could Hack the Nervous System With Ultrasound</title><link>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&width=1200&height=800&coordinates=0%2C800%2C0%2C800"/><br/><br/><p><strong>Inflammation: It’s the body’s</strong> natural response to injury and infection, but medical science now recognizes it as a double-edged sword. When inflammation becomes chronic, it can contribute to a host of serious health problems, including arthritis, heart disease, and certain cancers. As this understanding has grown, so too has the search for effective ways to manage harmful inflammation.</p><p>Doctors and researchers are exploring various approaches to tackle this pervasive health issue, from new medications to dietary interventions. But what if one of the most promising treatments relies on a familiar technology that’s been in hospitals for decades?</p><p>Enter <a href="https://www.sciencedirect.com/science/article/abs/pii/S0165027020301448" target="_blank">focused ultrasound stimulation</a> (FUS), a technique that uses sound waves to reduce inflammation in targeted areas of the body. It’s a surprising new application for ultrasound technology, which most people associate with prenatal checkups or diagnostic imaging. And FUS may help with many other disorders too, including diabetes and obesity. By modifying existing ultrasound technology, we might be able to offer a novel approach to some of today’s most pressing health challenges.</p><p>Our team of biomedical researchers at the <a href="https://feinstein.northwell.edu/institutes-researchers/bioelectronic-medicine" target="_blank">Institute of Bioelectronic Medicine</a> (part of the Feinstein Institutes for Medical Research), in Manhasset, N.Y., has made great strides in learning the electric language of the nervous system. Rather than treating disease with drugs that can have broad side effects throughout the body, we’re learning how to stimulate nerve cells, called neurons, to intervene in a more targeted way. Our goal is to activate or inhibit specific functions within organs.</p><p>The relatively new application of FUS for <a href="https://www.neuromodulation.com/about-neuromodulation" target="_blank">neuromodulation</a>, in which we hypothesize that sound waves activate neurons, may offer a precise and safe way to provide healing treatments for a wide range of both acute and chronic maladies. The treatment doesn’t require surgery and potentially could be used at home with a wearable device. People are accustomed to being prescribing pills for these ailments, but we imagine that one day, the prescriptions could be more like this: “Strap on your ultrasound belt once per day to receive your dose of stimulation.”</p><h2>How Ultrasound Stimulation Works</h2><p><a href="https://spectrum.ieee.org/mems-ultrasound-history" target="_self">Ultrasound</a> is a time-honored medical technology. Researchers began experimenting with ultrasound imaging in the 1940s, bouncing low-energy ultrasonic waves off internal organs to construct medical images, typically using intensities of a few hundred milliwatts per square centimeter of tissue. By the late 1950s, some doctors were using the technique to show expectant parents the <a href="https://en.wikipedia.org/wiki/Obstetric_ultrasonography" target="_blank">developing fetus inside the mother’s uterus</a>. And high-intensity ultrasound waves, which can be millions of milliwatts per square centimeter, have a variety of therapeutic uses, including <a href="https://my.clevelandclinic.org/health/treatments/16541-hifu-high-intensity-focused-ultrasound" target="_blank">destroying tumors</a>.</p><p>The use of low-intensity ultrasound (with intensities similar to that of imaging applications) to alter the activity of the nervous system, however, is relatively unexplored territory. To understand how it works, it’s helpful to compare FUS to the most common form of neuromodulation today, which uses electric current to alter the activity of neurons to treat conditions like Parkinson’s disease. In that technique, electric current increases the voltage inside a neuron, causing it to “fire” and release a neurotransmitter that’s received by connected neurons, which triggers those neurons to fire in turn. For example, the deep brain stimulation used to treat Parkinson’s activates certain neurons to restore healthy patterns of brain activity.</p><h3>How It Works</h3><br/><img alt="Neuron impulse transmission showing ion flow through cell membrane." class="rm-shortcode" data-rm-shortcode-id="7e69502283206bf183e56ebf15b850d4" data-rm-shortcode-name="rebelmouse-image" id="d2e57" loading="lazy" src="https://spectrum.ieee.org/media-library/neuron-impulse-transmission-showing-ion-flow-through-cell-membrane.png?id=60681884&width=980"/><h3></h3><br/><p><strong></strong>In FUS, by contrast, the sound waves’ vibrations interact with the membrane of the neuron, <a href="https://www.nature.com/articles/s41467-022-28040-1" target="_blank">opening channels</a> that allow ions to flow into the cell, thus indirectly changing the cell’s voltage and causing it to fire. One promising use is <a href="https://doi.org/10.1016/j.clinph.2021.12.010" target="_blank">transcranial ultrasound stimulation</a>, which is being tested extensively as a noninvasive way to stimulate the brain and treat neurological and psychiatric diseases.</p><p>We’re interested in FUS’s effect on the peripheral nerves—that is, the nerves outside the brain and spinal cord. We think that activating specific nerves in the abdomen that regulate inflammation or metabolism may help address the root causes of related diseases, rather than just treating the symptoms.</p><h2>FUS for Inflammation</h2><p>Inflammation is something that we know a lot about. Back in 2002, <a href="https://feinstein.northwell.edu/institutes-researchers/our-researchers/kevin-j-tracey-md" target="_blank">Kevin Tracey</a>, currently the president and CEO of the Feinstein Institutes, upset the conventional wisdom that the nervous system and the immune system operate independently and serve distinct roles. He discovered the body’s <a href="https://www.nature.com/articles/nature01321" rel="noopener noreferrer" target="_blank">inflammatory reflex</a>: a two-way neural circuit that sends signals between the brain and body via the vagus nerve and the nerves of the spleen. These nerves control the release of <a href="https://en.wikipedia.org/wiki/Cytokine" rel="noopener noreferrer" target="_blank">cytokines</a>, which are proteins released by immune cells to trigger inflammation. Tracey and colleagues found that stimulating nerves in this neural circuit suppressed the inflammatory response. The discoveries led to the first clinical trials of electrical neuromodulation devices to treat chronic inflammation and launched the field of bioelectronic medicine.</p><h3>Hacking the Immune System</h3><br/><img alt="Ultrasound transducer scans kidney, showing bacteria spreading and evading immune response." class="rm-shortcode" data-rm-shortcode-id="f79f5c1b9ad1a15af4a3d7e98b7b7946" data-rm-shortcode-name="rebelmouse-image" id="7e37e" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-transducer-scans-kidney-showing-bacteria-spreading-and-evading-immune-response.png?id=60559935&width=980"/><h3></h3><br/><p>Tracey has been a pioneer in treating inflammation with <a href="https://spectrum.ieee.org/the-vagus-nerve-a-back-door-for-brain-hacking" target="_self">vagus nerve stimulation</a> (VNS), in which electrical stimulation of the vagus nerve activates neurons in the spleen. In animals and humans, VNS has been shown to reduce harmful inflammation in both chronic diseases such as arthritis and acute conditions such as sepsis. But direct VNS requires surgery to place an implant in the body, which makes it risky for the patient and expensive. That’s why we’ve pursued noninvasive ultrasound stimulation of the spleen.</p><p>Working with Tracey, collaborators at <a href="https://www.gehealthcare.com/" target="_blank">GE Research</a>, and others, we first experimented with rodents to show that ultrasound stimulation of the spleen <a href="https://www.nature.com/articles/s41467-019-08750-9" rel="noopener noreferrer" target="_blank">affects an anti-inflammatory pathway</a>, just as VNS does, and reduces cytokine production as much as a VNS implant does. We then conducted the first-in-human trial of <a href="https://feinstein.northwell.edu/news/the-latest/non-invasive-ultrasound-stimulation-spleen-reduces-inflammation-in-humans-new-study" target="_blank">FUS for controlling inflammation</a>.</p><p>We initially enrolled 60 healthy people, none of whom had signs of chronic inflammation. To test the effect of a 3-minute ultrasound treatment, we were measuring the amount of a molecule called <a href="https://en.wikipedia.org/wiki/Tumor_necrosis_factor" rel="noopener noreferrer" target="_blank">tumor necrosis factor</a> (TNF), which is a biomarker of inflammation that’s released when white blood cells go into action against a perceived pathogen. At the beginning of the study, 40 people received focused ultrasound stimulation, while 20 others, serving as the control group, simply had their spleens imaged by ultrasound. Yet, when we looked at the early data, <em>everyone</em> had lower levels of TNF, even the control group. It seemed that even imaging with ultrasound for a few minutes had a moderate anti-inflammatory effect! To get a proper control group, we had to recruit 10 more people for the study and devise a different sham experiment, this time unplugging the ultrasound machine.</p><h3></h3><br/><img alt="Abstract collage with neuron, brain textures, and dynamic wave patterns in pastel colors." class="rm-shortcode" data-rm-shortcode-id="2ae16b235445ec02f72e63b3c909cc7a" data-rm-shortcode-name="rebelmouse-image" id="f166b" loading="lazy" src="https://spectrum.ieee.org/media-library/abstract-collage-with-neuron-brain-textures-and-dynamic-wave-patterns-in-pastel-colors.jpg?id=60560952&width=980"/><h3></h3><br/><p>After the subjects received either the real or sham stimulation, we took blood samples from all of them. We next simulated an infection by adding a bacterial toxin to the blood in the test tubes, then measured the amount of TNF released by the white blood cells to fight the toxin. The results, which we published in the journal <em>Brain Stimulation</em> in 2023, showed that people who had received FUS treatments <a href="https://www.sciencedirect.com/science/article/pii/S1935861X23017436" target="_blank">had lower levels of TNF</a> than the true control group. We saw no problematic side effects of the ultrasound: The treatment didn’t adversely affect heart rate, blood pressure, or the many other biomarkers that we checked.</p><p>The results also showed that when we repeated the blood draw and experiment 24 hours later, the treatment groups’ TNF levels had returned to baseline. This finding suggests that if FUS becomes a treatment option for inflammatory diseases, people might require regular, perhaps even daily, treatments.</p><p>One surprising result was that it didn’t seem to matter which location within the spleen we targeted—all the locations we tried produced similar results. Our hypothesis is that hitting any target within the spleen activates enough nerves to produce the beneficial effect. What’s more, it didn’t matter which energy intensity we used. We tried intensities ranging from about 10 to 200 mW per cm<sup>2</sup>, well within the range of intensities used in ultrasound imaging; remarkably, even the lowest intensity level caused subjects’ TNF levels to drop.</p><p>Our big takeaway from that first-in-human study was that targeting the spleen with FUS is not just a feasible treatment but could be a gamechanger for inflammatory diseases. Our next steps are to investigate the mechanisms by which FUS affects the inflammatory response, and to conduct more animal and human studies to see whether prolonged administration of FUS to the spleen can treat chronic inflammatory diseases.</p><h2>FUS for Obesity and Diabetes</h2><p>For much of our research on FUS, we’ve partnered with GE Research, whose parent company is one of the world’s leading makers of ultrasound equipment. One of our first projects together explored the potential of FUS as a treatment for the widespread inflammation that often accompanies obesity, a condition that now affects about <a href="https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight" target="_blank">890 million people</a> around the world. In this study, we fed lab mice a high-calorie and high-fat “Western diet” for eight weeks. During the following eight weeks, half of them received ultrasound stimulation while the other half received daily sham stimulation. We found that the <a href="https://feinstein.northwell.edu/news/the-latest/feinstein-institutes-and-ge-research-demonstrate-ultrasound-stimulation-reduces-obesity" target="_blank">mice that received FUS had lower levels of cytokines</a>—and to our surprise, those mice also ate less and lost weight.</p><p>In related work with our GE colleagues, we examined the potential of FUS as a <a href="https://www.northwell.edu/news/the-latest/research-team-treats-diabetes-using-ultrasound" rel="noopener noreferrer" target="_blank">treatment for diabetes</a>, which now affects <a href="https://www.who.int/news-room/fact-sheets/detail/diabetes" rel="noopener noreferrer" target="_blank">830 million people</a> around the world. In a healthy human body, the liver stores glucose as a reserve and releases it only when it registers that glucose levels in the bloodstream have dropped. But in people with diabetes, this sensing system is dysfunctional, and the liver releases glucose even when blood levels are already high, causing a host of health problems.</p><h3>Hacking the Metabolic System</h3><br/><img alt="Ultrasound scan diagram showing brain-liver connection through neural pathways." class="rm-shortcode" data-rm-shortcode-id="1c7dde5728bf4fc1d4f0d8460e4cde13" data-rm-shortcode-name="rebelmouse-image" id="257de" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-scan-diagram-showing-brain-liver-connection-through-neural-pathways.png?id=60681922&width=980"/><p><span>For diabetes, our ultrasound target was the network of nerves that transmit signals between the liver and the brain: specifically, glucose-sensing neurons in the </span><a href="https://en.wikipedia.org/wiki/Porta_hepatis" target="_blank">porta hepatis</a><span>, which is essentially the gateway to the liver. We gave diabetic rats 3-minute daily ultrasound stimulation over a period of 40 days. Within just a few days, the treatment <a href="https://www.nature.com/articles/s41551-022-00870-w" target="_blank">brought down the rats’ glucose levels</a> from dangerously high to normal range. We got similar results in mice and pigs, and </span>published these exciting results<span> in 2022 in </span><em>Nature Biomedical Engineering</em><span>.</span></p><p>Those diabetes experiments shed some light on why ultrasound had this effect. We decided to zero in on a brain region called the <a href="https://en.wikipedia.org/wiki/Hypothalamus" target="_blank">hypothalamus</a>, which controls many crucial automatic body functions, including metabolism, circadian rhythms, and body temperature. Our colleagues at GE Research started investigating by blocking the nerve signals that travel from the liver to the hypothalamus in two different ways—both cutting the nerves physically and using a local anesthetic. When we then applied FUS, we didn’t see the beneficial decrease in glucose levels. This result suggests that the ultrasound treatment works by changing glucose-sensing signals that travel from the liver to the brain—which in turn changes the commands the hypothalamus issues to the metabolic systems of the body, essentially telling them to lower glucose levels.</p><p>The next steps in this research involve both technical development and clinical testing. Currently, administering FUS requires technical expertise, with a sonographer looking at ultrasound images, locating the target, and triggering the stimulation. But if FUS is to become a practical treatment for a chronic disease, we’ll need to make it usable by anyone and available as an at-home system. That could be a wearable device that uses ultrasound imaging to automatically locate the anatomical target and then delivers the FUS dose: All the patient would have to do is put on the device and turn it on. But before we get to that point, FUS treatment will have to be tested clinically in randomized controlled trials for people with obesity and diabetes. GE HealthCare recently <a href="https://www.gehealthcare.com/about/newsroom/press-releases/ge-healthcare-and-novo-nordisk-to-collaborate-to-advance-novel-non-invasive-treatment-for-type-2-diabetes-and-obesity-with-ultrasound" target="_blank">partnered</a> with Novo Nordisk to work on the clinical and product development of FUS in these areas.</p><h2>FUS for Cardiopulmonary Diseases</h2><p>FUS may also help with chronic cardiovascular diseases, many of which are associated with immune dysfunction and inflammation. We began with a disorder called <a href="https://en.wikipedia.org/wiki/Pulmonary_hypertension" target="_blank">pulmonary arterial hypertension</a>, a rare but incurable disease in which blood pressure increases in the arteries within the lungs. At the start of our research, it wasn’t clear whether inflammation around the pulmonary arteries was a cause or a by-product of the disease, and whether targeting inflammation was a viable treatment. Our group was the first to try FUS of the spleen in order to reduce the inflammation associated <a href="https://feinstein.northwell.edu/feinstein-institutes-ultrasound-neuromodulation-for-pulmonary-hypertension" target="_blank">with pulmonary hypertension</a> in rats.</p><p>The results, published last year, were very encouraging. We found that 12-minute FUS sessions <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCRESAHA.123.323679" target="_blank">reduced pulmonary pressure</a>, improved heart function, and reduced lung inflammation in the animals in the experimental group (as compared to animals that received sham stimulation). What’s more, in the animals that received FUS, the progression of the disease slowed significantly even after the experiment ended, suggesting that this treatment could provide a lasting effect.</p><p class="pull-quote">One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation.</p><p>This study was, to our knowledge, the first to successfully demonstrate an ultrasound-based therapy for any cardiopulmonary disease. And we’re eager to build on it. We’re next interested in studying whether FUS can help with congestive heart failure, a condition in which the heart can’t pump enough blood to meet the body’s needs. In the United States alone, more than <a href="https://www.thecardiologyadvisor.com/ddi/heart-failure-in-the-united-states/" target="_blank">6 million people</a> are living with heart failure, and that number could surpass 8 million by 2030. We know that inflammation plays a significant role in heart failure by damaging the heart’s muscle cells and reducing their elasticity. We plan to test FUS of the spleen in mice with the condition. If those tests are successful, we could move toward clinical testing in humans.</p><h2>The Future of Ultrasound Stimulation</h2><p>We have one huge advantage as we think about how to bring these results from the lab to the clinic: The basic hardware for ultrasound already exists, it’s already FDA approved, and it has a stellar safety record through decades of use. Our collaborators at GE have already experimented with modifying the typical ultrasound devices used for imaging so that they can be used for FUS treatments.</p><p>Once we get to the point of optimizing FUS for clinical use, we’ll have to determine the best neuromodulation parameters. For instance, what are the right acoustic wavelengths and frequencies? Ultrasound imaging typically uses higher frequencies than FUS does, but human tissue absorbs more acoustic energy at higher frequencies than it does at lower frequencies. So to deliver a good dose of FUS, researchers are exploring a wide range of frequencies. We’ll also have to think about how long to transmit that ultrasound energy to make up a single pulse, what rate of pulses to use, and how long the treatment should be.</p><p>In addition, we need to determine how long the beneficial effect of the treatment lasts. For some of the ailments that researchers are exploring, like <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8261174/" target="_blank">FUS of the brain to treat chronic pain</a>, a patient might be able to go to the doctor’s office once every three months for a dose. But for diseases associated with inflammation, a regular, several-times-per-week regimen might prove most effective, which would require at-home treatments.</p><p>For home use to be possible, the wearable device would have to locate the targets automatically via ultrasound imaging. As vast databases already exist of human ultrasound images from the liver, spleen, and other organs, it seems feasible to train a machine-learning algorithm to detect targets automatically and in real time. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. A few startups are working on building such wearable devices, which could take the form of a belt or a vest. For example, the company <a href="https://www.secondwaveus.com/" target="_blank">SecondWave Systems</a>, which has partnered with the University of Minnesota, in Minneapolis, has already conducted a small <a href="https://www.businesswire.com/news/home/20240501993182/en/SecondWave-Systems-Demonstrates-Reduction-in-Disease-Activity-in-Clinical-Study-Evaluating-Novel-Ultrasound-Based-Anti-Inflammatory-Therapy-in-Rheumatoid-Arthritis" target="_blank">pilot study</a> of its wearable device, trying it out on 13 people with rheumatoid arthritis and seeing positive outcomes.</p><p>While it will be many years before FUS treatments are approved for clinical use, and likely still more years for wearable devices to be proven safe enough for home use, the path forward looks very promising. We believe that FUS and other forms of bioelectronic medicine offer a new paradigm for human health, one in which we reduce our reliance on pharmaceuticals and begin to speak directly to the body electric.</p>]]></description><pubDate>Mon, 09 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</guid><category>Ultrasound</category><category>Neuromodulation</category><category>Neural stimulation</category><category>Diabetes</category><category>Arthritis</category><dc:creator>Sangeeta S. Chavan</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&amp;width=980"></media:content></item><item><title>Intel Upgrades Chip Packaging for Bigger AI</title><link>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&width=1200&height=800&coordinates=116%2C0%2C116%2C0"/><br/><br/><p><span>This week at the <a href="https://ectc.net/" target="_blank">IEEE Electronic Components and Packaging Technology Conference</a>, <a href="https://www.intel.com/" target="_blank">Intel</a> unveiled that it is developing new chip-packaging technology that will allow for bigger processors for AI.</span></p><p>With Moore’s Law slowing down, makers of advanced GPUs and other data-center chips are having to add more silicon area to their products to keep up with the relentless rise of AI’s computing needs. But the maximum size of a single silicon chip is fixed at around 800 square millimeters (with <a href="https://spectrum.ieee.org/tag/cerebras" target="_blank">one exception</a>), so manufacturers have had to turn to <a href="https://spectrum.ieee.org/tag/advanced-packaging" target="_blank">advanced packaging technologies</a> that integrate multiple pieces of silicon in a way that lets them act like a single chip.</p><p>Three of the innovations Intel unveiled at ECTC were aimed at tackling limitations in just how much silicon you can squeeze into a single package and how big that package can be. They include improvements to the technology Intel uses to link adjacent silicon dies together, a more-accurate method for bonding silicon to the package substrate, and a system to expand the size of a critical part of the package that removes heat. Together, the technologies enable the integration of more than 10,000 square millimeters of silicon within a package that can be bigger than 21,000 mm<sup>2</sup>—a massive area about the size of four and a half credit cards.</p><h2>EMIB gets a 3D upgrade</h2><p>One of the limitations on how much silicon can fit in a single package has to do with connecting a large number of silicon dies at their edges. Using an organic polymer package substrate to interconnect the silicon dies is the most affordable option, but a silicon substrate allows you to make more dense connections at these edges.</p><p>Intel’s solution, introduced more than five years ago, is to embed a small sliver of silicon in the organic package beneath the adjoining edges of the silicon dies. That sliver of silicon, called EMIB, is etched with fine interconnects that increase the density of connections beyond what the organic substrate can handle.</p><p>At ECTC, Intel unveiled the latest twist on the EMIB technology, called EMIB-T. In addition to the usual fine horizontal interconnects, EMIB-T provides relatively thick vertical copper connections called through-silicon vias, or TSVs. The TSVs allow power from the circuit board below to directly connect to the chips above instead of having to route around the EMIB, reducing power lost by a longer journey. Additionally, EMIB-T contains a copper grid that acts as a ground plane to reduce noise in the power delivered due to process cores and other circuits suddenly ramping up their workloads.</p><p>“It sounds simple, but this is a technology that brings a lot of capability to us,” says Rahul Manepalli, vice president of substrate packaging technology at Intel. With it and the other technologies Intel described, a customer could connect silicon equivalent to more than 12 full-size silicon dies—10,000 mm<sup>2</sup> of silicon—in a single package using 38 or more EMIB-T bridges.</p><h2>Thermal control</h2><p>Another technology Intel reported at ECTC that helps increase the size of packages is low-thermal-gradient thermal compression bonding. It’s a variant of the technology used today to attach silicon dies to organic substrates. Micrometer-scale bumps of solder are positioned on the substrate where they will connect to a silicon die. The die is then heated and pressed onto the microbumps, melting them and connecting the package’s interconnects to the silicon’s.</p><p>Because the silicon and the substrate expand at different rates when heated, engineers have to limit the inter-bump distance, or pitch. Additionally, the expansion difference makes it difficult to reliably make very large substrates full of lots of silicon dies, which is the direction AI processors need to go.</p><p>The new Intel tech makes the thermal expansion mismatch more predictable and manageable, says Manepalli. The result is that very large substrates can be populated with dies. Alternatively, the same technology can be used to increase the density of connections to EMIB down to about one every 25 micrometers.</p><h2>A flatter heat spreader</h2><p>These bigger silicon assemblages will generate even more heat than today’s systems. So it’s critical that the heat’s pathway out of the silicon isn’t obstructed. An integrated piece of metal called a heat spreader is key to that, but making one big enough for these large packages is difficult. The package substrate can warp, and the metal heat spreader itself might not stay perfectly flat, so it might not touch the tops of the hot dies it’s supposed to be sucking the heat from. Intel’s solution was to assemble the integrated heat spreader in parts rather than as one piece. This allowed the company to add extra stiffening components, among other things, to keep everything flat and in place.</p><p> “Keeping it flat at higher temperatures is a big benefit for reliability and yield,” says Manepalli.</p><p>Intel says the technologies are still in the R&D stage and would not comment on when these technologies would debut commercially. However, they will likely have to arrive in the next few years for the Intel foundry to compete with <a href="https://spectrum.ieee.org/tsmc-advanced-packaging" target="_blank">TSMC’s planned packaging expansion</a>.</p>]]></description><pubDate>Sun, 08 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</guid><category>Chip packaging</category><category>Intel</category><category>Thermal control</category><category>Noise reduction</category><category>Interconnects</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&amp;width=980"></media:content></item><item><title>Video Friday: Hopping on One Robotic Leg</title><link>https://spectrum.ieee.org/video-friday-one-legged-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&width=1200&height=800&coordinates=181%2C0%2C182%2C0"/><br/><br/><p>
<span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span>
</p><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>
	Enjoy today’s videos!
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="fnzdkxl-jj0">
	This single-leg robot is designed to “form a foundation for future bipedal robot development,” but personally, I think it’s perfect as is.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="e263fb0233d0bb0d075d93a40d651be2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/FNzdKXl-jj0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://dynamicrobot.kaist.ac.kr/">KAIST Dynamic Robot Control and Design Lab</a> ]
</p><div class="horizontal-rule">
</div><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="8a6d56b1cad95583679b96d5194dd022" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wzYtsJwYfTM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Selling 17,000 
	<a data-linked-post="2655919083" href="https://spectrum.ieee.org/social-robots-children" target="_blank">social robots</a> still amazes me. <a data-linked-post="2650251656" href="https://spectrum.ieee.org/aldebaran-robotics-seeking-betatesters-for-its-nao-humanoid-robot" target="_blank">Aldebaran</a> will be missed.
</p><p>
	[ 
	<a href="https://aldebaran.com/en/">Aldebaran</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="udti_d_vif0">
	Nice to see some actual challenging shoves as part of biped testing.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="397e23922e40f8dda09c9558813c3604" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UdtI_D_vIF0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.ucr.bot/">Under Control Robotics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="j5cfeee5pyi">
<em>Ground Control made multilegged waves at IEEE’s International Conference on Robotics and Automation 2025 in Atlanta! We competed in the Startup Pitch Competition and demoed our robot at our booth, on NIST standard terrain, and around the convention. We were proud to be a finalist for Best Expo Demo and participate in the Robot Parade.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="f805b79697328de135747f04c5a7dac1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/J5cfeEe5pyI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://groundcontrolrobotics.com/">Ground Control Robotics</a> ]
</p><p>
	Thanks, Dan!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="agrtswo4snw">
<em>Humanoid is a U.K.-based robotics innovation company dedicated to building commercially scalable, reliable and safe robotic solutions for real-world applications.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c6f2dda46adfe06e68b0b4b335ec3291" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AgrTSWO4Snw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	It’s a nifty bootup screen, I’ll give them that.
</p><p>
	[ 
	<a href="https://thehumanoid.ai/product/">Humanoid</a> ]
</p><p>
	Thanks, Kristina!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="plm9gaq1jxo">
<em>Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they remain limited in performing object interactions that require sustained contact. In this work, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting mechanisms to maintain stability.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="5209d97768c506bd070b00ce7aa8e8b2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/pLm9gaQ1JXo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://linchangyi1.github.io/LocoTouch/">LocoTouch paper</a> ]
</p><p>
	Thanks, Changyi!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="2lg-4mdx210">
<em>In this video, Digit is performing tasks autonomously using a whole-body controller for mobile manipulation. This new controller was trained in simulation, enabling Digit to execute tasks while navigating new environments and manipulating objects it has never encountered before.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="4d2772b70353c22ada366d8040940a1a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2lG-4mdx210?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Not bad, although it’s worth pointing out that those shelves are not representative of any market I’ve ever been to.
</p><p>
	[ 
	<a href="https://www.agilityrobotics.com/">Agility Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="xwmwmhrt-fs">
	It’s always cool to see robots presented as an incidental solution to a problem as opposed to, you know, robots.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="686a7d77fbda850290710efc6140a527" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xWmWmhRt-fs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	The question that you really want answered, though, is “Why is there water on the floor?”
</p><p>
	[ 
	<a href="https://bostondynamics.com/products/orbit/">Boston Dynamics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="gqidyj-akaa">
<em>Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. We propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="daa86fab0c3d3f61cc1ab142a8056ca3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/gQidYj-AKaA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://la.disneyresearch.com/publication/amor-adaptive-character-control-through-multi-objective-reinforcement-learning/">Disney Research</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="igyjdvu2tc0">
	It’s been a week since ICRA 2025, and TRON 1 already misses all the new friends it made!
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="891a8f3fed5dda5103e1a2056cef57e4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iGyJdVu2tc0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.limxdynamics.com/en">LimX Dynamics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hjpps5vcftg">
	ROB 450 in Winter 2025 challenged students to synthesize the knowledge acquired through their Robotics undergraduate courses at the University of Michigan to use a systematic and iterative design and analysis process and apply it to solving a real open-ended Robotics problem.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="cb4df45971f7989fef2eafcf4708c497" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hjPPS5vcFtg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://robotics.umich.edu/">University of Michigan Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hh7fh5ys82q">
	What’s the Trick? A talk on human vs. current robot learning, given by Chris Atkeson at the Robotics and AI Institute.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="ad0b49c258ded8012bd36ea093692f33" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hh7Fh5YS82Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://rai-inst.com/">Robotics and AI Institute (RAI)</a> ]
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Fri, 06 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-one-legged-robot</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><category>Aldebaran robotics</category><category>Reinforcement learning</category><category>Quadruped robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&amp;width=980"></media:content></item><item><title>Getting Past Procastination</title><link>https://spectrum.ieee.org/getting-past-procastination</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p>Across a decade working at hypergrowth tech companies like Meta and Pinterest, I constantly struggled with procrastination. I’d be assigned an important project, but I simply couldn’t get myself to get started. The source of my distraction varied—I would constantly check my email, read random documentation, or even scroll through my social feeds. But the result was the same: I felt a deep sense of dread that I was not making progress on the things that mattered.</p><p>At the end of the day, time is the only resource that matters. With every minute, you are making a decision about how to spend your life. Most of the ways people spend their time are ineffective. Especially in the tech world, our tasks and tools are constantly changing, so we must be able to adapt. What separates the best engineers from the rest of the pack is that they create systems that allow them to be consistently productive.</p><p>Here’s the core idea that changed my perspective on productivity: <strong>Action leads to motivation</strong>, not the other way around. You should not check your email or scroll Instagram while you wait for motivation to “hit you.” Instead, just start doing something, anything, that makes progress toward your goal, and you’ll find that motivation will follow.</p><p>For example, if I have a high-priority, complex bug-fixing challenge at work, my approach is to decompose the problem into something much simpler. <em>Could I simply</em> add a log statement that prints the value of a relevant variable? My goal at this point is not to solve the bug, it’s simply to take a tiny step forward.</p><p>This creates a powerful flywheel: you’re productive → you feel good → you’re more productive.</p><p>Unfortunately, many engineers are stuck in the opposite flywheel, a downward spiral of procrastination: you’re unproductive → you feel bad → you’re unproductive.</p><p>The idea that motivation follows naturally from progress lets us lower the activation energy needed to enter the upward spiral. Author and motivational speaker Tony Robbins talks about a related concept that “motion creates emotion.” The actions we take, and even the way we move our body, affect how we feel. Once you realize that you can control your motivation, you can achieve stress-free productivity.</p><p>—Rahul</p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank">Overcoming Tech Workforce Shortages With IEEE Microcredentials</a></h2><p><span>A shortage of technical workers is coming. Currently, most of these roles require university degrees, but specialized training through focused, skills-based microcredential courses could provide an alternative and expand the workforce. IEEE’s microcredentials program offers credentials that focus on the skills needed to become a technician, electrician, or programmer, regardless of educational background.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="How Software Engineers Actually Use AI">How Software Engineers Actually Use AI</a></h2><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here."></a><span>Amidst conflicting accounts of how programmers use AI on the job, Wired surveyed 730 coders to get more clarity—then used ChatGPT to comb through the data, with plenty of help from human editors and fact-checkers. The survey asked coders how much they use AI, their outlook on the technology, and how it has changed their jobs, among other questions.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Profile: A Knee Injury Launched This VR Pioneer’s Career">Profile: A Knee Injury Launched This VR Pioneer’s Career</a></h2><p>Unlike many engineers, Carolina Cruz-Neira had little interest in technology as a child. Instead, she dreamed of becoming a professional ballerina. But when an injury forced her to pivot, Cruz-Neira found success in computer science, eventually blending her interests in art and science as a pioneer in virtual reality. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p>]]></description><pubDate>Thu, 05 Jun 2025 19:32:12 +0000</pubDate><guid>https://spectrum.ieee.org/getting-past-procastination</guid><category>Career development</category><category>Careers</category><category>Practical strategies</category><category>Tech careers</category><category>Careers newsletter</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>IEEE Honors Engineering Visionaries at Annual Summit</title><link>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&width=1200&height=800&coordinates=172%2C0%2C172%2C0"/><br/><br/><p>I attended this year’s <a href="https://corporate-awards.ieee.org/event/vic-summit-honors-ceremony-gala/" rel="noopener noreferrer" target="_blank">IEEE Vision, Innovation, and Challenges Summit and Honors Ceremony</a> on 23 and 24 April at the <a href="https://www.hilton.com/en/hotels/tyotohi-hilton-tokyo-odaiba/" rel="noopener noreferrer" target="_blank">Hilton Tokyo Odaiba</a> hotel. The event celebrates <a href="https://spectrum.ieee.org/ieee-celebrates-engineering-brilliance" target="_self">pioneers</a> in engineering who have developed technology that changes the way people connect and learn about the world. This year’s celebrants included the engineers behind the <a href="https://spectrum.ieee.org/henry-samueli-1999" target="_self">first digital cable set-top box modem chipset</a> and the <a href="https://science.nasa.gov/mission/webb/" rel="noopener noreferrer" target="_blank">James Webb Space Telescope</a>.</p><p>The event included the inaugural <a href="https://corporate-awards.ieee.org/yp_laureate_forum/" rel="noopener noreferrer" target="_blank">IEEE Distinguished Young Professionals and Laureate Forum</a>. Fifty young professionals attended the networking event with IEEE leaders, <a href="https://corporate-awards.ieee.org/recipients/ieee-medal-of-honor-recipients/" rel="noopener noreferrer" target="_blank">IEEE Medal of Honor</a> laureates, and award recipients.</p><p>Here are highlights of the <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-vic-summit-full-stream" rel="noopener noreferrer" target="_blank">sessions</a>, which are available to watch in full on <a href="https://ieeetv.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE.tv</a>.</p><h2>Networking opportunities for young professionals</h2><p>Before the VIC summit got underway on 23 April, the networking forum took place that morning. </p><p>In her speech welcoming the attendees, <a href="https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead" target="_self">Sophia Muirhead</a>, IEEE executive director and chief operating officer, encouraged the young professionals to engage in IEEE’s mission of developing technology for the benefit of humanity.</p><p>The participants heard from 2020 IEEE President <a href="https://ethw.org/Toshio_Fukuda" rel="noopener noreferrer" target="_blank">Toshio Fukuda</a> and award recipient <a href="https://sg.linkedin.com/in/aishbandla" rel="noopener noreferrer" target="_blank">Aishwarya Bandla</a> about their careers and volunteer work. Bandla received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-theodore-w-hissey-outstanding-young-professional-award/" rel="noopener noreferrer" target="_blank">IEEE Theodore W. Hissey Young Professionals Award</a> for her “leadership in patient-centric health technology innovation and inspiring IEEE young professionals to drive meaningful change.” The award is sponsored by the <a href="https://ieeephotonics.org/" rel="noopener noreferrer" target="_blank">IEEE Photonics</a> and <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power & Energy</a> societies, as well as <a href="https://yp.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Young Professionals</a>.</p><p>She is an IEEE senior member and the clinical innovation manager at <a href="https://paxmanscalpcooling.com/" rel="noopener noreferrer" target="_blank">Paxman</a>, a medical equipment manufacturer headquartered in Huddersfield, England. She is developing a wearable device that cools a person’s limbs. The Paxman limb “cryocompression” system helps prevent nerve damage associated with certain types of intravenous chemotherapy.</p><p>As someone who follows the Japanese concept of <em><em>ikigai</em></em>—a sense of purpose—Bandla said her “passion and profession intersected not at technology in the lab but at bringing technology to the people.”</p><p>She shows similar passion in her role as chair of <a href="https://yp.ieeer10.org/" rel="noopener noreferrer" target="_blank">IEEE Region 10’s Young Professionals group</a>. Encouraging attendees to become active in the organization, she said IEEE has given her a purpose and the opportunity to give back to the community.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A large group of business professionals posing together for a photo. A screen in the background reads, \u201cIEEE Vision and Innovation Challenges Summit\u201d." class="rm-shortcode" data-rm-shortcode-id="393c0426f1fd5e1752af1c9775d7caf9" data-rm-shortcode-name="rebelmouse-image" id="13537" loading="lazy" src="https://spectrum.ieee.org/media-library/a-large-group-of-business-professionals-posing-together-for-a-photo-a-screen-in-the-background-reads-u201cieee-vision-and-inn.jpg?id=60451480&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Fifty yong professionals attended the inaugural IEEE Distinguished Young Professionals and Laureate Forum.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Junko Kimura; Tomohiro Ohsumi</small></p><p>Attendees were surprised by a guest speaker whose name is well known outside of the technology and engineering fields: <a href="https://woz.org/" target="_blank">Steve Wozniak</a>. </p><p>The summit’s emcee <a href="https://www.miho-japanesevo.com/" rel="noopener noreferrer" target="_blank">Miho Noguchi</a> interviewed the IEEE Fellow and <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> cofounder about what inspired him to pursue a career in engineering and launch a startup. Noguchi, a former Japanese radio broadcaster, is the voice for <a href="https://maps.google.com/" rel="noopener noreferrer" target="_blank">Google Maps</a> navigation in Japanese.</p><p>Wozniak said he was inspired by his father, who was an engineer at <a href="https://www.lockheedmartin.com/en-us/index.html" rel="noopener noreferrer" target="_blank">Lockheed Martin</a> in Sunnyvale, Calif.</p><p>“I visited the company several times and watched him work and started asking him what engineers did,” he said. His father told him engineers build things that make life easier for people.</p><p>When asked what career advice he would give to young professionals, Wozniak said, “Always be a good person. Even if someone is bad to you, always be good to them.”</p><p>Reflecting on the benefits of IEEE membership, he said being an IEEE Fellow is the most important honor he has received.</p><p>The forum concluded with a networking opportunity. Each table was given a set of questions to break the ice. Attendees paired up and were given 10 minutes to ask each other about their schooling, work experiences, and career aspirations. When the time was up, they switched partners.</p><p>Several young professionals I interviewed about their experience said they enjoyed the event. One said she really liked learning about where everyone came from, their work, and their passions. All said they were in awe that they got the opportunity to see and hear Wozniak. </p><h2>Governing AI and a telescope’s foray into the past</h2><p>The summit featured a “fireside chat” with <a href="https://spectrum.ieee.org/henry-samueli-moh" target="_self">Henry Samueli</a>, this year’s <a href="https://spectrum.ieee.org/ieee-medal-of-honor-2025" target="_self">IEEE Medal of Honor</a> recipient for his “pioneering research and commercialization of <a href="https://spectrum.ieee.org/tag/broadband" target="_self">broadband</a> communication and networking technologies” and his promotion of STEM education. He is the first recipient of the award since its <a href="https://spectrum.ieee.org/ieee-medal-of-honor" target="_self">monetary prize was increased to US $2 million from $50,000</a>.</p><p><a href="https://spectrum.ieee.org/u/glenn-zorpette" target="_self">Glenn Zorpette</a>, <a href="https://spectrum.ieee.org/navigating-the-dual-use-dilemma" target="_self"><em><em>IEEE Spectrum</em></em></a>’s editorial director for content development, interviewed Samueli, who reminisced about working in his parents’ liquor/grocery store in Los Angeles as a teenager, where he stocked shelves, operated the cash register, and helped out with the bookkeeping. He told Zorpette that his parents inspired him to become an entrepreneur and that a hands-on project in a seventh-grade shop class prompted him to become an engineer.</p><p>Samueli helped to found <a href="https://spectrum.ieee.org/tag/broadcom" target="_self">Broadcom</a> in 1991 in San Jose, Calif. The company developed the first digital cable set-top box modem chipset, which served as the cable signal receiver. Today he is chairman of the company’s board.</p><p>When asked about the future of broadband, he said the application of existing technology is more important than its advancement. He added that he’s excited to see what the future will bring.</p><p>An audience member asked him what <a href="https://spectrum.ieee.org/henry-samueli-advice" target="_self">advice he would give</a> to engineers in developing countries.</p><p>“Take it one step at a time and let [your career] unfold how it is meant to,” he said.</p><p>The conversation was followed by keynote speeches and panel discussions with award recipients on topics including <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">artificial intelligence</a> and <a href="https://spectrum.ieee.org/topic/aerospace/" target="_self">space exploration</a>. </p><p class="pull-quote">“[The IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies] represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”<strong> </strong><span><strong>— Frederick A. Kish Jr</strong></span></p><p><span></span>During a presentation on artificial intelligence, <a href="https://unu.edu/about/staff/tshilidzi-marwala" target="_blank">Tshilidzi Marwala</a>, rector of the <a href="https://unu.edu/" target="_blank">United Nations University</a> in Tokyo, led a deep dive into how lawmakers can create policies to govern AI use.</p><p>While AI is already being used by <a href="https://google.com/" target="_blank">Google</a>, <a href="https://www.microsoft.com/" target="_blank">Microsoft</a>, and <a href="https://twitter.com/" target="_blank">X</a>, as well as students and professionals in different fields, Marwala noted there are still many concerns surrounding the technology, especially when it comes to safety and information accuracy.</p><p>He stressed the importance of international collaboration, and he called for lawmakers to involve technologists when creating policy.</p><p>AI is complicated, he pointed out and “needs consistency when it comes to writing rules for its use.”</p><p>AI might be the future, but innovations such as the <a href="https://spectrum.ieee.org/rogue-planet" target="_self">James Webb Space Telescope</a> (JWST) are helping people understand the past.</p><p>The telescope, which <a href="https://spectrum.ieee.org/at-last-first-light-for-the-james-webb-space-telescope" target="_self">gathers images of stars and galaxies created soon after the big bang</a>, took 20 years to develop and build. Its development was led by <a href="https://corporate-awards.ieee.org/speaker/bill-ochs/" rel="noopener noreferrer" target="_blank">Bill Ochs</a>, <a href="https://science.nasa.gov/people/webb-people-mike-menzel/" rel="noopener noreferrer" target="_blank">Mike Menzel</a>, and <a href="https://corporate-awards.ieee.org/speaker/scott-willoughby/" rel="noopener noreferrer" target="_blank">Scott Willoughby</a> at NASA’s <a href="https://www.nasa.gov/goddard/" rel="noopener noreferrer" target="_blank">Goddard Space Flight Center</a> in Greenbelt, Md. For their work, they received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-simon-ramo-medal/" rel="noopener noreferrer" target="_blank">IEEE Simon Ramo Medal</a>, sponsored by <a href="https://www.northropgrumman.com/" rel="noopener noreferrer" target="_blank">Northrop Grumman</a>.</p><p>Ochs, who was a project manager at the flight center during its development, is now the principal engineer at <a href="https://www.fts-intl.com/" rel="noopener noreferrer" target="_blank">FTS International</a>, in Chantilly, Va. Menzel is the telescope’s mission systems engineer, and Willoughby is vice president and project manager at the flight center.</p><p>In a panel session moderated by Noguchi, the three talked about the challenges they faced during the JWST spacecraft’s development and launch. </p><p>One hurdle was the inability to test the telescope’s flight capabilities before launch, Ochs said. The telescope was built to orbit the sun, and it isn’t possible to simulate that environment on Earth. Therefore, Ochs said, the team completed tests and analyses on the telescope’s components and systems to mitigate potential risks.</p><p>The Webb telescope, which launched in 2022, is still collecting data.</p><p>The three engineers also shared advice about what it takes to be a project manager. Take one big problem and break it down into several small problems you can solve, Willoughby said. He added that managers need to communicate with the entire team and “get empirical, fast.”</p><h2>A royal visitor and honoring innovators</h2><p>This year’s <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-honors-ceremony-full-stream" rel="noopener noreferrer" target="_blank">IEEE Honors Ceremony</a>, held on the evening of 24 April, recognized people who spearheaded innovations in areas including solid-state circuits, wireless communication, and broadband technology.</p><p>The opening speaker was <a href="https://en.wikipedia.org/wiki/Hisako,_Princess_Takamado" rel="noopener noreferrer" target="_blank">Hisako, Princess Takamodo</a> of Japan. “It is a great honor that Japan has been allowed to host this premier event,” she said.</p><p>This was the first time the ceremony was held in Asia.</p><p>“I stand here in total awe of how far the human brain has come in the past century,” the princess said.</p><p>LEDs play an important role in sustainability, as they reduce energy consumption and can be recycled, unlike incandescent lighting. The technology wouldn’t have been possible without photonic integrated circuits developed by <a href="https://ethw.org/Frederick_A._Kish,_Jr." rel="noopener noreferrer" target="_blank">Frederick A. Kish Jr</a>.</p><p>Kish, an IEEE Fellow, also advanced telecommunications technology by creating and integrating a full optical system for transmissions onto a single chip, reducing manufacturing costs and enabling significantly higher bandwidths and faster data transfer speeds. For his innovations, he received the <a href="https://corporate-awards.ieee.org/award/ieee-nick-holonyak-medal/" rel="noopener noreferrer" target="_blank">IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies</a>. The award is sponsored by Friends of Nick Holonyak Jr. The <a href="https://spectrum.ieee.org/red-hot" target="_self">2003 Medal of Honor recipient</a> invented the first practical visible-spectrum LED.</p><p>Kish thanked his former colleagues at <a href="https://www.google.com/aclk?sa=l&ai=DChcSEwick7_e1rWNAxVWSEcBHWS4F0EYABABGgJxdQ&co=1&ase=2&gclid=CjwKCAjw87XBBhBIEiwAxP3_AzMlWVi0er_7aNExyr7nFlrhOlrskfS4AAq6S-PdnD-37OFIvyXQThoCbmcQAvD_BwE&category=acrcp_v1_5&sig=AOD64_2_79wIKcMbzCyXgqxASx3pyYEDMg&q&nis=4&adurl&ved=2ahUKEwj8p7je1rWNAxXHF1kFHQIJDMAQ0Qx6BAgmEAE" rel="noopener noreferrer" target="_blank">Agilent Technologies</a>,<a href="https://www.hpe.com/us/en/home.html" rel="noopener noreferrer" target="_blank"> Hewlett Packard</a>, the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Urbana-Champaign</a>, and other organizations.</p><p>“We’ve worked together to leave the world brighter, greener, and more connected,” he said. “This medal represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”</p><p>Honoring work that helps connect the world continued with the presentation of the <a href="https://spectrum.ieee.org/mildred-dresselhaus-the-queen-of-carbon-science-has-ieee-medal-named-in-her-honor" target="_self">IEEE Mildred Dresselhaus Medal</a> to IEEE Fellow <a href="https://spectrum.ieee.org/princeton-dean-andrea-goldsmith" target="_self">Andrea Goldsmith</a>, who received the award for “contributions to and leadership in wireless communications theory and practice.” The award is sponsored by <a href="https://about.google/" rel="noopener noreferrer" target="_blank">Google</a>.</p><p>While at <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford</a>, Goldsmith developed foundational mathematical approaches for increasing the capacity, speed, and range of wireless systems. She helped found two communications startups: <a href="https://www.linkedin.com/company/quantenna-communications" rel="noopener noreferrer" target="_blank">Quantenna Communications</a> of San Jose, Calif., and <a href="https://www.plume.com/" rel="noopener noreferrer" target="_blank">Plume Design</a> of Palo Alto, Calif. This year, the current dean of engineering and applied sciences at <a href="https://www.princeton.edu/" rel="noopener noreferrer" target="_blank">Princeton</a> was appointed president of <a href="https://www.stonybrook.edu/" rel="noopener noreferrer" target="_blank">Stony Brook University</a>, in New York. She is set to start her new position on 1 August.</p><p>“Mildred Dresselhaus was a pioneer in the days when there were very few women in science and technology,” Goldsmith said. “She was a role model and an early champion of diversity, ensuring the best and the brightest could enter the field and thrive within it. Her contributions to science and engineering are unparalleled, and receiving an award named for her is deeply meaningful to me.”</p><p>The ceremony concluded with the presentation of the IEEE Medal of Honor to Samueli, who received a standing ovation.</p><p>At the end of his speech, he announced that he was giving his $2 million prize to <a href="https://hkn.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE–Eta Kappa Nu</a>, the organization’s honors society.</p><p>“I was made an eminent member of IEEE-HKN in 2019, and [the <a href="https://www.samueli.org/" rel="noopener noreferrer" target="_blank">Samueli Foundation</a>] has supported [the society] for years,” he said. “It is truly an honor for me to endow such a wonderful student organization.”</p>]]></description><pubDate>Wed, 04 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</guid><category>Broadband</category><category>Ieee awards</category><category>Ieee medal of honor</category><category>Ieee news</category><category>Semiconductors</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&amp;width=980"></media:content></item><item><title>Look for These 7 New Technologies at the Airport</title><link>https://spectrum.ieee.org/7-new-airport-tech</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&width=1200&height=800&coordinates=7%2C0%2C8%2C0"/><br/><br/><p><strong>Take a look around</strong> the airport during your travels this summer and you might spot a string of new technologies at every touchpoint: from pre-arrival, bag drop, and security to the moment you board the plane.</p><p>In this new world, your face is your boarding pass, your electronic luggage tag transforms itself for each new flight, and gate scanners catch line cutters trying to sneak onto the plane early.</p><p>It isn’t the future—it’s now. Each of the technologies to follow is in use at airports around the world today, transforming your journey-before-the-journey.</p><h2>Virtual queuing speeds up airport security</h2><p>As you pack the night before your trip, you ponder the age-old travel question: What time should I get to the airport? The right answer requires predicting the length of the security line. But at some airports, you no longer have to guess; in fact, you don’t have to wait in line at all.</p><p>Instead, you can book ahead and choose a specific time for your security screening—so you can arrive right before your reserved slot, confident that you’ll be whisked to the front of the line, thanks to <a href="https://copenhagenoptimization.com/" rel="noopener noreferrer" target="_blank">Copenhagen Optimization</a>’s Virtual Queuing system.</p><p>Copenhagen Optimization’s machine learning models use linear regression, heuristic models, and other techniques to forecast the volume of passenger arrivals based on historical data. The system is integrated with airport programs to access flight schedules and passenger-flow data from boarding-pass scans, and it also takes in data from lidar sensors and cameras at security checkpoints, X-ray luggage scanners, and other areas.</p><p>If a given day’s passenger volume ends up differing from historical projections, the platform can use real-time data from these inputs to adjust the Virtual Queuing time slots—and recommend that the airport make changes to security staffing and the number of open lanes. The Virtual Queuing system is constantly adjusting to flatten the passenger arrival curve, tactically redistributing demand across time slots to optimize resources and reduce congestion.</p><p>While this system is doing the most, you as a passenger can do the least. Just book a time slot on your airport’s website or app, and get some extra sleep knowing you’ll waltz right up to the security check tomorrow morning.</p><h2>Electronic bag tags</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Line drawing of a woman lifting suitcase at airport baggage check-in with barcode in focus." class="rm-shortcode" data-rm-shortcode-id="64ff97b084fbc93dd936889921e516d7" data-rm-shortcode-name="rebelmouse-image" id="f8bea" loading="lazy" src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-lifting-suitcase-at-airport-baggage-check-in-with-barcode-in-focus.png?id=60389664&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Checking a bag? Here’s another step you can take care of before you arrive: Skip the old-school paper tags and generate your own electronic <a href="https://bagtag.com/" rel="noopener noreferrer" target="_blank">Bagtag</a>. This e-ink device (costing about US $80, or €70) looks like a traditional luggage-tag holder, but it can generate a new, paperless tag for each one of your flights.</p><p>You provide your booking details through your airline’s app or the Bagtag app, and the Bagtag system then uses application programming interfaces and secure data protocols to retrieve the necessary information from the airline’s system: your name, flight details, the baggage you’re allowed, and the unique barcode that identifies your bag. The app uses this data to generate a digital tag. Hold your phone near your Bagtag, and it will transmit the encrypted tag data via Bluetooth or NFC. Simultaneously, your phone’s NFC antenna powers the battery-free Bagtag device.</p><p>On the Bagtag itself, a low-power microcontroller decrypts the tag data and displays the digital tag on the e-ink screen. Once you’re at the airport, the tag can be scanned at the airline’s self-service bag drop or desk, just like a traditional paper tag. The device also contains an RFID chip that’s compatible with the luggage-tracking systems that some airlines are using, allowing your bag to be identified and tracked—even if it takes a different journey than you do. When you arrive at the airport, just drop that checked bag and make your way to the security area.</p><h2>Biometric boarding passes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman using kiosk for facial recognition ID verification." class="rm-shortcode" data-rm-shortcode-id="af8a923d85c7eac7ca6873db756cc3fb" data-rm-shortcode-name="rebelmouse-image" id="3dfdf" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-using-kiosk-for-facial-recognition-id-verification.png?id=60389955&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Over at security, you’ll need your boarding pass and ID. Compared with the old days of printing a physical slip from a kiosk, digital QR code boarding passes are quite handy—but what if you didn’t need anything besides your face? That’s the premise of <a href="https://www.idemia.com/" rel="noopener noreferrer" target="_blank">Idemia Public Security</a>’s biometric boarding-pass technology.</p><p>Instead of waiting in a queue for a security agent, you’ll approach a self-service kiosk or check-in point and insert your government-issued identification document, such as a driver’s license or passport. The system uses visible light, infrared, and ultraviolet imaging to analyze the document’s embedded security features and verify its authenticity. Then, computer-vision algorithms locate and extract the image of your face on the ID for identity verification.</p><p>Next, it’s time for your close-up. High-resolution cameras within the system capture a live image of your face using 3D and infrared imaging. The system’s antispoofing technology prevents people from trying to trick the system with items like photos, videos, or masks. The technology compares your live image to the one extracted from your ID using facial-recognition algorithms. Each image is then converted into a compact biometric template—a mathematical representation of your facial features—and a similarity score is generated to confirm a match.</p><p>Finally, the system checks your travel information against secure flight databases to make sure the ticket is valid and that you’re authorized to fly that day. Assuming all checks out, you’re cleared to head to the body scanners—with no biometric data retained by Idemia Public Security’s system.</p><h2>X-rays that can tell ecstasy from eczema meds </h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of an X-ray machine scanning luggage with schematic view of interior components above." class="rm-shortcode" data-rm-shortcode-id="0ff27fb1769e9930b81f30bde1d86244" data-rm-shortcode-name="rebelmouse-image" id="9c471" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-an-x-ray-machine-scanning-luggage-with-schematic-view-of-interior-components-above.png?id=60389973&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>While you pass through your security screening, that luggage you checked is undergoing its own screening—with a major new upgrade that can tell exactly what’s inside.</p><p>Traditional scanners use one or a few X-ray sources and work by transmission, measuring the attenuation of the beam as it passes through the bag. These systems create a 2D “shadow” image based on differences in the amount and type of the materials inside. More recently, these systems have begun using <a href="https://spectrum.ieee.org/invention-of-ct-scanner" target="_blank">computed tomography</a> to scan the bag from all directions and to reconstruct 3D images of the objects inside. But even with CT, harmless objects may look similar to dangerous materials—which can lead to false positives and also require security staff to visually inspect the X-ray images or even bust open your luggage.</p><p>By contrast, <a href="https://www.smithsdetection.com/" rel="noopener noreferrer" target="_blank">Smiths Detection</a>’s new <a href="https://spectrum.ieee.org/future-baggage-scanners-will-tell-us-what-things-are-made-of" target="_blank">X-ray diffraction</a> machines measure the molecular structure of the items inside your bag to identify the exact materials—no human review required.</p><p>The machine uses a multifocus X-ray tube to quickly scan a bag from various angles, measuring the way the radiation diffracts while switching the position of the focal spots every few microseconds. Then, it analyzes the diffraction patterns to determine the crystal structure and molecular composition of the objects inside the bag—building a “fingerprint” of each material that can much more finely differentiate threats, like explosives and drugs, from benign items.</p><p>The system’s algorithms process this diffraction data and build a 3D spatial image, which allows real-time automated screening without the need for manual visual inspection by a human. After your bag passes through the X-ray diffraction machine without incident, it’s loaded into the cargo hold. Meanwhile, you’ve passed through your own scan at security and are ready to head toward your gate.</p><h2>Airport shops with no cashiers or checkout lanes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt='Illustration of a woman entering a store with a "Just Walk Out" shopping system.' class="rm-shortcode" data-rm-shortcode-id="995f20a20ef07fc6697d2cac5737b9c1" data-rm-shortcode-name="rebelmouse-image" id="8a4c0" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-entering-a-store-with-a-just-walk-out-shopping-system.png?id=60390007&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>While meandering over to your gate from security, you decide you could use a little pick-me-up. Just down the corridor is a convenience store with snacks, drinks, and other treats—but no cashiers. It’s a contactless shop that uses <a href="https://www.justwalkout.com/" rel="noopener noreferrer" target="_blank">Just Walk Out</a> technology by Amazon.</p><p>As you enter the store with the tap of a credit card or mobile wallet, a scanner reads the card and assigns you a unique session identifier that will let the Just Walk Out system link your actions in the store to your payment. Overhead cameras track you by the top of your head, not your face, as you move through the store.</p><p>The Just Walk Out system uses a deep-learning model to follow your movements and detect when you interact with items. In most cases, computer vision can identify a product you pick up simply based on the video feed, but sometimes weight sensors embedded in the shelves provide additional data to determine what you removed. The video and weight data are encoded as tokens, and a neural network processes those tokens in a way similar to how large language models encode text—determining the result of your actions to create a “virtual cart.”</p><p>While you shop, the system continuously updates this cart: adding a can of soda when you pick it up, swapping one brand of gum for another if you change your mind, or removing that bag of chips if you put it back on the shelf. Once your shopping is complete, you can indeed just walk out with your soda and gum. The items you take will make up your finalized virtual cart, and the credit card you entered the store with will be charged as usual. (You can look up a receipt, if you want.) With provisions procured, it’s onward to the gate.</p><h2>Airport-cleaning robots</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman watching an automated floor cleaning robot cleaning up a spilled drink in the airport." class="rm-shortcode" data-rm-shortcode-id="910995e84aefefbcacb0afaefa6e6a37" data-rm-shortcode-name="rebelmouse-image" id="a8ced" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-watching-an-automated-floor-cleaning-robot-cleaning-up-a-spilled-drink-in-the-airport.png?id=60390051&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>As you amble toward the gate with your luggage and snacks, you promptly spill that soda you just bought. Cleanup in Terminal C! Along comes <a href="https://avidbots.com/" rel="noopener noreferrer" target="_blank">Avidbots’ Neo</a>, a fully autonomous floor-scrubbing robot designed to clean commercial spaces like airports with minimal human intervention.</p><p>When a Neo is first delivered to the airport, the robot performs a comprehensive scan of the various areas it will be cleaning using lidar and 3D depth cameras. Avidbots software processes the data to create a detailed map of the environment, including walls and other obstacles, and this serves as the foundation for Neo’s cleaning plans and navigation.</p><p>Neo’s human overlords can use a touchscreen on the robot to direct it to the area that needs cleaning—either as part of scheduled upkeep, or when someone (ahem) spills their soda. The robot springs into action, and as it moves, it continuously locates itself within its map and plans its movements using data from wheel encoders, inertial measurement units, and a gyroscope. Neo also updates its map and adjusts its path in real time by using the lidar and depth cameras to detect any changes from its initial mapping, such as a translocated trash can or perambulating passengers.</p><p>Then comes the scrubbing. Neo’s software plans the optimal path for cleaning a given area at this moment in time, adjusting the robot’s speed and steering as it moves along. A water-delivery system pumps and controls the flow of cleaning solution to the motorized brushes, whose speed and pressure can also be adjusted based on the surface the robot is cleaning. A powerful vacuum system collects the dirty water, and a flexible squeegee prevents slippery floors from being left behind.</p><p>While the robot’s various sensors and planning algorithms continuously detect and avoid obstacles, any physical contact with the robot’s bumpers triggers an emergency stop. And if Neo finds itself in a situation it’s just not sure how to handle, the robot will stop and call for assistance from a human operator, who can review sensor data and camera feeds remotely to help it along.</p><h2>“Wrong group” plane-boarding alarm</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman waiting in line at boarding gate E6, with notification bell icon above." class="rm-shortcode" data-rm-shortcode-id="2b01cd5c1c6b0ed8dd2962347946988a" data-rm-shortcode-name="rebelmouse-image" id="bdce3" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-waiting-in-line-at-boarding-gate-e6-with-notification-bell-icon-above.png?id=60390066&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Your airport journey is coming to an end, and your real journey is about to begin. As you wait at the gate, you notice a fair number of your fellow passengers hovering to board even before the agent has made any announcements. And when boarding does begin, a surprising number of people hop in line. <em><em>Could all these people really be in boarding groups 1 and 2?</em></em> you wonder.</p><p>If they’re not…they’ll get called out. American Airlines’ new boarding technology stops those pesky passengers who try to join the wrong boarding group and sneak onto the plane early.</p><p>If one such passenger approaches the gate before their assigned group has been called, scanning their boarding pass will trigger an audible alert—notifying the airline crew, and everyone else for that matter. The passengers will be politely asked to wait to board. As they slink back into line, try not to look too smug. After all, it’s been a remarkably easy, tech-assisted journey through the airport today. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Wed, 04 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/7-new-airport-tech</guid><category>Airlines</category><category>Facial recognition</category><category>Robot cleaner</category><category>Airports</category><category>X-ray diffraction</category><dc:creator>Julianne Pepitone</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&amp;width=980"></media:content></item><item><title>Nvidia’s Blackwell Conquers Largest LLM Training Benchmark</title><link>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&width=1200&height=800&coordinates=0%2C137%2C0%2C137"/><br/><br/><p>
<span>For those who enjoy rooting for the underdog, the latest MLPerf benchmark results will disappoint: Nvidia’s GPUs have dominated the competition </span><a href="https://spectrum.ieee.org/mlperf-nvidia-conquers" target="_self">yet</a><span> </span><a href="https://spectrum.ieee.org/ai-training-2669810566" target="_self">again</a><span>. This includes chart-topping performance on the latest and most demanding benchmark, pretraining the Llama 3.1 403B large language model. That said, the computers built around the newest AMD GPU, MI325X, matched the performance of Nvidia’s H200, </span><a href="https://spectrum.ieee.org/nvidia-blackwell" target="_self">Blackwell’s</a><span> predecessor, on the most popular LLM fine-tuning benchmark. This suggests that AMD is one generation behind Nvidia.</span>
</p><p>
<a href="https://mlcommons.org/benchmarks/" target="_blank">MLPerf</a> training is one of the machine learning competitions run by the <a href="https://mlcommons.org/" rel="noopener noreferrer" target="_blank">MLCommons</a> consortium. “AI performance sometimes can be sort of the Wild West. MLPerf seeks to bring order to that chaos,” says <a href="https://www.linkedin.com/in/davesalvator/" rel="noopener noreferrer" target="_blank">Dave Salvator</a>, director of accelerated computing products at Nvidia. “This is not an easy task.”
</p><p>
	The competition consists of six benchmarks, each probing a different industry-relevant machine learning task. The benchmarks are content recommendation, large language model pretraining, large language model fine-tuning, object detection for machine vision applications, image generation, and graph node classification for applications such as fraud detection and drug discovery.
</p><p>
	The large language model pretraining task is the most resource intensive, and this round it was updated to be even more so. The term “pretraining” is somewhat misleading—it might give the impression that it’s followed by a phase called “training.” It’s not. Pretraining is where most of the number crunching happens, and what follows is usually fine-tuning, which refines the model for specific tasks.
</p><p>
	In previous iterations, the pretraining was done on the GPT3 model. This iteration, it was replaced by Meta’s Llama 3.1 403B, which is more than twice the size of GPT3 and uses a four times larger context window. The context window is how much input text the model can process at once. This larger benchmark represents the industry trend for ever larger models, as well as including some architectural updates.
</p><h2>Blackwell Tops the Charts, AMD on Its Tail </h2><p>
	For all six benchmarks, the fastest training time was on Nvidia’s Blackwell GPUs. Nvidia itself submitted to every benchmark (other companies also submitted using various computers built around Nvidia GPUs). Nvidia’s Salvator emphasized that this is the first deployment of Blackwell GPUs at scale and that this performance is only likely to improve. “We’re still fairly early in the Blackwell development life cycle,” he says.
</p><p>
	This is the first time AMD has submitted to the training benchmark, although in previous years other companies have submitted using computers that included AMD GPUs. In the most popular benchmark, LLM fine-tuning, AMD demonstrated that its latest Instinct MI325X GPU performed on par with Nvidia’s H200s. Additionally, the Instinct MI325X showed a 30 percent improvement over its predecessor, the <a href="https://spectrum.ieee.org/amd-mi300" target="_blank">Instinct MI300X</a>. (The main difference between the two is that MI325X comes with 30 percent more high-bandwidth memory than MI300X.)</p><p>For it’s part, Google submitted to a single benchmark, the image-generation task, with its <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus" target="_blank">Trillium TPU</a>.
</p><div class="flourish-embed flourish-scatter" data-src="visualisation/23471261?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="scatter visualization" src="https://public.flourish.studio/visualisation/23471261/thumbnail" width="100%"/></noscript></div><h2>The Importance of Networking</h2><p>
	Of all submissions to the LLM fine-tuning benchmarks, the system with the largest number of GPUs was submitted by Nvidia, a computer connecting 512 B200s. At this scale, networking between GPUs starts to play a significant role. Ideally, adding more than one GPU would divide the time to train by the number of GPUs. In reality, it is always less efficient than that, as some of the time is lost to communication. Minimizing that loss is key to efficiently training the largest models.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23550378?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23550378/thumbnail" width="100%"/></noscript></div><p>
	This becomes even more significant on the pretraining benchmark, where the smallest submission used 512 GPUs, and the largest used 8,192. For this new benchmark, the performance scaling with more GPUs was notably close to linear, achieving 90 percent of the ideal performance.
</p><p>
	Nvidia’s Salvator attributes this to the NVL72, an efficient package that connects 36 Grace CPUs and 72 Blackwell GPUs with <a href="https://www.nvidia.com/en-us/data-center/nvlink/" rel="noopener noreferrer" target="_blank">NVLink</a>, to form a system that “acts as a single, massive GPU,” the <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank">datasheet</a> claims. Multiple NVL72s were then connected with <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self">InfiniBand</a> network technology.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23508278?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23508278/thumbnail" width="100%"/></noscript></div><p>
	Notably, the largest submission for this round of MLPerf—at 8192 GPUs—is not the largest ever, despite the increased demands of the pretraining benchmark. Previous rounds saw submissions with over 10,000 GPUs. <a href="https://www.linkedin.com/in/kennethleach/" target="_blank">Kenneth Leach</a>, principal AI and machine learning engineer at Hewlett Packard Enterprise, attributes the reduction to improvements in GPUs, as well as networking between them. “Previously, we needed 16 server nodes [to pretrain LLMs], but today we’re able to do it with 4. I think that’s one reason we’re not seeing so many huge systems, because we’re getting a lot of efficient scaling.”
</p><p>
	One way to avoid the losses associated with networking is to put many AI accelerators on the same huge wafer, as done by <a href="https://spectrum.ieee.org/cerebrass-giant-chip-will-smash-deep-learnings-speed-barrier" target="_self">Cerebras</a>, which recently claimed to <a href="https://www.businesswire.com/news/home/20250528123694/en/Cerebras-Beats-NVIDIA-Blackwell-in-Llama-4-Maverick-Inference" rel="noopener noreferrer" target="_blank">beat</a> Nvidia’s Blackwell GPUs by more than a factor of two on inference tasks. However, that result was measured by <a href="https://artificialanalysis.ai/" rel="noopener noreferrer" target="_blank">Artificial Analysis</a>, which queries different providers without controlling how the workload is executed. So its not an apples-to-apples comparison in the way the MLPerf benchmark ensures.
</p><h2>A Paucity of Power</h2><p>
	The MLPerf benchmark also includes a power test, measuring how much power is consumed to achieve each training task. This round, only a single submitter—Lenovo—included a power measurement in its submission, making it impossible to make comparisons across performers. The energy it took to fine-tune an LLM on two Blackwell GPUs was 6.11 gigajoules, or 1,698 kilowatt-hours, or roughly the energy it would take to heat a small home for a winter. With growing <a href="https://spectrum.ieee.org/ai-energy-consumption" target="_self">concerns</a> about AI’s energy use, the power efficiency of training is crucial, and this author is perhaps not alone in hoping more companies submit these results in future rounds.
</p>]]></description><pubDate>Wed, 04 Jun 2025 15:59:50 +0000</pubDate><guid>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</guid><category>Mlperf</category><category>Nvidia</category><category>Amd</category><category>Networking</category><dc:creator>Dina Genkina</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&amp;width=980"></media:content></item><item><title>The Birth of the University as Innovation Incubator</title><link>https://spectrum.ieee.org/university-as-innovation-incubator</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-bearded-white-man-lying-on-the-floor-with-video-game-consoles-and-a-1970s-tv.jpg?id=60341590&width=1200&height=800&coordinates=0%2C188%2C0%2C188"/><br/><br/><div class="intro-text"><em>This article is excerpted from</em> <a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" target="_blank">Every American an Innovator: How Innovation Became a Way of Life</a><em>, by Matthew Wisnioski (The MIT Press, 2025).</em></div><p class="drop-caps"><strong>Imagine a point-to-point </strong>transportation service in which two parties communicate at a distance. A passenger in need of a ride contacts the service via phone. A complex algorithm based on time, distance, and volume informs both passenger and driver of the journey’s cost before it begins. This novel business plan promises efficient service and lower costs. It has the potential to disrupt an overregulated taxi monopoly in cities across the country. Its enhanced transparency may even reduce racial discrimination by preestablishing pickups regardless of race.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" target="_blank"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" style="background: gray;" title="Select for lightbox">aspect_ratio</a><img alt="Book cover with illustration of people engaged in various activities.   " class="rm-shortcode" data-rm-shortcode-id="a2bb1759ca4d53a4cb4aa8bacec42ccc" data-rm-shortcode-name="rebelmouse-image" id="c431e" loading="lazy" src="https://spectrum.ieee.org/media-library/book-cover-with-illustration-of-people-engaged-in-various-activities.jpg?id=60341772&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption..."><a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" target="_blank">Every American an Innovator: How Innovation Became a Way of Life</a>, by Matthew Wisnioski.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The MIT Press</small></p><p>Sounds like Uber, but it’s not. Prototyped in 1975, this automated taxi-dispatch system was the brainchild of mechanical engineer Dwight Baumann and his students at <a href="https://www.cmu.edu/" rel="noopener noreferrer" target="_blank">Carnegie Mellon University</a>. The dial-a-ride service was designed to resurrect a defunct cab company that had once served Pittsburgh’s African American neighborhoods.</p><p>The ride service was one of 11 entrepreneurial ventures supported by the university’s Center for Entrepreneurial Development. Funded by a million-dollar grant from the <a href="https://www.nsf.gov/" rel="noopener noreferrer" target="_blank">National Science Foundation</a>, the CED was envisioned as an innovation “hatchery,” intended to challenge the norms of research science and higher education, foster risk-taking, birth campus startups focused on market-based technological solutions to social problems, and remake American science to serve national needs.</p><p>Today, university incubators like the CED are commonplace. Whether they’re seeking to nurture the next Uber, or social ventures like the dial-a-ride service, they all aim to transform ideas into businesses, discoveries into applications, classroom assignments into revenue, and faculty and students into entrepreneurs. Indeed, the idea that universities are engines of innovation is so ingrained that we take it for granted that it was always the case. So it’s instructive to look back to the time when the first innovation incubators were themselves being incubated.</p><h2>Are innovators born or made?</h2><p>During the Cold War, the model for training scientists and engineers in the United States was one of manpower in service to a linear model of innovation: Scientists pursued “basic” discovery in universities and federal laboratories; engineer–scientists conducted “applied” research elsewhere on campus; engineers developed those ideas in giant teams for companies such as Lockheed and Boeing; and research managers oversaw the whole process. This model dictated national science policy, elevated the <a href="https://spectrum.ieee.org/the-essential-vannevar-bush/as-we-may-think" target="_self">scientist as a national hero</a> in pursuit of truth beyond politics, and pumped hundreds of millions of dollars into higher education. In practice, the lines between basic and applied research were blurred, but the perceived hierarchy was integral to the NSF and the university research culture that it helped to foster.</p><p>In the late 1960s, this postwar system of academic science and engineering appeared to be breaking down. Science and technology were seen as root causes of environmental destruction, the Vietnam War, job losses, and racial and economic inequality. A similar reckoning was taking place around national science policy, with critics on the left attacking the complicity of scientists in the military-industrial complex and those on the right assailing the wastefulness of ivory-tower spending on science.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/innovation-magazine-and-the-birth-of-a-buzzword" target="_self">Innovation Magazine and the Birth of a Buzzword</a></p><p>In this moment of revolt, innovation experts in Washington, D.C., and the booming technology regions of California and Massachusetts began to promote innovators as the people who would bring about change, because they were different from the established leaders of American science. Eventually, a wide range of constituents—bureaucrats, inventors, academics, business leaders, and engineers—came to identify innovators as agents of national progress, and they concluded that these innovators could indeed be taught in the nation’s universities.</p><p>The question was, how? And would the universities be willing to remake themselves to support innovation?</p><p>And so it fell to the NSF to develop successful models for producing these risk-taking sociotechnologists.</p><h2>The NSF experiments with innovation</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" rel="float: left;" style="float: left;"> <img alt="Black and white photo of two young men in 1970s casual clothing, one holding a circuit diagram." class="rm-shortcode" data-rm-shortcode-id="83dce65d75b331d5c4364e652815dba5" data-rm-shortcode-name="rebelmouse-image" id="a452a" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-two-young-men-in-1970s-casual-clothing-one-holding-a-circuit-diagram.jpg?id=60341592&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">At the Utah Innovation Center, engineering students John DeJong and Douglas Kihm worked on a programmable electronics breadboard.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Special Collections, J. Willard Marriott Library, The University of Utah</small></p><p>In 1972, NSF director <a href="https://ethw.org/Oral-History:H._Guyford_Stever" target="_blank">H. Guyford Stever</a> established the <a href="https://scholarworks.uni.edu/cgi/viewcontent.cgi?article=2550&context=istj" target="_blank">Office of Experimental R&D Incentives</a> to “incentivize” innovation for national needs by supporting research on “how the government [could] most effectively accelerate the transfer of new technology into productive enterprise.” Stever stressed the experimental nature of the program because many in the NSF and the scientific community resisted the idea of goal-directed research. Innovation, with its connotations of profit and social change, was even more suspect.</p><p>To lead the initiative, Stever appointed C.B. Smith, a research manager at United Aircraft Corp., who in turn brought in engineers with industrial experience, including Robert Colton, an automotive engineer. Colton led the university Innovation Center experiment that gave rise to Carnegie Mellon’s CED.</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>The NSF chose four universities that captured a range of approaches to innovation incubation. MIT targeted undergrads through formal coursework and an innovation “co-op” that assisted in turning ideas into products. The University of Oregon evaluated the ideas of garage inventors from across the country. The University of Utah emphasized an ecosystem of biotech and <a href="https://spectrum.ieee.org/sketchpad" target="_blank">computer graphics</a> startups coming out of its research labs. And Carnegie Mellon established a nonprofit corporation to support graduate student ventures, including the dial-a-ride service.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Black and white photo of a young man in 1970s clothing seated at a table with a telephone and radio and holding a digital device." class="rm-shortcode" data-rm-shortcode-id="128c785470ffd6b3bde84fb4d7424e90" data-rm-shortcode-name="rebelmouse-image" id="72485" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-young-man-in-1970s-clothing-seated-at-a-table-with-a-telephone-and-radio-and-holding-a-digital-device.jpg?id=60341597&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Grad student Fritz Faulhaber holds one of the radio-coupled taxi meters that Carnegie Mellon students installed in Pittsburgh cabs in the 1970s.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ralph Guggenheim;Jerome McCavitt/Carnegie-Mellon Alumni News</small></p><h2>Carnegie Mellon got one of the first university incubators</h2><p>Carnegie Mellon had all the components that experts believed were necessary for innovation: strong engineering, a world-class business school, novel approaches to urban planning with a focus on community needs, and a tradition of industrial design and the practical arts. CMU leaders claimed that the school was smaller, younger, more interdisciplinary, and more agile than MIT.</p><p>The main reason that CMU received an NSF Innovation Center, however, was its director, <a href="https://www.cmu.edu/swartz-center-for-entrepreneurship/assets/dwight-m.b-baumann/article-in-honor-and-memory-of-professor-dwight-baumann.pdf" target="_blank">Dwight Baumann</a>. Baumann exemplified a new kind of educator-entrepreneur. The son of North Dakota farmers, he had graduated from North Dakota State University, then headed to MIT for a Ph.D. in mechanical engineering, where he discovered a love of teaching. He also garnered a reputation as an unusually creative engineer with an interest in solving problems that addressed human needs. In the 1950s and 1960s, first as a student and then as an MIT professor, Baumann helped develop one of the first computer-aided-design programs, as well as computer interfaces for the blind and the nation’s first dial-a-ride paratransit system.</p><p>But Baumann was frustrated with MIT’s culture of defense research and engineering science, and so he left his tenured position in 1970 to join CMU and continue his work on transportation systems. There, he chartered the NSF-funded CED as a nonprofit. He purchased the bankrupt Peoples Cab Co. for a dollar, convinced the university to let him use a former parking garage as an incubator space, and worked across colleges to establish a master’s program in engineering design.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Black and white photo of a white man in a suit in front of a blackboard talking to students and leaning on a slide projector." class="rm-shortcode" data-rm-shortcode-id="92961fa4c92ac7b71730756430a42457" data-rm-shortcode-name="rebelmouse-image" id="3fcf2" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-white-man-in-a-suit-in-front-of-a-blackboard-talking-to-students-and-leaning-on-a-slide-projector.jpg?id=60341736&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Dwight Baumann, director of Carnegie Mellon’s Center for Entrepreneurial Development, believed that a modern university should provide entrepreneurial education.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Carnegie Mellon University Archives</small></p><p>Baumann’s goal was to establish entrepreneurship education as a core function of a modern technological university. He wasn’t especially concerned with making money, and he cared little for nationalist rhetoric about global competition. Rather, his professed goal was to unlock human creativity in a “studio without walls, an association of people, loosely related, who communicate with each other and can get help when they need it.” Technological innovation, he argued, could never be entirely predictable because it was a project, rather than an act of scientific discovery. “A project,” he wrote, “is something that hasn’t yet happened. And the instructors and students have the common goal of seeing how it’ll turn out.”</p><p>The CED’s mission was to support entrepreneurs in the earliest stages of the innovation process when they needed space and seed funding. It created an environment for students to make a “sequence of nonfatal mistakes,” so they could fail and develop self-confidence for navigating the risks and uncertainties of entrepreneurial life. It targeted graduate students who already had advanced scientific and engineering training and a viable idea for a business.</p><p>In its first five years, the center launched 11 ventures. In addition to the reboot of the Peoples Cab Company, projects included a blood oximeter, a computer-hardware company, and a newspaper-printing technique. Many of these endeavors failed. Founders had health problems, patent disputes arose, and competitors claimed that the CED’s ventures had an unfair advantage through the weight of CMU.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Black and white photo of a Black man leaning out the driver\u2019s side of a 1950s taxi with \u201cPeoples Cab\u201d printed on the side. " class="rm-shortcode" data-rm-shortcode-id="961806d3e494d864b538f2af59212d18" data-rm-shortcode-name="rebelmouse-image" id="9d61a" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-black-man-leaning-out-the-driver-u2019s-side-of-a-1950s-taxi-with-u201cpeoples-cab-u201d-printed-on.jpg?id=60341570&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Carnegie Mellon’s dial-a-ride service replicated the Peoples Cab Co., which had provided taxi service to Black communities in Pittsburgh.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Charles “Teenie” Harris/Carnegie Museum of Art/Getty Images</small></p><p>The CED distilled these lessons in brochures and public seminars, while faculty incorporated them into new classes. A 10-point “readiness assessment” emphasized personal reflection before any technology or market evaluation. The first rule: “Only if you have sincerely made the decision within yourself to invest time and effort, and understand that sacrifice and risk are inevitable, should you consider the life of an entrepreneur.” It aimed to show that innovation was a difficult path that could result in “personal dissatisfaction” and that one’s “family goals” must not be sacrificed in single-minded pursuit of an entrepreneurial opportunity.</p><p>A few CED students did create successful startups. The breakout hit was Compuguard, founded by electrical engineering Ph.D. students <a href="https://www.forbes.com/profile/romesh-t-wadhwani/" target="_blank">Romesh Wadhwani </a>and Krishnahadi Pribad, who hailed from India and Indonesia, respectively. The pair spent 18 months developing a security bracelet that used wireless signals to protect vulnerable people in dangerous work environments. But after failing to convert their prototype into a working design, they pivoted to a security- and energy-monitoring system for schools, prisons, and warehouses.</p><p>With CED assistance, Compuguard secured government contracts and millions in venture capital and grew to over 100 employees. Its first major client was the Los Angeles city school district. The two founders sold the company for what was then the largest ever return on investment by a minority-run enterprise. Wadhwani became a serial entrepreneur and is now one of Silicon Valley’s leading billionaire philanthropists. His <a href="https://wadhwanifoundation.org/" target="_blank">Wadhwani Foundation</a> supports innovation and entrepreneurship education worldwide, particularly in emerging economies.</p><p>When NSF funding for the CED ran out in 1978, a series of long-simmering tensions erupted. At the heart of most of them was the cult of personality around Baumann, whose slapdash style conflicted with CMU’s desire to compete with new technology entrepreneurship programs at the University of Pennsylvania’s <a href="https://www.wharton.upenn.edu/" target="_blank">Wharton School</a> and elsewhere. In 1983, Baumann’s onetime partner Jack Thorne took the lead of the new Enterprise Corp., which aimed to help Pittsburgh’s entrepreneurs raise venture capital. Baumann was kicked out of his garage to make room for the initiative.</p><p>Baumann moved the CED to an abandoned YMCA building and attempted, with limited results, to help unemployed skilled laborers become innovators. The center faded, as CMU’s faculty continued to fight over the proper role of university innovation and who had the authority to teach it.</p><h2>Was the NSF’s experiment in innovation a success?</h2><p>As the university Innovation Center experiment wrapped up in the late 1970s, the NSF patted itself on the back in a series of reports, conferences, and articles. “The ultimate effect of the Innovation Centers,” it stated, would be “the regrowth of invention, innovation, and entrepreneurship in the American economic system.” The NSF claimed that the experiment produced dozens of new ventures with US $20 million in gross revenue, employed nearly 800 people, and yielded $4 million in tax revenue. Yet, by 1979, license returns from intellectual property had generated only $100,000.</p><p>The Innovation Centers garnered intense national and international interest. Established business schools in the United States created competing technology-innovation tracks. Visiting contingents from Canada, Sweden, and the United Kingdom hoped to re-create it.</p><p class="pull-quote">Today, the legacies of the NSF experiment are visible on nearly every college campus.</p><p>Critics included Senator <a href="https://en.wikipedia.org/wiki/William_Proxmire" target="_blank">William Proxmire</a> of Wisconsin, who pointed to the banana peelers, video games, and sports equipment pursued in the centers to lambast them as “wasteful federal spending” of “questionable benefit to the American taxpayer.”</p><p>African American chemist Grant Venerable faulted the program for its narrow conception of innovation as the purview of white men at elite universities. If supposed innovators could not address gender and racial equity “by more than a token nod,” he wrote, “they are guilty of being part of the problem.”</p><p>And so the impacts of the NSF’s Innovation Center experiment weren’t immediately obvious. Many faculty and administrators of that era were still apt to view such programs as frivolous, nonacademic, or not worth the investment.</p><p>Today, though, the legacies of the NSF experiment are visible on nearly every college campus. It institutionalized the scientific innovator-entrepreneur as a risk-taker who understood the probabilities of capital just as well as thermodynamics. And it established that the purpose of innovation education wasn’t just about breeding winners. All students, even those who never intended to commercialize their ideas or launch a startup, would benefit from learning to be entrepreneurial. And so the NSF’s experiment created another path by which innovation, a concept that prior to World War II barely registered as a cultural touchstone, became ingrained in our institutions, our educational system, and our beliefs about ourselves. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Wed, 04 Jun 2025 15:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/university-as-innovation-incubator</guid><category>Innovation</category><category>Entrepreneurs</category><category>Carnegie mellon university</category><category>National science foundation</category><category>History of engineering</category><category>Incubator</category><dc:creator>Matthew Wisnioski</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-bearded-white-man-lying-on-the-floor-with-video-game-consoles-and-a-1970s-tv.jpg?id=60341590&amp;width=980"></media:content></item><item><title>Who Gives a S#!t About Cursing Robots?</title><link>https://spectrum.ieee.org/cursing-social-robot-interaction</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-a-robot-tripping-on-a-banana-peel-its-head-is-covered-by-a-speech-bubble-and-symbols-representing-an-expleti.jpg?id=60333045&width=1200&height=800&coordinates=0%2C166%2C0%2C167"/><br/><br/><p>
	The robots that share our public spaces today are so demure. Social robots and service robots aim to avoid offense, erring toward polite airs, positive emotions, and obedience. In some ways, this makes sense—would you really want to have a yelling match with a delivery robot in a hotel? Probably not, even if you’re in New York City and trying to absorb the local culture.
</p><p>
	In other ways, this passive social robot design aligns with paternalistic standards that link assistance to subservience. Thoughtlessly following such outdated social norms in robot design may be ill-advised, since it <a href="https://pubmed.ncbi.nlm.nih.gov/37123285/" rel="noopener noreferrer" target="_blank">can help to reinforce outdated or harmful ideas</a> such as restricting people’s rights and reflecting only the needs of majority-identity users.
</p><p>
	In <a href="https://osusharelab.com/" rel="noopener noreferrer" target="_blank">my robotics lab at Oregon State University</a>, <a href="https://spectrum.ieee.org/how-high-fives-help-us-get-in-touch-with-robots" target="_blank">we work with</a> <a href="https://spectrum.ieee.org/whats-the-deal-with-robot-comedy" target="_blank">a playful spirit</a> and enjoy challenging the problematic norms that are entrenched within “polite” interactions and social roles. So we decided to experiment with robots that use foul language around humans. After all, many people are using foul language more than ever in 2025. Why not let robots have a chance, too?
</p><h3>Why and How to Study Cursing Robots</h3><p>
	Societal standards in the United States suggest that cursing robots would likely rub people the wrong way in most contexts, as swearing has a predominantly negative connotation. Although some past research shows that cursing <a href="https://www.researchgate.net/publication/238326248_Swearing_at_work_and_permissive_leadership_culture_When_anti-social_becomes_social_and_incivility_is_acceptable" rel="noopener noreferrer" target="_blank">can enhance team cohesion</a> and <a href="https://hrcak.srce.hr/file/159883" rel="noopener noreferrer" target="_blank">elicit humor</a>, certain members of society (such as women) are often expected to <a href="https://link.springer.com/article/10.1023/A:1022986429748" rel="noopener noreferrer" target="_blank">avoid risking offense</a> through profanity. We wondered whether cursing robots would be viewed negatively, or if they might perhaps offer benefits in certain situations.
</p><p>
	We decided to study cursing robots in the context of responding to mistakes. Past work in human-robot interaction has already shown that <a href="https://ieeexplore.ieee.org/abstract/document/5453195" rel="noopener noreferrer" target="_blank">responding to error</a> (rather than ignoring it) can help robots be perceived more positively in human-populated spaces, especially in the case of personal and service robots. And one <a href="https://par.nsf.gov/biblio/10284325-perceived-agency-social-norm-violating-robot" rel="noopener noreferrer" target="_blank">study</a> found that compared to other faux pas, foul language is more forgivable in a robot.
</p><p>
	With this past work in mind, we generated videos with three common types of robot failure: bumping into a table, dropping an object, and failing to grasp an object. We crossed these situations with three types of responses from the robot: no verbal reaction, a non-expletive verbal declaration, and an expletive verbal declaration. We then asked people to rate the robots on things like competence, discomfort, and likability, using standard scales in an online survey.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="27aa73d6ea081bc41fc24b217317e021" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hYdN5zLa07Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">What If Robots Cursed? These Videos Helped Us Learn How People Feel about Profane Robots</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: Naomi Fitter</small></p><h3>What People Thought of Our Cursing Robots</h3><p>
	On the whole, we were surprised by how acceptable swearing seemed to the study participants, especially within an initial group of Oregon State University students, but even among the general public as well. Cursing had no negative impact, and even some positive impacts, among the college students after we removed one religiously connotated curse (god***it), which seemed to be received in a stronger negative way than other cuss words.
</p><p>
	In fact, university participants rated swearing robots as the <a href="https://sparqtools.org/mobility-measure/inclusion-of-other-in-the-self-ios-scale/" rel="noopener noreferrer" target="_blank">most socially close</a> and most humorous, and rated non-expletive and expletive robot reactions equivalent on social warmth, competence, discomfort, anthropomorphism, and likability scales. The general public judged non-profane and profane robots as equivalent on most scales, although expletive reactions were deemed most discomforting and non-expletive responses seemed most likable. We believe that the university students were slightly more accepting of cursing robots because of the campus’s progressive culture, where cursing is considered a peccadillo.
</p><p class="ieee-inbody-related">Related: <a href="https://spectrum.ieee.org/whats-the-deal-with-robot-comedy" target="_blank">What’s the Deal With Robot Comedy?</a></p><p>
	Since experiments run solely in an online setting do not always represent real-life interactions well, we also conducted a final replication study in person with a robot that made errors while distributing goodie bags to campus community members at Oregon State, which reinforced our prior results.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="10b0ac5703e47214353a0b0843085bf0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DhHhh4yni1I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Humans React to a Cursing Robot in the Wild<a href="https://www.youtube.com/@naomi_fitter" rel="noopener noreferrer" target="_blank"></a></small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: Naomi Fitter</small></p><p>
	We have submitted this work, which represents a well-designed series of empirical experiments with interesting results and replications along the way, to several different journals and conferences. Despite consistently enthusiastic reviewer comments, no editors have yet accepted our work for publication—it seems to be the type of paper that editors are nervous to touch. Currently, the work is under review for a fourth time, for possible inclusion in the 2025 IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), in a paper titled “<a href="https://arxiv.org/abs/2505.05831" rel="noopener noreferrer" target="_blank">Oh F**k! How Do People Feel About Robots That Leverage Profanity?</a>”
</p><h3>Give Cursing Robots a Chance </h3><p>
	Based on our results, we think cursing robots deserve a chance! Our findings show that swearing robots would typically have little downside and some upside, especially in open-minded spaces such as university campuses. Even for the general public, reactions to errors with profanity yielded much less distaste than we expected. Our data showed that people cared more about whether robots acknowledged their error at all than whether or not they swore.
</p><p>
	People do have some reservations about cursing robots, especially when it comes to comfort and likability, so thoughtfulness may be required to apply curse words at the right time. For example, just as humans do, robots should likely hold back their swear words around children and be more careful in settings that typically demand cleaner language. Robot practitioners might also consider surveying individual users about profanity acceptance as they set up new technology in personal settings—rather than letting robotic systems learn the hard way, perhaps alienating users in the process.
</p><p>
	As more robots enter our day-to-day spaces, they are bound to make mistakes. How they react to these errors is important. Fundamentally, our work shows that people prefer robots that notice when a mistake has occurred and react to this error in a relatable way. And it seems that a range of styles in the response itself, from the profane to the mundane, can work well. So we invite designers to give cursing robots a chance!
</p>]]></description><pubDate>Tue, 03 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/cursing-social-robot-interaction</guid><category>Social robots</category><category>Human robot interaction</category><dc:creator>Naomi Fitter</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-a-robot-tripping-on-a-banana-peel-its-head-is-covered-by-a-speech-bubble-and-symbols-representing-an-expleti.jpg?id=60333045&amp;width=980"></media:content></item><item><title>Disaster Awaits if We Don’t Secure IoT Now</title><link>https://spectrum.ieee.org/iot-security-root-of-trust</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-robotic-arms-attached-to-an-assembly-line-equipped-with-internet-of-things-technology.jpg?id=60339966&width=1200&height=800&coordinates=0%2C129%2C0%2C130"/><br/><br/><p>In 2015, Ukraine experienced a slew of unexpected <a href="https://www.cisa.gov/news-events/ics-alerts/ir-alert-h-16-056-01" rel="noopener noreferrer" target="_blank">power outages</a>. Much of the country went dark. The U.S. investigation has concluded that this was due to a Russian state cyberattack on Ukrainian computers running critical infrastructure.</p><p>In the decade that followed, <a data-linked-post="2658630634" href="https://spectrum.ieee.org/cyberattacks" target="_blank">cyberattacks</a> on critical infrastructure and near misses <a href="https://www.rand.org/pubs/commentary/2024/02/threats-to-americas-critical-infrastructure-are-now-a-terrifying-reality.html" rel="noopener noreferrer" target="_blank">continued</a>. In 2017, a <a href="https://www.kcur.org/news/2022-03-24/federal-officials-say-russian-spies-tried-to-hack-into-a-kansas-nuclear-power-plant" rel="noopener noreferrer" target="_blank">nuclear power plant</a> in Kansas was the subject of a Russian cyberattack. In 2021, Chinese state actors reportedly gained access to parts of the New York City subway computer system. Later in 2021, a <a href="https://www.cnn.com/2021/06/02/business/beef-hack-jbs/index.html" rel="noopener noreferrer" target="_blank">cyberattack</a> temporarily closed down beef processing plants. In 2023, Microsoft reported a cyberattack on its <a href="https://www.microsoft.com/en-us/security/blog/2023/05/24/volt-typhoon-targets-us-critical-infrastructure-with-living-off-the-land-techniques/" rel="noopener noreferrer" target="_blank">IT systems</a>, likely by Chinese-backed actors.</p><p>The risk is growing, particularly when it comes to <a data-linked-post="2659970239" href="https://spectrum.ieee.org/iot-for-arson-forensics" target="_blank">Internet of things</a> (IoT) devices. Just below the veneer of popular fad gadgets (does anyone <em><em>really</em></em> want their refrigerator to automatically place orders for groceries?) is an increasing army of more prosaic Internet-connected devices that take care of keeping our world running. This is particularly true of a subclass called <a data-linked-post="2650277653" href="https://spectrum.ieee.org/the-industrial-internet-of-things" target="_blank">Industrial Internet of Things</a> (IIoT), devices that implement our communication networks, or control infrastructure such as power grids or chemical plants. IIoT devices can be small devices like valves or sensors, but also can include very substantial pieces of gear, such as an HVAC system, an MRI machine, a dual-use aerial drone, an elevator, a nuclear centrifuge, or a jet engine. </p><p>The number of current IoT devices is growing rapidly. In 2019, there were an <a href="https://explodingtopics.com/blog/iot-stats" rel="noopener noreferrer" target="_blank">estimated</a> 10 billion IoT devices in operation. At the end of 2024, it had almost doubled to <a href="https://iot-analytics.com/number-connected-iot-devices/" rel="noopener noreferrer" target="_blank">approximately</a> 19 billion. This number is set to more than double again by 2030. Cyberattacks aimed at those devices, motivated either by political or financial gain, can cause very real physical-world damage to entire communities, far beyond damage to the device itself.</p><p>Security for IoT devices is often an afterthought, as they often have little need for a “human interface” (i.e., maybe a valve in a chemical plant only needs commands to Open, Close, and Report), and usually they don’t contain information that would be viewed as sensitive (for example, thermostats don’t need credit cards, a medical device doesn’t have a Social Security number). What could go wrong?</p><p>Of course, “what could go wrong” depends on the device, but especially with carefully planned, at-scale attacks, it’s already been shown that a lot can go wrong. For example, armies of poorly secured, Internet-connected security cameras have<a href="https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/" rel="noopener noreferrer" target="_blank"> already</a> been put to use in coordinated distributed-denial-of-service attacks, where each camera makes a few harmless requests of some victim service, causing the service to collapse under the load.</p><h2>How to Secure IoT Devices</h2><p>Measures to defend these devices generally fall into two categories: basic cybersecurity hygiene and defense in depth.</p><p>Cybersecurity hygiene consists of a few rules: Don’t use default passwords on admin accounts, apply software updates regularly to remove newly discovered vulnerabilities, require cryptographic signatures to validate updates, and understand your “<a href="https://datatracker.ietf.org/group/scitt/about/" rel="noopener noreferrer" target="_blank">software supply chain</a>:” where your software comes from, where the supplier obtains components that it may simply be passing through from open-source projects.</p><p>The rapid profusion of open-source software has prompted development of the U.S. Government’s Software Bill of Materials (<a href="https://www.cisa.gov/sbom" rel="noopener noreferrer" target="_blank">SBOM</a>). This is a document that conveys supply-chain provenance, indicating which version of what packages went into making the product’s software. Both IIoT device suppliers and device users benefit from accurate SBOMs, shortening the path to determining if a specific device’s software may contain a version of a package vulnerable to attack. If the SBOM shows an up-to-date package version where the vulnerability has been addressed, both the IIoT vendor and user can breathe easy; if the package version listed in the SBOM is vulnerable, remediation may be in order.</p><p>Defense in depth is less well-known, and deserves more attention.</p><p>It is tempting to implement the easiest approach to cybersecurity, a “hard and crunchy on the outside, soft and chewy inside” model. This emphasizes perimeter defense, on the theory that if hackers can’t get in, they can’t do damage. But even the smallest IoT devices may have a software stack that’s too complex for the designers to fully comprehend, usually leading to obscure vulnerabilities in dark corners of the code. As soon as these vulnerabilities become known, the device transitions from tight, well-managed security to no security, as there’s no second line of defense.</p><p>Defense in depth is the answer. A National Institute of Standards and Technology <a href="https://csrc.nist.gov/pubs/sp/800/193/final" rel="noopener noreferrer" target="_blank">publication</a> breaks down this approach to cyber-resilience into three basic functions: <em><em>protect</em></em>, meaning use cybersecurity engineering to keep hackers out; <em><em>detect</em></em>, meaning add mechanisms to detect unexpected intrusions; and <em><em>remediate</em></em>, meaning take action to expel intruders to prevent subsequent damage. We will explore each of these in turn.</p><h2>Protect</h2><p>Systems that are designed for security use a layered approach, with most of the device’s “normal behavior” in an outer layer, while inner layers form a series of shells, each of which has smaller, more constrained functionality, making the inner shells progressively simpler to defend. These layers are often related to the sequence of steps followed during the initialization of the device, where the device starts in the inner layer with the smallest possible functionality, with just enough to get the next stage running, and so on until the outer layer is functional. </p><p>To ensure correct operation, each layer must also perform an integrity check on the next layer before starting it. In each ring, the current layer computes a fingerprint or signature of the next layer out.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Concentric circles with labels: hardware root of trust (if present), firmware, operating system loader, operating system kernel, application software. " class="rm-shortcode" data-rm-shortcode-id="a2918f8a33c7d367e9d5745220e28a3c" data-rm-shortcode-name="rebelmouse-image" id="a1c56" loading="lazy" src="https://spectrum.ieee.org/media-library/concentric-circles-with-labels-hardware-root-of-trust-if-present-firmware-operating-system-loader-operating-system-kernel.png?id=60339975&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">To make a defensible IoT device, the software needs to be layered, with each layer running only if the previous layer has deemed it safe. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Guy Fedorkow, Mark Montgomery </small></p><p>But there’s a puzzle here. Each layer is checking the next one before starting it, but who checks the first one? No one! The inner layer, whether the first checker is implemented in hardware or firmware, must be implicitly trusted for the rest of the system to be worthy of trust. As such, it’s called a Root of Trust (RoT). </p><p>Roots of Trust must be carefully protected, because a compromise of the Root of Trust may be impossible to detect without specialized test hardware. One approach is to put the firmware that implements the Root of Trust into read-only memory that can’t be modified once the device is manufactured. That’s great if you know your RoT code doesn’t have any bugs, and uses algorithms that can’t go obsolete. But few of us live in that world, so, at a minimum, we usually must protect the RoT code with some simple hardware that makes the firmware read-only after it’s done its job, but writable during its startup phase, allowing for carefully vetted, cryptographically signed updates. </p><p>Newer processor chips move this Root of Trust one step back into the processor chip itself, a hardware Root of Trust. This makes the RoT much more resistant to firmware vulnerabilities or a hardware-based attack, because firmware boot code is usually stored in nonvolatile flash memory where it can be reprogrammed by the system manufacturer (and also by hackers). An RoT inside the processor can be made much more difficult to hack.</p><h2>Detect</h2><p>Having a reliable Root of Trust, we can arrange so each layer is able to check the next for hacks. This process can be augmented with <a href="https://datatracker.ietf.org/doc/rfc9683/" rel="noopener noreferrer" target="_blank">Remote Attestation</a>, where we collect and report the fingerprints (called <em><em>attestation evidence</em></em>) gathered by each layer during the startup process. We can’t just ask the outer application layer if it’s been hacked; of course, any good hacker would ensure the answer is “No Way! You can trust me!”, no matter what.</p><p>But remote attestation adds a small bit of hardware, such as the <a href="https://trustedcomputinggroup.org/new-tcg-guidance-simplifies-creating-cyber-resilient-devices/" rel="noopener noreferrer" target="_blank">Trusted Platform Module</a> (TPM) defined by the Trusted Computing Group. This bit of hardware collects evidence in shielded locations made of special-purpose, hardware-isolated memory cells that can’t be directly changed by the processor at all. The TPM also provides protected capability, which ensures that new information can be added to the shielded locations, but previously stored information cannot be changed. And, it provides a protected capability that attaches a cryptographic signature to the contents of the Shielded Location to serve as evidence of the state of the machine, using a key known only to the Root of Trust hardware, called an Attestation Key (AK).</p><p>Given these functions, the application layer has no choice but to accurately report the attestation evidence, as proven by use of the RoT’s AK secret key. Any attempt to tamper with the evidence would invalidate the signature provided by the AK. At a remote location, a verifier can then validate the signature and check that all the fingerprints reported line up with known, trusted, versions of the device’s software. These known-good fingerprints, called endorsements, must come from a trusted source, such as the device manufacturer.  </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A flow chart showing device manufacturer flowing to attester and verifier. " class="rm-shortcode" data-rm-shortcode-id="1ae39c03bcd527b4c6371f010c8e3fe3" data-rm-shortcode-name="rebelmouse-image" id="2f6fb" loading="lazy" src="https://spectrum.ieee.org/media-library/a-flow-chart-showing-device-manufacturer-flowing-to-attester-and-verifier.png?id=60339987&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">To verify that it’s safe to turn on an IoT device, one can use an attestation and verification protocol provided by the Trusted Computing Group. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Guy Fedorkow, Mark Montgomery</small></p><p>In practice, the Root of Trust may contain several separate mechanisms to protect individual functions, such as boot integrity, attestation and device identity, and the device designer is always responsible for assembling the specific components most appropriate for the device, then carefully integrating them, but organizations like Trusted Computing Group offer guidance and specifications for components that can offer considerable help, such as the Trusted Platform Module (TPM) commonly used in many larger computer systems.</p><h2>Remediate</h2><p>Once an anomaly is detected, there are a wide range of actions to remediate. A simple option is power-cycling the device or refreshing its software. However, trusted components inside the devices themselves may help with remediation through the use of authenticated watchdog timers or other approaches that cause the device to reset itself if it can’t demonstrate good health. <a href="https://trustedcomputinggroup.org/work-groups/cyber-resilient-technologies/" rel="noopener noreferrer" target="_blank"> Trusted Computing Group Cyber Resilience</a> provides guidance for these techniques.</p><p>The requirements outlined here have been available and used in specialized high-security applications for some years, and many of the attacks have been known for a decade. In the last few years, Root of Trust implementations have become widely used in <a href="https://techcommunity.microsoft.com/blog/surfaceitpro/study-highlights-critical-role-of-surface-firmware-protection/2245244" rel="noopener noreferrer" target="_blank">some laptop families</a>. But until recently, blocking Root of Trust attacks has been challenging and expensive even for cyberexperts in the IIoT space. Fortunately, many of the silicon vendors that supply the underlying IoT hardware are <a href="https://docs.broadcom.com/doc/53650-PB" rel="noopener noreferrer" target="_blank">now</a><a href="https://docs.broadcom.com/doc/Data-Center-Security-WP100" rel="noopener noreferrer" target="_blank"> including</a><a href="https://www.marvell.com/content/dam/marvell/en/public-collateral/embedded-processors/marvell-infrastructure-processors-octeon-tx2-cn913x-product-brief.pdf" rel="noopener noreferrer" target="_blank"> these</a> high-security<a href="https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/010/boot-guard-technology/" rel="noopener noreferrer" target="_blank"> mechanisms</a> even in the budget-minded embedded chips, and reliable software stacks have evolved to make mechanisms for Root of Trust defense more available to any designer who wants to use it.</p><p>While the IIoT device designer has the responsibility to provide these cybersecurity mechanisms, it’s up to system integrators, who are responsible for the security of an overall service interconnecting IoT devices, to require the features from their suppliers, and to coordinate features inside the device with external resilience and monitoring mechanisms, all to take full advantage of the improved security now more readily available than ever.</p><p>Mind your roots of trust!</p>]]></description><pubDate>Mon, 02 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/iot-security-root-of-trust</guid><category>Cybersecurity</category><category>Internet of things</category><category>Cyberattacks</category><dc:creator>Guy Fedorkow</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-robotic-arms-attached-to-an-assembly-line-equipped-with-internet-of-things-technology.jpg?id=60339966&amp;width=980"></media:content></item><item><title>Startup’s Analog AI Promises Power for PCs</title><link>https://spectrum.ieee.org/analog-ai-chip-architecture</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-five-silver-chips-and-many-black-components.jpg?id=60345071&width=1200&height=800&coordinates=188%2C0%2C188%2C0"/><br/><br/><p><a href="https://ece.princeton.edu/people/naveen-verma" rel="noopener noreferrer" target="_blank">Naveen Verma’</a>s lab at Princeton University is like a museum of all the ways engineers have tried to make AI ultra-efficient by using analog phenomena instead of digital computing. At one bench lies the most energy-efficient magnetic-memory-based neural-network computer ever made. At another you’ll find a resistive-memory-based chip that can compute the largest matrix of numbers of any <a href="https://spectrum.ieee.org/tag/analog-ai" target="_self">analog AI</a> system yet.</p><p>Neither has a commercial future, according to Verma. Less charitably, this part of his lab is a graveyard.</p><p>Analog AI has captured chip architects’ imagination for years. It combines two key concepts that should make machine learning massively less energy intensive. First, it limits the costly movement of bits between memory chips and processors. Second, instead of the 1s and 0s of logic, it uses the physics of the flow of current to efficiently do machine learning’s key computation.</p><p>As attractive as the idea has been, various analog AI schemes have not delivered in a way that could really take a bite out of AI’s stupefying energy appetite. Verma would know. He’s tried them all.</p><p>But when <em>IEEE Spectrum</em> visited a year ago, there was a chip at the back of Verma’s lab that represents some hope for analog AI and for the energy-efficient computing needed to make AI useful and ubiquitous. Instead of calculating with current, the chip sums up charge. It might seem like an inconsequential difference, but it could be the key to overcoming the noise that hinders every other analog AI scheme.</p><p>This week, Verma’s startup <a href="https://www.enchargeai.com/" rel="noopener noreferrer" target="_blank">EnCharge AI</a> unveiled the first chip based on this new architecture, the EN100. The startup claims the chip tackles various AI work with performance per watt up to 20 times better than competing chips. It’s designed into a single processor card that adds 200 trillion operations per second at 8.25 watts, aimed at conserving battery life in AI-capable laptops. On top of that, a 4-chip, 1,000-trillion-operations-per-second card is targeted for AI workstations.</p><h2>Current and Coincidence</h2><p>In machine learning, “it turns out, by dumb luck, the main operation we’re doing is matrix multiplies,” says Verma. That’s basically taking an array of numbers, multiplying it by another array, and adding up the result of all those multiplications. Early on, engineers noticed a coincidence: Two fundamental rules of electrical engineering can do exactly that operation. Ohm’s Law says that you get current by multiplying voltage and conductance. And Kirchoff’s Current Law says that if you have a bunch of currents coming into a point from a bunch of wires, the sum of those currents is what leaves that point. So basically, each of a bunch of input voltages pushes current through a resistance (conductance is the inverse of resistance), multiplying the voltage value, and all those currents add up to produce a single value. Math, done.</p><p>Sound good? Well, it gets better. Much of the data that makes up a neural network are the “weights,” the things by which you multiply the input. And moving that data from memory into a processor’s logic to do the work is responsible for a big fraction of the energy GPUs expend. Instead, in most analog AI schemes, the weights are stored in one of several types of nonvolatile memory as a conductance value (the resistances above). Because weight data is already where it needs to be to do the computation, it doesn’t have to be moved as much, saving a pile of energy.</p><p>The combination of free math and stationary data promises calculations that need just <a href="https://spectrum.ieee.org/analog-ai" target="_self">thousandths of a trillionth of joule of energy</a>. Unfortunately, that’s not nearly what analog AI efforts have been delivering.</p><h2>The Trouble With Current</h2><p>The fundamental problem with any kind of analog computing has always been the signal-to-noise ratio. Analog AI has it by the truckload. The signal, in this case the sum of all those multiplications, tends to be overwhelmed by the many possible sources of noise.</p><p>“The problem is, semiconductor devices are messy things,” says Verma. Say you’ve got an analog neural network where the weights are stored as conductances in individual RRAM cells. Such weight values are stored by setting a relatively high voltage across the RRAM cell for a defined period of time. The trouble is, you could set the exact same voltage on two cells for the same amount of time, and those two cells would wind up with slightly different conductance values. Worse still, those conductance values might change with temperature.</p><p>The differences might be small, but recall that the operation is adding up many multiplications, so the noise gets magnified. Worse, the resulting current is then turned into a voltage that is the input of the next layer of neural networks, a step that adds to the noise even more.</p><p>Researchers have attacked this problem from both a computer science perspective and a device physics one. In the hope of compensating for the noise, researchers have invented ways to bake some knowledge of the physical foibles of devices into their neural network models. Others have focused on making devices that behave as predictably as possible. IBM, which has done <a href="https://spectrum.ieee.org/analog-ai" target="_self">extensive research in this area</a>, does both.</p><p>Such techniques are competitive, if not yet commercially successful, in smaller-scale systems, chips meant to provide low-power machine learning to devices at the edges of IoT networks. Early entrant <a href="https://spectrum.ieee.org/two-startups-use-processing-in-flash-memory-for-ai-at-the-edge" target="_blank">Mythic AI </a>has produced more than one generation of its analog AI chip, but it’s competing in a field where low-power digital chips are succeeding.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black circuit board with a large silver chip at center." class="rm-shortcode" data-rm-shortcode-id="46bc434f675fd57bfe4383c42e4d2043" data-rm-shortcode-name="rebelmouse-image" id="71fa2" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-a-large-silver-chip-at-center.jpg?id=60345147&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The EN100 card for PCs is a new analog AI chip architecture.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">EnCharge AI</small></p><h2>Capacitors All the Way Down</h2><p>EnCharge’s solution strips out the noise by measuring the amount of charge instead of flow of charge in machine learning’s multiply-and-accumulate mantra. In traditional analog AI, multiplication depends on the relationship among voltage, conductance, and current. In this new scheme, it depends on the relationship among voltage, capacitance, and charge—where basically, charge equals capacitance times voltage.</p><p>Why is that difference important? It comes down to the component that’s doing the multiplication. Instead of using some finicky, vulnerable device like RRAM, EnCharge uses capacitors.</p><p>A capacitor is basically two conductors sandwiching an insulator. A voltage difference between the conductors causes charge to accumulate on one of them. The thing that’s key about them for the purpose of machine learning is that their value, the capacitance, is determined by their size. (More conductor area or less space between the conductors means more capacitance.)</p><p>“The only thing they depend on is geometry, basically the space between wires,” Verma says. “And that’s the one thing you can control very, very well in CMOS technologies.” EnCharge builds an array of precisely valued capacitors in the layers of copper interconnect above the silicon of its processors.</p><p>The data that makes up most of a neural network model, the weights, are stored in an array of digital memory cells, each connected to a capacitor. The data the neural network is analyzing is then multiplied by the weight bits using simple logic built into the cell, and the results are stored as charge on the capacitors. Then the array switches into a mode where all the charges from the results of multiplications accumulate and the result is digitized. </p><p>While the initial <a href="https://www.enchargeai.com/technology" rel="noopener noreferrer" target="_blank">invention</a>, which dates back to 2017, was a big moment for Verma’s lab, he says the basic concept is quite old. “It’s called switched capacitor operation; it turns out we’ve been doing it for decades,” he says. It’s used, for example, in commercial high-precision analog-to-digital converters. “Our innovation was figuring out how you can use it in an architecture that does in-memory computing.”</p><h2>Competition</h2><p>Verma’s lab and EnCharge spent years proving that the technology was programmable and scalable and co-optimizing it with an architecture and software stack that suits AI needs that are vastly different than they were in 2017. The resulting products are with early-access developers now, and the company—which <a href="https://www.enchargeai.com/news-and-publications/series-b" rel="noopener noreferrer" target="_blank">recently raised US $100 million</a> from Samsung Venture, Foxconn, and others—plans another round of early access collaborations.</p><p>But EnCharge is entering a competitive field, and among the competitors is the big kahuna, Nvidia. At its big developer event in March, GTC, Nvidia announced plans for a <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/" rel="noopener noreferrer" target="_blank">PC product</a> built around its GB10 CPU-GPU combination and <a href="https://www.nvidia.com/en-us/products/workstations/dgx-station/" rel="noopener noreferrer" target="_blank">workstation</a> built around the upcoming <a href="https://www.nvidia.com/en-us/data-center/gb300-nvl72/" rel="noopener noreferrer" target="_blank">GB300</a>.</p><p>And there will be plenty of competition in the low-power space EnCharge is after. Some of them even use a form of computing-in-memory. <a href="https://www.d-matrix.ai/" target="_blank">D-Matrix</a> and <a href="https://axelera.ai/" target="_blank">Axelera</a>, for example, took part of analog AI’s promise, embedding the memory in the computing, but do everything digitally. They each developed custom SRAM memory cells that both store and multiply and do the summation operation digitally, as well. There’s even at least one more-traditional analog AI startup in the mix, <a href="https://spectrum.ieee.org/analog-ai-2669898661" target="_self">Sagence</a>.</p><p>Verma is, unsurprisingly, optimistic. The new technology “means advanced, secure, and personalized AI can run locally, without relying on cloud infrastructure,” he said in a <a href="https://www.businesswire.com/news/home/20250529108055/en/EnCharge-AI-Announces-EN100-First-of-its-Kind-AI-Accelerator-for-On-Device-Computing" target="_blank">statement</a>. “We hope this will radically expand what you can do with AI.”</p>]]></description><pubDate>Mon, 02 Jun 2025 14:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/analog-ai-chip-architecture</guid><category>Analog ai</category><category>Artificial intelligence</category><category>Ibm</category><category>Nvidia</category><category>Capacitors</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-five-silver-chips-and-many-black-components.jpg?id=60345071&amp;width=980"></media:content></item><item><title>How Ukraine’s Killer Drones Are Beating Russian Jamming</title><link>https://spectrum.ieee.org/ukraine-killer-drones</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-soldiers-in-full-military-dress-stand-on-a-hill-while-one-of-them-releases-a-drone.jpg?id=59800494&width=1200&height=800&coordinates=80%2C0%2C81%2C0"/><br/><br/><div class="intro-text">
<em><a href="https://www.axios.com/2025/06/01/ukraine-drone-strikes-russia" rel="noopener noreferrer" target="_blank">Ukraine’s 1 June attack</a> on multiple Russian military bases destroyed or damaged as many as 41 Russian aircraft, including some of the country’s most advanced bombers. Estimates of the sum total of the damage range from US $2 billion to $7 billion. Supposedly planned for a <a href="https://www.theguardian.com/world/2025/jun/02/operation-spiderweb-visual-guide-ukraine-drone-attack-russian-aircraft" rel="noopener noreferrer" target="_blank">year and a half</a>, the Ukrainian operation was exceptional in its sophistication: Ukrainian agents reportedly smuggled dozens of first-person-view attack drones into Russia on trucks, <a href="https://www.nytimes.com/2025/06/02/world/europe/ukraine-russia-drone-strikes.html" rel="noopener noreferrer" target="_blank">situating them close to the air bases</a> where the target aircraft were vulnerable on tarmacs. The bases included one in Irkutsk, 4,300 kilometers from Ukraine, and another in south Murmansk, 1,800 km away. Remote pilots in Ukraine then launched the killer drones simultaneously.
	</em>
</div><div class="intro-text">
<em>
	The far-reaching operation was being hailed as the most inventive and bold of the war so far. Indeed, </em>IEEE Spectrum<em> has been regularly covering the ascent of Ukraine’s military drone programs, both </em><em><a href="https://spectrum.ieee.org/ukraine-hackers-war" target="_self">offensive</a></em><em> and </em><em><a href="https://spectrum.ieee.org/ukraine-air-defense" target="_self">defensive</a></em><em>, and for </em><em><a href="https://spectrum.ieee.org/drone-warfare-ukraine">air</a></em><em>, </em><em><a href="https://spectrum.ieee.org/sea-drone" target="_self">marine</a></em><em>, and </em><em><a href="https://spectrum.ieee.org/ukraine-drones-2671254184" target="_self">land</a></em><em> missions. In this article, originally posted on April 6, we described another bold Ukrainian drone initiative, which was applying artificial intelligence-based navigational software to enable killer drones to navigate to targets even in the presence of heavy jamming.</em>
</div><p class="drop-caps">
<strong><span></span>After the Estonian startup </strong><a href="https://www.krattworks.com/" target="_blank">KrattWorks</a> dispatched the first batch of its <a href="https://www.krattworks.com/isr-ghostdragon" target="_blank">Ghost Dragon ISR</a> quadcopters to Ukraine in mid-2022, the company’s officers thought they might have six months or so before they’d need to reconceive the drones in response to new battlefield realities. The 46-centimeter-wide flier was far more robust than the hobbyist-grade UAVs that came to define the <a href="https://spectrum.ieee.org/ukraine-hackers-war" target="_self">early days of the drone war</a> against Russia. But within a scant three months, the Estonian team realized their painstakingly fine-tuned device had already become obsolete.
</p><p class="ieee-inbody-related">
	Related: 
	<a href="https://spectrum.ieee.org/ukraine-drones-2671254184" target="_blank">Ukraine Tech Turns Combat into Real-Life “Game”</a>
</p><p>
	Rapid advances in 
	<a href="https://spectrum.ieee.org/tag/jamming" target="_self">jamming</a> and <a href="https://spectrum.ieee.org/tag/spoofing" target="_self">spoofing</a>—the only efficient defense against drone attacks—set the team on an unceasing marathon of innovation. Its latest technology is a neural-network-driven optical navigation system, which allows the drone to continue its mission even when all radio and satellite-navigation links are jammed. It began tests in <a href="https://spectrum.ieee.org/tag/ukraine" target="_self">Ukraine</a> in December, part of a trend toward jam-resistant, <a href="https://spectrum.ieee.org/search/?q=autonomous+drones" target="_self">autonomous UAVs</a> (uncrewed aerial vehicles). The new fliers herald yet another phase in the unending struggle that pits drones against the jamming and spoofing of <a href="https://spectrum.ieee.org/the-fall-and-rise-of-russian-electronic-warfare" target="_self">electronic warfare,</a> which aims to sever links between drones and their operators. There are now <a href="https://www.nytimes.com/interactive/2025/03/03/world/europe/ukraine-russia-war-drones-deaths.html" rel="noopener noreferrer" target="_blank">tens of thousands</a> of jammers straddling the front lines of the war, defending against drones that are not just killing soldiers but also destroying armored vehicles, other drones, <a href="https://kyivindependent.com/russia-missile-attack/" rel="noopener noreferrer" target="_blank">industrial infrastructure</a>, and even tanks.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="A man wearing a dark-green long-sleeve t-shirt, seen from behind, holds a drone with both hands above his head." class="rm-shortcode" data-rm-shortcode-id="670ed8b9ef5644576b6f8f9836a06576" data-rm-shortcode-name="rebelmouse-image" id="090b5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-wearing-a-dark-green-long-sleeve-t-shirt-seen-from-behind-holds-a-drone-with-both-hands-above-his-head.jpg?id=59800469&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">During tests near Kyiv, Ukraine, in 2024, a technician prepared to release a drone outfitted with software by Auterion.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Justyna Mielnikiewicz
	</small>
</p><p>
	“The situation with electronic warfare is moving extremely fast,” says Martin Karmin, KrattWorks’ cofounder and chief operations officer. “We have to constantly iterate. It’s like a cat-and-mouse game.”
</p><p>
	I met Karmin at the company’s headquarters in the outskirts of Estonia’s capital, Tallinn. Just a couple of hundred kilometers to the east is the tiny nation’s border with Russia, its former oppressor. At 38, Karmin is barely old enough to remember what life was like under Russian rule, but he’s heard plenty. He and his colleagues, most of them volunteer members of the 
	<a href="https://www.kaitseliit.ee/en/edl" rel="noopener noreferrer" target="_blank">Estonian Defense League</a>, have “no illusions” about <a href="https://spectrum.ieee.org/tag/russia" target="_self">Russia</a>, he says with a shrug.
</p><p>
	His company is as much about arming Estonia as it is about helping Ukraine, he acknowledges. Estonia is not officially at war with Russia, of course, but regions around the border between the two countries have for years been subjected to persistent jamming of satellite-based navigation systems, such as the 
	<a href="https://defence-industry-space.ec.europa.eu/eu-space/galileo-satellite-navigation_en" rel="noopener noreferrer" target="_blank">European Union’s Galileo satellites</a>, forcing occasional flight cancellations at Tartu airport. In November, <a href="https://spectrum.ieee.org/tag/satellite" target="_self">satellite</a> imagery revealed that Russia is expanding its military bases along the Baltic states’ borders.
</p><p>
	“We are a small country,” Karmin says. “Innovation is our only chance.”
</p><h2>Navigating by Neural Network</h2><p>
	In KrattWorks’ spacious, white-walled workshop, a handful of engineers are testing software. On the large ocher desk that dominates the room, a selection of KrattWorks’ devices is on display, including a couple of fixed-wing, smoke-colored UAVs designed to serve as aerial decoys, and the Ghost Dragon ISR 
	<a href="https://spectrum.ieee.org/tag/quadcopter" target="_self">quadcopter</a>, the company’s flagship product.
</p><p>
	Now in its third generation, the Ghost Dragon has come a long way since 2022. Its original command-and-control-band 
	<a href="https://spectrum.ieee.org/tag/radio" target="_self">radio</a> was quickly replaced with a smart frequency-hopping system that constantly scans the available spectrum, looking for bands that aren’t jammed. It allows operators to switch among six radio-frequency bands to maintain control and also send back video even in the face of hostile jamming.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black quadcopter drone hovers in front of a coniferous tree." class="rm-shortcode" data-rm-shortcode-id="a63ed6b62a8e73e583b5ce084557e634" data-rm-shortcode-name="rebelmouse-image" id="ca339" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-quadcopter-drone-hovers-in-front-of-a-coniferous-tree.jpg?id=59800498&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Ghost Dragon reconnaissance drone from Krattworks can navigate autonomously, by detecting landmarks as it flies over them. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">KrattWorks</small>
</p><p>
	The drone’s dual-band satellite-navigation receiver can switch among the four main satellite positioning services: 
	<a href="https://family1st.io/gps-vs-glonass-vs-galileo-whats-the-best-gnss/" rel="noopener noreferrer" target="_blank">GPS, Galileo</a>, China’s BeiDou, and Russia’s GLONASS. It’s been augmented with a spoof-proof algorithm that compares the satellite-navigation input with data from onboard sensors. The system provides protection against sophisticated spoofing attacks that attempt to trick drones into self-destruction by persuading them they’re flying at a much higher altitude than they actually are.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>
	At the heart of the quadcopter’s matte grey body is a machine-vision-enabled computer running a 1-gigahertz Arm processor that provides the Ghost Dragon with its latest superpower: the ability to navigate autonomously, without access to any global navigation satellite system (GNSS). To do that, the computer runs a 
	<a href="https://spectrum.ieee.org/tag/neural-network" target="_self">neural network</a> that, like an old-fashioned traveler, compares views of landmarks with positions on a map to determine its position. More precisely, the drone uses real-time views from a downward-facing optical camera, comparing them against stored satellite images, to determine its position.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c7a71f63bfede8e7831651bd1a76a5e8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/tGCFBHbw6HQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A promotional video from Krattworks depicts scenarios in which the company’s drones augment soldiers on offensive maneuvers.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">KrattWorks</small>
</p><p>
	“Even if it gets lost, it can recognize some patterns, like crossroads, and update its position,” Karmin says. “It can make its own decisions, somewhat, either to return home or to fly through the jamming bubble until it can reestablish the GNSS link again.”
</p><h2>Designing Drones for High Lethality per Cost</h2><p>
	Just as machine guns and tanks defined the First World War, drones have become emblematic of Ukraine’s struggle against Russia. It was the besieged Ukraine that first turned the concept of a military drone on its head. Instead of Predators and Reapers worth tens of millions of dollars each, Ukraine began purchasing huge numbers of off-the-shelf fliers worth a few hundred dollars apiece—the kind used by filmmakers and enthusiasts—and turned them into highly lethal weapons. A recent 
	<a href="https://www.nytimes.com/interactive/2025/03/03/world/europe/ukraine-russia-war-drones-deaths.html" rel="noopener noreferrer" target="_blank"><em><em>New York Times</em></em> investigation</a> found that drones account for 70 percent of deaths and injuries in the ongoing conflict.
</p><p>
	“We have much less artillery than Russia, so we had to compensate with drones,” says 
	<a href="https://www.linkedin.com/in/serhii-skoryk-20b02728b/?originalSubdomain=ua" rel="noopener noreferrer" target="_blank">Serhii Skoryk</a>, commercial director at <a href="https://kvertus.ua/" rel="noopener noreferrer" target="_blank">Kvertus</a>, a Kyiv-based electronic-warfare company. “A missile is worth perhaps a million dollars and can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A man in camouflage uniform is surrounded by military gear, including drones. " class="rm-shortcode" data-rm-shortcode-id="c6a0b6470088c9d32d62087a112dae9a" data-rm-shortcode-name="rebelmouse-image" id="a5d2c" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-camouflage-uniform-is-surrounded-by-military-gear-including-drones.jpg?id=59800500&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Near the Russian border in Kharkiv Oblast, a Ukrainian soldier prepared first-person-view drones for an attack on 16 January 2025.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Jose Colon/Anadolu/Getty Images</small>
</p><p>
	Electronic warfare techniques such as jamming and spoofing aim to neutralize the drone threat. A drone that gets jammed and loses contact with its pilot and also loses its spatial bearings will either crash or fly off randomly until its battery dies.
	<a href="https://static.rusi.org/403-SR-Russian-Tactics-web-final.pdf" rel="noopener noreferrer" target="_blank"> According to the Royal United Services Institute</a>, a U.K. defense think tank, Ukraine may be losing about 10,000 drones per month, mostly due to jamming. That number includes explosives-laden kamikaze drones that don’t reach their targets, as well as surveillance and reconnaissance drones like KrattWorks’ Ghost Dragon, meant for longer service.
</p><p>
	“Drones have become a consumable item,” says Karmin. “You will get maybe 10 or 15 missions out of a reconnaissance drone, and then it has to be already paid off because you will lose it sooner or later.”
</p><p class="pull-quote">
	 Russia took an unexpected step in the summer of 2024, ditching sophisticated wireless control in favor of hard-wired drones fitted with spools of optical fiber.
</p><p>
	Tech minds on both sides of the conflict have therefore been working hard to circumvent electronic defenses. Russia took an unexpected step starting in early 2024, deploying hard-wired drones fitted with spools of optical fiber. Like a twisted variation on a child’s kite, the lethal UAVs can venture 20 or more kilometers away from the controller, the hair-thin fiber floating behind them, providing an unjammable connection.
</p><p>
	“Right now, there is no protection against fiber-optic drones,” 
	<a href="https://www.linkedin.com/in/vadym-burukin-1378abbb/?originalSubdomain=ua" rel="noopener noreferrer" target="_blank">Vadym Burukin</a>, cofounder of the Ukrainian drone startup <a href="https://huless.com/" rel="noopener noreferrer" target="_blank">Huless</a>, tells <em><em>IEEE</em></em> <em><em>Spectrum</em></em>. “The Russians scaled this solution pretty fast, and now they are saturating the battle front with these drones. It’s a huge problem for Ukraine.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A drone carrying a large cylindrical object flies over a blurry forest background." class="rm-shortcode" data-rm-shortcode-id="c5a9250fbac728b7b2a951af2e4492d2" data-rm-shortcode-name="rebelmouse-image" id="b73cf" loading="lazy" src="https://spectrum.ieee.org/media-library/a-drone-carrying-a-large-cylindrical-object-flies-over-a-blurry-forest-background.jpg?id=59800502&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">One way that drone operators can defeat electronic jamming is by communicating with their drone via a fiber optic line that pays out of a spool as the drone flies. This is a tactic favored by Russian units, although this particular first-person-view drone is Ukrainian. It was demonstrated near Kyiv on 29 January 2025.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Efrem Lukatsky/AP</small>
</p><p>
	Ukraine, too, has experimented with optical fiber, but the technology didn’t take off, as it were. “The optical fiber costs upwards from $500, which is, in many cases, more than the drone itself,” Burukin says. “If you use it in a drone that carries explosives, you lose some of that capacity because you have the weight of the cable.” The extra weight also means less capacity for better-quality cameras, sensors, and computers in reconnaissance drones.
</p><h2>Small Drones May Soon Be Making Kill-or-No-Kill Decisions</h2><p>
	Instead, Ukraine sees the future in autonomous navigation. This past July, kamikaze drones equipped with an autonomous navigation system from U.S. supplier
	<a href="https://auterion.com/" rel="noopener noreferrer" target="_blank"> Auterion</a> destroyed a column of Russian tanks fitted with jamming devices.
</p><p>
	“It was really hard to strike these tanks because they were jamming everything,” says Burukin. “The drones with the autopilot were the only equipment that could stop them.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A diagram shows a quadcopter drone flying above a communications tower as it attempts to navigate to an enemy tank." class="rm-shortcode" data-rm-shortcode-id="4009b55dda0d71df9507b2de4cc944ef" data-rm-shortcode-name="rebelmouse-image" id="3ddb9" loading="lazy" src="https://spectrum.ieee.org/media-library/a-diagram-shows-a-quadcopter-drone-flying-above-a-communications-tower-as-it-attempts-to-navigate-to-an-enemy-tank.jpg?id=59800510&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Auterion’s “terminal guidance” system uses known landmarks to orient a drone as it seeks out a target. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Auterion</small>
</p><p>
	The technology used to hit those tanks is called terminal guidance and is the first step toward smart, fully autonomous drones, according to Auterion’s CEO, Lorenz Meier. The system allows the drone to directly overcome the jamming whether the protected target is a tank, a trench, or a military airfield.
</p><p>
	“If you lock on the target from, let’s say, a kilometer away and you get jammed as you approach the target, it doesn’t matter,” Meier says in an interview. “You’re not losing the target as a manual operator would.”
</p><p>
	The visual navigation technology trialed by KrattWorks is the next step and an innovation that has only reached the battlefield this year. Meier expects that by the end of 2025, firms including his own will introduce fully autonomous solutions encompassing visual navigation to overcome GPS jamming, as well as terminal guidance and smart target recognition.
</p><p>
	“The operator would only decide the area where to strike, but the decision about the target is made by the drone,” Meier explains. “It’s already done with guided shells, but with drones you can do that at mass scale and over much greater distances.”
</p><p>
	Auterion, founded in 2017 to produce drone software for civilian applications such as grocery delivery, threw itself into the war effort in early 2024, motivated by a desire to equip democratic countries with technologies to help them defend themselves against authoritarian regimes. Since then, the company has made rapid strides, working closely with Ukrainian drone makers and troops.
</p><p class="pull-quote">
<span>“A missile worth perhaps a million dollars can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.” <strong>—Serhii Skoryk, Kvertus</strong></span>
</p><p>
	But purchasing Western equipment is, in the long term, not affordable for Ukraine, a country with a per capita GDP of 
	<a href="https://www.imf.org/external/datamapper/profile/UKR" rel="noopener noreferrer" target="_blank">US $5,760</a>—much lower than the European average of <a href="https://www.imf.org/external/datamapper/profile/EUQ" rel="noopener noreferrer" target="_blank">$38,270</a>. Fortunately, Ukraine can tap its engineering workforce, which is among the largest in Europe. Before the war, Ukraine was a go-to place for Western companies looking to set up IT- and software-development centers. Many of these workers have since joined Ukraine’s DIY military-technician (“miltech”) development movement.
</p><p>
	An engineer and founder at a Ukrainian startup that produces long-range kamikaze drones, who didn’t want to be named because of security concerns, told 
	<em><em>Spectrum</em></em> that the company began developing its own computers and autonomous navigation software for target tracking “just to keep the price down.” The engineer said Ukrainian startups offer advanced military-drone technology at a price that is a small fraction of what established competitors in the West are charging.
</p><p>
	Within three years of the February 2022 Russian invasion, Ukraine produced a world-class defense-tech ecosystem that is not only attracting Western innovators into its fold, but also regularly surpassing them. The keys to Ukraine’s success are rapid iterations and close cooperation with frontline troops. It’s a formula that’s working for Auterion as well. “If you want to build a leading product, you need to be where the product is needed the most,” says Meier. “That’s why we’re in Ukraine.”
</p><p>
	Burukin, from Ukrainian startup Huless, believes that autonomy will play a bigger role in the future of drone warfare than 
	<a href="https://www.rferl.org/a/russia-fiber-optic-drones-ukraine-battlefield/33270243.html" rel="noopener noreferrer" target="_blank">Russia’s optical fibers</a> will. Autonomous drones not only evade jamming, but their range is limited only by their battery storage. They also can carry more explosives or better cameras and sensors than the wired drones can. On top of that, they don’t place high demands on their operators.
</p><p>
	“In the perfect world, the drone should take off, fly, find the target, strike it, and report back on the task,” Burukin says. “That’s where the development is heading.”
</p><p>
	The cat-and-mouse game is nowhere near over. Companies including KrattWorks are already thinking about the next innovation that would make drone warfare cheaper and more lethal. By creating a drone mesh network, for example, they could send a sophisticated intelligence, surveillance, and reconnaissance drone followed by a swarm of simpler kamikaze drones to find and attack a target using visual navigation.
</p><p>
	“You can send, like, 10 drones, but because they can fly themselves, you don’t need a superskilled operator controlling every single one of these,” notes KrattWorks’ Karmin, who keeps tabs on tech developments in Ukraine with a mixture of professional interest, personal empathy, and foreboding. Rarely does a day go by that he does not think about the expanding Russian military presence near Estonia’s eastern borders.
</p><p>
	“We don’t have a lot of people in Estonia,” he says. “We will never have enough skilled drone pilots. We must find another way.” 
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Mon, 02 Jun 2025 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ukraine-killer-drones</guid><category>Drone war</category><category>Drones</category><category>Neural network</category><category>Russian jamming</category><category>Ukraine</category><dc:creator>Tereza Pultarova</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-soldiers-in-full-military-dress-stand-on-a-hill-while-one-of-them-releases-a-drone.jpg?id=59800494&amp;width=980"></media:content></item><item><title>IEEE President’s Note: One IEEE for Education</title><link>https://spectrum.ieee.org/ieee-presidents-note-june-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-smiling-woman-in-a-magenta-suit.jpg?id=56605851&width=1200&height=800&coordinates=0%2C406%2C0%2C406"/><br/><br/><p>
	Education plays a critical role in building the abilities and interests of our next generation, as well as developing the technical expertise and knowledge needed for our near-career technologists to join the professional world and be among those who design, build, and maintain the systems and devices that form the foundation of modern society and advance us forward.
</p><p>
	IEEE’s role in the education of engineers and technologists, and its devotion to knowledge-sharing as a trusted source across our fields, has been part of our organization from its creation. The effects are so pervasive that goals for degreed education and ideals of professional responsibility stemming from IEEE (or its historic predecessors AIEE and IRE) have shaped the knowledge and perspectives of most engineers and degreed computing professionals. IEEE is a mostly unsung world leader to the public because these contributions often go unnoticed.
</p><p>
	To support IEEE’s continued role in education as a positive force for advancing technology, we must ensure that our efforts are more widely recognized as we fulfill our mission and expand our value in engineering and technology across the globe.
</p><p>
	Strategic initiatives are a catalyst to prioritize our shared goal of better education for engineers and technologists and can be achieved by promoting collaboration and excellence across the organization.
</p><p>
	Advancing One IEEE for Education enables us to leverage our collective strengths. As part of this, I have created a committee whose focus is to map and strengthen IEEE’s collective future by developing a One IEEE strategy to empower technical innovation through education.
</p><p>
	Fostering education is a primary purpose for our organization, which is a public charity. IEEE’s focus on professional development and commitment to providing opportunities for lifelong learning, from preuniversity, university, and graduate students to professionals across our fields of interest is crucial not only for members but also the broader IEEE mission of advancing technology education and ensuring workforce readiness around the globe.
</p><h2>A primer on IEEE educational programs</h2><p>
	Education is the pathway to becoming an engineer or technologist. It is, perhaps because of this, that education is an almost universal value across IEEE, one that drives activities throughout our technical communities.
</p><p>
	The <a href="https://www.ieee.org/education/eab.html" rel="noopener noreferrer" target="_blank">IEEE Educational Activities Board</a> is dedicated to offering valuable programs to the engineering community and the public. There are educational offerings and activities from all our 47 technical societies and councils including, of course, the <a href="https://ieee-edusociety.org/" rel="noopener noreferrer" target="_blank">IEEE Education Society</a>. There are thousands of conferences, webinars, and seminars; dedicated collaborations for local sections and regional approaches; and education for those involved in creating our trusted publications, standards, and public policy.
</p><p>
	IEEE’s student and academic education programs include both university and preuniversity initiatives. Together, these provide support for IEEE members who work to inspire the next generation of engineers and technologists.
</p><h3>Volunteers in action</h3><br/><p>
	The 2025 IEEE Ad Hoc Committee on One IEEE Education Strategy for Empowering Technical Innovation, chaired by IEEE Fellow <a href="https://www.karenpanetta.com/#about-overview" rel="noopener noreferrer" target="_blank">Karen Panetta</a>, is working to build an ecosystem of support for advancing education across IEEE’s fields of interest.
</p><p>
	Additionally, a new task force under IEEE Educational Activities, led by 2021 IEEE President and IEEE Fellow <a href="https://spectrum.ieee.org/u/susan-k-kathy-land" target="_self">Susan K. “Kathy” Land</a>, is working to better understand ongoing preuniversity STEM activities across IEEE dedicated to outreach for students ages 5 through 18.
</p><p>
	Among its university programs, Educational Activities supports <a href="https://epics.ieee.org/" rel="noopener noreferrer" target="_blank">EPICS in IEEE</a>, a service-learning program; IEEE–Eta Kappa Nu (<a href="https://hkn.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE-HKN</a>), the <a href="https://spectrum.ieee.org/ieees-honor-society-expands-to-more-countries" target="_self">honor society</a> promoting scholarship, character, and attitude for undergraduates, graduate students and professionals; and IEEE’s significant accreditation activities collaboratively as part of <a href="https://www.abet.org/" rel="noopener noreferrer" target="_blank">ABET</a>, which contributes heavily as a major force for engineering and technical program accreditation worldwide.
</p><p>
	IEEE’s <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering</a> umbrella of programs focuses on precollege students and the adults who work with them, with the goal of increasing awareness of engineering and ensuring any student can see that they too could have a future in our professions. TryEngineering has developed strong collaborations with IEEE societies to provide opportunities for school-age children to learn about engineering technologies and career potential in those areas. The world benefits as more young people are empowered through IEEE efforts to see themselves as future innovators in engineering and computing.
</p><p>
	The <a href="https://spectrum.ieee.org/ieee-tryengineering-summer-institute-2024" target="_self">IEEE TryEngineering Summer Institute</a> provides camp-type experiences at select U.S. universities. I am proud to share that one of the most successful locations is held where I’m a professor: the <a href="https://www.sandiego.edu/" rel="noopener noreferrer" target="_blank">University of San Diego</a>. The program was expanded internationally with <a href="https://tryengineering.org/tryengineering-on-campus/" rel="noopener noreferrer" target="_blank">TryEngineering On Campus</a> pilot programs held in Hong Kong and Arta, Greece.
</p><p>
	The <a href="https://www.computer.org/membership/juniors" rel="noopener noreferrer" target="_blank">IEEE Computer Society Juniors Program</a> introduces preuniversity students to computing fundamentals through engaging, age-appropriate content and hands-on learning. Developed to align with TryEngineering, the program aims to inspire interest in computer science early on and support the global STEM talent pipeline.
</p><p>
	IEEE’s commitment to accreditation activities is critical to the development and success of future professionals. As a representative and advocate for the engineering and computer-science professions for ABET, IEEE has a strong, continued interest in sustaining and improving engineering and computing programs worldwide, and the programs in IEEE lead fields, including electrical, electronic, computer, and communications engineering, are the most numerous.
</p><p>
	The involvement of IEEE volunteers as ABET delegates, commissioners, and program evaluators ensures that the next generation of engineers who graduate from accredited programs will be prepared to handle the challenges facing IEEE’s fields of interest, including the creation of new program criteria in robotics and mechatronics.
</p><h2>Practical professional development</h2><p>
	The <a href="https://spectrum.ieee.org/ieee-learning-network-anniversary" target="_self">IEEE Learning Network</a> gathers education offerings from across IEEE, allowing learners who are professionals to advance or expand their specific technical expertise. Some of the new e-learning course programs recently launched include those produced in partnership with the <a href="https://iln.ieee.org/public/contentdetails.aspx?id=A8B87554A5914826AD91C19986C0D74F" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a>, the <a href="https://iln.ieee.org/public/contentdetails.aspx?id=706DBC956996482182A5232D95410F99" rel="noopener noreferrer" target="_blank">IEEE Computer Society</a>, <a href="https://iln.ieee.org/public/contentdetails.aspx?id=D02A42B64A834CC1A698ADC1ABAB9523" rel="noopener noreferrer" target="_blank">IEEE Future Directions</a>, and <a href="https://iln.ieee.org/public/contentdetails.aspx?id=D02A42B64A834CC1A698ADC1ABAB9523" rel="noopener noreferrer" target="_blank">IEEE Global Semiconductors</a>.
</p><p>
	The <a href="https://blended-learning.ieee.org/Portal/" rel="noopener noreferrer" target="_blank">IEEE Blended Learning Program</a> combines e-learning techniques with hands-on practice, designed to empower engineers with short lessons in tech to become future-ready.
</p><h2>Designing your future</h2><p>
	IEEE’s greatest opportunities for engagement and its advantage are the ability to offer individuals numerous avenues to contribute, collaborate, and advance both their professional careers and the broader field of technology. For our members, IEEE provides opportunities to engage, showcase their work, and grow professionally.
</p><p>
	Members thrive through the connections IEEE can facilitate. IEEE provides a professional home at every career stage, even pre-career, connecting you with a world of possibilities. This requires active effort and engagement, where membership is just one step. It is up to everyone to take charge of their own professional development and to look for ways to help others succeed too. The best opportunities lie in how each of us actively participates in shaping our career journey.
</p><p>
<strong>—Kathleen Kramer</strong>
</p><p>
<strong>IEEE president and CEO</strong>
</p><p>
	Please share your thoughts with me: <a href="mailto:president@ieee.org">president@ieee.org</a>.
</p>]]></description><pubDate>Sun, 01 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-presidents-note-june-2025</guid><category>Ieee news</category><category>Type:ti</category><category>Ieee presidents column</category><category>Presidents column</category><category>Education</category><category>Continuing education</category><category>Tryengineering</category><dc:creator>Kathleen Kramer</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-smiling-woman-in-a-magenta-suit.jpg?id=56605851&amp;width=980"></media:content></item><item><title>The Data Reveals Top Patent Portfolios</title><link>https://spectrum.ieee.org/top-patents-scorecard-rankings</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-white-men-in-polo-shirts-with-a-laptop-on-the-table.png?id=60312174&width=1200&height=800&coordinates=0%2C43%2C0%2C44"/><br/><br/><p>Eight years is a long time in the world of patents. When we last published what we then called the Patent Power Scorecard, in 2017, it was a different technological and social landscape—Google had just filed a <a href="https://patents.google.com/patent/US10452978B2/en" rel="noopener noreferrer" target="_blank">patent application on the transformer architecture</a>, a momentous advance that spawned the generative AI revolution. China was just beginning to produce quality, affordable <a href="https://en.wikipedia.org/wiki/Electric_vehicle_industry_in_China" rel="noopener noreferrer" target="_blank">electric vehicles at scale</a>. And the COVID pandemic wasn’t on anyone’s dance card. </p><p>Eight years is also a long time in the world of magazines, where we regularly play around with formats for articles and infographics. We now have more readers online than we do in print, so our art team is leveraging advances in interactive design software to make complex datasets grokkable at a glance, whether you’re on your phone or flipping through the pages of the magazine.</p><p>The scorecard’s return in <a href="https://spectrum.ieee.org/magazine/2025/june/" target="_blank">this issue</a> follows the return last month of The Data, which ran as our back page for several years; it’s curated by a different editor every month and edited by Editorial Director for Content Development Glenn Zorpette. </p><p>As we set out to recast the scorecard for this decade, we sought to strike the right balance between comprehensiveness and clarity, especially on a mobile-phone screen. As our Digital Product Designer Erik Vrielink, Assistant Editor Gwendolyn Rak, and Community Manager Kohava Mendelsohn explained to me, they wanted something that would be eye-catching while avoiding information overload. The solution they arrived at—a dynamic sunburst visualization—lets readers grasp the essential takeaways at glance in print, while <a href="https://spectrum.ieee.org/patent-power-2025" target="_self">the digital version</a>, allows readers to dive as deep as they want into the data.</p><p>Working with sci-tech-focused data-mining company <a href="https://1790analytics.com/" rel="noopener noreferrer" target="_blank">1790 Analytics</a>, which we partnered with on the original <a href="https://spectrum.ieee.org/patent-power" target="_self">Patent Power Scorecard</a>, the team prioritized three key metrics or characteristics: patent Pipeline Power (which goes beyond mere quantity to assess quality and impact), number of patents, and the country where companies are based. This last characteristic has become increasingly significant as geopolitical tensions reshape the global technology landscape. As 1790 Analytics cofounders Anthony Breitzman and Patrick Thomas note, the next few years could be particularly interesting as organizations adjust their patenting strategies in response to changing market access.</p><p>Some trends leap out immediately. In consumer electronics, <a href="https://insights.greyb.com/apple-patents/" rel="noopener noreferrer" target="_blank">Apple</a> dominates Pipeline Power despite having a patent portfolio one-third the size of <a href="https://www.samsung.com/global/business/networks/" rel="noopener noreferrer" target="_blank">Samsung’s</a>—a testament to the Cupertino company’s focus on high-impact innovations. The aerospace sector has seen dramatic consolidation, with <a href="https://www.rtx.com/" rel="noopener noreferrer" target="_blank">RTX</a> (formerly Raytheon Technologies) now encompassing multiple subsidiaries that appear separately on our scorecard. </p><p>And in the university rankings, <a href="https://otd.harvard.edu/faculty-inventors/protecting-intellectual-property/" rel="noopener noreferrer" target="_blank">Harvard</a> has seized the top spot from traditional tech powerhouses like MIT and Stanford, driven by patents that are more often cited as prior art in other recent patents. And then there are the subtle shifts that become apparent only when you dig deeper into the data. The rise of <a href="https://www.sel.co.jp/en/" rel="noopener noreferrer" target="_blank">SEL (Semiconductor Energy Laboratory)</a> over <a href="https://spectrum.ieee.org/tag/tsmc" target="_self">TSMC (Taiwan Semiconductor Manufacturing Co.)</a> in semiconductor design, despite having far fewer patents, suggests again that true innovation isn’t just about filing patents—it’s about creating technologies that others build upon.</p><p>Looking ahead, the real test will be how these patent portfolios translate into actual products and services. Patents are promises of innovation; the scorecard helps us see what companies are making those promises and the R&D investments to realize them. As we enter an era when technological leadership increasingly determines economic and strategic power, understanding these patterns is more crucial than ever.</p>]]></description><pubDate>Sun, 01 Jun 2025 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/top-patents-scorecard-rankings</guid><category>Patents</category><category>Patent power</category><category>1790 analytics</category><category>Innovation</category><category>Intellectual property</category><category>Ip</category><category>Sel</category><category>Apple</category><category>Harvard</category><dc:creator>Harry Goldstein</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/two-white-men-in-polo-shirts-with-a-laptop-on-the-table.png?id=60312174&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1200&height=800&coordinates=0%2C0%2C0%2C210"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Deep learning</category><category>Artificial intelligence</category><category>Andrew ng</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Matlab</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Quantum computing</category><category>2d materials</category><category>Ibm</category><category>Qubits</category><category>Hexagonal boron nitride</category><category>Superconducting qubits</category><category>Mit</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>