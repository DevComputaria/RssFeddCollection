<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Tue, 10 Jun 2025 20:23:43 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2MzA3MTQzOX0.SxRBIud_XE2YWQFaIJD9BPB1w-3JsFhiRkJIIe9Yq-g/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>IBM Says It’s Cracked Quantum Error Correction</title><link>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&width=1245&height=700&coordinates=0%2C338%2C0%2C338"/><br/><br/><p>IBM has unveiled a new quantum computing architecture it says will slash the number of qubits required for error correction. The advance will underpin its goal of <a href="https://spectrum.ieee.org/ibm-quantum-computer-2668978269" target="_blank">building a large-scale, fault-tolerant quantum computer</a>, called Starling, that will be available to customers by 2029.</p><p>Because of the inherent unreliability of the qubits (the quantum equivalent of bits) that quantum computers are built from, error correction will be crucial for building reliable, large-scale devices. <a data-linked-post="2650272311" href="https://spectrum.ieee.org/google-tests-first-error-correction-in-quantum-computing" target="_blank">Error-correction approaches</a> spread each unit of information across many physical qubits to create “logical qubits.” This provides redundancy against errors in individual physical qubits.</p><p>One of the most popular approaches is known as a surface code, which requires roughly 1,000 physical qubits to make up one logical qubit. This was the approach IBM focused on initially, but the company eventually realized that creating the hardware to support it was an “engineering pipe dream,” <a href="https://research.ibm.com/people/jay-gambetta" target="_blank">Jay Gambetta</a>, the vice president of IBM Quantum, said in a press briefing.</p><p>Around 2019, the company began to investigate alternatives. In a <a href="https://www.nature.com/articles/s41586-024-07107-7" target="_blank">paper</a> published in <em>Nature</em> last year, IBM researchers outlined a new error-correction scheme called quantum low-density parity check (qLDPC) codes that would require roughly one-tenth of the number of qubits that surface codes need. Now, the company <a href="https://newsroom.ibm.com/2025-06-10-IBM-Sets-the-Course-to-Build-Worlds-First-Large-Scale,-Fault-Tolerant-Quantum-Computer-at-New-IBM-Quantum-Data-Center" target="_blank">has unveiled a new quantum-computing architecture</a> that can realize this new approach.</p><p>“We’ve cracked the code to quantum error correction and it’s our plan to build the first large-scale, fault-tolerant quantum computer,” said Gambetta, who is also an IBM Fellow. “We feel confident it is now a question of engineering to build these machines, rather than science.”</p><h2>IBM Unveils New Quantum Roadmap</h2><p>IBM will take the first step towards realizing this architecture later this year with a processor called Loon. This chip will feature couplers that can connect distant qubits on the same chip, which is key for implementing qLDPC codes. These “non-local” interactions are what make the approach more efficient than the surface code, which relies solely on qubits communicating with their neighbors.</p><p>According to <a href="https://www.ibm.com/downloads/documents/us-en/131cf87ab63319bf" rel="noopener noreferrer" target="_blank">a roadmap</a> released alongside details of the new architecture, the company plans to build a follow-on processor called Kookaburra in 2026 that will feature both a logical processing unit and a quantum memory. This will be the first demonstration of the kind of base module that subsequent systems will be built from. The following year IBM plans to link two of these modules together to create a device called Cockatoo.</p><p>The road map doesn’t detail how many modules will be used to create Starling, IBM’s planned commercial offering, but the computer will feature 200 logical qubits and be capable of running 100 million quantum operations. Exactly how many physical qubits will be required is yet to be finalized, said <a href="https://research.ibm.com/people/matthias-steffen" target="_blank">Matthias Steffen</a>, IBM Fellow, who leads the quantum-processor technology team. But the new architecture is likely to require on the order of several hundred physical qubits to create 10 logical qubits, he added.</p><p>IBM plans to build Starling by 2028, before making it available on the cloud the following year. It will be housed in a new quantum data center in Poughkeepsie, N.Y., and will lay the foundations for the final system on IBM’s current road map, a 2,000 logical qubit machine codenamed Blue Jay.</p><p>IBM’s new architecture is a significant advance over its previous technology, says <a href="https://www.gartner.com/en/experts/mark-horvath" target="_blank">Mark Horvath</a>, a vice president analyst at Gartner, who was briefed in advance of the announcement. The new chip’s increased connectivity makes it substantially more powerful and is backed up by significant breakthroughs in 3D fabrication. And if it helps IBM reach 200 logical qubits, that would bring quantum computers into the realm of solving practical problems, Horvath says.</p><p>However, Horvath adds that the modular approach IBM is banking on to get there could prove challenging. “That’s a very complicated task,” he says. “I think it will eventually work. It’s just, it’s a lot further off than people think it is.”</p><p>One of biggest remaining hurdles is improving gate fidelities across the device. To successfully implement this new architecture, error rates need to come down by an order of magnitude, admitted IBM’s Steffen, though the company is confident this is achievable. One of the main paths forward will be to improve the coherence times of the underlying qubits, which refers to how long they can maintain their quantum state. “We do have evidence that this is really one of the main bottlenecks to improving gate errors,” Steffen says.</p><p>In isolated test devices, IBM has managed to push average coherence times to 2 milliseconds but translating that to larger chips is not straightforward. Steffen said the company recently made progress with its Heron chips, going from around 150 to 250 microseconds.</p><p>Significant engineering challenges remain in supporting infrastructure as well, said Steffen, including connectors that link together different parts of the system and amplifiers. But a big advantage of the new architecture is that it requires far fewer components due to the reduced number of physical qubits. “This is one of the reasons why we’re so excited about these qLDPC codes, because it also reduces all of the nonquantum-processor overhead,” he says.<br/></p><p><em>This story was updated on 10 June 2025 to correct some details of IBM’s current roadmap.</em><br/></p>]]></description><pubDate>Tue, 10 Jun 2025 14:00:10 +0000</pubDate><guid>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</guid><category>Quantum computing</category><category>Ibm</category><category>Error correction</category><category>Quantum computers</category><category>Qubits</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&amp;width=980"></media:content></item><item><title>Navigating the Dual-Use Dilemma</title><link>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&width=1245&height=700&coordinates=0%2C65%2C0%2C65"/><br/><br/><p>Open-source technology developed in the civilian sector has the capacity to also be used in military applications or be simply misused. Navigating this <a href="https://link.springer.com/article/10.1007/s11948-009-9159-9" rel="noopener noreferrer" target="_blank">dual-use</a> potential is becoming more important across engineering fields, as innovation goes both ways. While the “openness” of open-source technology is part of what drives innovation and allows everyone access, it also, unfortunately, means it’s just as easily accessible to others, including the military and criminals.</p><p>What happens when a rogue state, a nonstate militia, or a school shooter displays the same creativity and innovation with open-source technology that engineers do? This is the question we are discussing here: How can we uphold our principles of open research and innovation to drive progress while mitigating the inherent risks that come with accessible technology?</p><p>More than just open-ended risk, let’s discuss the specific challenges open-source technology and its dual-use potential have on robotics. Understanding these challenges can help engineers learn what to look for in their own disciplines.</p><h2>The Power and Peril of Openness</h2><p>Open-access publications, software, and educational content are fundamental to advancing robotics. They have democratized access to knowledge, enabled reproducibility, and fostered a vibrant, collaborative international community of scientists. Platforms like arXiv and GitHub and open-source initiatives like the <a href="https://www.ros.org/" rel="noopener noreferrer" target="_blank">Robot Operating System</a> (ROS) and the <a href="https://github.com/open-dynamic-robot-initiative/" rel="noopener noreferrer" target="_blank">Open Dynamic Robot Initiative</a> have been pivotal in accelerating robotics research and innovation, and there is no doubt that they should remain openly accessible. Losing access to these resources would be devastating to the robotics field.</p><p>However, robotics carries inherent dual-use risks since most robotics technology can be repurposed <a href="https://spectrum.ieee.org/autonomous-weapons-challenges" target="_blank">for military use</a> or <a href="https://spectrum.ieee.org/why-you-should-fear-slaughterbots-a-response" target="_blank">harmful purposes</a>. One recent example of custom-made drones in current conflicts is particularly insightful. The resourcefulness displayed by Ukrainian soldiers in repurposing and sometimes <a href="https://www.cnas.org/publications/reports/evolution-not-revolution" rel="noopener noreferrer" target="_blank">augmenting civilian drone technology</a> received worldwide, often admiring, news coverage. Their creativity has been made possible through the affordability of commercial drones, spare parts, 3D printers, and the availability of open-source software and hardware. This allows people with little technological background and money to easily create, control, and repurpose robots for military applications. One can certainly argue that this has had an empowering effect on Ukrainians defending their country. However, these same conditions also present opportunities for a wide range of potential bad actors.</p><p>Openly available knowledge, designs, and software can be misused to enhance existing weapons systems with capabilities like vision-based <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12663" rel="noopener noreferrer" target="_blank">navigation, autonomous targeting, or swarming</a>. Additionally, unless proper security measures are taken, the public nature of open-source code makes it vulnerable to cyberattacks, potentially allowing malicious actors to gain control of robotic systems and cause them to malfunction or be used for <a href="https://www.sciencedirect.com/science/article/pii/S2667305323000625" rel="noopener noreferrer" target="_blank">malevolent purposes</a>. Many ROS users already recognize that they do not invest enough in <a href="https://aliasrobotics.com/files/robot_cybersecurity_review.pdf" rel="noopener noreferrer" target="_blank">cybersecurity</a> for their applications.</p><h2>Guidance Is Necessary</h2><p>Dual-use risks stemming from openness in research and innovation are a concern for many engineering fields. Did you know that engineering was originally a military-only activity? The word “engineer” was coined in the Middle Ages to describe “a designer and constructor of fortifications and weapons.” Some engineering specializations, especially those that include the development of weapons of mass destruction (chemical, biological, radiological, and nuclear), have developed clear guidance, and in some cases, regulations for how research and innovation can be conducted and disseminated. They also have community-driven processes intended to mitigate dual-use risks associated with spreading knowledge. For instance, BioRxiv and MedRxiv—the preprint servers for biology and health sciences—screen submissions for material that poses a biosecurity or health risk before publishing them.</p><p>The field of robotics, in comparison, offers no specific regulation and little guidance as to how roboticists should think of and address the risks associated with openness. Dual-use risk is not taught in most universities, despite it being something that students will likely face in their careers, such as when assessing whether their work is subject to <a href="https://www.sipri.org/publications/2020/policy-reports/responsible-artificial-intelligence-research-and-innovation-international-peace-and-security" rel="noopener noreferrer" target="_blank">export-control regulations on dual-use items</a>.</p><p>As a result, roboticists may not feel they have an incentive or are equipped to evaluate and mitigate the dual-use risks associated with their work. This represents a major problem, as the likelihood of harm associated with the misuse of open robotic research and innovation is likely higher than that of nuclear and biological research, both of which require significantly more resources. Producing “do-it-yourself” robotic weapon systems using open-source design and software and off-the-shelf commercial components is relatively easy and accessible. With this in mind, we think that it’s high time for the robotics community to work toward its own set of sector-specific guidance for how researchers and companies can best navigate the dual-use risks associated with the open diffusion of their work.</p><h2>A Road Map for Responsible Robotics</h2><p>Striking a balance between security and openness is a complex challenge, but one that the robotics community must embrace. We cannot afford to stifle innovation, nor can we ignore the potential for harm. A proactive, multipronged approach is needed to navigate this dual-use dilemma. Drawing lessons from other fields of engineering, we propose a road map focusing on four key areas: education, incentives, moderation, and red lines.</p><h3>Education</h3><p>Integrating responsible research and innovation into robotics education at all levels is paramount. This includes not only dedicated courses but also the <a href="https://journals.uclpress.co.uk/lre/article/id/129/" rel="noopener noreferrer" target="_blank">systematic inclusion</a> of dual-use and cybersecurity considerations within core <a href="https://link.springer.com/article/10.1007/s11948-019-00164-6" rel="noopener noreferrer" target="_blank">robotics curricula</a>. We must foster a culture of responsible innovation so that we can empower roboticists to make informed decisions and proactively address potential risks.</p><p>Educational initiatives could include:</p><ul><li>Developing and disseminating open-source educational materials on responsible robotics for robotics teachers, researchers, and professionals from resources such as the <a href="https://disarmament.unoda.org/responsible-innovation-ai/resources/" rel="noopener noreferrer" target="_blank">United Nations Office for Disarmament Affairs</a> (UNODA) and the <a href="https://airesponsibly.net/education/" rel="noopener noreferrer" target="_blank">Center for Responsible AI</a> at New York University. </li><li>Organizing workshops and seminars on dual-use and ethical considerations at robotics conferences and universities.</li><li>Encouraging universities to offer courses or modules dedicated to <a href="https://journals.sagepub.com/doi/10.1177/20539517231219958" rel="noopener noreferrer" target="_blank">responsible research and innovation in robotics</a>.</li></ul><h3>Incentives</h3><p>Everyone should be encouraged to assess the potential negative consequences of making their work fully or partially open. Funding agencies can mandate risk assessments as a condition for project funding, signaling their importance. Professional organizations, like the <a href="https://www.ieee-ras.org/" rel="noopener noreferrer" target="_blank">IEEE Robotics and Automation Society</a> (RAS), can adopt and promote <a href="https://www.ieee-ras.org/industry-government/standards" rel="noopener noreferrer" target="_blank">best practices</a>, providing tools and frameworks for researchers to identify, assess, and mitigate risks. Such tools could include self-assessment checklists for individual researchers and guidance for how faculties and labs can set up ethical review boards. Academic journals and conferences can make peer-review risk assessments an integral part of the publication process, especially for high-risk applications.</p><p>Additionally, incentives like awards and recognition programs can highlight exemplary contributions to risk assessment and mitigation, fostering a culture of responsibility within the community. Risk assessment can also be encouraged and rewarded in more informal ways. People in leadership positions, such as Ph.D. supervisors and heads of labs, could build ad hoc opportunities for students and researchers to discuss possible risks. They can hold seminars on the topic and provide introductions to external experts and stakeholders like social scientists and experts from NGOs.</p><h3>Moderation</h3><p>The robotics community can implement <a href="https://dl.acm.org/doi/10.1145/3593013.3593981" rel="noopener noreferrer" target="_blank">self-regulation mechanisms</a> to moderate the diffusion of high-risk material. This could involve:</p><ul><li>Screening work prior to publication to prevent the dissemination of content posing serious risks.</li><li>Implementing graduated access controls (“gating”) to certain source code or data on open-source repositories, potentially requiring users to identify themselves and specify their intended use.</li><li>Establishing clear guidelines and community oversight to ensure transparency and prevent misuse of these moderation mechanisms. For example, organizations like RAS could design categories of risk levels for robotics research and applications and create a monitoring committee to track and document real cases of the misuse of robotics research to understand and visualize the scale of the risks and create better mitigation strategies.</li></ul><h3>Red Lines</h3><p>The robotics community should also seek to define and enforce red lines for the development and deployment of robotics technologies. Efforts to define red lines have already been made in that direction, notably in the context of the <a href="https://standards.ieee.org/industry-connections/ec/autonomous-systems/" rel="noopener noreferrer" target="_blank">IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</a>. Companies, including <a href="https://bostondynamics.com/" rel="noopener noreferrer" target="_blank">Boston Dynamics</a>, <a href="https://www.unitree.com/" rel="noopener noreferrer" target="_blank">Unitree</a>, <a href="https://www.agilityrobotics.com/" rel="noopener noreferrer" target="_blank">Agility Robotics</a>, <a href="https://clearpathrobotics.com/" rel="noopener noreferrer" target="_blank">Clearpath Robotics</a>, <a href="https://www.anybotics.com/" rel="noopener noreferrer" target="_blank">ANYbotics</a>, and <a href="https://www.openrobotics.org/" rel="noopener noreferrer" target="_blank">Open Robotics</a> wrote an open letter calling for regulations on the <a href="https://bostondynamics.com/news/general-purpose-robots-should-not-be-weaponized/" rel="noopener noreferrer" target="_blank">weaponization of general-purpose robots</a>. Unfortunately, their efforts were very narrow in scope, and there is a lot of value in further mapping end uses of robotics that should be deemed off-limits or demand extra caution.</p><p>It will absolutely be difficult for the community to agree on standard red lines, because what is considered ethically acceptable or problematic is highly subjective. To support the process, individuals and companies can reflect on what they consider to be unacceptable use of their work. This could result in policies and terms of use that beneficiaries of open research and open-source design software would have to formally agree to (such as specific-use open-source licenses). This would provide a basis for revoking access, denying software updates, and potentially suing or blacklisting people who misuse the technology. Some companies, including Boston Dynamics, have already implemented these measures to some extent. Any person or company conducting open research could replicate this example.</p><p>Openness is the key to innovation and the democratization of many engineering disciplines, including robotics, but it also amplifies the potential for misuse. The engineering community has a responsibility to proactively address the dual-use dilemma. By embracing responsible practices, from education and risk assessment to moderation and red lines, we can foster an ecosystem where openness and security coexist. The challenges are significant, but the stakes are too high to ignore. It is crucial to ensure that research and innovation benefit society globally and do not become a driver of instability in the world. This goal, we believe, aligns with the mission of the IEEE, which is to “advance technology for the benefit of humanity.” The engineering community, especially roboticists, needs to be proactive on these issues to prevent any backlash from society and to preempt potentially counterproductive measures or international regulations that could harm open science.</p>]]></description><pubDate>Tue, 10 Jun 2025 13:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</guid><category>Robotics</category><category>Guest articles</category><category>Dual-use</category><dc:creator>Vincent Boulanin</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&amp;width=980"></media:content></item><item><title>IEEE’s 5 New E-Books Provide On-ramp to Engineering</title><link>https://spectrum.ieee.org/tryengineering-5-new-ebooks</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&width=1245&height=700&coordinates=0%2C141%2C0%2C141"/><br/><br/><p>As the home for IEEE’s preuniversity resources, activities, and hands-on experiences, <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering</a> serves as a hub for educators, parents, and IEEE volunteers to teach school-age children about engineering.</p><p>With support from IEEE partners, TryEngineering has launched a series of <a href="https://tryengineering.org/news/discover-tryengineerings-ebooks/" rel="noopener noreferrer" target="_blank">e-books</a>. Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way. Visually appealing, the books use colorful charts and graphs to grab children’s attention.</p><p>Each of the five English-language publications provides an overview of a technology or topic. The books include stories about <a href="https://spectrum.ieee.org/topic/careers/" target="_self">engineers</a>, technologists, and early pioneers.</p><h2>Engineering disciplines, solutions, and ethics</h2><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2023/" rel="noopener noreferrer" target="_blank"><em><em>Engineers Make the World a Better Place</em></em></a> was created with funding from the <a href="https://www.ieee.org/about/corporate/initiatives/initiatives-committee.html" rel="noopener noreferrer" target="_blank">IEEE New Initiatives Committee</a>. The book introduces students to engineering disciplines and explains how engineers improve society by solving challenging problems, such as<a href="https://tryengineering.org/news/high-school-students-modify-ride-on-cars-for-disabled-children/" rel="noopener noreferrer" target="_blank"> improving access for children with limited physical mobility</a>.</p><p>With support from <a href="https://www.onsemi.com/" rel="noopener noreferrer" target="_blank">Onsemi</a>’s <a href="https://spectrum.ieee.org/ieee-tryengineering-onsemi-grant" target="_self">Giving Now</a> program, IEEE <a href="https://spectrum.ieee.org/topic/semiconductors/" target="_self">semiconductor</a> experts wrote <a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2024/" rel="noopener noreferrer" target="_blank"><em><em>Microchip Adventures: A Journey Into the World of Semiconductors</em></em></a>. It includes an introduction to the field, a list of commonly used terms, an explanation of how chips are made, and an overview of the <a href="https://spectrum.ieee.org/topic/tech-history/" target="_self">technology’s history</a>.</p><p class="pull-quote">Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_signalprocessing_2025/" rel="noopener noreferrer" target="_blank"><em>Wave Wonders: A Signal Processing Journey</em></a> was written with experts from the <a href="https://signalprocessingsociety.org/" rel="noopener noreferrer" target="_blank">IEEE Signal Processing Society</a>. It teaches students how to tell the difference between digital and analog signals. The e-book introduces readers to the inventor of the <a href="https://spectrum.ieee.org/the-first-transatlantic-telegraph-cable-was-a-bold-beautiful-failure" target="_self">telegraph</a>, <a href="https://www.britannica.com/biography/Samuel-F-B-Morse" rel="noopener noreferrer" target="_blank">Samuel Morse</a>. Also included is the <a href="https://tryengineering.org/resource/lesson-plan/electric-messages-then-and-now/" rel="noopener noreferrer" target="_blank">Electric Messages lesson plan</a>, which explains how early telegraphs worked.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_oceanengineering/" target="_blank"><em><em>Ocean Engineering Heroes: Making the Oceans and the World a Better Place</em></em></a> was created in partnership with the <a href="https://ieeeoes.org/" rel="noopener noreferrer" target="_blank">IEEE Oceanic Engineering Society</a>. It includes video interviews with several society leaders about oceans and ways to help keep them clean. It also discusses the impact of pollution including sound pollution from ships and other sources. Included are links to resources from other organizations such as the <a href="https://www.noaa.gov/" rel="noopener noreferrer" target="_blank">National Oceanic and Atmospheric Administration</a>.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_aiadventures/" rel="noopener noreferrer" target="_blank"><em><em>AI Adventures: Exploring the World of Artificial Intelligence</em></em></a> was written with assistance from the <a href="https://www.computer.org/" rel="noopener noreferrer" target="_blank">IEEE Computer Society</a>. The publication describes how <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">AI</a> models work and explains commonly used terms including <em><em>machine learning</em></em> and <em><em>neural networks</em></em>. The book covers the importance of <a href="https://spectrum.ieee.org/ai-ethics-advice" target="_self">ethics when using AI</a>.</p><p>Visit the <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering website</a> for the e-books and many other resources for educators, parents, and volunteers. To help expand the site’s pool of offerings, consider donating to the <a href="https://secure.ieeefoundation.org/site/Donation2?df_id=1720&mfc_pref=T&1720.donation=form1" rel="noopener noreferrer" target="_blank">IEEE TryEngineering Fund</a>.</p>]]></description><pubDate>Mon, 09 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/tryengineering-5-new-ebooks</guid><category>Artificial intelligence</category><category>Education</category><category>Ieee products and services</category><category>Ocean engineering</category><category>Signal processing</category><category>Students</category><category>Type:ti</category><dc:creator>Debra Gulick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&amp;width=980"></media:content></item><item><title>Doctors Could Hack the Nervous System With Ultrasound</title><link>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&width=1245&height=700&coordinates=0%2C927%2C0%2C927"/><br/><br/><p><strong>Inflammation: It’s the body’s</strong> natural response to injury and infection, but medical science now recognizes it as a double-edged sword. When inflammation becomes chronic, it can contribute to a host of serious health problems, including arthritis, heart disease, and certain cancers. As this understanding has grown, so too has the search for effective ways to manage harmful inflammation.</p><p>Doctors and researchers are exploring various approaches to tackle this pervasive health issue, from new medications to dietary interventions. But what if one of the most promising treatments relies on a familiar technology that’s been in hospitals for decades?</p><p>Enter <a href="https://www.sciencedirect.com/science/article/abs/pii/S0165027020301448" target="_blank">focused ultrasound stimulation</a> (FUS), a technique that uses sound waves to reduce inflammation in targeted areas of the body. It’s a surprising new application for ultrasound technology, which most people associate with prenatal checkups or diagnostic imaging. And FUS may help with many other disorders too, including diabetes and obesity. By modifying existing ultrasound technology, we might be able to offer a novel approach to some of today’s most pressing health challenges.</p><p>Our team of biomedical researchers at the <a href="https://feinstein.northwell.edu/institutes-researchers/bioelectronic-medicine" target="_blank">Institute of Bioelectronic Medicine</a> (part of the Feinstein Institutes for Medical Research), in Manhasset, N.Y., has made great strides in learning the electric language of the nervous system. Rather than treating disease with drugs that can have broad side effects throughout the body, we’re learning how to stimulate nerve cells, called neurons, to intervene in a more targeted way. Our goal is to activate or inhibit specific functions within organs.</p><p>The relatively new application of FUS for <a href="https://www.neuromodulation.com/about-neuromodulation" target="_blank">neuromodulation</a>, in which we hypothesize that sound waves activate neurons, may offer a precise and safe way to provide healing treatments for a wide range of both acute and chronic maladies. The treatment doesn’t require surgery and potentially could be used at home with a wearable device. People are accustomed to being prescribing pills for these ailments, but we imagine that one day, the prescriptions could be more like this: “Strap on your ultrasound belt once per day to receive your dose of stimulation.”</p><h2>How Ultrasound Stimulation Works</h2><p><a href="https://spectrum.ieee.org/mems-ultrasound-history" target="_self">Ultrasound</a> is a time-honored medical technology. Researchers began experimenting with ultrasound imaging in the 1940s, bouncing low-energy ultrasonic waves off internal organs to construct medical images, typically using intensities of a few hundred milliwatts per square centimeter of tissue. By the late 1950s, some doctors were using the technique to show expectant parents the <a href="https://en.wikipedia.org/wiki/Obstetric_ultrasonography" target="_blank">developing fetus inside the mother’s uterus</a>. And high-intensity ultrasound waves, which can be millions of milliwatts per square centimeter, have a variety of therapeutic uses, including <a href="https://my.clevelandclinic.org/health/treatments/16541-hifu-high-intensity-focused-ultrasound" target="_blank">destroying tumors</a>.</p><p>The use of low-intensity ultrasound (with intensities similar to that of imaging applications) to alter the activity of the nervous system, however, is relatively unexplored territory. To understand how it works, it’s helpful to compare FUS to the most common form of neuromodulation today, which uses electric current to alter the activity of neurons to treat conditions like Parkinson’s disease. In that technique, electric current increases the voltage inside a neuron, causing it to “fire” and release a neurotransmitter that’s received by connected neurons, which triggers those neurons to fire in turn. For example, the deep brain stimulation used to treat Parkinson’s activates certain neurons to restore healthy patterns of brain activity.</p><h3>How It Works</h3><br/><img alt="Neuron impulse transmission showing ion flow through cell membrane." class="rm-shortcode" data-rm-shortcode-id="7e69502283206bf183e56ebf15b850d4" data-rm-shortcode-name="rebelmouse-image" id="d2e57" loading="lazy" src="https://spectrum.ieee.org/media-library/neuron-impulse-transmission-showing-ion-flow-through-cell-membrane.png?id=60681884&width=980"/><h3></h3><br/><p><strong></strong>In FUS, by contrast, the sound waves’ vibrations interact with the membrane of the neuron, <a href="https://www.nature.com/articles/s41467-022-28040-1" target="_blank">opening channels</a> that allow ions to flow into the cell, thus indirectly changing the cell’s voltage and causing it to fire. One promising use is <a href="https://doi.org/10.1016/j.clinph.2021.12.010" target="_blank">transcranial ultrasound stimulation</a>, which is being tested extensively as a noninvasive way to stimulate the brain and treat neurological and psychiatric diseases.</p><p>We’re interested in FUS’s effect on the peripheral nerves—that is, the nerves outside the brain and spinal cord. We think that activating specific nerves in the abdomen that regulate inflammation or metabolism may help address the root causes of related diseases, rather than just treating the symptoms.</p><h2>FUS for Inflammation</h2><p>Inflammation is something that we know a lot about. Back in 2002, <a href="https://feinstein.northwell.edu/institutes-researchers/our-researchers/kevin-j-tracey-md" target="_blank">Kevin Tracey</a>, currently the president and CEO of the Feinstein Institutes, upset the conventional wisdom that the nervous system and the immune system operate independently and serve distinct roles. He discovered the body’s <a href="https://www.nature.com/articles/nature01321" rel="noopener noreferrer" target="_blank">inflammatory reflex</a>: a two-way neural circuit that sends signals between the brain and body via the vagus nerve and the nerves of the spleen. These nerves control the release of <a href="https://en.wikipedia.org/wiki/Cytokine" rel="noopener noreferrer" target="_blank">cytokines</a>, which are proteins released by immune cells to trigger inflammation. Tracey and colleagues found that stimulating nerves in this neural circuit suppressed the inflammatory response. The discoveries led to the first clinical trials of electrical neuromodulation devices to treat chronic inflammation and launched the field of bioelectronic medicine.</p><h3>Hacking the Immune System</h3><br/><img alt="Ultrasound transducer scans kidney, showing bacteria spreading and evading immune response." class="rm-shortcode" data-rm-shortcode-id="f79f5c1b9ad1a15af4a3d7e98b7b7946" data-rm-shortcode-name="rebelmouse-image" id="7e37e" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-transducer-scans-kidney-showing-bacteria-spreading-and-evading-immune-response.png?id=60559935&width=980"/><h3></h3><br/><p>Tracey has been a pioneer in treating inflammation with <a href="https://spectrum.ieee.org/the-vagus-nerve-a-back-door-for-brain-hacking" target="_self">vagus nerve stimulation</a> (VNS), in which electrical stimulation of the vagus nerve activates neurons in the spleen. In animals and humans, VNS has been shown to reduce harmful inflammation in both chronic diseases such as arthritis and acute conditions such as sepsis. But direct VNS requires surgery to place an implant in the body, which makes it risky for the patient and expensive. That’s why we’ve pursued noninvasive ultrasound stimulation of the spleen.</p><p>Working with Tracey, collaborators at <a href="https://www.gehealthcare.com/" target="_blank">GE Research</a>, and others, we first experimented with rodents to show that ultrasound stimulation of the spleen <a href="https://www.nature.com/articles/s41467-019-08750-9" rel="noopener noreferrer" target="_blank">affects an anti-inflammatory pathway</a>, just as VNS does, and reduces cytokine production as much as a VNS implant does. We then conducted the first-in-human trial of <a href="https://feinstein.northwell.edu/news/the-latest/non-invasive-ultrasound-stimulation-spleen-reduces-inflammation-in-humans-new-study" target="_blank">FUS for controlling inflammation</a>.</p><p>We initially enrolled 60 healthy people, none of whom had signs of chronic inflammation. To test the effect of a 3-minute ultrasound treatment, we were measuring the amount of a molecule called <a href="https://en.wikipedia.org/wiki/Tumor_necrosis_factor" rel="noopener noreferrer" target="_blank">tumor necrosis factor</a> (TNF), which is a biomarker of inflammation that’s released when white blood cells go into action against a perceived pathogen. At the beginning of the study, 40 people received focused ultrasound stimulation, while 20 others, serving as the control group, simply had their spleens imaged by ultrasound. Yet, when we looked at the early data, <em>everyone</em> had lower levels of TNF, even the control group. It seemed that even imaging with ultrasound for a few minutes had a moderate anti-inflammatory effect! To get a proper control group, we had to recruit 10 more people for the study and devise a different sham experiment, this time unplugging the ultrasound machine.</p><h3></h3><br/><img alt="Abstract collage with neuron, brain textures, and dynamic wave patterns in pastel colors." class="rm-shortcode" data-rm-shortcode-id="2ae16b235445ec02f72e63b3c909cc7a" data-rm-shortcode-name="rebelmouse-image" id="f166b" loading="lazy" src="https://spectrum.ieee.org/media-library/abstract-collage-with-neuron-brain-textures-and-dynamic-wave-patterns-in-pastel-colors.jpg?id=60560952&width=980"/><h3></h3><br/><p>After the subjects received either the real or sham stimulation, we took blood samples from all of them. We next simulated an infection by adding a bacterial toxin to the blood in the test tubes, then measured the amount of TNF released by the white blood cells to fight the toxin. The results, which we published in the journal <em>Brain Stimulation</em> in 2023, showed that people who had received FUS treatments <a href="https://www.sciencedirect.com/science/article/pii/S1935861X23017436" target="_blank">had lower levels of TNF</a> than the true control group. We saw no problematic side effects of the ultrasound: The treatment didn’t adversely affect heart rate, blood pressure, or the many other biomarkers that we checked.</p><p>The results also showed that when we repeated the blood draw and experiment 24 hours later, the treatment groups’ TNF levels had returned to baseline. This finding suggests that if FUS becomes a treatment option for inflammatory diseases, people might require regular, perhaps even daily, treatments.</p><p>One surprising result was that it didn’t seem to matter which location within the spleen we targeted—all the locations we tried produced similar results. Our hypothesis is that hitting any target within the spleen activates enough nerves to produce the beneficial effect. What’s more, it didn’t matter which energy intensity we used. We tried intensities ranging from about 10 to 200 mW per cm<sup>2</sup>, well within the range of intensities used in ultrasound imaging; remarkably, even the lowest intensity level caused subjects’ TNF levels to drop.</p><p>Our big takeaway from that first-in-human study was that targeting the spleen with FUS is not just a feasible treatment but could be a gamechanger for inflammatory diseases. Our next steps are to investigate the mechanisms by which FUS affects the inflammatory response, and to conduct more animal and human studies to see whether prolonged administration of FUS to the spleen can treat chronic inflammatory diseases.</p><h2>FUS for Obesity and Diabetes</h2><p>For much of our research on FUS, we’ve partnered with GE Research, whose parent company is one of the world’s leading makers of ultrasound equipment. One of our first projects together explored the potential of FUS as a treatment for the widespread inflammation that often accompanies obesity, a condition that now affects about <a href="https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight" target="_blank">890 million people</a> around the world. In this study, we fed lab mice a high-calorie and high-fat “Western diet” for eight weeks. During the following eight weeks, half of them received ultrasound stimulation while the other half received daily sham stimulation. We found that the <a href="https://feinstein.northwell.edu/news/the-latest/feinstein-institutes-and-ge-research-demonstrate-ultrasound-stimulation-reduces-obesity" target="_blank">mice that received FUS had lower levels of cytokines</a>—and to our surprise, those mice also ate less and lost weight.</p><p>In related work with our GE colleagues, we examined the potential of FUS as a <a href="https://www.northwell.edu/news/the-latest/research-team-treats-diabetes-using-ultrasound" rel="noopener noreferrer" target="_blank">treatment for diabetes</a>, which now affects <a href="https://www.who.int/news-room/fact-sheets/detail/diabetes" rel="noopener noreferrer" target="_blank">830 million people</a> around the world. In a healthy human body, the liver stores glucose as a reserve and releases it only when it registers that glucose levels in the bloodstream have dropped. But in people with diabetes, this sensing system is dysfunctional, and the liver releases glucose even when blood levels are already high, causing a host of health problems.</p><h3>Hacking the Metabolic System</h3><br/><img alt="Ultrasound scan diagram showing brain-liver connection through neural pathways." class="rm-shortcode" data-rm-shortcode-id="1c7dde5728bf4fc1d4f0d8460e4cde13" data-rm-shortcode-name="rebelmouse-image" id="257de" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-scan-diagram-showing-brain-liver-connection-through-neural-pathways.png?id=60681922&width=980"/><p><span>For diabetes, our ultrasound target was the network of nerves that transmit signals between the liver and the brain: specifically, glucose-sensing neurons in the </span><a href="https://en.wikipedia.org/wiki/Porta_hepatis" target="_blank">porta hepatis</a><span>, which is essentially the gateway to the liver. We gave diabetic rats 3-minute daily ultrasound stimulation over a period of 40 days. Within just a few days, the treatment <a href="https://www.nature.com/articles/s41551-022-00870-w" target="_blank">brought down the rats’ glucose levels</a> from dangerously high to normal range. We got similar results in mice and pigs, and </span>published these exciting results<span> in 2022 in </span><em>Nature Biomedical Engineering</em><span>.</span></p><p>Those diabetes experiments shed some light on why ultrasound had this effect. We decided to zero in on a brain region called the <a href="https://en.wikipedia.org/wiki/Hypothalamus" target="_blank">hypothalamus</a>, which controls many crucial automatic body functions, including metabolism, circadian rhythms, and body temperature. Our colleagues at GE Research started investigating by blocking the nerve signals that travel from the liver to the hypothalamus in two different ways—both cutting the nerves physically and using a local anesthetic. When we then applied FUS, we didn’t see the beneficial decrease in glucose levels. This result suggests that the ultrasound treatment works by changing glucose-sensing signals that travel from the liver to the brain—which in turn changes the commands the hypothalamus issues to the metabolic systems of the body, essentially telling them to lower glucose levels.</p><p>The next steps in this research involve both technical development and clinical testing. Currently, administering FUS requires technical expertise, with a sonographer looking at ultrasound images, locating the target, and triggering the stimulation. But if FUS is to become a practical treatment for a chronic disease, we’ll need to make it usable by anyone and available as an at-home system. That could be a wearable device that uses ultrasound imaging to automatically locate the anatomical target and then delivers the FUS dose: All the patient would have to do is put on the device and turn it on. But before we get to that point, FUS treatment will have to be tested clinically in randomized controlled trials for people with obesity and diabetes. GE HealthCare recently <a href="https://www.gehealthcare.com/about/newsroom/press-releases/ge-healthcare-and-novo-nordisk-to-collaborate-to-advance-novel-non-invasive-treatment-for-type-2-diabetes-and-obesity-with-ultrasound" target="_blank">partnered</a> with Novo Nordisk to work on the clinical and product development of FUS in these areas.</p><h2>FUS for Cardiopulmonary Diseases</h2><p>FUS may also help with chronic cardiovascular diseases, many of which are associated with immune dysfunction and inflammation. We began with a disorder called <a href="https://en.wikipedia.org/wiki/Pulmonary_hypertension" target="_blank">pulmonary arterial hypertension</a>, a rare but incurable disease in which blood pressure increases in the arteries within the lungs. At the start of our research, it wasn’t clear whether inflammation around the pulmonary arteries was a cause or a by-product of the disease, and whether targeting inflammation was a viable treatment. Our group was the first to try FUS of the spleen in order to reduce the inflammation associated <a href="https://feinstein.northwell.edu/feinstein-institutes-ultrasound-neuromodulation-for-pulmonary-hypertension" target="_blank">with pulmonary hypertension</a> in rats.</p><p>The results, published last year, were very encouraging. We found that 12-minute FUS sessions <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCRESAHA.123.323679" target="_blank">reduced pulmonary pressure</a>, improved heart function, and reduced lung inflammation in the animals in the experimental group (as compared to animals that received sham stimulation). What’s more, in the animals that received FUS, the progression of the disease slowed significantly even after the experiment ended, suggesting that this treatment could provide a lasting effect.</p><p class="pull-quote">One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation.</p><p>This study was, to our knowledge, the first to successfully demonstrate an ultrasound-based therapy for any cardiopulmonary disease. And we’re eager to build on it. We’re next interested in studying whether FUS can help with congestive heart failure, a condition in which the heart can’t pump enough blood to meet the body’s needs. In the United States alone, more than <a href="https://www.thecardiologyadvisor.com/ddi/heart-failure-in-the-united-states/" target="_blank">6 million people</a> are living with heart failure, and that number could surpass 8 million by 2030. We know that inflammation plays a significant role in heart failure by damaging the heart’s muscle cells and reducing their elasticity. We plan to test FUS of the spleen in mice with the condition. If those tests are successful, we could move toward clinical testing in humans.</p><h2>The Future of Ultrasound Stimulation</h2><p>We have one huge advantage as we think about how to bring these results from the lab to the clinic: The basic hardware for ultrasound already exists, it’s already FDA approved, and it has a stellar safety record through decades of use. Our collaborators at GE have already experimented with modifying the typical ultrasound devices used for imaging so that they can be used for FUS treatments.</p><p>Once we get to the point of optimizing FUS for clinical use, we’ll have to determine the best neuromodulation parameters. For instance, what are the right acoustic wavelengths and frequencies? Ultrasound imaging typically uses higher frequencies than FUS does, but human tissue absorbs more acoustic energy at higher frequencies than it does at lower frequencies. So to deliver a good dose of FUS, researchers are exploring a wide range of frequencies. We’ll also have to think about how long to transmit that ultrasound energy to make up a single pulse, what rate of pulses to use, and how long the treatment should be.</p><p>In addition, we need to determine how long the beneficial effect of the treatment lasts. For some of the ailments that researchers are exploring, like <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8261174/" target="_blank">FUS of the brain to treat chronic pain</a>, a patient might be able to go to the doctor’s office once every three months for a dose. But for diseases associated with inflammation, a regular, several-times-per-week regimen might prove most effective, which would require at-home treatments.</p><p>For home use to be possible, the wearable device would have to locate the targets automatically via ultrasound imaging. As vast databases already exist of human ultrasound images from the liver, spleen, and other organs, it seems feasible to train a machine-learning algorithm to detect targets automatically and in real time. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. A few startups are working on building such wearable devices, which could take the form of a belt or a vest. For example, the company <a href="https://www.secondwaveus.com/" target="_blank">SecondWave Systems</a>, which has partnered with the University of Minnesota, in Minneapolis, has already conducted a small <a href="https://www.businesswire.com/news/home/20240501993182/en/SecondWave-Systems-Demonstrates-Reduction-in-Disease-Activity-in-Clinical-Study-Evaluating-Novel-Ultrasound-Based-Anti-Inflammatory-Therapy-in-Rheumatoid-Arthritis" target="_blank">pilot study</a> of its wearable device, trying it out on 13 people with rheumatoid arthritis and seeing positive outcomes.</p><p>While it will be many years before FUS treatments are approved for clinical use, and likely still more years for wearable devices to be proven safe enough for home use, the path forward looks very promising. We believe that FUS and other forms of bioelectronic medicine offer a new paradigm for human health, one in which we reduce our reliance on pharmaceuticals and begin to speak directly to the body electric.</p>]]></description><pubDate>Mon, 09 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</guid><category>Ultrasound</category><category>Neuromodulation</category><category>Neural stimulation</category><category>Diabetes</category><category>Arthritis</category><dc:creator>Sangeeta S. Chavan</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&amp;width=980"></media:content></item><item><title>Intel Upgrades Chip Packaging for Bigger AI</title><link>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&width=1245&height=700&coordinates=0%2C117%2C0%2C117"/><br/><br/><p><span>This week at the <a href="https://ectc.net/" target="_blank">IEEE Electronic Components and Packaging Technology Conference</a>, <a href="https://www.intel.com/" target="_blank">Intel</a> unveiled that it is developing new chip-packaging technology that will allow for bigger processors for AI.</span></p><p>With Moore’s Law slowing down, makers of advanced GPUs and other data-center chips are having to add more silicon area to their products to keep up with the relentless rise of AI’s computing needs. But the maximum size of a single silicon chip is fixed at around 800 square millimeters (with <a href="https://spectrum.ieee.org/tag/cerebras" target="_blank">one exception</a>), so manufacturers have had to turn to <a href="https://spectrum.ieee.org/tag/advanced-packaging" target="_blank">advanced packaging technologies</a> that integrate multiple pieces of silicon in a way that lets them act like a single chip.</p><p>Three of the innovations Intel unveiled at ECTC were aimed at tackling limitations in just how much silicon you can squeeze into a single package and how big that package can be. They include improvements to the technology Intel uses to link adjacent silicon dies together, a more-accurate method for bonding silicon to the package substrate, and a system to expand the size of a critical part of the package that removes heat. Together, the technologies enable the integration of more than 10,000 square millimeters of silicon within a package that can be bigger than 21,000 mm<sup>2</sup>—a massive area about the size of four and a half credit cards.</p><h2>EMIB gets a 3D upgrade</h2><p>One of the limitations on how much silicon can fit in a single package has to do with connecting a large number of silicon dies at their edges. Using an organic polymer package substrate to interconnect the silicon dies is the most affordable option, but a silicon substrate allows you to make more dense connections at these edges.</p><p>Intel’s solution, introduced more than five years ago, is to embed a small sliver of silicon in the organic package beneath the adjoining edges of the silicon dies. That sliver of silicon, called EMIB, is etched with fine interconnects that increase the density of connections beyond what the organic substrate can handle.</p><p>At ECTC, Intel unveiled the latest twist on the EMIB technology, called EMIB-T. In addition to the usual fine horizontal interconnects, EMIB-T provides relatively thick vertical copper connections called through-silicon vias, or TSVs. The TSVs allow power from the circuit board below to directly connect to the chips above instead of having to route around the EMIB, reducing power lost by a longer journey. Additionally, EMIB-T contains a copper grid that acts as a ground plane to reduce noise in the power delivered due to process cores and other circuits suddenly ramping up their workloads.</p><p>“It sounds simple, but this is a technology that brings a lot of capability to us,” says Rahul Manepalli, vice president of substrate packaging technology at Intel. With it and the other technologies Intel described, a customer could connect silicon equivalent to more than 12 full-size silicon dies—10,000 mm<sup>2</sup> of silicon—in a single package using 38 or more EMIB-T bridges.</p><h2>Thermal control</h2><p>Another technology Intel reported at ECTC that helps increase the size of packages is low-thermal-gradient thermal compression bonding. It’s a variant of the technology used today to attach silicon dies to organic substrates. Micrometer-scale bumps of solder are positioned on the substrate where they will connect to a silicon die. The die is then heated and pressed onto the microbumps, melting them and connecting the package’s interconnects to the silicon’s.</p><p>Because the silicon and the substrate expand at different rates when heated, engineers have to limit the inter-bump distance, or pitch. Additionally, the expansion difference makes it difficult to reliably make very large substrates full of lots of silicon dies, which is the direction AI processors need to go.</p><p>The new Intel tech makes the thermal expansion mismatch more predictable and manageable, says Manepalli. The result is that very large substrates can be populated with dies. Alternatively, the same technology can be used to increase the density of connections to EMIB down to about one every 25 micrometers.</p><h2>A flatter heat spreader</h2><p>These bigger silicon assemblages will generate even more heat than today’s systems. So it’s critical that the heat’s pathway out of the silicon isn’t obstructed. An integrated piece of metal called a heat spreader is key to that, but making one big enough for these large packages is difficult. The package substrate can warp, and the metal heat spreader itself might not stay perfectly flat, so it might not touch the tops of the hot dies it’s supposed to be sucking the heat from. Intel’s solution was to assemble the integrated heat spreader in parts rather than as one piece. This allowed the company to add extra stiffening components, among other things, to keep everything flat and in place.</p><p> “Keeping it flat at higher temperatures is a big benefit for reliability and yield,” says Manepalli.</p><p>Intel says the technologies are still in the R&D stage and would not comment on when these technologies would debut commercially. However, they will likely have to arrive in the next few years for the Intel foundry to compete with <a href="https://spectrum.ieee.org/tsmc-advanced-packaging" target="_blank">TSMC’s planned packaging expansion</a>.</p>]]></description><pubDate>Sun, 08 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</guid><category>Chip packaging</category><category>Intel</category><category>Thermal control</category><category>Noise reduction</category><category>Interconnects</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&amp;width=980"></media:content></item><item><title>Video Friday: Hopping on One Robotic Leg</title><link>https://spectrum.ieee.org/video-friday-one-legged-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&width=1245&height=700&coordinates=69%2C0%2C69%2C0"/><br/><br/><p>
<span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span>
</p><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>
	Enjoy today’s videos!
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="fnzdkxl-jj0">
	This single-leg robot is designed to “form a foundation for future bipedal robot development,” but personally, I think it’s perfect as is.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="e263fb0233d0bb0d075d93a40d651be2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/FNzdKXl-jj0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://dynamicrobot.kaist.ac.kr/">KAIST Dynamic Robot Control and Design Lab</a> ]
</p><div class="horizontal-rule">
</div><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="8a6d56b1cad95583679b96d5194dd022" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wzYtsJwYfTM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Selling 17,000 
	<a data-linked-post="2655919083" href="https://spectrum.ieee.org/social-robots-children" target="_blank">social robots</a> still amazes me. <a data-linked-post="2650251656" href="https://spectrum.ieee.org/aldebaran-robotics-seeking-betatesters-for-its-nao-humanoid-robot" target="_blank">Aldebaran</a> will be missed.
</p><p>
	[ 
	<a href="https://aldebaran.com/en/">Aldebaran</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="udti_d_vif0">
	Nice to see some actual challenging shoves as part of biped testing.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="397e23922e40f8dda09c9558813c3604" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UdtI_D_vIF0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.ucr.bot/">Under Control Robotics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="j5cfeee5pyi">
<em>Ground Control made multilegged waves at IEEE’s International Conference on Robotics and Automation 2025 in Atlanta! We competed in the Startup Pitch Competition and demoed our robot at our booth, on NIST standard terrain, and around the convention. We were proud to be a finalist for Best Expo Demo and participate in the Robot Parade.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="f805b79697328de135747f04c5a7dac1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/J5cfeEe5pyI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://groundcontrolrobotics.com/">Ground Control Robotics</a> ]
</p><p>
	Thanks, Dan!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="agrtswo4snw">
<em>Humanoid is a U.K.-based robotics innovation company dedicated to building commercially scalable, reliable and safe robotic solutions for real-world applications.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c6f2dda46adfe06e68b0b4b335ec3291" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AgrTSWO4Snw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	It’s a nifty bootup screen, I’ll give them that.
</p><p>
	[ 
	<a href="https://thehumanoid.ai/product/">Humanoid</a> ]
</p><p>
	Thanks, Kristina!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="plm9gaq1jxo">
<em>Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they remain limited in performing object interactions that require sustained contact. In this work, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting mechanisms to maintain stability.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="5209d97768c506bd070b00ce7aa8e8b2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/pLm9gaQ1JXo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://linchangyi1.github.io/LocoTouch/">LocoTouch paper</a> ]
</p><p>
	Thanks, Changyi!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="2lg-4mdx210">
<em>In this video, Digit is performing tasks autonomously using a whole-body controller for mobile manipulation. This new controller was trained in simulation, enabling Digit to execute tasks while navigating new environments and manipulating objects it has never encountered before.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="4d2772b70353c22ada366d8040940a1a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2lG-4mdx210?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Not bad, although it’s worth pointing out that those shelves are not representative of any market I’ve ever been to.
</p><p>
	[ 
	<a href="https://www.agilityrobotics.com/">Agility Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="xwmwmhrt-fs">
	It’s always cool to see robots presented as an incidental solution to a problem as opposed to, you know, robots.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="686a7d77fbda850290710efc6140a527" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xWmWmhRt-fs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	The question that you really want answered, though, is “Why is there water on the floor?”
</p><p>
	[ 
	<a href="https://bostondynamics.com/products/orbit/">Boston Dynamics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="gqidyj-akaa">
<em>Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. We propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="daa86fab0c3d3f61cc1ab142a8056ca3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/gQidYj-AKaA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://la.disneyresearch.com/publication/amor-adaptive-character-control-through-multi-objective-reinforcement-learning/">Disney Research</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="igyjdvu2tc0">
	It’s been a week since ICRA 2025, and TRON 1 already misses all the new friends it made!
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="891a8f3fed5dda5103e1a2056cef57e4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iGyJdVu2tc0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.limxdynamics.com/en">LimX Dynamics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hjpps5vcftg">
	ROB 450 in Winter 2025 challenged students to synthesize the knowledge acquired through their Robotics undergraduate courses at the University of Michigan to use a systematic and iterative design and analysis process and apply it to solving a real open-ended Robotics problem.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="cb4df45971f7989fef2eafcf4708c497" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hjPPS5vcFtg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://robotics.umich.edu/">University of Michigan Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hh7fh5ys82q">
	What’s the Trick? A talk on human vs. current robot learning, given by Chris Atkeson at the Robotics and AI Institute.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="ad0b49c258ded8012bd36ea093692f33" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hh7Fh5YS82Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://rai-inst.com/">Robotics and AI Institute (RAI)</a> ]
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Fri, 06 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-one-legged-robot</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><category>Aldebaran robotics</category><category>Reinforcement learning</category><category>Quadruped robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&amp;width=980"></media:content></item><item><title>Getting Past Procastination</title><link>https://spectrum.ieee.org/getting-past-procastination</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1245&height=700&coordinates=0%2C232%2C0%2C233"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p>Across a decade working at hypergrowth tech companies like Meta and Pinterest, I constantly struggled with procrastination. I’d be assigned an important project, but I simply couldn’t get myself to get started. The source of my distraction varied—I would constantly check my email, read random documentation, or even scroll through my social feeds. But the result was the same: I felt a deep sense of dread that I was not making progress on the things that mattered.</p><p>At the end of the day, time is the only resource that matters. With every minute, you are making a decision about how to spend your life. Most of the ways people spend their time are ineffective. Especially in the tech world, our tasks and tools are constantly changing, so we must be able to adapt. What separates the best engineers from the rest of the pack is that they create systems that allow them to be consistently productive.</p><p>Here’s the core idea that changed my perspective on productivity: <strong>Action leads to motivation</strong>, not the other way around. You should not check your email or scroll Instagram while you wait for motivation to “hit you.” Instead, just start doing something, anything, that makes progress toward your goal, and you’ll find that motivation will follow.</p><p>For example, if I have a high-priority, complex bug-fixing challenge at work, my approach is to decompose the problem into something much simpler. <em>Could I simply</em> add a log statement that prints the value of a relevant variable? My goal at this point is not to solve the bug, it’s simply to take a tiny step forward.</p><p>This creates a powerful flywheel: you’re productive → you feel good → you’re more productive.</p><p>Unfortunately, many engineers are stuck in the opposite flywheel, a downward spiral of procrastination: you’re unproductive → you feel bad → you’re unproductive.</p><p>The idea that motivation follows naturally from progress lets us lower the activation energy needed to enter the upward spiral. Author and motivational speaker Tony Robbins talks about a related concept that “motion creates emotion.” The actions we take, and even the way we move our body, affect how we feel. Once you realize that you can control your motivation, you can achieve stress-free productivity.</p><p>—Rahul</p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank">Overcoming Tech Workforce Shortages With IEEE Microcredentials</a></h2><p><span>A shortage of technical workers is coming. Currently, most of these roles require university degrees, but specialized training through focused, skills-based microcredential courses could provide an alternative and expand the workforce. IEEE’s microcredentials program offers credentials that focus on the skills needed to become a technician, electrician, or programmer, regardless of educational background.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="How Software Engineers Actually Use AI">How Software Engineers Actually Use AI</a></h2><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here."></a><span>Amidst conflicting accounts of how programmers use AI on the job, Wired surveyed 730 coders to get more clarity—then used ChatGPT to comb through the data, with plenty of help from human editors and fact-checkers. The survey asked coders how much they use AI, their outlook on the technology, and how it has changed their jobs, among other questions.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Profile: A Knee Injury Launched This VR Pioneer’s Career">Profile: A Knee Injury Launched This VR Pioneer’s Career</a></h2><p>Unlike many engineers, Carolina Cruz-Neira had little interest in technology as a child. Instead, she dreamed of becoming a professional ballerina. But when an injury forced her to pivot, Cruz-Neira found success in computer science, eventually blending her interests in art and science as a pioneer in virtual reality. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p>]]></description><pubDate>Thu, 05 Jun 2025 19:32:12 +0000</pubDate><guid>https://spectrum.ieee.org/getting-past-procastination</guid><category>Career development</category><category>Careers</category><category>Practical strategies</category><category>Tech careers</category><category>Careers newsletter</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>IEEE Honors Engineering Visionaries at Annual Summit</title><link>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&width=1245&height=700&coordinates=0%2C173%2C0%2C174"/><br/><br/><p>I attended this year’s <a href="https://corporate-awards.ieee.org/event/vic-summit-honors-ceremony-gala/" rel="noopener noreferrer" target="_blank">IEEE Vision, Innovation, and Challenges Summit and Honors Ceremony</a> on 23 and 24 April at the <a href="https://www.hilton.com/en/hotels/tyotohi-hilton-tokyo-odaiba/" rel="noopener noreferrer" target="_blank">Hilton Tokyo Odaiba</a> hotel. The event celebrates <a href="https://spectrum.ieee.org/ieee-celebrates-engineering-brilliance" target="_self">pioneers</a> in engineering who have developed technology that changes the way people connect and learn about the world. This year’s celebrants included the engineers behind the <a href="https://spectrum.ieee.org/henry-samueli-1999" target="_self">first digital cable set-top box modem chipset</a> and the <a href="https://science.nasa.gov/mission/webb/" rel="noopener noreferrer" target="_blank">James Webb Space Telescope</a>.</p><p>The event included the inaugural <a href="https://corporate-awards.ieee.org/yp_laureate_forum/" rel="noopener noreferrer" target="_blank">IEEE Distinguished Young Professionals and Laureate Forum</a>. Fifty young professionals attended the networking event with IEEE leaders, <a href="https://corporate-awards.ieee.org/recipients/ieee-medal-of-honor-recipients/" rel="noopener noreferrer" target="_blank">IEEE Medal of Honor</a> laureates, and award recipients.</p><p>Here are highlights of the <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-vic-summit-full-stream" rel="noopener noreferrer" target="_blank">sessions</a>, which are available to watch in full on <a href="https://ieeetv.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE.tv</a>.</p><h2>Networking opportunities for young professionals</h2><p>Before the VIC summit got underway on 23 April, the networking forum took place that morning. </p><p>In her speech welcoming the attendees, <a href="https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead" target="_self">Sophia Muirhead</a>, IEEE executive director and chief operating officer, encouraged the young professionals to engage in IEEE’s mission of developing technology for the benefit of humanity.</p><p>The participants heard from 2020 IEEE President <a href="https://ethw.org/Toshio_Fukuda" rel="noopener noreferrer" target="_blank">Toshio Fukuda</a> and award recipient <a href="https://sg.linkedin.com/in/aishbandla" rel="noopener noreferrer" target="_blank">Aishwarya Bandla</a> about their careers and volunteer work. Bandla received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-theodore-w-hissey-outstanding-young-professional-award/" rel="noopener noreferrer" target="_blank">IEEE Theodore W. Hissey Young Professionals Award</a> for her “leadership in patient-centric health technology innovation and inspiring IEEE young professionals to drive meaningful change.” The award is sponsored by the <a href="https://ieeephotonics.org/" rel="noopener noreferrer" target="_blank">IEEE Photonics</a> and <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power & Energy</a> societies, as well as <a href="https://yp.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Young Professionals</a>.</p><p>She is an IEEE senior member and the clinical innovation manager at <a href="https://paxmanscalpcooling.com/" rel="noopener noreferrer" target="_blank">Paxman</a>, a medical equipment manufacturer headquartered in Huddersfield, England. She is developing a wearable device that cools a person’s limbs. The Paxman limb “cryocompression” system helps prevent nerve damage associated with certain types of intravenous chemotherapy.</p><p>As someone who follows the Japanese concept of <em><em>ikigai</em></em>—a sense of purpose—Bandla said her “passion and profession intersected not at technology in the lab but at bringing technology to the people.”</p><p>She shows similar passion in her role as chair of <a href="https://yp.ieeer10.org/" rel="noopener noreferrer" target="_blank">IEEE Region 10’s Young Professionals group</a>. Encouraging attendees to become active in the organization, she said IEEE has given her a purpose and the opportunity to give back to the community.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A large group of business professionals posing together for a photo. A screen in the background reads, \u201cIEEE Vision and Innovation Challenges Summit\u201d." class="rm-shortcode" data-rm-shortcode-id="393c0426f1fd5e1752af1c9775d7caf9" data-rm-shortcode-name="rebelmouse-image" id="13537" loading="lazy" src="https://spectrum.ieee.org/media-library/a-large-group-of-business-professionals-posing-together-for-a-photo-a-screen-in-the-background-reads-u201cieee-vision-and-inn.jpg?id=60451480&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Fifty yong professionals attended the inaugural IEEE Distinguished Young Professionals and Laureate Forum.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Junko Kimura; Tomohiro Ohsumi</small></p><p>Attendees were surprised by a guest speaker whose name is well known outside of the technology and engineering fields: <a href="https://woz.org/" target="_blank">Steve Wozniak</a>. </p><p>The summit’s emcee <a href="https://www.miho-japanesevo.com/" rel="noopener noreferrer" target="_blank">Miho Noguchi</a> interviewed the IEEE Fellow and <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> cofounder about what inspired him to pursue a career in engineering and launch a startup. Noguchi, a former Japanese radio broadcaster, is the voice for <a href="https://maps.google.com/" rel="noopener noreferrer" target="_blank">Google Maps</a> navigation in Japanese.</p><p>Wozniak said he was inspired by his father, who was an engineer at <a href="https://www.lockheedmartin.com/en-us/index.html" rel="noopener noreferrer" target="_blank">Lockheed Martin</a> in Sunnyvale, Calif.</p><p>“I visited the company several times and watched him work and started asking him what engineers did,” he said. His father told him engineers build things that make life easier for people.</p><p>When asked what career advice he would give to young professionals, Wozniak said, “Always be a good person. Even if someone is bad to you, always be good to them.”</p><p>Reflecting on the benefits of IEEE membership, he said being an IEEE Fellow is the most important honor he has received.</p><p>The forum concluded with a networking opportunity. Each table was given a set of questions to break the ice. Attendees paired up and were given 10 minutes to ask each other about their schooling, work experiences, and career aspirations. When the time was up, they switched partners.</p><p>Several young professionals I interviewed about their experience said they enjoyed the event. One said she really liked learning about where everyone came from, their work, and their passions. All said they were in awe that they got the opportunity to see and hear Wozniak. </p><h2>Governing AI and a telescope’s foray into the past</h2><p>The summit featured a “fireside chat” with <a href="https://spectrum.ieee.org/henry-samueli-moh" target="_self">Henry Samueli</a>, this year’s <a href="https://spectrum.ieee.org/ieee-medal-of-honor-2025" target="_self">IEEE Medal of Honor</a> recipient for his “pioneering research and commercialization of <a href="https://spectrum.ieee.org/tag/broadband" target="_self">broadband</a> communication and networking technologies” and his promotion of STEM education. He is the first recipient of the award since its <a href="https://spectrum.ieee.org/ieee-medal-of-honor" target="_self">monetary prize was increased to US $2 million from $50,000</a>.</p><p><a href="https://spectrum.ieee.org/u/glenn-zorpette" target="_self">Glenn Zorpette</a>, <a href="https://spectrum.ieee.org/navigating-the-dual-use-dilemma" target="_self"><em><em>IEEE Spectrum</em></em></a>’s editorial director for content development, interviewed Samueli, who reminisced about working in his parents’ liquor/grocery store in Los Angeles as a teenager, where he stocked shelves, operated the cash register, and helped out with the bookkeeping. He told Zorpette that his parents inspired him to become an entrepreneur and that a hands-on project in a seventh-grade shop class prompted him to become an engineer.</p><p>Samueli helped to found <a href="https://spectrum.ieee.org/tag/broadcom" target="_self">Broadcom</a> in 1991 in San Jose, Calif. The company developed the first digital cable set-top box modem chipset, which served as the cable signal receiver. Today he is chairman of the company’s board.</p><p>When asked about the future of broadband, he said the application of existing technology is more important than its advancement. He added that he’s excited to see what the future will bring.</p><p>An audience member asked him what <a href="https://spectrum.ieee.org/henry-samueli-advice" target="_self">advice he would give</a> to engineers in developing countries.</p><p>“Take it one step at a time and let [your career] unfold how it is meant to,” he said.</p><p>The conversation was followed by keynote speeches and panel discussions with award recipients on topics including <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">artificial intelligence</a> and <a href="https://spectrum.ieee.org/topic/aerospace/" target="_self">space exploration</a>. </p><p class="pull-quote">“[The IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies] represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”<strong> </strong><span><strong>— Frederick A. Kish Jr</strong></span></p><p><span></span>During a presentation on artificial intelligence, <a href="https://unu.edu/about/staff/tshilidzi-marwala" target="_blank">Tshilidzi Marwala</a>, rector of the <a href="https://unu.edu/" target="_blank">United Nations University</a> in Tokyo, led a deep dive into how lawmakers can create policies to govern AI use.</p><p>While AI is already being used by <a href="https://google.com/" target="_blank">Google</a>, <a href="https://www.microsoft.com/" target="_blank">Microsoft</a>, and <a href="https://twitter.com/" target="_blank">X</a>, as well as students and professionals in different fields, Marwala noted there are still many concerns surrounding the technology, especially when it comes to safety and information accuracy.</p><p>He stressed the importance of international collaboration, and he called for lawmakers to involve technologists when creating policy.</p><p>AI is complicated, he pointed out and “needs consistency when it comes to writing rules for its use.”</p><p>AI might be the future, but innovations such as the <a href="https://spectrum.ieee.org/rogue-planet" target="_self">James Webb Space Telescope</a> (JWST) are helping people understand the past.</p><p>The telescope, which <a href="https://spectrum.ieee.org/at-last-first-light-for-the-james-webb-space-telescope" target="_self">gathers images of stars and galaxies created soon after the big bang</a>, took 20 years to develop and build. Its development was led by <a href="https://corporate-awards.ieee.org/speaker/bill-ochs/" rel="noopener noreferrer" target="_blank">Bill Ochs</a>, <a href="https://science.nasa.gov/people/webb-people-mike-menzel/" rel="noopener noreferrer" target="_blank">Mike Menzel</a>, and <a href="https://corporate-awards.ieee.org/speaker/scott-willoughby/" rel="noopener noreferrer" target="_blank">Scott Willoughby</a> at NASA’s <a href="https://www.nasa.gov/goddard/" rel="noopener noreferrer" target="_blank">Goddard Space Flight Center</a> in Greenbelt, Md. For their work, they received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-simon-ramo-medal/" rel="noopener noreferrer" target="_blank">IEEE Simon Ramo Medal</a>, sponsored by <a href="https://www.northropgrumman.com/" rel="noopener noreferrer" target="_blank">Northrop Grumman</a>.</p><p>Ochs, who was a project manager at the flight center during its development, is now the principal engineer at <a href="https://www.fts-intl.com/" rel="noopener noreferrer" target="_blank">FTS International</a>, in Chantilly, Va. Menzel is the telescope’s mission systems engineer, and Willoughby is vice president and project manager at the flight center.</p><p>In a panel session moderated by Noguchi, the three talked about the challenges they faced during the JWST spacecraft’s development and launch. </p><p>One hurdle was the inability to test the telescope’s flight capabilities before launch, Ochs said. The telescope was built to orbit the sun, and it isn’t possible to simulate that environment on Earth. Therefore, Ochs said, the team completed tests and analyses on the telescope’s components and systems to mitigate potential risks.</p><p>The Webb telescope, which launched in 2022, is still collecting data.</p><p>The three engineers also shared advice about what it takes to be a project manager. Take one big problem and break it down into several small problems you can solve, Willoughby said. He added that managers need to communicate with the entire team and “get empirical, fast.”</p><h2>A royal visitor and honoring innovators</h2><p>This year’s <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-honors-ceremony-full-stream" rel="noopener noreferrer" target="_blank">IEEE Honors Ceremony</a>, held on the evening of 24 April, recognized people who spearheaded innovations in areas including solid-state circuits, wireless communication, and broadband technology.</p><p>The opening speaker was <a href="https://en.wikipedia.org/wiki/Hisako,_Princess_Takamado" rel="noopener noreferrer" target="_blank">Hisako, Princess Takamodo</a> of Japan. “It is a great honor that Japan has been allowed to host this premier event,” she said.</p><p>This was the first time the ceremony was held in Asia.</p><p>“I stand here in total awe of how far the human brain has come in the past century,” the princess said.</p><p>LEDs play an important role in sustainability, as they reduce energy consumption and can be recycled, unlike incandescent lighting. The technology wouldn’t have been possible without photonic integrated circuits developed by <a href="https://ethw.org/Frederick_A._Kish,_Jr." rel="noopener noreferrer" target="_blank">Frederick A. Kish Jr</a>.</p><p>Kish, an IEEE Fellow, also advanced telecommunications technology by creating and integrating a full optical system for transmissions onto a single chip, reducing manufacturing costs and enabling significantly higher bandwidths and faster data transfer speeds. For his innovations, he received the <a href="https://corporate-awards.ieee.org/award/ieee-nick-holonyak-medal/" rel="noopener noreferrer" target="_blank">IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies</a>. The award is sponsored by Friends of Nick Holonyak Jr. The <a href="https://spectrum.ieee.org/red-hot" target="_self">2003 Medal of Honor recipient</a> invented the first practical visible-spectrum LED.</p><p>Kish thanked his former colleagues at <a href="https://www.google.com/aclk?sa=l&ai=DChcSEwick7_e1rWNAxVWSEcBHWS4F0EYABABGgJxdQ&co=1&ase=2&gclid=CjwKCAjw87XBBhBIEiwAxP3_AzMlWVi0er_7aNExyr7nFlrhOlrskfS4AAq6S-PdnD-37OFIvyXQThoCbmcQAvD_BwE&category=acrcp_v1_5&sig=AOD64_2_79wIKcMbzCyXgqxASx3pyYEDMg&q&nis=4&adurl&ved=2ahUKEwj8p7je1rWNAxXHF1kFHQIJDMAQ0Qx6BAgmEAE" rel="noopener noreferrer" target="_blank">Agilent Technologies</a>,<a href="https://www.hpe.com/us/en/home.html" rel="noopener noreferrer" target="_blank"> Hewlett Packard</a>, the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Urbana-Champaign</a>, and other organizations.</p><p>“We’ve worked together to leave the world brighter, greener, and more connected,” he said. “This medal represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”</p><p>Honoring work that helps connect the world continued with the presentation of the <a href="https://spectrum.ieee.org/mildred-dresselhaus-the-queen-of-carbon-science-has-ieee-medal-named-in-her-honor" target="_self">IEEE Mildred Dresselhaus Medal</a> to IEEE Fellow <a href="https://spectrum.ieee.org/princeton-dean-andrea-goldsmith" target="_self">Andrea Goldsmith</a>, who received the award for “contributions to and leadership in wireless communications theory and practice.” The award is sponsored by <a href="https://about.google/" rel="noopener noreferrer" target="_blank">Google</a>.</p><p>While at <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford</a>, Goldsmith developed foundational mathematical approaches for increasing the capacity, speed, and range of wireless systems. She helped found two communications startups: <a href="https://www.linkedin.com/company/quantenna-communications" rel="noopener noreferrer" target="_blank">Quantenna Communications</a> of San Jose, Calif., and <a href="https://www.plume.com/" rel="noopener noreferrer" target="_blank">Plume Design</a> of Palo Alto, Calif. This year, the current dean of engineering and applied sciences at <a href="https://www.princeton.edu/" rel="noopener noreferrer" target="_blank">Princeton</a> was appointed president of <a href="https://www.stonybrook.edu/" rel="noopener noreferrer" target="_blank">Stony Brook University</a>, in New York. She is set to start her new position on 1 August.</p><p>“Mildred Dresselhaus was a pioneer in the days when there were very few women in science and technology,” Goldsmith said. “She was a role model and an early champion of diversity, ensuring the best and the brightest could enter the field and thrive within it. Her contributions to science and engineering are unparalleled, and receiving an award named for her is deeply meaningful to me.”</p><p>The ceremony concluded with the presentation of the IEEE Medal of Honor to Samueli, who received a standing ovation.</p><p>At the end of his speech, he announced that he was giving his $2 million prize to <a href="https://hkn.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE–Eta Kappa Nu</a>, the organization’s honors society.</p><p>“I was made an eminent member of IEEE-HKN in 2019, and [the <a href="https://www.samueli.org/" rel="noopener noreferrer" target="_blank">Samueli Foundation</a>] has supported [the society] for years,” he said. “It is truly an honor for me to endow such a wonderful student organization.”</p>]]></description><pubDate>Wed, 04 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</guid><category>Broadband</category><category>Ieee awards</category><category>Ieee medal of honor</category><category>Ieee news</category><category>Semiconductors</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&amp;width=980"></media:content></item><item><title>Look for These 7 New Technologies at the Airport</title><link>https://spectrum.ieee.org/7-new-airport-tech</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&width=1245&height=700&coordinates=0%2C115%2C0%2C115"/><br/><br/><p>
<strong>Take a look around</strong> the airport during your travels this summer and you might spot a string of new technologies at every touchpoint: from pre-arrival, bag drop, and security to the moment you board the plane.
</p><p>
	In this new world, your face is your boarding pass, your electronic luggage tag transforms itself for each new flight, and gate scanners catch line cutters trying to sneak onto the plane early.
</p><p>
	It isn’t the future—it’s now. Each of the technologies to follow is in use at airports around the world today, transforming your journey-before-the-journey.
</p><h2>Virtual queuing speeds up airport security</h2><p>
	As you pack the night before your trip, you ponder the age-old travel question: What time should I get to the airport? The right answer requires predicting the length of the security line. But at some airports, you no longer have to guess; in fact, you don’t have to wait in line at all.
</p><p>
	Instead, you can book ahead and choose a specific time for your security screening—so you can arrive right before your reserved slot, confident that you’ll be whisked to the front of the line, thanks to <a href="https://copenhagenoptimization.com/" rel="noopener noreferrer" target="_blank">Copenhagen Optimization</a>’s Virtual Queuing system.
</p><p>
	Copenhagen Optimization’s machine learning models use linear regression, heuristic models, and other techniques to forecast the volume of passenger arrivals based on historical data. The system is integrated with airport programs to access flight schedules and passenger-flow data from boarding-pass scans, and it also takes in data from lidar sensors and cameras at security checkpoints, X-ray luggage scanners, and other areas.
</p><p>
	If a given day’s passenger volume ends up differing from historical projections, the platform can use real-time data from these inputs to adjust the Virtual Queuing time slots—and recommend that the airport make changes to security staffing and the number of open lanes. The Virtual Queuing system is constantly adjusting to flatten the passenger arrival curve, tactically redistributing demand across time slots to optimize resources and reduce congestion.
</p><p>
	While this system is doing the most, you as a passenger can do the least. Just book a time slot on your airport’s website or app, and get some extra sleep knowing you’ll waltz right up to the security check tomorrow morning.
</p><h2>Electronic bag tags</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Line drawing of a woman lifting suitcase at airport baggage check-in with barcode in focus." class="rm-shortcode" data-rm-shortcode-id="64ff97b084fbc93dd936889921e516d7" data-rm-shortcode-name="rebelmouse-image" id="f8bea" loading="lazy" src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-lifting-suitcase-at-airport-baggage-check-in-with-barcode-in-focus.png?id=60389664&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	Checking a bag? Here’s another step you can take care of before you arrive: Skip the old-school paper tags and generate your own electronic <a href="https://bagtag.com/" rel="noopener noreferrer" target="_blank">Bagtag</a>. This e-ink device (costing about US $80, or €70) looks like a traditional luggage-tag holder, but it can generate a new, paperless tag for each one of your flights.
</p><p>
	You provide your booking details through your airline’s app or the Bagtag app, and the Bagtag system then uses application programming interfaces and secure data protocols to retrieve the necessary information from the airline’s system: your name, flight details, the baggage you’re allowed, and the unique barcode that identifies your bag. The app uses this data to generate a digital tag. Hold your phone near your Bagtag, and it will transmit the encrypted tag data via Bluetooth or NFC. Simultaneously, your phone’s NFC antenna powers the battery-free Bagtag device.
</p><p>
	On the Bagtag itself, a low-power microcontroller decrypts the tag data and displays the digital tag on the e-ink screen. Once you’re at the airport, the tag can be scanned at the airline’s self-service bag drop or desk, just like a traditional paper tag. The device also contains an RFID chip that’s compatible with the luggage-tracking systems that some airlines are using, allowing your bag to be identified and tracked—even if it takes a different journey than you do. When you arrive at the airport, just drop that checked bag and make your way to the security area.
</p><h2>Biometric boarding passes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Illustration of a woman using kiosk for facial recognition ID verification." class="rm-shortcode" data-rm-shortcode-id="af8a923d85c7eac7ca6873db756cc3fb" data-rm-shortcode-name="rebelmouse-image" id="3dfdf" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-using-kiosk-for-facial-recognition-id-verification.png?id=60389955&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	Over at security, you’ll need your boarding pass and ID. Compared with the old days of printing a physical slip from a kiosk, digital QR code boarding passes are quite handy—but what if you didn’t need anything besides your face? That’s the premise of <a href="https://www.idemia.com/" rel="noopener noreferrer" target="_blank">Idemia Public Security</a>’s biometric boarding-pass technology.
</p><p>
	Instead of waiting in a queue for a security agent, you’ll approach a self-service kiosk or check-in point and insert your government-issued identification document, such as a driver’s license or passport. The system uses visible light, infrared, and ultraviolet imaging to analyze the document’s embedded security features and verify its authenticity. Then, computer-vision algorithms locate and extract the image of your face on the ID for identity verification.
</p><p>
	Next, it’s time for your close-up. High-resolution cameras within the system capture a live image of your face using 3D and infrared imaging. The system’s antispoofing technology prevents people from trying to trick the system with items like photos, videos, or masks. The technology compares your live image to the one extracted from your ID using facial-recognition algorithms. Each image is then converted into a compact biometric template—a mathematical representation of your facial features—and a similarity score is generated to confirm a match.
</p><p>
	Finally, the system checks your travel information against secure flight databases to make sure the ticket is valid and that you’re authorized to fly that day. Assuming all checks out, you’re cleared to head to the body scanners—with no biometric data retained by Idemia Public Security’s system.
</p><h2>X-rays that can tell ecstasy from eczema meds </h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Illustration of an X-ray machine scanning luggage with schematic view of interior components above." class="rm-shortcode" data-rm-shortcode-id="0ff27fb1769e9930b81f30bde1d86244" data-rm-shortcode-name="rebelmouse-image" id="9c471" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-an-x-ray-machine-scanning-luggage-with-schematic-view-of-interior-components-above.png?id=60389973&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	While you pass through your security screening, that luggage you checked is undergoing its own screening—with a major new upgrade that can tell exactly what’s inside.
</p><p>
	Traditional scanners use one or a few X-ray sources and work by transmission, measuring the attenuation of the beam as it passes through the bag. These systems create a 2D “shadow” image based on differences in the amount and type of the materials inside. More recently, these systems have begun using <a href="https://spectrum.ieee.org/invention-of-ct-scanner" target="_blank">computed tomography</a> to scan the bag from all directions and to reconstruct 3D images of the objects inside. But even with CT, harmless objects may look similar to dangerous materials—which can lead to false positives and also require security staff to visually inspect the X-ray images or even bust open your luggage.
</p><p>
	By contrast, <a href="https://www.smithsdetection.com/" rel="noopener noreferrer" target="_blank">Smiths Detection</a>’s new <a href="https://spectrum.ieee.org/future-baggage-scanners-will-tell-us-what-things-are-made-of" target="_blank">X-ray diffraction</a> machines measure the molecular structure of the items inside your bag to identify the exact materials—no human review required.
</p><p>
	The machine uses a multifocus X-ray tube to quickly scan a bag from various angles, measuring the way the radiation diffracts while switching the position of the focal spots every few microseconds. Then, it analyzes the diffraction patterns to determine the crystal structure and molecular composition of the objects inside the bag—building a “fingerprint” of each material that can much more finely differentiate threats, like explosives and drugs, from benign items.
</p><p>
	The system’s algorithms process this diffraction data and build a 3D spatial image, which allows real-time automated screening without the need for manual visual inspection by a human. After your bag passes through the X-ray diffraction machine without incident, it’s loaded into the cargo hold. Meanwhile, you’ve passed through your own scan at security and are ready to head toward your gate.
</p><h2>Airport shops with no cashiers or checkout lanes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt='Illustration of a woman entering a store with a "Just Walk Out" shopping system.' class="rm-shortcode" data-rm-shortcode-id="995f20a20ef07fc6697d2cac5737b9c1" data-rm-shortcode-name="rebelmouse-image" id="8a4c0" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-entering-a-store-with-a-just-walk-out-shopping-system.png?id=60390007&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	While meandering over to your gate from security, you decide you could use a little pick-me-up. Just down the corridor is a convenience store with snacks, drinks, and other treats—but no cashiers. It’s a contactless shop that uses <a href="https://www.justwalkout.com/" rel="noopener noreferrer" target="_blank">Just Walk Out</a> technology by Amazon.
</p><p>
	As you enter the store with the tap of a credit card or mobile wallet, a scanner reads the card and assigns you a unique session identifier that will let the Just Walk Out system link your actions in the store to your payment. Overhead cameras track you by the top of your head, not your face, as you move through the store.
</p><p>
	The Just Walk Out system uses a deep-learning model to follow your movements and detect when you interact with items. In most cases, computer vision can identify a product you pick up simply based on the video feed, but sometimes weight sensors embedded in the shelves provide additional data to determine what you removed. The video and weight data are encoded as tokens, and a neural network processes those tokens in a way similar to how large language models encode text—determining the result of your actions to create a “virtual cart.”
</p><p>
	While you shop, the system continuously updates this cart: adding a can of soda when you pick it up, swapping one brand of gum for another if you change your mind, or removing that bag of chips if you put it back on the shelf. Once your shopping is complete, you can indeed just walk out with your soda and gum. The items you take will make up your finalized virtual cart, and the credit card you entered the store with will be charged as usual. (You can look up a receipt, if you want.) With provisions procured, it’s onward to the gate.
</p><h2>Airport-cleaning robots</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Illustration of a woman watching an automated floor cleaning robot cleaning up a spilled drink in the airport." class="rm-shortcode" data-rm-shortcode-id="910995e84aefefbcacb0afaefa6e6a37" data-rm-shortcode-name="rebelmouse-image" id="a8ced" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-watching-an-automated-floor-cleaning-robot-cleaning-up-a-spilled-drink-in-the-airport.png?id=60390051&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	As you amble toward the gate with your luggage and snacks, you promptly spill that soda you just bought. Cleanup in Terminal C! Along comes <a href="https://avidbots.com/" rel="noopener noreferrer" target="_blank">Avidbots’ Neo</a>, a fully autonomous floor-scrubbing robot designed to clean commercial spaces like airports with minimal human intervention.
</p><p>
	When a Neo is first delivered to the airport, the robot performs a comprehensive scan of the various areas it will be cleaning using lidar and 3D depth cameras. Avidbots software processes the data to create a detailed map of the environment, including walls and other obstacles, and this serves as the foundation for Neo’s cleaning plans and navigation.
</p><p>
	Neo’s human overlords can use a touchscreen on the robot to direct it to the area that needs cleaning—either as part of scheduled upkeep, or when someone (ahem) spills their soda. The robot springs into action, and as it moves, it continuously locates itself within its map and plans its movements using data from wheel encoders, inertial measurement units, and a gyroscope. Neo also updates its map and adjusts its path in real time by using the lidar and depth cameras to detect any changes from its initial mapping, such as a translocated trash can or perambulating passengers.
</p><p>
	Then comes the scrubbing. Neo’s software plans the optimal path for cleaning a given area at this moment in time, adjusting the robot’s speed and steering as it moves along. A water-delivery system pumps and controls the flow of cleaning solution to the motorized brushes, whose speed and pressure can also be adjusted based on the surface the robot is cleaning. A powerful vacuum system collects the dirty water, and a flexible squeegee prevents slippery floors from being left behind.
</p><p>
	While the robot’s various sensors and planning algorithms continuously detect and avoid obstacles, any physical contact with the robot’s bumpers triggers an emergency stop. And if Neo finds itself in a situation it’s just not sure how to handle, the robot will stop and call for assistance from a human operator, who can review sensor data and camera feeds remotely to help it along.
</p><h2>“Wrong group” plane-boarding alarm</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Illustration of a woman waiting in line at boarding gate E6, with notification bell icon above." class="rm-shortcode" data-rm-shortcode-id="2b01cd5c1c6b0ed8dd2962347946988a" data-rm-shortcode-name="rebelmouse-image" id="bdce3" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-waiting-in-line-at-boarding-gate-e6-with-notification-bell-icon-above.png?id=60390066&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>
	Your airport journey is coming to an end, and your real journey is about to begin. As you wait at the gate, you notice a fair number of your fellow passengers hovering to board even before the agent has made any announcements. And when boarding does begin, a surprising number of people hop in line. <em><em>Could all these people really be in boarding groups 1 and 2?</em></em> you wonder.
</p><p>
	If they’re not…they’ll get called out. American Airlines’ new boarding technology stops those pesky passengers who try to join the wrong boarding group and sneak onto the plane early.
</p><p>
	If one such passenger approaches the gate before their assigned group has been called, scanning their boarding pass will trigger an audible alert—notifying the airline crew, and everyone else for that matter. The passengers will be politely asked to wait to board. As they slink back into line, try not to look too smug. After all, it’s been a remarkably easy, tech-assisted journey through the airport today. <span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Wed, 04 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/7-new-airport-tech</guid><category>Airlines</category><category>Facial recognition</category><category>Robot cleaner</category><category>Airports</category><category>X-ray diffraction</category><dc:creator>Julianne Pepitone</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&amp;width=980"></media:content></item><item><title>Nvidia’s Blackwell Conquers Largest LLM Training Benchmark</title><link>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&width=1245&height=700&coordinates=0%2C309%2C0%2C309"/><br/><br/><p>
<span>For those who enjoy rooting for the underdog, the latest MLPerf benchmark results will disappoint: Nvidia’s GPUs have dominated the competition </span><a href="https://spectrum.ieee.org/mlperf-nvidia-conquers" target="_self">yet</a><span> </span><a href="https://spectrum.ieee.org/ai-training-2669810566" target="_self">again</a><span>. This includes chart-topping performance on the latest and most demanding benchmark, pretraining the Llama 3.1 403B large language model. That said, the computers built around the newest AMD GPU, MI325X, matched the performance of Nvidia’s H200, </span><a href="https://spectrum.ieee.org/nvidia-blackwell" target="_self">Blackwell’s</a><span> predecessor, on the most popular LLM fine-tuning benchmark. This suggests that AMD is one generation behind Nvidia.</span>
</p><p>
<a href="https://mlcommons.org/benchmarks/" target="_blank">MLPerf</a> training is one of the machine learning competitions run by the <a href="https://mlcommons.org/" rel="noopener noreferrer" target="_blank">MLCommons</a> consortium. “AI performance sometimes can be sort of the Wild West. MLPerf seeks to bring order to that chaos,” says <a href="https://www.linkedin.com/in/davesalvator/" rel="noopener noreferrer" target="_blank">Dave Salvator</a>, director of accelerated computing products at Nvidia. “This is not an easy task.”
</p><p>
	The competition consists of six benchmarks, each probing a different industry-relevant machine learning task. The benchmarks are content recommendation, large language model pretraining, large language model fine-tuning, object detection for machine vision applications, image generation, and graph node classification for applications such as fraud detection and drug discovery.
</p><p>
	The large language model pretraining task is the most resource intensive, and this round it was updated to be even more so. The term “pretraining” is somewhat misleading—it might give the impression that it’s followed by a phase called “training.” It’s not. Pretraining is where most of the number crunching happens, and what follows is usually fine-tuning, which refines the model for specific tasks.
</p><p>
	In previous iterations, the pretraining was done on the GPT3 model. This iteration, it was replaced by Meta’s Llama 3.1 403B, which is more than twice the size of GPT3 and uses a four times larger context window. The context window is how much input text the model can process at once. This larger benchmark represents the industry trend for ever larger models, as well as including some architectural updates.
</p><h2>Blackwell Tops the Charts, AMD on Its Tail </h2><p>
	For all six benchmarks, the fastest training time was on Nvidia’s Blackwell GPUs. Nvidia itself submitted to every benchmark (other companies also submitted using various computers built around Nvidia GPUs). Nvidia’s Salvator emphasized that this is the first deployment of Blackwell GPUs at scale and that this performance is only likely to improve. “We’re still fairly early in the Blackwell development life cycle,” he says.
</p><p>
	This is the first time AMD has submitted to the training benchmark, although in previous years other companies have submitted using computers that included AMD GPUs. In the most popular benchmark, LLM fine-tuning, AMD demonstrated that its latest Instinct MI325X GPU performed on par with Nvidia’s H200s. Additionally, the Instinct MI325X showed a 30 percent improvement over its predecessor, the <a href="https://spectrum.ieee.org/amd-mi300" target="_blank">Instinct MI300X</a>. (The main difference between the two is that MI325X comes with 30 percent more high-bandwidth memory than MI300X.)</p><p>For it’s part, Google submitted to a single benchmark, the image-generation task, with its <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus" target="_blank">Trillium TPU</a>.
</p><div class="flourish-embed flourish-scatter" data-src="visualisation/23471261?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="scatter visualization" src="https://public.flourish.studio/visualisation/23471261/thumbnail" width="100%"/></noscript></div><h2>The Importance of Networking</h2><p>
	Of all submissions to the LLM fine-tuning benchmarks, the system with the largest number of GPUs was submitted by Nvidia, a computer connecting 512 B200s. At this scale, networking between GPUs starts to play a significant role. Ideally, adding more than one GPU would divide the time to train by the number of GPUs. In reality, it is always less efficient than that, as some of the time is lost to communication. Minimizing that loss is key to efficiently training the largest models.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23550378?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23550378/thumbnail" width="100%"/></noscript></div><p>
	This becomes even more significant on the pretraining benchmark, where the smallest submission used 512 GPUs, and the largest used 8,192. For this new benchmark, the performance scaling with more GPUs was notably close to linear, achieving 90 percent of the ideal performance.
</p><p>
	Nvidia’s Salvator attributes this to the NVL72, an efficient package that connects 36 Grace CPUs and 72 Blackwell GPUs with <a href="https://www.nvidia.com/en-us/data-center/nvlink/" rel="noopener noreferrer" target="_blank">NVLink</a>, to form a system that “acts as a single, massive GPU,” the <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank">datasheet</a> claims. Multiple NVL72s were then connected with <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self">InfiniBand</a> network technology.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23508278?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23508278/thumbnail" width="100%"/></noscript></div><p>
	Notably, the largest submission for this round of MLPerf—at 8192 GPUs—is not the largest ever, despite the increased demands of the pretraining benchmark. Previous rounds saw submissions with over 10,000 GPUs. <a href="https://www.linkedin.com/in/kennethleach/" target="_blank">Kenneth Leach</a>, principal AI and machine learning engineer at Hewlett Packard Enterprise, attributes the reduction to improvements in GPUs, as well as networking between them. “Previously, we needed 16 server nodes [to pretrain LLMs], but today we’re able to do it with 4. I think that’s one reason we’re not seeing so many huge systems, because we’re getting a lot of efficient scaling.”
</p><p>
	One way to avoid the losses associated with networking is to put many AI accelerators on the same huge wafer, as done by <a href="https://spectrum.ieee.org/cerebrass-giant-chip-will-smash-deep-learnings-speed-barrier" target="_self">Cerebras</a>, which recently claimed to <a href="https://www.businesswire.com/news/home/20250528123694/en/Cerebras-Beats-NVIDIA-Blackwell-in-Llama-4-Maverick-Inference" rel="noopener noreferrer" target="_blank">beat</a> Nvidia’s Blackwell GPUs by more than a factor of two on inference tasks. However, that result was measured by <a href="https://artificialanalysis.ai/" rel="noopener noreferrer" target="_blank">Artificial Analysis</a>, which queries different providers without controlling how the workload is executed. So its not an apples-to-apples comparison in the way the MLPerf benchmark ensures.
</p><h2>A Paucity of Power</h2><p>
	The MLPerf benchmark also includes a power test, measuring how much power is consumed to achieve each training task. This round, only a single submitter—Lenovo—included a power measurement in its submission, making it impossible to make comparisons across performers. The energy it took to fine-tune an LLM on two Blackwell GPUs was 6.11 gigajoules, or 1,698 kilowatt-hours, or roughly the energy it would take to heat a small home for a winter. With growing <a href="https://spectrum.ieee.org/ai-energy-consumption" target="_self">concerns</a> about AI’s energy use, the power efficiency of training is crucial, and this author is perhaps not alone in hoping more companies submit these results in future rounds.
</p>]]></description><pubDate>Wed, 04 Jun 2025 15:59:50 +0000</pubDate><guid>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</guid><category>Mlperf</category><category>Nvidia</category><category>Amd</category><category>Networking</category><dc:creator>Dina Genkina</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&amp;width=980"></media:content></item><item><title>The Birth of the University as Innovation Incubator</title><link>https://spectrum.ieee.org/university-as-innovation-incubator</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-bearded-white-man-lying-on-the-floor-with-video-game-consoles-and-a-1970s-tv.jpg?id=60341590&width=1245&height=700&coordinates=0%2C329%2C0%2C330"/><br/><br/><div class="intro-text">
<em>This article is excerpted from</em> <a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" target="_blank">Every American an Innovator: How Innovation Became a Way of Life</a><em>, by Matthew Wisnioski (The MIT Press, 2025).</em>
</div><p class="drop-caps">
<strong>Imagine a point-to-point </strong>transportation service in which two parties communicate at a distance. A passenger in need of a ride contacts the service via phone. A complex algorithm based on time, distance, and volume informs both passenger and driver of the journey’s cost before it begins. This novel business plan promises efficient service and lower costs. It has the potential to disrupt an overregulated taxi monopoly in cities across the country. Its enhanced transparency may even reduce racial discrimination by preestablishing pickups regardless of race.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" target="_blank"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><img alt="Book cover with illustration of people engaged in various activities.   " class="rm-shortcode" data-rm-shortcode-id="a2bb1759ca4d53a4cb4aa8bacec42ccc" data-rm-shortcode-name="rebelmouse-image" id="c431e" loading="lazy" src="https://spectrum.ieee.org/media-library/book-cover-with-illustration-of-people-engaged-in-various-activities.jpg?id=60341772&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..."><a href="https://mitpress.mit.edu/9780262550734/every-american-an-innovator/" rel="noopener noreferrer" target="_blank">Every American an Innovator: How Innovation Became a Way of Life</a>, by Matthew Wisnioski.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">The MIT Press</small>
</p><p>
	Sounds like Uber, but it’s not. Prototyped in 1975, this automated taxi-dispatch system was the brainchild of mechanical engineer Dwight Baumann and his students at 
	<a href="https://www.cmu.edu/" rel="noopener noreferrer" target="_blank">Carnegie Mellon University</a>. The dial-a-ride service was designed to resurrect a defunct cab company that had once served Pittsburgh’s African American neighborhoods.
</p><p>
	The ride service was one of 11 entrepreneurial ventures supported by the university’s Center for Entrepreneurial Development. Funded by a million-dollar grant from the 
	<a href="https://www.nsf.gov/" rel="noopener noreferrer" target="_blank">National Science Foundation</a>, the CED was envisioned as an innovation “hatchery,” intended to challenge the norms of research science and higher education, foster risk-taking, birth campus startups focused on market-based technological solutions to social problems, and remake American science to serve national needs.
</p><p>
	Today, university incubators like the CED are commonplace. Whether they’re seeking to nurture the next Uber, or social ventures like the dial-a-ride service, they all aim to transform ideas into businesses, discoveries into applications, classroom assignments into revenue, and faculty and students into entrepreneurs. Indeed, the idea that universities are engines of innovation is so ingrained that we take it for granted that it was always the case. So it’s instructive to look back to the time when the first innovation incubators were themselves being incubated.
</p><h2>Are innovators born or made?</h2><p>
	During the Cold War, the model for training scientists and engineers in the United States was one of manpower in service to a linear model of innovation: Scientists pursued “basic” discovery in universities and federal laboratories; engineer–scientists conducted “applied” research elsewhere on campus; engineers developed those ideas in giant teams for companies such as Lockheed and Boeing; and research managers oversaw the whole process. This model dictated national science policy, elevated the 
	<a href="https://spectrum.ieee.org/the-essential-vannevar-bush/as-we-may-think" target="_self">scientist as a national hero</a> in pursuit of truth beyond politics, and pumped hundreds of millions of dollars into higher education. In practice, the lines between basic and applied research were blurred, but the perceived hierarchy was integral to the NSF and the university research culture that it helped to foster.
</p><p>
	In the late 1960s, this postwar system of academic science and engineering appeared to be breaking down. Science and technology were seen as root causes of environmental destruction, the Vietnam War, job losses, and racial and economic inequality. A similar reckoning was taking place around national science policy, with critics on the left attacking the complicity of scientists in the military-industrial complex and those on the right assailing the wastefulness of ivory-tower spending on science.
</p><p class="ieee-inbody-related">
	RELATED: 
	<a href="https://spectrum.ieee.org/innovation-magazine-and-the-birth-of-a-buzzword" target="_self">Innovation Magazine and the Birth of a Buzzword</a>
</p><p>
	In this moment of revolt, innovation experts in Washington, D.C., and the booming technology regions of California and Massachusetts began to promote innovators as the people who would bring about change, because they were different from the established leaders of American science. Eventually, a wide range of constituents—bureaucrats, inventors, academics, business leaders, and engineers—came to identify innovators as agents of national progress, and they concluded that these innovators could indeed be taught in the nation’s universities.
</p><p>
	The question was, how? And would the universities be willing to remake themselves to support innovation?
</p><p>
	And so it fell to the NSF to develop successful models for producing these risk-taking sociotechnologists.
</p><h2>The NSF experiments with innovation</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Black and white photo of two young men in 1970s casual clothing, one holding a circuit diagram." class="rm-shortcode" data-rm-shortcode-id="83dce65d75b331d5c4364e652815dba5" data-rm-shortcode-name="rebelmouse-image" id="a452a" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-two-young-men-in-1970s-casual-clothing-one-holding-a-circuit-diagram.jpg?id=60341592&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">At the Utah Innovation Center, engineering students John DeJong and Douglas Kihm worked on a programmable electronics breadboard.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Special Collections, J. Willard Marriott Library, The University of Utah</small>
</p><p>
In 1972, NSF director 
<a href="https://ethw.org/Oral-History:H._Guyford_Stever" rel="noopener noreferrer" target="_blank">H. Guyford Stever</a> established the <a href="https://scholarworks.uni.edu/cgi/viewcontent.cgi?article=2550&context=istj" target="_blank">Office of Experimental R&D Incentives</a> to “incentivize” innovation for national needs by supporting research on “how the government [could] most effectively accelerate the transfer of new technology into productive enterprise.” Stever stressed the experimental nature of the program because many in the NSF and the scientific community resisted the idea of goal-directed research. Innovation, with its connotations of profit and social change, was even more suspect.
</p><p>
	To lead the initiative, Stever appointed C.B. Smith, a research manager at United Aircraft Corp., who in turn brought in engineers with industrial experience, including Robert Colton, an automotive engineer. Colton led the university Innovation Center experiment that gave rise to Carnegie Mellon’s CED.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>
<span>The NSF chose four universities that captured a range of approaches to innovation incubation. MIT targeted undergrads through formal coursework and an innovation “co-op” that assisted in turning ideas into products. The University of Oregon evaluated the ideas of garage inventors from across the country. The University of Utah emphasized an ecosystem of biotech and <a href="https://spectrum.ieee.org/sketchpad" target="_blank">computer graphics</a> startups coming out of its research labs. And Carnegie Mellon established a nonprofit corporation to support graduate student ventures, including the dial-a-ride service.</span>
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a young man in 1970s clothing seated at a table with a telephone and radio and holding a digital device." class="rm-shortcode" data-rm-shortcode-id="128c785470ffd6b3bde84fb4d7424e90" data-rm-shortcode-name="rebelmouse-image" id="72485" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-young-man-in-1970s-clothing-seated-at-a-table-with-a-telephone-and-radio-and-holding-a-digital-device.jpg?id=60341597&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Grad student Fritz Faulhaber holds one of the radio-coupled taxi meters that Carnegie Mellon students installed in Pittsburgh cabs in the 1970s.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ralph Guggenheim;Jerome McCavitt/Carnegie-Mellon Alumni News</small>
</p><h2>Carnegie Mellon got one of the first university incubators</h2><p>
	Carnegie Mellon had all the components that experts believed were necessary for innovation: strong engineering, a world-class business school, novel approaches to urban planning with a focus on community needs, and a tradition of industrial design and the practical arts. CMU leaders claimed that the school was smaller, younger, more interdisciplinary, and more agile than MIT.
</p><p>
	The main reason that CMU received an NSF Innovation Center, however, was its director, 
	<a href="https://www.cmu.edu/swartz-center-for-entrepreneurship/assets/dwight-m.b-baumann/article-in-honor-and-memory-of-professor-dwight-baumann.pdf" target="_blank">Dwight Baumann</a>. Baumann exemplified a new kind of educator-entrepreneur. The son of North Dakota farmers, he had graduated from North Dakota State University, then headed to MIT for a Ph.D. in mechanical engineering, where he discovered a love of teaching. He also garnered a reputation as an unusually creative engineer with an interest in solving problems that addressed human needs. In the 1950s and 1960s, first as a student and then as an MIT professor, Baumann helped develop one of the first computer-aided-design programs, as well as computer interfaces for the blind and the nation’s first dial-a-ride paratransit system.
</p><p>
	But Baumann was frustrated with MIT’s culture of defense research and engineering science, and so he left his tenured position in 1970 to join CMU and continue his work on transportation systems. There, he chartered the NSF-funded CED as a nonprofit. He purchased the bankrupt Peoples Cab Co. for a dollar, convinced the university to let him use a former parking garage as an incubator space, and worked across colleges to establish a master’s program in engineering design.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a white man in a suit in front of a blackboard talking to students and leaning on a slide projector." class="rm-shortcode" data-rm-shortcode-id="92961fa4c92ac7b71730756430a42457" data-rm-shortcode-name="rebelmouse-image" id="3fcf2" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-white-man-in-a-suit-in-front-of-a-blackboard-talking-to-students-and-leaning-on-a-slide-projector.jpg?id=60341736&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Dwight Baumann, director of Carnegie Mellon’s Center for Entrepreneurial Development, believed that a modern university should provide entrepreneurial education.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Carnegie Mellon University Archives
	</small>
</p><p>
	Baumann’s goal was to establish entrepreneurship education as a core function of a modern technological university. He wasn’t especially concerned with making money, and he cared little for nationalist rhetoric about global competition. Rather, his professed goal was to unlock human creativity in a “studio without walls, an association of people, loosely related, who communicate with each other and can get help when they need it.” Technological innovation, he argued, could never be entirely predictable because it was a project, rather than an act of scientific discovery. “A project,” he wrote, “is something that hasn’t yet happened. And the instructors and students have the common goal of seeing how it’ll turn out.”
</p><p>
	The CED’s mission was to support entrepreneurs in the earliest stages of the innovation process when they needed space and seed funding. It created an environment for students to make a “sequence of nonfatal mistakes,” so they could fail and develop self-confidence for navigating the risks and uncertainties of entrepreneurial life. It targeted graduate students who already had advanced scientific and engineering training and a viable idea for a business.
</p><p>
	In its first five years, the center launched 11 ventures. In addition to the reboot of the Peoples Cab Company, projects included a blood oximeter, a computer-hardware company, and a newspaper-printing technique. Many of these endeavors failed. Founders had health problems, patent disputes arose, and competitors claimed that the CED’s ventures had an unfair advantage through the weight of CMU.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a Black man leaning out the driver\u2019s side of a 1950s taxi with \u201cPeoples Cab\u201d printed on the side. " class="rm-shortcode" data-rm-shortcode-id="961806d3e494d864b538f2af59212d18" data-rm-shortcode-name="rebelmouse-image" id="9d61a" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-black-man-leaning-out-the-driver-u2019s-side-of-a-1950s-taxi-with-u201cpeoples-cab-u201d-printed-on.jpg?id=60341570&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Carnegie Mellon’s dial-a-ride service replicated the Peoples Cab Co., which had provided taxi service to Black communities in Pittsburgh.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Charles “Teenie” Harris/Carnegie Museum of Art/Getty Images
	</small>
</p><p>
	The CED distilled these lessons in brochures and public seminars, while faculty incorporated them into new classes. A 10-point “readiness assessment” emphasized personal reflection before any technology or market evaluation. The first rule: “Only if you have sincerely made the decision within yourself to invest time and effort, and understand that sacrifice and risk are inevitable, should you consider the life of an entrepreneur.” It aimed to show that innovation was a difficult path that could result in “personal dissatisfaction” and that one’s “family goals” must not be sacrificed in single-minded pursuit of an entrepreneurial opportunity.
</p><p>
	A few CED students did create successful startups. The breakout hit was Compuguard, founded by electrical engineering Ph.D. students 
	<a href="https://www.forbes.com/profile/romesh-t-wadhwani/" target="_blank">Romesh Wadhwani </a>and Krishnahadi Pribad, who hailed from India and Indonesia, respectively. The pair spent 18 months developing a security bracelet that used wireless signals to protect vulnerable people in dangerous work environments. But after failing to convert their prototype into a working design, they pivoted to a security- and energy-monitoring system for schools, prisons, and warehouses.
</p><p>
	With CED assistance, Compuguard secured government contracts and millions in venture capital and grew to over 100 employees. Its first major client was the Los Angeles city school district. The two founders sold the company for what was then the largest ever return on investment by a minority-run enterprise. Wadhwani became a serial entrepreneur and is now one of Silicon Valley’s leading billionaire philanthropists. His 
	<a href="https://wadhwanifoundation.org/" rel="noopener noreferrer" target="_blank">Wadhwani Foundation</a> supports innovation and entrepreneurship education worldwide, particularly in emerging economies.
</p><p>
	When NSF funding for the CED ran out in 1978, a series of long-simmering tensions erupted. At the heart of most of them was the cult of personality around Baumann, whose slapdash style conflicted with CMU’s desire to compete with new technology entrepreneurship programs at the University of Pennsylvania’s 
	<a href="https://www.wharton.upenn.edu/" target="_blank">Wharton School</a> and elsewhere. In 1983, Baumann’s onetime partner Jack Thorne took the lead of the new Enterprise Corp., which aimed to help Pittsburgh’s entrepreneurs raise venture capital. Baumann was kicked out of his garage to make room for the initiative.
</p><p>
	Baumann moved the CED to an abandoned YMCA building and attempted, with limited results, to help unemployed skilled laborers become innovators. The center faded, as CMU’s faculty continued to fight over the proper role of university innovation and who had the authority to teach it.
</p><h2>Was the NSF’s experiment in innovation a success?</h2><p>
	As the university Innovation Center experiment wrapped up in the late 1970s, the NSF patted itself on the back in a series of reports, conferences, and articles. “The ultimate effect of the Innovation Centers,” it stated, would be “the regrowth of invention, innovation, and entrepreneurship in the American economic system.” The NSF claimed that the experiment produced dozens of new ventures with US $20 million in gross revenue, employed nearly 800 people, and yielded $4 million in tax revenue. Yet, by 1979, license returns from intellectual property had generated only $100,000.
</p><p>
	The Innovation Centers garnered intense national and international interest. Established business schools in the United States created competing technology-innovation tracks. Visiting contingents from Canada, Sweden, and the United Kingdom hoped to re-create it.
</p><p class="pull-quote">Today, the legacies of the NSF experiment are visible on nearly every college campus.</p><p>
	Critics included Senator 
	<a href="https://en.wikipedia.org/wiki/William_Proxmire" target="_blank">William Proxmire</a> of Wisconsin, who pointed to the banana peelers, video games, and sports equipment pursued in the centers to lambast them as “wasteful federal spending” of “questionable benefit to the American taxpayer.”
</p><p>
	African American chemist 
	Grant Venerable faulted the program for its narrow conception of innovation as the purview of white men at elite universities. If supposed innovators could not address gender and racial equity “by more than a token nod,” he wrote, “they are guilty of being part of the problem.”
</p><p>
	And so the impacts of the NSF’s Innovation Center experiment weren’t immediately obvious. Many faculty and administrators of that era were still apt to view such programs as frivolous, nonacademic, or not worth the investment.
</p><p>
	Today, though, the legacies of the NSF experiment are visible on nearly every college campus. It institutionalized the scientific innovator-entrepreneur as a risk-taker who understood the probabilities of capital just as well as thermodynamics. And it established that the purpose of innovation education wasn’t just about breeding winners. All students, even those who never intended to commercialize their ideas or launch a startup, would benefit from learning to be entrepreneurial. And so the NSF’s experiment created another path by which innovation, a concept that prior to World War II barely registered as a cultural touchstone, became ingrained in our institutions, our educational system, and our beliefs about ourselves. 
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Wed, 04 Jun 2025 15:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/university-as-innovation-incubator</guid><category>Innovation</category><category>Entrepreneurs</category><category>Carnegie mellon university</category><category>National science foundation</category><category>History of engineering</category><category>Incubator</category><dc:creator>Matthew Wisnioski</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-bearded-white-man-lying-on-the-floor-with-video-game-consoles-and-a-1970s-tv.jpg?id=60341590&amp;width=980"></media:content></item><item><title>Who Gives a S#!t About Cursing Robots?</title><link>https://spectrum.ieee.org/cursing-social-robot-interaction</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-a-robot-tripping-on-a-banana-peel-its-head-is-covered-by-a-speech-bubble-and-symbols-representing-an-expleti.jpg?id=60333045&width=1245&height=700&coordinates=0%2C375%2C0%2C375"/><br/><br/><p>
	The robots that share our public spaces today are so demure. Social robots and service robots aim to avoid offense, erring toward polite airs, positive emotions, and obedience. In some ways, this makes sense—would you really want to have a yelling match with a delivery robot in a hotel? Probably not, even if you’re in New York City and trying to absorb the local culture.
</p><p>
	In other ways, this passive social robot design aligns with paternalistic standards that link assistance to subservience. Thoughtlessly following such outdated social norms in robot design may be ill-advised, since it <a href="https://pubmed.ncbi.nlm.nih.gov/37123285/" rel="noopener noreferrer" target="_blank">can help to reinforce outdated or harmful ideas</a> such as restricting people’s rights and reflecting only the needs of majority-identity users.
</p><p>
	In <a href="https://osusharelab.com/" rel="noopener noreferrer" target="_blank">my robotics lab at Oregon State University</a>, <a href="https://spectrum.ieee.org/how-high-fives-help-us-get-in-touch-with-robots" target="_blank">we work with</a> <a href="https://spectrum.ieee.org/whats-the-deal-with-robot-comedy" target="_blank">a playful spirit</a> and enjoy challenging the problematic norms that are entrenched within “polite” interactions and social roles. So we decided to experiment with robots that use foul language around humans. After all, many people are using foul language more than ever in 2025. Why not let robots have a chance, too?
</p><h3>Why and How to Study Cursing Robots</h3><p>
	Societal standards in the United States suggest that cursing robots would likely rub people the wrong way in most contexts, as swearing has a predominantly negative connotation. Although some past research shows that cursing <a href="https://www.researchgate.net/publication/238326248_Swearing_at_work_and_permissive_leadership_culture_When_anti-social_becomes_social_and_incivility_is_acceptable" rel="noopener noreferrer" target="_blank">can enhance team cohesion</a> and <a href="https://hrcak.srce.hr/file/159883" rel="noopener noreferrer" target="_blank">elicit humor</a>, certain members of society (such as women) are often expected to <a href="https://link.springer.com/article/10.1023/A:1022986429748" rel="noopener noreferrer" target="_blank">avoid risking offense</a> through profanity. We wondered whether cursing robots would be viewed negatively, or if they might perhaps offer benefits in certain situations.
</p><p>
	We decided to study cursing robots in the context of responding to mistakes. Past work in human-robot interaction has already shown that <a href="https://ieeexplore.ieee.org/abstract/document/5453195" rel="noopener noreferrer" target="_blank">responding to error</a> (rather than ignoring it) can help robots be perceived more positively in human-populated spaces, especially in the case of personal and service robots. And one <a href="https://par.nsf.gov/biblio/10284325-perceived-agency-social-norm-violating-robot" rel="noopener noreferrer" target="_blank">study</a> found that compared to other faux pas, foul language is more forgivable in a robot.
</p><p>
	With this past work in mind, we generated videos with three common types of robot failure: bumping into a table, dropping an object, and failing to grasp an object. We crossed these situations with three types of responses from the robot: no verbal reaction, a non-expletive verbal declaration, and an expletive verbal declaration. We then asked people to rate the robots on things like competence, discomfort, and likability, using standard scales in an online survey.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="27aa73d6ea081bc41fc24b217317e021" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hYdN5zLa07Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">What If Robots Cursed? These Videos Helped Us Learn How People Feel about Profane Robots</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: Naomi Fitter</small></p><h3>What People Thought of Our Cursing Robots</h3><p>
	On the whole, we were surprised by how acceptable swearing seemed to the study participants, especially within an initial group of Oregon State University students, but even among the general public as well. Cursing had no negative impact, and even some positive impacts, among the college students after we removed one religiously connotated curse (god***it), which seemed to be received in a stronger negative way than other cuss words.
</p><p>
	In fact, university participants rated swearing robots as the <a href="https://sparqtools.org/mobility-measure/inclusion-of-other-in-the-self-ios-scale/" rel="noopener noreferrer" target="_blank">most socially close</a> and most humorous, and rated non-expletive and expletive robot reactions equivalent on social warmth, competence, discomfort, anthropomorphism, and likability scales. The general public judged non-profane and profane robots as equivalent on most scales, although expletive reactions were deemed most discomforting and non-expletive responses seemed most likable. We believe that the university students were slightly more accepting of cursing robots because of the campus’s progressive culture, where cursing is considered a peccadillo.
</p><p class="ieee-inbody-related">Related: <a href="https://spectrum.ieee.org/whats-the-deal-with-robot-comedy" target="_blank">What’s the Deal With Robot Comedy?</a></p><p>
	Since experiments run solely in an online setting do not always represent real-life interactions well, we also conducted a final replication study in person with a robot that made errors while distributing goodie bags to campus community members at Oregon State, which reinforced our prior results.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="10b0ac5703e47214353a0b0843085bf0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DhHhh4yni1I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Humans React to a Cursing Robot in the Wild<a href="https://www.youtube.com/@naomi_fitter" rel="noopener noreferrer" target="_blank"></a></small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: Naomi Fitter</small></p><p>
	We have submitted this work, which represents a well-designed series of empirical experiments with interesting results and replications along the way, to several different journals and conferences. Despite consistently enthusiastic reviewer comments, no editors have yet accepted our work for publication—it seems to be the type of paper that editors are nervous to touch. Currently, the work is under review for a fourth time, for possible inclusion in the 2025 IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), in a paper titled “<a href="https://arxiv.org/abs/2505.05831" rel="noopener noreferrer" target="_blank">Oh F**k! How Do People Feel About Robots That Leverage Profanity?</a>”
</p><h3>Give Cursing Robots a Chance </h3><p>
	Based on our results, we think cursing robots deserve a chance! Our findings show that swearing robots would typically have little downside and some upside, especially in open-minded spaces such as university campuses. Even for the general public, reactions to errors with profanity yielded much less distaste than we expected. Our data showed that people cared more about whether robots acknowledged their error at all than whether or not they swore.
</p><p>
	People do have some reservations about cursing robots, especially when it comes to comfort and likability, so thoughtfulness may be required to apply curse words at the right time. For example, just as humans do, robots should likely hold back their swear words around children and be more careful in settings that typically demand cleaner language. Robot practitioners might also consider surveying individual users about profanity acceptance as they set up new technology in personal settings—rather than letting robotic systems learn the hard way, perhaps alienating users in the process.
</p><p>
	As more robots enter our day-to-day spaces, they are bound to make mistakes. How they react to these errors is important. Fundamentally, our work shows that people prefer robots that notice when a mistake has occurred and react to this error in a relatable way. And it seems that a range of styles in the response itself, from the profane to the mundane, can work well. So we invite designers to give cursing robots a chance!
</p>]]></description><pubDate>Tue, 03 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/cursing-social-robot-interaction</guid><category>Social robots</category><category>Human robot interaction</category><dc:creator>Naomi Fitter</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-a-robot-tripping-on-a-banana-peel-its-head-is-covered-by-a-speech-bubble-and-symbols-representing-an-expleti.jpg?id=60333045&amp;width=980"></media:content></item><item><title>Disaster Awaits if We Don’t Secure IoT Now</title><link>https://spectrum.ieee.org/iot-security-root-of-trust</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-robotic-arms-attached-to-an-assembly-line-equipped-with-internet-of-things-technology.jpg?id=60339966&width=1245&height=700&coordinates=0%2C291%2C0%2C292"/><br/><br/><p>In 2015, Ukraine experienced a slew of unexpected <a href="https://www.cisa.gov/news-events/ics-alerts/ir-alert-h-16-056-01" rel="noopener noreferrer" target="_blank">power outages</a>. Much of the country went dark. The U.S. investigation has concluded that this was due to a Russian state cyberattack on Ukrainian computers running critical infrastructure.</p><p>In the decade that followed, <a data-linked-post="2658630634" href="https://spectrum.ieee.org/cyberattacks" target="_blank">cyberattacks</a> on critical infrastructure and near misses <a href="https://www.rand.org/pubs/commentary/2024/02/threats-to-americas-critical-infrastructure-are-now-a-terrifying-reality.html" rel="noopener noreferrer" target="_blank">continued</a>. In 2017, a <a href="https://www.kcur.org/news/2022-03-24/federal-officials-say-russian-spies-tried-to-hack-into-a-kansas-nuclear-power-plant" rel="noopener noreferrer" target="_blank">nuclear power plant</a> in Kansas was the subject of a Russian cyberattack. In 2021, Chinese state actors reportedly gained access to parts of the New York City subway computer system. Later in 2021, a <a href="https://www.cnn.com/2021/06/02/business/beef-hack-jbs/index.html" rel="noopener noreferrer" target="_blank">cyberattack</a> temporarily closed down beef processing plants. In 2023, Microsoft reported a cyberattack on its <a href="https://www.microsoft.com/en-us/security/blog/2023/05/24/volt-typhoon-targets-us-critical-infrastructure-with-living-off-the-land-techniques/" rel="noopener noreferrer" target="_blank">IT systems</a>, likely by Chinese-backed actors.</p><p>The risk is growing, particularly when it comes to <a data-linked-post="2659970239" href="https://spectrum.ieee.org/iot-for-arson-forensics" target="_blank">Internet of things</a> (IoT) devices. Just below the veneer of popular fad gadgets (does anyone <em><em>really</em></em> want their refrigerator to automatically place orders for groceries?) is an increasing army of more prosaic Internet-connected devices that take care of keeping our world running. This is particularly true of a subclass called <a data-linked-post="2650277653" href="https://spectrum.ieee.org/the-industrial-internet-of-things" target="_blank">Industrial Internet of Things</a> (IIoT), devices that implement our communication networks, or control infrastructure such as power grids or chemical plants. IIoT devices can be small devices like valves or sensors, but also can include very substantial pieces of gear, such as an HVAC system, an MRI machine, a dual-use aerial drone, an elevator, a nuclear centrifuge, or a jet engine. </p><p>The number of current IoT devices is growing rapidly. In 2019, there were an <a href="https://explodingtopics.com/blog/iot-stats" rel="noopener noreferrer" target="_blank">estimated</a> 10 billion IoT devices in operation. At the end of 2024, it had almost doubled to <a href="https://iot-analytics.com/number-connected-iot-devices/" rel="noopener noreferrer" target="_blank">approximately</a> 19 billion. This number is set to more than double again by 2030. Cyberattacks aimed at those devices, motivated either by political or financial gain, can cause very real physical-world damage to entire communities, far beyond damage to the device itself.</p><p>Security for IoT devices is often an afterthought, as they often have little need for a “human interface” (i.e., maybe a valve in a chemical plant only needs commands to Open, Close, and Report), and usually they don’t contain information that would be viewed as sensitive (for example, thermostats don’t need credit cards, a medical device doesn’t have a Social Security number). What could go wrong?</p><p>Of course, “what could go wrong” depends on the device, but especially with carefully planned, at-scale attacks, it’s already been shown that a lot can go wrong. For example, armies of poorly secured, Internet-connected security cameras have<a href="https://www.cloudflare.com/learning/ddos/glossary/mirai-botnet/" rel="noopener noreferrer" target="_blank"> already</a> been put to use in coordinated distributed-denial-of-service attacks, where each camera makes a few harmless requests of some victim service, causing the service to collapse under the load.</p><h2>How to Secure IoT Devices</h2><p>Measures to defend these devices generally fall into two categories: basic cybersecurity hygiene and defense in depth.</p><p>Cybersecurity hygiene consists of a few rules: Don’t use default passwords on admin accounts, apply software updates regularly to remove newly discovered vulnerabilities, require cryptographic signatures to validate updates, and understand your “<a href="https://datatracker.ietf.org/group/scitt/about/" rel="noopener noreferrer" target="_blank">software supply chain</a>:” where your software comes from, where the supplier obtains components that it may simply be passing through from open-source projects.</p><p>The rapid profusion of open-source software has prompted development of the U.S. Government’s Software Bill of Materials (<a href="https://www.cisa.gov/sbom" rel="noopener noreferrer" target="_blank">SBOM</a>). This is a document that conveys supply-chain provenance, indicating which version of what packages went into making the product’s software. Both IIoT device suppliers and device users benefit from accurate SBOMs, shortening the path to determining if a specific device’s software may contain a version of a package vulnerable to attack. If the SBOM shows an up-to-date package version where the vulnerability has been addressed, both the IIoT vendor and user can breathe easy; if the package version listed in the SBOM is vulnerable, remediation may be in order.</p><p>Defense in depth is less well-known, and deserves more attention.</p><p>It is tempting to implement the easiest approach to cybersecurity, a “hard and crunchy on the outside, soft and chewy inside” model. This emphasizes perimeter defense, on the theory that if hackers can’t get in, they can’t do damage. But even the smallest IoT devices may have a software stack that’s too complex for the designers to fully comprehend, usually leading to obscure vulnerabilities in dark corners of the code. As soon as these vulnerabilities become known, the device transitions from tight, well-managed security to no security, as there’s no second line of defense.</p><p>Defense in depth is the answer. A National Institute of Standards and Technology <a href="https://csrc.nist.gov/pubs/sp/800/193/final" rel="noopener noreferrer" target="_blank">publication</a> breaks down this approach to cyber-resilience into three basic functions: <em><em>protect</em></em>, meaning use cybersecurity engineering to keep hackers out; <em><em>detect</em></em>, meaning add mechanisms to detect unexpected intrusions; and <em><em>remediate</em></em>, meaning take action to expel intruders to prevent subsequent damage. We will explore each of these in turn.</p><h2>Protect</h2><p>Systems that are designed for security use a layered approach, with most of the device’s “normal behavior” in an outer layer, while inner layers form a series of shells, each of which has smaller, more constrained functionality, making the inner shells progressively simpler to defend. These layers are often related to the sequence of steps followed during the initialization of the device, where the device starts in the inner layer with the smallest possible functionality, with just enough to get the next stage running, and so on until the outer layer is functional. </p><p>To ensure correct operation, each layer must also perform an integrity check on the next layer before starting it. In each ring, the current layer computes a fingerprint or signature of the next layer out.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Concentric circles with labels: hardware root of trust (if present), firmware, operating system loader, operating system kernel, application software. " class="rm-shortcode" data-rm-shortcode-id="a2918f8a33c7d367e9d5745220e28a3c" data-rm-shortcode-name="rebelmouse-image" id="a1c56" loading="lazy" src="https://spectrum.ieee.org/media-library/concentric-circles-with-labels-hardware-root-of-trust-if-present-firmware-operating-system-loader-operating-system-kernel.png?id=60339975&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">To make a defensible IoT device, the software needs to be layered, with each layer running only if the previous layer has deemed it safe. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Guy Fedorkow, Mark Montgomery </small></p><p>But there’s a puzzle here. Each layer is checking the next one before starting it, but who checks the first one? No one! The inner layer, whether the first checker is implemented in hardware or firmware, must be implicitly trusted for the rest of the system to be worthy of trust. As such, it’s called a Root of Trust (RoT). </p><p>Roots of Trust must be carefully protected, because a compromise of the Root of Trust may be impossible to detect without specialized test hardware. One approach is to put the firmware that implements the Root of Trust into read-only memory that can’t be modified once the device is manufactured. That’s great if you know your RoT code doesn’t have any bugs, and uses algorithms that can’t go obsolete. But few of us live in that world, so, at a minimum, we usually must protect the RoT code with some simple hardware that makes the firmware read-only after it’s done its job, but writable during its startup phase, allowing for carefully vetted, cryptographically signed updates. </p><p>Newer processor chips move this Root of Trust one step back into the processor chip itself, a hardware Root of Trust. This makes the RoT much more resistant to firmware vulnerabilities or a hardware-based attack, because firmware boot code is usually stored in nonvolatile flash memory where it can be reprogrammed by the system manufacturer (and also by hackers). An RoT inside the processor can be made much more difficult to hack.</p><h2>Detect</h2><p>Having a reliable Root of Trust, we can arrange so each layer is able to check the next for hacks. This process can be augmented with <a href="https://datatracker.ietf.org/doc/rfc9683/" rel="noopener noreferrer" target="_blank">Remote Attestation</a>, where we collect and report the fingerprints (called <em><em>attestation evidence</em></em>) gathered by each layer during the startup process. We can’t just ask the outer application layer if it’s been hacked; of course, any good hacker would ensure the answer is “No Way! You can trust me!”, no matter what.</p><p>But remote attestation adds a small bit of hardware, such as the <a href="https://trustedcomputinggroup.org/new-tcg-guidance-simplifies-creating-cyber-resilient-devices/" rel="noopener noreferrer" target="_blank">Trusted Platform Module</a> (TPM) defined by the Trusted Computing Group. This bit of hardware collects evidence in shielded locations made of special-purpose, hardware-isolated memory cells that can’t be directly changed by the processor at all. The TPM also provides protected capability, which ensures that new information can be added to the shielded locations, but previously stored information cannot be changed. And, it provides a protected capability that attaches a cryptographic signature to the contents of the Shielded Location to serve as evidence of the state of the machine, using a key known only to the Root of Trust hardware, called an Attestation Key (AK).</p><p>Given these functions, the application layer has no choice but to accurately report the attestation evidence, as proven by use of the RoT’s AK secret key. Any attempt to tamper with the evidence would invalidate the signature provided by the AK. At a remote location, a verifier can then validate the signature and check that all the fingerprints reported line up with known, trusted, versions of the device’s software. These known-good fingerprints, called endorsements, must come from a trusted source, such as the device manufacturer.  </p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A flow chart showing device manufacturer flowing to attester and verifier. " class="rm-shortcode" data-rm-shortcode-id="1ae39c03bcd527b4c6371f010c8e3fe3" data-rm-shortcode-name="rebelmouse-image" id="2f6fb" loading="lazy" src="https://spectrum.ieee.org/media-library/a-flow-chart-showing-device-manufacturer-flowing-to-attester-and-verifier.png?id=60339987&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">To verify that it’s safe to turn on an IoT device, one can use an attestation and verification protocol provided by the Trusted Computing Group. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Guy Fedorkow, Mark Montgomery</small></p><p>In practice, the Root of Trust may contain several separate mechanisms to protect individual functions, such as boot integrity, attestation and device identity, and the device designer is always responsible for assembling the specific components most appropriate for the device, then carefully integrating them, but organizations like Trusted Computing Group offer guidance and specifications for components that can offer considerable help, such as the Trusted Platform Module (TPM) commonly used in many larger computer systems.</p><h2>Remediate</h2><p>Once an anomaly is detected, there are a wide range of actions to remediate. A simple option is power-cycling the device or refreshing its software. However, trusted components inside the devices themselves may help with remediation through the use of authenticated watchdog timers or other approaches that cause the device to reset itself if it can’t demonstrate good health. <a href="https://trustedcomputinggroup.org/work-groups/cyber-resilient-technologies/" rel="noopener noreferrer" target="_blank"> Trusted Computing Group Cyber Resilience</a> provides guidance for these techniques.</p><p>The requirements outlined here have been available and used in specialized high-security applications for some years, and many of the attacks have been known for a decade. In the last few years, Root of Trust implementations have become widely used in <a href="https://techcommunity.microsoft.com/blog/surfaceitpro/study-highlights-critical-role-of-surface-firmware-protection/2245244" rel="noopener noreferrer" target="_blank">some laptop families</a>. But until recently, blocking Root of Trust attacks has been challenging and expensive even for cyberexperts in the IIoT space. Fortunately, many of the silicon vendors that supply the underlying IoT hardware are <a href="https://docs.broadcom.com/doc/53650-PB" rel="noopener noreferrer" target="_blank">now</a><a href="https://docs.broadcom.com/doc/Data-Center-Security-WP100" rel="noopener noreferrer" target="_blank"> including</a><a href="https://www.marvell.com/content/dam/marvell/en/public-collateral/embedded-processors/marvell-infrastructure-processors-octeon-tx2-cn913x-product-brief.pdf" rel="noopener noreferrer" target="_blank"> these</a> high-security<a href="https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/010/boot-guard-technology/" rel="noopener noreferrer" target="_blank"> mechanisms</a> even in the budget-minded embedded chips, and reliable software stacks have evolved to make mechanisms for Root of Trust defense more available to any designer who wants to use it.</p><p>While the IIoT device designer has the responsibility to provide these cybersecurity mechanisms, it’s up to system integrators, who are responsible for the security of an overall service interconnecting IoT devices, to require the features from their suppliers, and to coordinate features inside the device with external resilience and monitoring mechanisms, all to take full advantage of the improved security now more readily available than ever.</p><p>Mind your roots of trust!</p>]]></description><pubDate>Mon, 02 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/iot-security-root-of-trust</guid><category>Cybersecurity</category><category>Internet of things</category><category>Cyberattacks</category><dc:creator>Guy Fedorkow</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-robotic-arms-attached-to-an-assembly-line-equipped-with-internet-of-things-technology.jpg?id=60339966&amp;width=980"></media:content></item><item><title>Startup’s Analog AI Promises Power for PCs</title><link>https://spectrum.ieee.org/analog-ai-chip-architecture</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-five-silver-chips-and-many-black-components.jpg?id=60345071&width=1245&height=700&coordinates=0%2C187%2C0%2C187"/><br/><br/><p><a href="https://ece.princeton.edu/people/naveen-verma" rel="noopener noreferrer" target="_blank">Naveen Verma’</a>s lab at Princeton University is like a museum of all the ways engineers have tried to make AI ultra-efficient by using analog phenomena instead of digital computing. At one bench lies the most energy-efficient magnetic-memory-based neural-network computer ever made. At another you’ll find a resistive-memory-based chip that can compute the largest matrix of numbers of any <a href="https://spectrum.ieee.org/tag/analog-ai" target="_self">analog AI</a> system yet.</p><p>Neither has a commercial future, according to Verma. Less charitably, this part of his lab is a graveyard.</p><p>Analog AI has captured chip architects’ imagination for years. It combines two key concepts that should make machine learning massively less energy intensive. First, it limits the costly movement of bits between memory chips and processors. Second, instead of the 1s and 0s of logic, it uses the physics of the flow of current to efficiently do machine learning’s key computation.</p><p>As attractive as the idea has been, various analog AI schemes have not delivered in a way that could really take a bite out of AI’s stupefying energy appetite. Verma would know. He’s tried them all.</p><p>But when <em>IEEE Spectrum</em> visited a year ago, there was a chip at the back of Verma’s lab that represents some hope for analog AI and for the energy-efficient computing needed to make AI useful and ubiquitous. Instead of calculating with current, the chip sums up charge. It might seem like an inconsequential difference, but it could be the key to overcoming the noise that hinders every other analog AI scheme.</p><p>This week, Verma’s startup <a href="https://www.enchargeai.com/" rel="noopener noreferrer" target="_blank">EnCharge AI</a> unveiled the first chip based on this new architecture, the EN100. The startup claims the chip tackles various AI work with performance per watt up to 20 times better than competing chips. It’s designed into a single processor card that adds 200 trillion operations per second at 8.25 watts, aimed at conserving battery life in AI-capable laptops. On top of that, a 4-chip, 1,000-trillion-operations-per-second card is targeted for AI workstations.</p><h2>Current and Coincidence</h2><p>In machine learning, “it turns out, by dumb luck, the main operation we’re doing is matrix multiplies,” says Verma. That’s basically taking an array of numbers, multiplying it by another array, and adding up the result of all those multiplications. Early on, engineers noticed a coincidence: Two fundamental rules of electrical engineering can do exactly that operation. Ohm’s Law says that you get current by multiplying voltage and conductance. And Kirchoff’s Current Law says that if you have a bunch of currents coming into a point from a bunch of wires, the sum of those currents is what leaves that point. So basically, each of a bunch of input voltages pushes current through a resistance (conductance is the inverse of resistance), multiplying the voltage value, and all those currents add up to produce a single value. Math, done.</p><p>Sound good? Well, it gets better. Much of the data that makes up a neural network are the “weights,” the things by which you multiply the input. And moving that data from memory into a processor’s logic to do the work is responsible for a big fraction of the energy GPUs expend. Instead, in most analog AI schemes, the weights are stored in one of several types of nonvolatile memory as a conductance value (the resistances above). Because weight data is already where it needs to be to do the computation, it doesn’t have to be moved as much, saving a pile of energy.</p><p>The combination of free math and stationary data promises calculations that need just <a href="https://spectrum.ieee.org/analog-ai" target="_self">thousandths of a trillionth of joule of energy</a>. Unfortunately, that’s not nearly what analog AI efforts have been delivering.</p><h2>The Trouble With Current</h2><p>The fundamental problem with any kind of analog computing has always been the signal-to-noise ratio. Analog AI has it by the truckload. The signal, in this case the sum of all those multiplications, tends to be overwhelmed by the many possible sources of noise.</p><p>“The problem is, semiconductor devices are messy things,” says Verma. Say you’ve got an analog neural network where the weights are stored as conductances in individual RRAM cells. Such weight values are stored by setting a relatively high voltage across the RRAM cell for a defined period of time. The trouble is, you could set the exact same voltage on two cells for the same amount of time, and those two cells would wind up with slightly different conductance values. Worse still, those conductance values might change with temperature.</p><p>The differences might be small, but recall that the operation is adding up many multiplications, so the noise gets magnified. Worse, the resulting current is then turned into a voltage that is the input of the next layer of neural networks, a step that adds to the noise even more.</p><p>Researchers have attacked this problem from both a computer science perspective and a device physics one. In the hope of compensating for the noise, researchers have invented ways to bake some knowledge of the physical foibles of devices into their neural network models. Others have focused on making devices that behave as predictably as possible. IBM, which has done <a href="https://spectrum.ieee.org/analog-ai" target="_self">extensive research in this area</a>, does both.</p><p>Such techniques are competitive, if not yet commercially successful, in smaller-scale systems, chips meant to provide low-power machine learning to devices at the edges of IoT networks. Early entrant <a href="https://spectrum.ieee.org/two-startups-use-processing-in-flash-memory-for-ai-at-the-edge" target="_blank">Mythic AI </a>has produced more than one generation of its analog AI chip, but it’s competing in a field where low-power digital chips are succeeding.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black circuit board with a large silver chip at center." class="rm-shortcode" data-rm-shortcode-id="46bc434f675fd57bfe4383c42e4d2043" data-rm-shortcode-name="rebelmouse-image" id="71fa2" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-a-large-silver-chip-at-center.jpg?id=60345147&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The EN100 card for PCs is a new analog AI chip architecture.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">EnCharge AI</small></p><h2>Capacitors All the Way Down</h2><p>EnCharge’s solution strips out the noise by measuring the amount of charge instead of flow of charge in machine learning’s multiply-and-accumulate mantra. In traditional analog AI, multiplication depends on the relationship among voltage, conductance, and current. In this new scheme, it depends on the relationship among voltage, capacitance, and charge—where basically, charge equals capacitance times voltage.</p><p>Why is that difference important? It comes down to the component that’s doing the multiplication. Instead of using some finicky, vulnerable device like RRAM, EnCharge uses capacitors.</p><p>A capacitor is basically two conductors sandwiching an insulator. A voltage difference between the conductors causes charge to accumulate on one of them. The thing that’s key about them for the purpose of machine learning is that their value, the capacitance, is determined by their size. (More conductor area or less space between the conductors means more capacitance.)</p><p>“The only thing they depend on is geometry, basically the space between wires,” Verma says. “And that’s the one thing you can control very, very well in CMOS technologies.” EnCharge builds an array of precisely valued capacitors in the layers of copper interconnect above the silicon of its processors.</p><p>The data that makes up most of a neural network model, the weights, are stored in an array of digital memory cells, each connected to a capacitor. The data the neural network is analyzing is then multiplied by the weight bits using simple logic built into the cell, and the results are stored as charge on the capacitors. Then the array switches into a mode where all the charges from the results of multiplications accumulate and the result is digitized. </p><p>While the initial <a href="https://www.enchargeai.com/technology" rel="noopener noreferrer" target="_blank">invention</a>, which dates back to 2017, was a big moment for Verma’s lab, he says the basic concept is quite old. “It’s called switched capacitor operation; it turns out we’ve been doing it for decades,” he says. It’s used, for example, in commercial high-precision analog-to-digital converters. “Our innovation was figuring out how you can use it in an architecture that does in-memory computing.”</p><h2>Competition</h2><p>Verma’s lab and EnCharge spent years proving that the technology was programmable and scalable and co-optimizing it with an architecture and software stack that suits AI needs that are vastly different than they were in 2017. The resulting products are with early-access developers now, and the company—which <a href="https://www.enchargeai.com/news-and-publications/series-b" rel="noopener noreferrer" target="_blank">recently raised US $100 million</a> from Samsung Venture, Foxconn, and others—plans another round of early access collaborations.</p><p>But EnCharge is entering a competitive field, and among the competitors is the big kahuna, Nvidia. At its big developer event in March, GTC, Nvidia announced plans for a <a href="https://www.nvidia.com/en-us/products/workstations/dgx-spark/" rel="noopener noreferrer" target="_blank">PC product</a> built around its GB10 CPU-GPU combination and <a href="https://www.nvidia.com/en-us/products/workstations/dgx-station/" rel="noopener noreferrer" target="_blank">workstation</a> built around the upcoming <a href="https://www.nvidia.com/en-us/data-center/gb300-nvl72/" rel="noopener noreferrer" target="_blank">GB300</a>.</p><p>And there will be plenty of competition in the low-power space EnCharge is after. Some of them even use a form of computing-in-memory. <a href="https://www.d-matrix.ai/" target="_blank">D-Matrix</a> and <a href="https://axelera.ai/" target="_blank">Axelera</a>, for example, took part of analog AI’s promise, embedding the memory in the computing, but do everything digitally. They each developed custom SRAM memory cells that both store and multiply and do the summation operation digitally, as well. There’s even at least one more-traditional analog AI startup in the mix, <a href="https://spectrum.ieee.org/analog-ai-2669898661" target="_self">Sagence</a>.</p><p>Verma is, unsurprisingly, optimistic. The new technology “means advanced, secure, and personalized AI can run locally, without relying on cloud infrastructure,” he said in a <a href="https://www.businesswire.com/news/home/20250529108055/en/EnCharge-AI-Announces-EN100-First-of-its-Kind-AI-Accelerator-for-On-Device-Computing" target="_blank">statement</a>. “We hope this will radically expand what you can do with AI.”</p>]]></description><pubDate>Mon, 02 Jun 2025 14:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/analog-ai-chip-architecture</guid><category>Analog ai</category><category>Artificial intelligence</category><category>Ibm</category><category>Nvidia</category><category>Capacitors</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-circuit-board-with-five-silver-chips-and-many-black-components.jpg?id=60345071&amp;width=980"></media:content></item><item><title>How Ukraine’s Killer Drones Are Beating Russian Jamming</title><link>https://spectrum.ieee.org/ukraine-killer-drones</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-soldiers-in-full-military-dress-stand-on-a-hill-while-one-of-them-releases-a-drone.jpg?id=59800494&width=1245&height=700&coordinates=0%2C80%2C0%2C80"/><br/><br/><div class="intro-text">
<em><a href="https://www.axios.com/2025/06/01/ukraine-drone-strikes-russia" rel="noopener noreferrer" target="_blank">Ukraine’s 1 June attack</a> on multiple Russian military bases destroyed or damaged as many as 41 Russian aircraft, including some of the country’s most advanced bombers. Estimates of the sum total of the damage range from US $2 billion to $7 billion. Supposedly planned for a <a href="https://www.theguardian.com/world/2025/jun/02/operation-spiderweb-visual-guide-ukraine-drone-attack-russian-aircraft" rel="noopener noreferrer" target="_blank">year and a half</a>, the Ukrainian operation was exceptional in its sophistication: Ukrainian agents reportedly smuggled dozens of first-person-view attack drones into Russia on trucks, <a href="https://www.nytimes.com/2025/06/02/world/europe/ukraine-russia-drone-strikes.html" rel="noopener noreferrer" target="_blank">situating them close to the air bases</a> where the target aircraft were vulnerable on tarmacs. The bases included one in Irkutsk, 4,300 kilometers from Ukraine, and another in south Murmansk, 1,800 km away. Remote pilots in Ukraine then launched the killer drones simultaneously.
	</em>
</div><div class="intro-text">
<em>
	The far-reaching operation was being hailed as the most inventive and bold of the war so far. Indeed, </em>IEEE Spectrum<em> has been regularly covering the ascent of Ukraine’s military drone programs, both </em><em><a href="https://spectrum.ieee.org/ukraine-hackers-war" target="_self">offensive</a></em><em> and </em><em><a href="https://spectrum.ieee.org/ukraine-air-defense" target="_self">defensive</a></em><em>, and for </em><em><a href="https://spectrum.ieee.org/drone-warfare-ukraine">air</a></em><em>, </em><em><a href="https://spectrum.ieee.org/sea-drone" target="_self">marine</a></em><em>, and </em><em><a href="https://spectrum.ieee.org/ukraine-drones-2671254184" target="_self">land</a></em><em> missions. In this article, originally posted on April 6, we described another bold Ukrainian drone initiative, which was applying artificial intelligence-based navigational software to enable killer drones to navigate to targets even in the presence of heavy jamming.</em>
</div><p class="drop-caps">
<strong><span></span>After the Estonian startup </strong><a href="https://www.krattworks.com/" target="_blank">KrattWorks</a> dispatched the first batch of its <a href="https://www.krattworks.com/isr-ghostdragon" target="_blank">Ghost Dragon ISR</a> quadcopters to Ukraine in mid-2022, the company’s officers thought they might have six months or so before they’d need to reconceive the drones in response to new battlefield realities. The 46-centimeter-wide flier was far more robust than the hobbyist-grade UAVs that came to define the <a href="https://spectrum.ieee.org/ukraine-hackers-war" target="_self">early days of the drone war</a> against Russia. But within a scant three months, the Estonian team realized their painstakingly fine-tuned device had already become obsolete.
</p><p class="ieee-inbody-related">
	Related: 
	<a href="https://spectrum.ieee.org/ukraine-drones-2671254184" target="_blank">Ukraine Tech Turns Combat into Real-Life “Game”</a>
</p><p>
	Rapid advances in 
	<a href="https://spectrum.ieee.org/tag/jamming" target="_self">jamming</a> and <a href="https://spectrum.ieee.org/tag/spoofing" target="_self">spoofing</a>—the only efficient defense against drone attacks—set the team on an unceasing marathon of innovation. Its latest technology is a neural-network-driven optical navigation system, which allows the drone to continue its mission even when all radio and satellite-navigation links are jammed. It began tests in <a href="https://spectrum.ieee.org/tag/ukraine" target="_self">Ukraine</a> in December, part of a trend toward jam-resistant, <a href="https://spectrum.ieee.org/search/?q=autonomous+drones" target="_self">autonomous UAVs</a> (uncrewed aerial vehicles). The new fliers herald yet another phase in the unending struggle that pits drones against the jamming and spoofing of <a href="https://spectrum.ieee.org/the-fall-and-rise-of-russian-electronic-warfare" target="_self">electronic warfare,</a> which aims to sever links between drones and their operators. There are now <a href="https://www.nytimes.com/interactive/2025/03/03/world/europe/ukraine-russia-war-drones-deaths.html" rel="noopener noreferrer" target="_blank">tens of thousands</a> of jammers straddling the front lines of the war, defending against drones that are not just killing soldiers but also destroying armored vehicles, other drones, <a href="https://kyivindependent.com/russia-missile-attack/" rel="noopener noreferrer" target="_blank">industrial infrastructure</a>, and even tanks.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="A man wearing a dark-green long-sleeve t-shirt, seen from behind, holds a drone with both hands above his head." class="rm-shortcode" data-rm-shortcode-id="670ed8b9ef5644576b6f8f9836a06576" data-rm-shortcode-name="rebelmouse-image" id="090b5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-wearing-a-dark-green-long-sleeve-t-shirt-seen-from-behind-holds-a-drone-with-both-hands-above-his-head.jpg?id=59800469&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">During tests near Kyiv, Ukraine, in 2024, a technician prepared to release a drone outfitted with software by Auterion.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
	Justyna Mielnikiewicz
	</small>
</p><p>
	“The situation with electronic warfare is moving extremely fast,” says Martin Karmin, KrattWorks’ cofounder and chief operations officer. “We have to constantly iterate. It’s like a cat-and-mouse game.”
</p><p>
	I met Karmin at the company’s headquarters in the outskirts of Estonia’s capital, Tallinn. Just a couple of hundred kilometers to the east is the tiny nation’s border with Russia, its former oppressor. At 38, Karmin is barely old enough to remember what life was like under Russian rule, but he’s heard plenty. He and his colleagues, most of them volunteer members of the 
	<a href="https://www.kaitseliit.ee/en/edl" rel="noopener noreferrer" target="_blank">Estonian Defense League</a>, have “no illusions” about <a href="https://spectrum.ieee.org/tag/russia" target="_self">Russia</a>, he says with a shrug.
</p><p>
	His company is as much about arming Estonia as it is about helping Ukraine, he acknowledges. Estonia is not officially at war with Russia, of course, but regions around the border between the two countries have for years been subjected to persistent jamming of satellite-based navigation systems, such as the 
	<a href="https://defence-industry-space.ec.europa.eu/eu-space/galileo-satellite-navigation_en" rel="noopener noreferrer" target="_blank">European Union’s Galileo satellites</a>, forcing occasional flight cancellations at Tartu airport. In November, <a href="https://spectrum.ieee.org/tag/satellite" target="_self">satellite</a> imagery revealed that Russia is expanding its military bases along the Baltic states’ borders.
</p><p>
	“We are a small country,” Karmin says. “Innovation is our only chance.”
</p><h2>Navigating by Neural Network</h2><p>
	In KrattWorks’ spacious, white-walled workshop, a handful of engineers are testing software. On the large ocher desk that dominates the room, a selection of KrattWorks’ devices is on display, including a couple of fixed-wing, smoke-colored UAVs designed to serve as aerial decoys, and the Ghost Dragon ISR 
	<a href="https://spectrum.ieee.org/tag/quadcopter" target="_self">quadcopter</a>, the company’s flagship product.
</p><p>
	Now in its third generation, the Ghost Dragon has come a long way since 2022. Its original command-and-control-band 
	<a href="https://spectrum.ieee.org/tag/radio" target="_self">radio</a> was quickly replaced with a smart frequency-hopping system that constantly scans the available spectrum, looking for bands that aren’t jammed. It allows operators to switch among six radio-frequency bands to maintain control and also send back video even in the face of hostile jamming.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A black quadcopter drone hovers in front of a coniferous tree." class="rm-shortcode" data-rm-shortcode-id="a63ed6b62a8e73e583b5ce084557e634" data-rm-shortcode-name="rebelmouse-image" id="ca339" loading="lazy" src="https://spectrum.ieee.org/media-library/a-black-quadcopter-drone-hovers-in-front-of-a-coniferous-tree.jpg?id=59800498&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The Ghost Dragon reconnaissance drone from Krattworks can navigate autonomously, by detecting landmarks as it flies over them. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">KrattWorks</small>
</p><p>
	The drone’s dual-band satellite-navigation receiver can switch among the four main satellite positioning services: 
	<a href="https://family1st.io/gps-vs-glonass-vs-galileo-whats-the-best-gnss/" rel="noopener noreferrer" target="_blank">GPS, Galileo</a>, China’s BeiDou, and Russia’s GLONASS. It’s been augmented with a spoof-proof algorithm that compares the satellite-navigation input with data from onboard sensors. The system provides protection against sophisticated spoofing attacks that attempt to trick drones into self-destruction by persuading them they’re flying at a much higher altitude than they actually are.
</p><h3></h3><br/><div class="rblad-ieee_in_content"></div><p>
	At the heart of the quadcopter’s matte grey body is a machine-vision-enabled computer running a 1-gigahertz Arm processor that provides the Ghost Dragon with its latest superpower: the ability to navigate autonomously, without access to any global navigation satellite system (GNSS). To do that, the computer runs a 
	<a href="https://spectrum.ieee.org/tag/neural-network" target="_self">neural network</a> that, like an old-fashioned traveler, compares views of landmarks with positions on a map to determine its position. More precisely, the drone uses real-time views from a downward-facing optical camera, comparing them against stored satellite images, to determine its position.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c7a71f63bfede8e7831651bd1a76a5e8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/tGCFBHbw6HQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A promotional video from Krattworks depicts scenarios in which the company’s drones augment soldiers on offensive maneuvers.</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">KrattWorks</small>
</p><p>
	“Even if it gets lost, it can recognize some patterns, like crossroads, and update its position,” Karmin says. “It can make its own decisions, somewhat, either to return home or to fly through the jamming bubble until it can reestablish the GNSS link again.”
</p><h2>Designing Drones for High Lethality per Cost</h2><p>
	Just as machine guns and tanks defined the First World War, drones have become emblematic of Ukraine’s struggle against Russia. It was the besieged Ukraine that first turned the concept of a military drone on its head. Instead of Predators and Reapers worth tens of millions of dollars each, Ukraine began purchasing huge numbers of off-the-shelf fliers worth a few hundred dollars apiece—the kind used by filmmakers and enthusiasts—and turned them into highly lethal weapons. A recent 
	<a href="https://www.nytimes.com/interactive/2025/03/03/world/europe/ukraine-russia-war-drones-deaths.html" rel="noopener noreferrer" target="_blank"><em><em>New York Times</em></em> investigation</a> found that drones account for 70 percent of deaths and injuries in the ongoing conflict.
</p><p>
	“We have much less artillery than Russia, so we had to compensate with drones,” says 
	<a href="https://www.linkedin.com/in/serhii-skoryk-20b02728b/?originalSubdomain=ua" rel="noopener noreferrer" target="_blank">Serhii Skoryk</a>, commercial director at <a href="https://kvertus.ua/" rel="noopener noreferrer" target="_blank">Kvertus</a>, a Kyiv-based electronic-warfare company. “A missile is worth perhaps a million dollars and can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A man in camouflage uniform is surrounded by military gear, including drones. " class="rm-shortcode" data-rm-shortcode-id="c6a0b6470088c9d32d62087a112dae9a" data-rm-shortcode-name="rebelmouse-image" id="a5d2c" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-in-camouflage-uniform-is-surrounded-by-military-gear-including-drones.jpg?id=59800500&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Near the Russian border in Kharkiv Oblast, a Ukrainian soldier prepared first-person-view drones for an attack on 16 January 2025.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Jose Colon/Anadolu/Getty Images</small>
</p><p>
	Electronic warfare techniques such as jamming and spoofing aim to neutralize the drone threat. A drone that gets jammed and loses contact with its pilot and also loses its spatial bearings will either crash or fly off randomly until its battery dies.
	<a href="https://static.rusi.org/403-SR-Russian-Tactics-web-final.pdf" rel="noopener noreferrer" target="_blank"> According to the Royal United Services Institute</a>, a U.K. defense think tank, Ukraine may be losing about 10,000 drones per month, mostly due to jamming. That number includes explosives-laden kamikaze drones that don’t reach their targets, as well as surveillance and reconnaissance drones like KrattWorks’ Ghost Dragon, meant for longer service.
</p><p>
	“Drones have become a consumable item,” says Karmin. “You will get maybe 10 or 15 missions out of a reconnaissance drone, and then it has to be already paid off because you will lose it sooner or later.”
</p><p class="pull-quote">
	 Russia took an unexpected step in the summer of 2024, ditching sophisticated wireless control in favor of hard-wired drones fitted with spools of optical fiber.
</p><p>
	Tech minds on both sides of the conflict have therefore been working hard to circumvent electronic defenses. Russia took an unexpected step starting in early 2024, deploying hard-wired drones fitted with spools of optical fiber. Like a twisted variation on a child’s kite, the lethal UAVs can venture 20 or more kilometers away from the controller, the hair-thin fiber floating behind them, providing an unjammable connection.
</p><p>
	“Right now, there is no protection against fiber-optic drones,” 
	<a href="https://www.linkedin.com/in/vadym-burukin-1378abbb/?originalSubdomain=ua" rel="noopener noreferrer" target="_blank">Vadym Burukin</a>, cofounder of the Ukrainian drone startup <a href="https://huless.com/" rel="noopener noreferrer" target="_blank">Huless</a>, tells <em><em>IEEE</em></em> <em><em>Spectrum</em></em>. “The Russians scaled this solution pretty fast, and now they are saturating the battle front with these drones. It’s a huge problem for Ukraine.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A drone carrying a large cylindrical object flies over a blurry forest background." class="rm-shortcode" data-rm-shortcode-id="c5a9250fbac728b7b2a951af2e4492d2" data-rm-shortcode-name="rebelmouse-image" id="b73cf" loading="lazy" src="https://spectrum.ieee.org/media-library/a-drone-carrying-a-large-cylindrical-object-flies-over-a-blurry-forest-background.jpg?id=59800502&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">One way that drone operators can defeat electronic jamming is by communicating with their drone via a fiber optic line that pays out of a spool as the drone flies. This is a tactic favored by Russian units, although this particular first-person-view drone is Ukrainian. It was demonstrated near Kyiv on 29 January 2025.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Efrem Lukatsky/AP</small>
</p><p>
	Ukraine, too, has experimented with optical fiber, but the technology didn’t take off, as it were. “The optical fiber costs upwards from $500, which is, in many cases, more than the drone itself,” Burukin says. “If you use it in a drone that carries explosives, you lose some of that capacity because you have the weight of the cable.” The extra weight also means less capacity for better-quality cameras, sensors, and computers in reconnaissance drones.
</p><h2>Small Drones May Soon Be Making Kill-or-No-Kill Decisions</h2><p>
	Instead, Ukraine sees the future in autonomous navigation. This past July, kamikaze drones equipped with an autonomous navigation system from U.S. supplier
	<a href="https://auterion.com/" rel="noopener noreferrer" target="_blank"> Auterion</a> destroyed a column of Russian tanks fitted with jamming devices.
</p><p>
	“It was really hard to strike these tanks because they were jamming everything,” says Burukin. “The drones with the autopilot were the only equipment that could stop them.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A diagram shows a quadcopter drone flying above a communications tower as it attempts to navigate to an enemy tank." class="rm-shortcode" data-rm-shortcode-id="4009b55dda0d71df9507b2de4cc944ef" data-rm-shortcode-name="rebelmouse-image" id="3ddb9" loading="lazy" src="https://spectrum.ieee.org/media-library/a-diagram-shows-a-quadcopter-drone-flying-above-a-communications-tower-as-it-attempts-to-navigate-to-an-enemy-tank.jpg?id=59800510&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Auterion’s “terminal guidance” system uses known landmarks to orient a drone as it seeks out a target. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Auterion</small>
</p><p>
	The technology used to hit those tanks is called terminal guidance and is the first step toward smart, fully autonomous drones, according to Auterion’s CEO, Lorenz Meier. The system allows the drone to directly overcome the jamming whether the protected target is a tank, a trench, or a military airfield.
</p><p>
	“If you lock on the target from, let’s say, a kilometer away and you get jammed as you approach the target, it doesn’t matter,” Meier says in an interview. “You’re not losing the target as a manual operator would.”
</p><p>
	The visual navigation technology trialed by KrattWorks is the next step and an innovation that has only reached the battlefield this year. Meier expects that by the end of 2025, firms including his own will introduce fully autonomous solutions encompassing visual navigation to overcome GPS jamming, as well as terminal guidance and smart target recognition.
</p><p>
	“The operator would only decide the area where to strike, but the decision about the target is made by the drone,” Meier explains. “It’s already done with guided shells, but with drones you can do that at mass scale and over much greater distances.”
</p><p>
	Auterion, founded in 2017 to produce drone software for civilian applications such as grocery delivery, threw itself into the war effort in early 2024, motivated by a desire to equip democratic countries with technologies to help them defend themselves against authoritarian regimes. Since then, the company has made rapid strides, working closely with Ukrainian drone makers and troops.
</p><p class="pull-quote">
<span>“A missile worth perhaps a million dollars can kill maybe 12 or 20 people. But for one million dollars, you can buy 10,000 drones, put four grenades on each, and they will kill 1,000 or even 2,000 people or destroy 200 tanks.” <strong>—Serhii Skoryk, Kvertus</strong></span>
</p><p>
	But purchasing Western equipment is, in the long term, not affordable for Ukraine, a country with a per capita GDP of 
	<a href="https://www.imf.org/external/datamapper/profile/UKR" rel="noopener noreferrer" target="_blank">US $5,760</a>—much lower than the European average of <a href="https://www.imf.org/external/datamapper/profile/EUQ" rel="noopener noreferrer" target="_blank">$38,270</a>. Fortunately, Ukraine can tap its engineering workforce, which is among the largest in Europe. Before the war, Ukraine was a go-to place for Western companies looking to set up IT- and software-development centers. Many of these workers have since joined Ukraine’s DIY military-technician (“miltech”) development movement.
</p><p>
	An engineer and founder at a Ukrainian startup that produces long-range kamikaze drones, who didn’t want to be named because of security concerns, told 
	<em><em>Spectrum</em></em> that the company began developing its own computers and autonomous navigation software for target tracking “just to keep the price down.” The engineer said Ukrainian startups offer advanced military-drone technology at a price that is a small fraction of what established competitors in the West are charging.
</p><p>
	Within three years of the February 2022 Russian invasion, Ukraine produced a world-class defense-tech ecosystem that is not only attracting Western innovators into its fold, but also regularly surpassing them. The keys to Ukraine’s success are rapid iterations and close cooperation with frontline troops. It’s a formula that’s working for Auterion as well. “If you want to build a leading product, you need to be where the product is needed the most,” says Meier. “That’s why we’re in Ukraine.”
</p><p>
	Burukin, from Ukrainian startup Huless, believes that autonomy will play a bigger role in the future of drone warfare than 
	<a href="https://www.rferl.org/a/russia-fiber-optic-drones-ukraine-battlefield/33270243.html" rel="noopener noreferrer" target="_blank">Russia’s optical fibers</a> will. Autonomous drones not only evade jamming, but their range is limited only by their battery storage. They also can carry more explosives or better cameras and sensors than the wired drones can. On top of that, they don’t place high demands on their operators.
</p><p>
	“In the perfect world, the drone should take off, fly, find the target, strike it, and report back on the task,” Burukin says. “That’s where the development is heading.”
</p><p>
	The cat-and-mouse game is nowhere near over. Companies including KrattWorks are already thinking about the next innovation that would make drone warfare cheaper and more lethal. By creating a drone mesh network, for example, they could send a sophisticated intelligence, surveillance, and reconnaissance drone followed by a swarm of simpler kamikaze drones to find and attack a target using visual navigation.
</p><p>
	“You can send, like, 10 drones, but because they can fly themselves, you don’t need a superskilled operator controlling every single one of these,” notes KrattWorks’ Karmin, who keeps tabs on tech developments in Ukraine with a mixture of professional interest, personal empathy, and foreboding. Rarely does a day go by that he does not think about the expanding Russian military presence near Estonia’s eastern borders.
</p><p>
	“We don’t have a lot of people in Estonia,” he says. “We will never have enough skilled drone pilots. We must find another way.” 
	<span class="ieee-end-mark"></span>
</p>]]></description><pubDate>Mon, 02 Jun 2025 14:00:00 +0000</pubDate><guid>https://spectrum.ieee.org/ukraine-killer-drones</guid><category>Drone war</category><category>Drones</category><category>Neural network</category><category>Russian jamming</category><category>Ukraine</category><dc:creator>Tereza Pultarova</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-soldiers-in-full-military-dress-stand-on-a-hill-while-one-of-them-releases-a-drone.jpg?id=59800494&amp;width=980"></media:content></item><item><title>IEEE President’s Note: One IEEE for Education</title><link>https://spectrum.ieee.org/ieee-presidents-note-june-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-photo-of-a-smiling-woman-in-a-magenta-suit.jpg?id=56605851&width=1245&height=700&coordinates=0%2C517%2C0%2C517"/><br/><br/><p>
	Education plays a critical role in building the abilities and interests of our next generation, as well as developing the technical expertise and knowledge needed for our near-career technologists to join the professional world and be among those who design, build, and maintain the systems and devices that form the foundation of modern society and advance us forward.
</p><p>
	IEEE’s role in the education of engineers and technologists, and its devotion to knowledge-sharing as a trusted source across our fields, has been part of our organization from its creation. The effects are so pervasive that goals for degreed education and ideals of professional responsibility stemming from IEEE (or its historic predecessors AIEE and IRE) have shaped the knowledge and perspectives of most engineers and degreed computing professionals. IEEE is a mostly unsung world leader to the public because these contributions often go unnoticed.
</p><p>
	To support IEEE’s continued role in education as a positive force for advancing technology, we must ensure that our efforts are more widely recognized as we fulfill our mission and expand our value in engineering and technology across the globe.
</p><p>
	Strategic initiatives are a catalyst to prioritize our shared goal of better education for engineers and technologists and can be achieved by promoting collaboration and excellence across the organization.
</p><p>
	Advancing One IEEE for Education enables us to leverage our collective strengths. As part of this, I have created a committee whose focus is to map and strengthen IEEE’s collective future by developing a One IEEE strategy to empower technical innovation through education.
</p><p>
	Fostering education is a primary purpose for our organization, which is a public charity. IEEE’s focus on professional development and commitment to providing opportunities for lifelong learning, from preuniversity, university, and graduate students to professionals across our fields of interest is crucial not only for members but also the broader IEEE mission of advancing technology education and ensuring workforce readiness around the globe.
</p><h2>A primer on IEEE educational programs</h2><p>
	Education is the pathway to becoming an engineer or technologist. It is, perhaps because of this, that education is an almost universal value across IEEE, one that drives activities throughout our technical communities.
</p><p>
	The <a href="https://www.ieee.org/education/eab.html" rel="noopener noreferrer" target="_blank">IEEE Educational Activities Board</a> is dedicated to offering valuable programs to the engineering community and the public. There are educational offerings and activities from all our 47 technical societies and councils including, of course, the <a href="https://ieee-edusociety.org/" rel="noopener noreferrer" target="_blank">IEEE Education Society</a>. There are thousands of conferences, webinars, and seminars; dedicated collaborations for local sections and regional approaches; and education for those involved in creating our trusted publications, standards, and public policy.
</p><p>
	IEEE’s student and academic education programs include both university and preuniversity initiatives. Together, these provide support for IEEE members who work to inspire the next generation of engineers and technologists.
</p><h3>Volunteers in action</h3><br/><p>
	The 2025 IEEE Ad Hoc Committee on One IEEE Education Strategy for Empowering Technical Innovation, chaired by IEEE Fellow <a href="https://www.karenpanetta.com/#about-overview" rel="noopener noreferrer" target="_blank">Karen Panetta</a>, is working to build an ecosystem of support for advancing education across IEEE’s fields of interest.
</p><p>
	Additionally, a new task force under IEEE Educational Activities, led by 2021 IEEE President and IEEE Fellow <a href="https://spectrum.ieee.org/u/susan-k-kathy-land" target="_self">Susan K. “Kathy” Land</a>, is working to better understand ongoing preuniversity STEM activities across IEEE dedicated to outreach for students ages 5 through 18.
</p><p>
	Among its university programs, Educational Activities supports <a href="https://epics.ieee.org/" rel="noopener noreferrer" target="_blank">EPICS in IEEE</a>, a service-learning program; IEEE–Eta Kappa Nu (<a href="https://hkn.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE-HKN</a>), the <a href="https://spectrum.ieee.org/ieees-honor-society-expands-to-more-countries" target="_self">honor society</a> promoting scholarship, character, and attitude for undergraduates, graduate students and professionals; and IEEE’s significant accreditation activities collaboratively as part of <a href="https://www.abet.org/" rel="noopener noreferrer" target="_blank">ABET</a>, which contributes heavily as a major force for engineering and technical program accreditation worldwide.
</p><p>
	IEEE’s <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering</a> umbrella of programs focuses on precollege students and the adults who work with them, with the goal of increasing awareness of engineering and ensuring any student can see that they too could have a future in our professions. TryEngineering has developed strong collaborations with IEEE societies to provide opportunities for school-age children to learn about engineering technologies and career potential in those areas. The world benefits as more young people are empowered through IEEE efforts to see themselves as future innovators in engineering and computing.
</p><p>
	The <a href="https://spectrum.ieee.org/ieee-tryengineering-summer-institute-2024" target="_self">IEEE TryEngineering Summer Institute</a> provides camp-type experiences at select U.S. universities. I am proud to share that one of the most successful locations is held where I’m a professor: the <a href="https://www.sandiego.edu/" rel="noopener noreferrer" target="_blank">University of San Diego</a>. The program was expanded internationally with <a href="https://tryengineering.org/tryengineering-on-campus/" rel="noopener noreferrer" target="_blank">TryEngineering On Campus</a> pilot programs held in Hong Kong and Arta, Greece.
</p><p>
	The <a href="https://www.computer.org/membership/juniors" rel="noopener noreferrer" target="_blank">IEEE Computer Society Juniors Program</a> introduces preuniversity students to computing fundamentals through engaging, age-appropriate content and hands-on learning. Developed to align with TryEngineering, the program aims to inspire interest in computer science early on and support the global STEM talent pipeline.
</p><p>
	IEEE’s commitment to accreditation activities is critical to the development and success of future professionals. As a representative and advocate for the engineering and computer-science professions for ABET, IEEE has a strong, continued interest in sustaining and improving engineering and computing programs worldwide, and the programs in IEEE lead fields, including electrical, electronic, computer, and communications engineering, are the most numerous.
</p><p>
	The involvement of IEEE volunteers as ABET delegates, commissioners, and program evaluators ensures that the next generation of engineers who graduate from accredited programs will be prepared to handle the challenges facing IEEE’s fields of interest, including the creation of new program criteria in robotics and mechatronics.
</p><h2>Practical professional development</h2><p>
	The <a href="https://spectrum.ieee.org/ieee-learning-network-anniversary" target="_self">IEEE Learning Network</a> gathers education offerings from across IEEE, allowing learners who are professionals to advance or expand their specific technical expertise. Some of the new e-learning course programs recently launched include those produced in partnership with the <a href="https://iln.ieee.org/public/contentdetails.aspx?id=A8B87554A5914826AD91C19986C0D74F" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a>, the <a href="https://iln.ieee.org/public/contentdetails.aspx?id=706DBC956996482182A5232D95410F99" rel="noopener noreferrer" target="_blank">IEEE Computer Society</a>, <a href="https://iln.ieee.org/public/contentdetails.aspx?id=D02A42B64A834CC1A698ADC1ABAB9523" rel="noopener noreferrer" target="_blank">IEEE Future Directions</a>, and <a href="https://iln.ieee.org/public/contentdetails.aspx?id=D02A42B64A834CC1A698ADC1ABAB9523" rel="noopener noreferrer" target="_blank">IEEE Global Semiconductors</a>.
</p><p>
	The <a href="https://blended-learning.ieee.org/Portal/" rel="noopener noreferrer" target="_blank">IEEE Blended Learning Program</a> combines e-learning techniques with hands-on practice, designed to empower engineers with short lessons in tech to become future-ready.
</p><h2>Designing your future</h2><p>
	IEEE’s greatest opportunities for engagement and its advantage are the ability to offer individuals numerous avenues to contribute, collaborate, and advance both their professional careers and the broader field of technology. For our members, IEEE provides opportunities to engage, showcase their work, and grow professionally.
</p><p>
	Members thrive through the connections IEEE can facilitate. IEEE provides a professional home at every career stage, even pre-career, connecting you with a world of possibilities. This requires active effort and engagement, where membership is just one step. It is up to everyone to take charge of their own professional development and to look for ways to help others succeed too. The best opportunities lie in how each of us actively participates in shaping our career journey.
</p><p>
<strong>—Kathleen Kramer</strong>
</p><p>
<strong>IEEE president and CEO</strong>
</p><p>
	Please share your thoughts with me: <a href="mailto:president@ieee.org">president@ieee.org</a>.
</p>]]></description><pubDate>Sun, 01 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-presidents-note-june-2025</guid><category>Ieee news</category><category>Type:ti</category><category>Ieee presidents column</category><category>Presidents column</category><category>Education</category><category>Continuing education</category><category>Tryengineering</category><dc:creator>Kathleen Kramer</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-photo-of-a-smiling-woman-in-a-magenta-suit.jpg?id=56605851&amp;width=980"></media:content></item><item><title>The Data Reveals Top Patent Portfolios</title><link>https://spectrum.ieee.org/top-patents-scorecard-rankings</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-white-men-in-polo-shirts-with-a-laptop-on-the-table.png?id=60312174&width=1245&height=700&coordinates=0%2C97%2C0%2C98"/><br/><br/><p>Eight years is a long time in the world of patents. When we last published what we then called the Patent Power Scorecard, in 2017, it was a different technological and social landscape—Google had just filed a <a href="https://patents.google.com/patent/US10452978B2/en" rel="noopener noreferrer" target="_blank">patent application on the transformer architecture</a>, a momentous advance that spawned the generative AI revolution. China was just beginning to produce quality, affordable <a href="https://en.wikipedia.org/wiki/Electric_vehicle_industry_in_China" rel="noopener noreferrer" target="_blank">electric vehicles at scale</a>. And the COVID pandemic wasn’t on anyone’s dance card. </p><p>Eight years is also a long time in the world of magazines, where we regularly play around with formats for articles and infographics. We now have more readers online than we do in print, so our art team is leveraging advances in interactive design software to make complex datasets grokkable at a glance, whether you’re on your phone or flipping through the pages of the magazine.</p><p>The scorecard’s return in <a href="https://spectrum.ieee.org/magazine/2025/june/" target="_blank">this issue</a> follows the return last month of The Data, which ran as our back page for several years; it’s curated by a different editor every month and edited by Editorial Director for Content Development Glenn Zorpette. </p><p>As we set out to recast the scorecard for this decade, we sought to strike the right balance between comprehensiveness and clarity, especially on a mobile-phone screen. As our Digital Product Designer Erik Vrielink, Assistant Editor Gwendolyn Rak, and Community Manager Kohava Mendelsohn explained to me, they wanted something that would be eye-catching while avoiding information overload. The solution they arrived at—a dynamic sunburst visualization—lets readers grasp the essential takeaways at glance in print, while <a href="https://spectrum.ieee.org/patent-power-2025" target="_self">the digital version</a>, allows readers to dive as deep as they want into the data.</p><p>Working with sci-tech-focused data-mining company <a href="https://1790analytics.com/" rel="noopener noreferrer" target="_blank">1790 Analytics</a>, which we partnered with on the original <a href="https://spectrum.ieee.org/patent-power" target="_self">Patent Power Scorecard</a>, the team prioritized three key metrics or characteristics: patent Pipeline Power (which goes beyond mere quantity to assess quality and impact), number of patents, and the country where companies are based. This last characteristic has become increasingly significant as geopolitical tensions reshape the global technology landscape. As 1790 Analytics cofounders Anthony Breitzman and Patrick Thomas note, the next few years could be particularly interesting as organizations adjust their patenting strategies in response to changing market access.</p><p>Some trends leap out immediately. In consumer electronics, <a href="https://insights.greyb.com/apple-patents/" rel="noopener noreferrer" target="_blank">Apple</a> dominates Pipeline Power despite having a patent portfolio one-third the size of <a href="https://www.samsung.com/global/business/networks/" rel="noopener noreferrer" target="_blank">Samsung’s</a>—a testament to the Cupertino company’s focus on high-impact innovations. The aerospace sector has seen dramatic consolidation, with <a href="https://www.rtx.com/" rel="noopener noreferrer" target="_blank">RTX</a> (formerly Raytheon Technologies) now encompassing multiple subsidiaries that appear separately on our scorecard. </p><p>And in the university rankings, <a href="https://otd.harvard.edu/faculty-inventors/protecting-intellectual-property/" rel="noopener noreferrer" target="_blank">Harvard</a> has seized the top spot from traditional tech powerhouses like MIT and Stanford, driven by patents that are more often cited as prior art in other recent patents. And then there are the subtle shifts that become apparent only when you dig deeper into the data. The rise of <a href="https://www.sel.co.jp/en/" rel="noopener noreferrer" target="_blank">SEL (Semiconductor Energy Laboratory)</a> over <a href="https://spectrum.ieee.org/tag/tsmc" target="_self">TSMC (Taiwan Semiconductor Manufacturing Co.)</a> in semiconductor design, despite having far fewer patents, suggests again that true innovation isn’t just about filing patents—it’s about creating technologies that others build upon.</p><p>Looking ahead, the real test will be how these patent portfolios translate into actual products and services. Patents are promises of innovation; the scorecard helps us see what companies are making those promises and the R&D investments to realize them. As we enter an era when technological leadership increasingly determines economic and strategic power, understanding these patterns is more crucial than ever.</p>]]></description><pubDate>Sun, 01 Jun 2025 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/top-patents-scorecard-rankings</guid><category>Patents</category><category>Patent power</category><category>1790 analytics</category><category>Innovation</category><category>Intellectual property</category><category>Ip</category><category>Sel</category><category>Apple</category><category>Harvard</category><dc:creator>Harry Goldstein</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/two-white-men-in-polo-shirts-with-a-laptop-on-the-table.png?id=60312174&amp;width=980"></media:content></item><item><title>This Little Mars Rover Stayed Home</title><link>https://spectrum.ieee.org/mars-pathfinder-rover</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-six-wheeled-robot-with-a-flat-metal-rectangular-top.jpg?id=60333415&width=1245&height=700&coordinates=0%2C375%2C0%2C375"/><br/><br/><p>
<em></em><em></em>As a mere earthling, I remember watching in fascination as 
	<em>Sojourner </em>sent back photos of the Martian surface during the summer of 1997. I was not alone. The servers at NASA’s Jet Propulsion Lab slowed to a crawl when they got more than 47 million hits (a record number!) from people attempting to download those early images of the Red Planet. To be fair, it was the late 1990s, the Internet was still young, and most people were using dial-up modems. By the end of the 83-day mission, <em>Sojourner </em>had sent back 550 photos and performed more than 15 chemical analyses of Martian rocks and soil.
</p><p>
<em>Sojourner</em>, of course, remains on Mars. Pictured here is <em>Marie Curie,</em> its twin. Functionally identical, either one of the rovers could have made the voyage to Mars, but one of them was bound to become the famous face of the mission, while the other was destined to be left behind in obscurity. Did I write this piece because I feel a little bad for <em>Marie Curie</em>? Maybe. But it also gave me a chance to revisit this pioneering Mars mission, which established that robots could effectively explore the surface of planets and captivate the public imagination.
</p><h2><em>Sojourner</em>’s sojourn on Mars</h2><p>
	On 4 July 1997, the 
	<a href="https://science.nasa.gov/mission/mars-pathfinder/" rel="noopener noreferrer" target="_blank"><em>Mars Pathfinder</em></a> parachuted through the Martian atmosphere and bounced about 15 times on glorified airbags before finally coming to a rest. The lander, <a href="https://www.jpl.nasa.gov/news/nasa-renames-mars-lander-in-honor-of-late-carl-sagan/" rel="noopener noreferrer" target="_blank">renamed the <em>Carl Sagan Memorial Station</em></a>, carried precious cargo stowed inside. The next day, after the airbags retracted, the solar-powered <em>Sojourner </em>eased its way down the ramp, the first human-made vehicle to roll around on the surface of another planet. (It wasn’t the first extraterrestrial body, though. The <a href="https://nssdc.gsfc.nasa.gov/nmc/spacecraft/display.action?id=1973-001A" rel="noopener noreferrer" target="_blank">Soviet Lunokhod rovers</a> conducted two successful missions on the moon in 1970 and 1973. The Soviets had also landed a rover on Mars back in 1971, but communication was lost before the <a href="https://spectrum.ieee.org/meet-the-very-first-rover-to-land-on-mars" target="_self">PROP-M</a> ever deployed.)
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Photo of a man pulling on a cable attached to a small wheeled robot, in a large room filled with sand and rocks." class="rm-shortcode" data-rm-shortcode-id="9a0472467f342ba65cbfc5b938087628" data-rm-shortcode-name="rebelmouse-image" id="bcc72" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-a-man-pulling-on-a-cable-attached-to-a-small-wheeled-robot-in-a-large-room-filled-with-sand-and-rocks.jpg?id=60333457&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This giant sandbox at JPL provided <i>Marie Curie</i> with an approximation of Martian terrain. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Mike Nelson/AFP/Getty Images</small>
</p><p>
	The six-wheeled, 10.6-kilogram, microwave-oven-size 
	<em>Sojourner </em>was equipped with three low-resolution cameras (two on the front for black-and-white images and a color camera on the rear), a laser hazard–avoidance system, an alpha-proton X-ray spectrometer, experiments for testing wheel abrasion and material adherence, and several accelerometers. The robot also demonstrated the value of the six-wheeled “rocker-bogie” suspension system that became NASA’s go-to design for all later Mars rovers. <em>Sojourner</em> never roamed more than about 12 meters from the lander due to the limited range of its radio<em>. </em>
</p><p>
<em>Pathfinder</em> had landed in <a href="https://en.wikipedia.org/wiki/Ares_Vallis" rel="noopener noreferrer" target="_blank">Ares Vallis</a>, an assumed ancient floodplain chosen because of the wide variety of rocks present. Scientists hoped to confirm the past existence of water on the surface of Mars. <em>Sojourner </em>did discover rounded pebbles that suggested running water, and later missions confirmed it.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Black and white photo of a small wheeled robot on sandy terrain with a large rock in the background." class="rm-shortcode" data-rm-shortcode-id="96ac1c45cafe3432d409797b52507ea8" data-rm-shortcode-name="rebelmouse-image" id="d8cf7" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-small-wheeled-robot-on-sandy-terrain-with-a-large-rock-in-the-background.jpg?id=60333452&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">A highlight of <i>Sojourner</i>’s 83-day mission on Mars was its encounter with a rock nicknamed Barnacle Bill [to the rover’s left]. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">JPL/NASA</small>
</p><p>
	As its first act of exploration, 
	<em>Sojourner </em>rolled forward 36 centimeters and encountered a rock, dubbed Barnacle Bill due to its rough surface. The rover<em> </em>spent about 10 hours analyzing the rock, using its spectrometer to determine the elemental composition. Over the next few weeks, while the lander collected atmospheric information and took photos, the rover studied rocks in detail and tested the Martian soil.
</p><h2><em>Marie Curie</em>’s sojourn…in a JPL sandbox</h2><p>
	Meanwhile back on Earth, engineers at JPL used 
	<em>Marie Curie</em> to mimic <em>Sojourner’s </em>movements in a Mars-like setting<em>.</em> During the original design and testing of the rovers, the team had set up giant sandboxes, each holding thousands of kilograms of playground sand, in the Space Flight Operations Facility at JPL. They exhaustively practiced the remote operation of <em>Sojourner</em>, including an 11-minute delay in communications between Mars and Earth. (The actual delay can vary from 7 to 20 minutes.) Even after <em>Sojourner</em> landed, <em>Marie Curie</em> continued to help them strategize.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Photo of a man wearing VR goggles and looking at a computer screen, with his right hand on a large track ball." class="rm-shortcode" data-rm-shortcode-id="fe9372a23dae6bcb728214d1172b838b" data-rm-shortcode-name="rebelmouse-image" id="6f5e6" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-a-man-wearing-vr-goggles-and-looking-at-a-computer-screen-with-his-right-hand-on-a-large-track-ball.jpg?id=60333425&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Initially, <i>Sojourner</i> was remotely operated from Earth, which was tricky given the lengthy communication delay. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Mike Nelson/AFP/Getty Images</small>
</p><p>
	During its first few days on Mars, 
	<em>Sojourner</em> was maneuvered by an Earth-based operator wearing 3D goggles and using a funky input device called a <a href="https://www.youtube.com/watch?v=0zzY8cxKauU" rel="noopener noreferrer" target="_blank">Spaceball 2003</a>. Images pieced together from both the lander and the rover guided the operator. It was like a very, very slow video game—the rover sometimes moved only a few centimeters a day. NASA then turned on <em>Sojourner’s </em>hazard-avoidance system, which allowed the rover <a href="https://www-robotics.jpl.nasa.gov/media/documents/ICESpaper.pdf" rel="noopener noreferrer" target="_blank">some autonomy</a> to explore its world. A human would suggest a path for that day’s exploration, and then the rover had to autonomously avoid any obstacles in its way, such as a big rock, a cliff, or a steep slope.
</p><p>
	JPL designed 
	<em>Sojourner </em>to operate for a week. But the little rover that could kept chugging along for 83 Martian days before NASA finally lost contact, on 7 October 1997. The lander had conked out on 27 September. In all, the mission collected 1.2 gigabytes of data (which at the time was a <em>lot</em>) and sent back 10,000 images of the planet’s surface.
</p><p>
	NASA held on to 
	<em>Marie Curie </em>with the hopes of sending it on another mission to Mars. For a while, it was slated to be part of the <em>Mars 2001 </em>set of missions, but that didn’t happen. In 2015, JPL transferred the rover to the <a href="https://collections.si.edu/search/detail/edanmdm:nasm_A20150317000?" rel="noopener noreferrer" target="_blank">Smithsonian’s National Air and Space Museum</a>.
</p><h2>When NASA Embraced Faster, Better, Cheaper</h2><p>
	The 
	<em>Pathfinder </em>mission was the second one in NASA administrator <a href="https://www.nasa.gov/people/daniel-s-goldin/" rel="noopener noreferrer" target="_blank">Daniel S. Goldin</a>’s Discovery Program, which embodied his “faster, better, cheaper” philosophy of making NASA more nimble and efficient. (The first Discovery mission was to the asteroid Eros.) In the financial climate of the early 1990s, the space agency couldn’t risk a billion-dollar loss if a major mission failed. Goldin opted for smaller projects; the <em>Pathfinder</em> mission’s overall budget, including flight and operations, was capped at US $300 million.
</p><p class="ieee-inbody-related">
	RELATED: <a href="https://spectrum.ieee.org/planetary-rovers-are-we-alone" target="_self">How NASA Built Its Mars Rovers</a>
</p><p>
	In his 2014 book 
	<a href="https://www.amazon.com/Curiosity-Inside-Mission-People-Happen/dp/1616149337" rel="noopener noreferrer" target="_blank"><em>Curiosity: An Inside Look at the Mars Rover Mission and the People Who Made It Happen</em></a> (Prometheus)<em>, </em>science writer Rod Pyle interviews <a href="https://science.nasa.gov/people/rob-manning/" rel="noopener noreferrer" target="_blank">Rob Manning</a>, chief engineer for the <em>Pathfinder </em>mission and subsequent Mars rovers. Manning recalled that one of the best things about the mission was its relatively minimal requirements. The team was responsible for landing on Mars, delivering the rover, and transmitting images—technically challenging, to be sure, but beyond that the team had no constraints.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Photo of two people in white lab coats standing in a dry landscape surrounded by several wheeled robots." class="rm-shortcode" data-rm-shortcode-id="c652ceb39c6da6d2c08a959847f796e0" data-rm-shortcode-name="rebelmouse-image" id="04b0b" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-two-people-in-white-lab-coats-standing-in-a-dry-landscape-surrounded-by-several-wheeled-robots.jpg?id=60333422&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..."><i>Sojourner</i> was succeeded by the rovers <i>Spirit</i>, <i>Opportunity</i>, and <i>Curiosity</i>. Shown here are four mission spares, including Marie Curie [foreground]. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">JPL-Caltech/NASA</small>
</p><p>
	The real mission was to prove to Congress and the American public that NASA could do groundbreaking work more efficiently. Behind the scenes, there was a little bit of accounting magic happening, with the “faster, better, cheaper” missions often being silently underwritten by larger, older projects. For example, the radioisotope heater units that kept 
	<em>Sojourner</em>’s electronics warm enough to operate were leftover spares from the <em>Galileo</em> mission to Jupiter, so they were “free.”
</p><p>
	Not only was the 
	<em>Pathfinder</em> mission successful but it captured the hearts of Americans and reinvigorated an interest in exploring Mars. In the process, it set the foundation for the future missions that allowed the rovers <a href="https://science.nasa.gov/mission/mer-spirit/" rel="noopener noreferrer" target="_blank"><em>Spirit</em></a><em>, </em><a href="https://science.nasa.gov/mission/mer-opportunity/" rel="noopener noreferrer" target="_blank"><em>Opportunity</em></a><em>, </em>and <a href="https://science.nasa.gov/mission/msl-curiosity/" rel="noopener noreferrer" target="_blank"><em>Curiosity</em></a> (which, incredibly, is still operating nearly 13 years after it landed) to explore even more of the Red Planet.
</p><h2>How the rovers <em>Sojourner</em> and <em>Marie Curie</em> got their names</h2><p>
	To name its first Mars rovers, NASA launched a student contest in March 1994, with the specific guidance of choosing a “heroine.” Entry essays were judged on their quality and creativity, the appropriateness of the name for a rover, and the student’s knowledge of the woman to be honored as well as the mission’s goals. Students from all over the world entered.
</p><p>
	Twelve-year-old Valerie Ambroise of Bridgeport, Conn., won for her essay on 
	<a href="https://www.smithsonianmag.com/history/remarkable-untold-story-sojourner-truth-180983691/" rel="noopener noreferrer" target="_blank">Sojourner Truth</a>, while 18-year-old Deepti Rohatgi of Rockville, Md., came in second for hers on <a href="https://www.mariecurie.org.uk/about-us/our-history/marie-curie-the-scientist" rel="noopener noreferrer" target="_blank">Marie Curie</a>. Truth was a Black woman born into slavery at the end of the 18th century. She escaped with her infant daughter and two years later won freedom for her son through legal action. She became a vocal advocate for civil rights, women’s rights, and alcohol temperance. Curie<em> </em>was a Polish-French physicist and chemist famous for her studies of radioactivity, a term she coined. She was the first woman to win a Nobel Prize, as well as the first person to win a second Nobel.
</p><p>
	NASA subsequently recognized several other women with named structures. One of the last women to be so honored was 
	<a href="https://spacenews.com/nasa-renames-wfirst-space-telescope-after-pioneering-woman-astronomer/" rel="noopener noreferrer" target="_blank">Nancy Grace Roman</a>, the space agency’s first chief of astronomy. In May 2020, NASA announced it would name the Wide Field Infrared Survey Telescope after Roman; the space telescope is set to launch as early as October 2026, although the Trump administration has repeatedly said it wants to <a href="https://www.scientificamerican.com/article/nasas-next-major-space-telescope-is-ready-to-launch-trump-wants-to-kill-it/" rel="noopener noreferrer" target="_blank">cancel the project</a>.
</p><p class="ieee-inbody-related">
	Related: 
	<a href="https://spectrum.ieee.org/rogue-planet" target="_self">A Trillion Rogue Planets and Not One Sun to Shine on Them</a>
</p><p>
	These days, NASA tries to avoid naming its major projects after people. It quietly changed 
	<a href="https://spacenews.com/nasa-policy-discourages-naming-missions-after-individuals/" rel="noopener noreferrer" target="_blank">its naming policy</a> in December 2022 after allegations came to light that James Webb, for whom the <a href="https://spectrum.ieee.org/collections/james-webb-telescope/" target="_self">James Webb Space Telescope</a> is named, had fired LGBTQ+ employees at NASA and, before that, the State Department. A <a href="https://spacenews.com/nasa-confirms-decision-to-keep-jwst-name-after-historical-report/" rel="noopener noreferrer" target="_blank">NASA investigation</a> couldn’t substantiate the allegations, and so the telescope retained Webb’s name. But the bar is now much higher for NASA projects to memorialize anyone, deserving or otherwise. (The agency did allow the hopping lunar robot <a href="https://www.intuitivemachines.com/micro-nova" rel="noopener noreferrer" target="_blank">IM-2 Micro Nova Hopper</a>, built by Intuitive Machines, to be named for computer-software pioneer <a href="https://www.gracehopper.com/blog/grace-hopper-the-person-programmer-and-pioneer" rel="noopener noreferrer" target="_blank">Grace Hopper</a>.)
</p><p>
	And so 
	<em>Marie Curie </em>and <em>Sojourner</em> will remain part of a rarefied clique. <em>Sojourner</em>, inducted into the <a href="http://www.robothalloffame.org/inductees/03inductees/mars.html" rel="noopener noreferrer" target="_blank">Robot Hall of Fame</a> in 2003, will always be the celebrity of the pair. And <em>Marie Curie</em> will always remain on the sidelines. But think about it this way: <em>Marie Curie </em>is now on exhibit at one of the most popular museums in the world, where millions of visitors can see the rover up close. That’s not too shabby a legacy either.
</p><p>
<em>Part of a </em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><em>continuing series</em></a><em> </em><em>looking at historical artifacts that embrace the boundless potential of technology.</em>
</p><p>
<em>An abridged version of this article appears in the June 2025 print issue.</em>
</p><h3>References</h3><br/><p>Curator Matthew Shindell of the National Air and Space Museum first suggested I feature <em>Marie Curie</em>.<em> </em>I found additional information from the museum’s <a href="https://airandspace.si.edu/collection-objects/rover-marie-curie-mars-pathfinder-engineering-test-vehicle/nasm_A20150317000" target="_blank">collections website</a>, an article by David Kindy in <a href="https://www.smithsonianmag.com/smithsonian-institution/recalling-thrill-pathfinders-mission-mars-180977008/" target="_blank"><em><em>Smithsonian</em></em> magazine</a>, and the book <a href="https://airandspace.si.edu/research/publications/after-sputnik-50-years-space-age" target="_blank"><em>After Sputnik: 50 Years of the Space Age</em></a> (Smithsonian Books/HarperCollins, 2007) by Smithsonian curator Martin Collins.</p><p>NASA has numerous resources documenting the <em>Mars Pathfinder </em>mission, such as the <a href="https://science.nasa.gov/mission/mars-pathfinder/" target="_blank">mission website</a>, <a href="https://d2pn8kiwq2w21t.cloudfront.net/documents/mpf_bQIcJKD.pdf" rel="noopener noreferrer" target="_blank">fact sheet</a>, and many lovely photos (including some of <a href="https://science.nasa.gov/resource/super-resolution-view-of-barnacle-bill/" rel="noopener noreferrer" target="_blank">Barnacle Bill</a> and a composite of <a href="https://science.nasa.gov/resource/marie-curie-during-ort6/" rel="noopener noreferrer" target="_blank"><em>Marie Curie</em></a> during a prelaunch test).</p><p><a href="https://www.amazon.com/Curiosity-Inside-Mission-People-Happen/dp/1616149337" rel="noopener noreferrer" target="_blank"><em>Curiosity: An Inside Look at the Mars Rover Mission and the People Who Made It Happen</em></a> (Prometheus, 2014) by Rod Pyle and <a href="https://www.amazon.com/Roving-Mars-Spirit-Opportunity-Exploration/dp/1401301495" rel="noopener noreferrer" target="_blank"><em>Roving Mars: Spirit, Opportunity, and the Exploration of the Red Planet</em></a> (Hyperion, 2005) by planetary scientist Steve Squyres are both about later Mars missions and their rovers, but they include foundational information about <em>Sojourner</em>.</p>]]></description><pubDate>Sat, 31 May 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/mars-pathfinder-rover</guid><category>Jpl</category><category>Mars rover</category><category>Nasa</category><category>Past forward</category><category>Planetary science</category><category>Type:departments</category><category>Pathfinder</category><dc:creator>Allison Marsh</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/photo-of-a-six-wheeled-robot-with-a-flat-metal-rectangular-top.jpg?id=60333415&amp;width=980"></media:content></item><item><title>IEEE Awardee’s Tech Prevents Chemotherapy-Induced Nerve Damage</title><link>https://spectrum.ieee.org/cooling-tech-nerve-damage</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/close-up-portrait-of-aishwarya-bandla-speaking-into-microphones-behind-a-podium-on-stage.jpg?id=60339007&width=1245&height=700&coordinates=0%2C461%2C0%2C462"/><br/><br/><p><a href="https://www.linkedin.com/in/aishbandla/" rel="noopener noreferrer" target="_blank">Aishwarya Bandla</a> tries to center her work around passion, people, and purpose, following the Japanese concept of <em><em>ikigai</em></em>, or a sense of purpose.</p><p>For the IEEE senior member, that involves transforming patient care through innovative health technology. Bandla is developing a means to help prevent nerve damage in cancer patients resulting from chemotherapy treatment, a condition known as chemotherapy-induced peripheral neuropathy</p><p>Chemotherapy is known to cause a variety of side effects including nausea, fatigue, and hair loss, according to the <a href="https://www.cancer.org/cancer/managing-cancer/treatment-types/chemotherapy/chemotherapy-side-effects.html" rel="noopener noreferrer" target="_blank">American Cancer Society</a>. But one lesser-known effect is neuropathy, Bandla says.</p><h3>Aishwarya Bandla</h3><br/><p><strong>Employer:</strong></p><p>Paxman Coolers of Huddersfield, England</p><p><strong>Title: </strong></p><p><strong></strong>Clinical innovation manager</p><p><strong>Member grade: </strong></p><p><strong></strong>Senior member</p><p><strong>Alma maters: </strong></p><p><strong></strong>Anna University in Chennai, India, and the National University of Singapore in Queenstown</p><p>Peripheral neuropathy nerve damage—which also can stem from diabetes, vitamin deficiencies, and other causes—affects mostly the tips of the patient’s hands and feet. Symptoms range from persistent tingling to excruciating pain. Currently there are no approved preventative measures for the condition; cancer patients try to manage it with painkillers or, in severe cases, reducing or stopping their chemotherapy, Bandla says.</p><p>Bandla is the clinical innovation manager at <a href="https://paxmanscalpcooling.com/" rel="noopener noreferrer" target="_blank">Paxman Coolers</a>, a medical equipment manufacturer headquartered in Huddersfield, England. She is developing a wearable device that cools a person’s limbs. Called the Paxman limb cryocompression system (PLCS), it’s designed to help prevent nerve damage from certain types of intravenous chemotherapy drugs. The cold temperature slows blood flow to the area, allowing less of the injected medication to reach the nerves there.</p><p>Bandla, who is based in Singapore, is also a principal investigator at the <a href="https://n1labs.org/" rel="noopener noreferrer" target="_blank">N.1 Institute for Health</a>, the <a href="https://www.nus.edu.sg/" rel="noopener noreferrer" target="_blank">National University of Singapore</a> (NUS), and at the <a href="https://www.ncis.com.sg/" rel="noopener noreferrer" target="_blank">National University Cancer Institute of Singapore</a>.</p><p>An active IEEE volunteer, she follows <em><em>ikigai</em></em> in her work with the organization, she says, and she encourages other young professionals to do the same. She has overseen the launch of several career development and mentorship programs for <a href="https://ewh.ieee.org/r10/singapore/wie/" rel="noopener noreferrer" target="_blank">IEEE Women in Engineering Singapore</a>, <a href="https://wie.ieeer10.org/" rel="noopener noreferrer" target="_blank">IEEE Region 10 Women in Engineering</a>, and <a href="https://yp.ieeer10.org/" rel="noopener noreferrer" target="_blank">IEEE Region 10 Young Professionals</a>.</p><p>“Being an IEEE member,” she says, “has helped me nurture my purpose in rallying my efforts toward creating meaningful impact.”</p><p>For “her leadership in patient-centric health technology innovation and inspiring IEEE Young Professionals to drive meaningful change,” she is the recipient of this year’s <a href="https://corporate-awards.ieee.org/award/ieee-theodore-w-hissey-outstanding-young-professional-award/" rel="noopener noreferrer" target="_blank">IEEE Theodore W. Hissey Outstanding Young Professional Award</a>. The award is sponsored by the <a href="https://ieeephotonics.org/" rel="noopener noreferrer" target="_blank">IEEE Photonics</a> and <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power & Energy</a> societies, as well as IEEE Young Professionals.</p><p>“This <a href="https://spectrum.ieee.org/ted-hissey-a-tireless-volunteer" target="_self">recognition</a> fuels me to continue the work IEEE is doing globally to make the world a better place,” she says.</p><h2>Engineering is a superpower</h2><p>Bandla had a difficult time deciding whether to pursue medicine or engineering as a career, she says, but she chose the latter because it’s “a superpower that can help you create things to make life better.”</p><p>After earning her bachelor’s degree in electrical and electronics engineering in 2009 from <a href="https://www.annauniv.edu/" rel="noopener noreferrer" target="_blank">Anna University</a>, in Chennai, India, she joined software engineering company <a href="https://www.infosys.com/" rel="noopener noreferrer" target="_blank">Infosys</a> in Mysuru, India, as a technical consultant. She left three years later after being accepted into the neurotechnology doctoral program at NUS in Queenstown. Neurotechnology encompasses ways of directly engaging with the human brain and nervous system, including brain-computer interfaces, magnetic resonance imaging, and brain-wave monitors.</p><p>Bandla conducted her research under biomedical engineer <a href="https://cde.nus.edu.sg/bme/staff/dr-nitish-v-thakor/" rel="noopener noreferrer" target="_blank">Nitish V. Thakor</a>, who specializes in developing brain-monitoring technologies and <a href="https://spectrum.ieee.org/a-prosthetic-that-feels-pain" target="_self">neuroprostheses</a>. The IEEE Life Fellow is a professor of biomedical engineering at <a href="https://www.jhu.edu/" rel="noopener noreferrer" target="_blank">Johns Hopkins University</a>, in Baltimore. He also is director of the <a href="https://neuroeng.org/" rel="noopener noreferrer" target="_blank">Singapore Institute for Neurotechnology</a>, SINAPSE, a collaboration among six research universities including Johns Hopkins, NUS, and the <a href="https://www.upatras.gr/en/" rel="noopener noreferrer" target="_blank">University of Patras</a>, in Greece.</p><p>Under Thakor’s tutelage, Bandla began her work in developing the technology she is involved with today.</p><h2>Using technology to address nerve damage</h2><p>In 2012 Bandla and other researchers from Thakor’s lab met with neurologist <a href="https://neuropathycommons.org/experts/einar-wilder-smith-md-dtmh-london-fams-neurology-habilitation" rel="noopener noreferrer" target="_blank">Einar Wilder Smith</a> and oncologist <a href="https://www.ncis.com.sg/for-patients-and-visitors/find-a-doctor/doctor-details/Raghav_Sundar" rel="noopener noreferrer" target="_blank">Raghav Sundar</a> from <a href="https://www.nuh.com.sg/" rel="noopener noreferrer" target="_blank">National University Hospital</a> in Kent Ridge, Singapore, to explore how the technology could help cancer patients with peripheral neuropathy.</p><p>During chemotherapy, patients are injected with an individualized drug mixture that kills fast-dividing cells or prevents them from multiplying by damaging the cells’ DNA. But the mixture also can attack healthy cells and damage nervous-system structures, causing pain and sensitivity in the patient’s hands and feet, as explained in an <a href="https://pubmed.ncbi.nlm.nih.gov/18571399/" rel="noopener noreferrer" target="_blank">article published in the <em><em>International Journal of Molecular Sciences</em></em></a>.</p><p>In the meeting, the team learned about a scalp-cooling technology that helps prevent a different side effect: hair loss. A special cap is placed on the patient’s head to cool the scalp.</p><p>Inspired by that cold cap, the team set out to develop similar technology for the hands and feet. But first, in 2014, the SINAPSE lab conducted a clinical trial with National University Hospital to see if cooling the limbs would help patients with peripheral neuropathy. Existing localized cryotherapy machines used for sports therapy—which circulate ice-cooled liquid to cool an area on the body, were tested on 15 chemotherapy patients at the hospital. The team found that patients could not comfortably tolerate temperatures below 22 °C during the three-hour treatment, Bandla says.</p><p class="pull-quote"><span>“Being an IEEE member has helped me nurture my purpose in rallying my efforts toward creating meaningful impact.” </span></p><p>She suggested conducting another clinical trial, this time testing cryocompression tools rather than cryotherapy ones. Cryocompression is used for sports therapy and rehab. It combines cooling and compression—which helps reduce swelling. In the second trial, the team found that patients could tolerate temperatures as low as 11 °C for three hours, Bandla says.</p><p>The second trial ended in 2017. Bandla earned her Ph.D. that year but continued to work on the project as a SINAPSE research fellow.</p><p>In 2018 the team members began another clinical study, testing if they could safely cool a patient’s scalp and limbs simultaneously to prevent multiple side effects at once.</p><p>Throughout the five-year trial period, Bandla collected data to understand the best way to deliver cooling therapy that was safe, comfortable, and effective. The feedback she received from patients, caregivers, and the medical staff demonstrated a clear need for a device to use in the chemotherapy suite.</p><p>After the pilot trials ended in 2019, the team began designing a device alongside <a href="https://www.lbbc.org/about-us/reviewer/richard-paxman" target="_blank">Richard Paxman</a> and his team at Paxman Coolers, who leveraged their expertise in cryotherapy for side-effect management.</p><p>The portable PLCS connects to four insulated wraps, each containing a bladder filled with coolant. The wraps cover a patient’s forearms, hands, shins, and feet and include velcro flaps that can be adjusted for a better fit. The PLCS circulates the coolant through the wraps and powers the compression. It also keeps the coolant temperature at 11 °C.</p><p>During every chemotherapy cycle, 30 minutes before the medication is administered, the wraps are placed on the patient’s forearms and shins to begin the cooling process. After the session ends, the device is used on the patient for 30 more minutes, Bandla says.</p><p>The team was granted two U.S. patents for the PLCS.</p><p>In 2022 Bandla joined Paxman as a research and development manager, and she was promoted to clinical innovation manager two years later.</p><p>The impact her work has had keeps her motivated to continue, she says.</p><p>The PLCS is being tested in a large-scale clinical trial in 25 U.S. hospitals in collaboration with the <a href="https://www.cancer.gov/" target="_blank">National Cancer Institute</a>.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Aishwarya Bandla smiling for a selfie with a group of young students in India. " class="rm-shortcode" data-rm-shortcode-id="cb5a2732f225eaf9c3405334a0980c00" data-rm-shortcode-name="rebelmouse-image" id="f2c3c" loading="lazy" src="https://spectrum.ieee.org/media-library/aishwarya-bandla-smiling-for-a-selfie-with-a-group-of-young-students-in-india.jpg?id=60339176&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Two years ago Bandla attended a social innovation camp for school students in India.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Aishwarya Bandla</small></p><h2>Starting her IEEE volunteer journey</h2><p>Thakor introduced Bandla to IEEE. An active member of the <a href="https://www.embs.org/" target="_blank">IEEE Engineering in Medicine and Biology Society</a>, he encourages his students to participate in its conferences and to publish papers in its journals.</p><p>Bandla says volunteering with IEEE was a no-brainer for her. Her volunteerism began in 2012 with IEEE Women in Engineering Singapore. In 2019 she became its chair and launched the WIE Singapore Networking Night to help build camaraderie between the <a href="https://www.ieeesingapore.org/" target="_blank">IEEE Singapore Section</a> and technologists in industry, academia, and government. The annual event includes panel discussions.</p><p>In 2021 Bandla joined the IEEE Region 10 Women in Engineering committee as the technical and Young Professionals lead. There she helped launch <a href="https://wie.ieeer10.org/mentorher2021/" target="_blank">MentorHer</a>, an eight-week program in which experts help their mentees design and implement a professional development plan. Bandla created the program’s framework.</p><p>“After the pilot program was completed in 2021, we received nice feedback from participants,” she says. “Many people said they interacted with people they wouldn’t normally work with and enjoyed the experience.”</p><p>In 2020 Bandla began participating in virtual events and conferences held by Region 10’s Young Professionals group as a speaker and panel moderator. Last year she became the chair.</p><h2>Guiding young professionals</h2><p>Volunteering for the YP group is special to her, she says, because she has been able to “build a community and help other young professionals become well-rounded leaders and decision-makers.”</p><p>She helped develop the <a href="https://yp.ieeer10.org/career-and-leadership-aid-program-2024/" target="_blank">Career and Leadership Aid Program</a> (CLAP) at the Region 10 <a href="https://ieee-jp.org/japancouncil/affinitygroup/R10SYWL2024/" target="_blank">Students, Young Professionals,Women in Engineering,Life Members Congress</a> held in August in Tokyo.</p><p>She introduced the concept of <em><em>ikigai</em></em> to <a href="https://spectrum.ieee.org/collections/celebrating-young-professionals-and-students/" target="_self">young professionals</a> by centering the event around it. The congress included what she calls a “human library” session. Ten IEEE members from different engineering fields were positioned around the meeting room, and attendees had an hour to learn about each of the “human books.”</p><p>The group received positive feedback, with participants saying they enjoyed the focus on professional and leadership development. They said they liked how extraordinary the event was, in particular the “human library” session.</p><p>Based on the success of the CLAP event, the team is building an <a href="https://yp.ieeer10.org/hive/" rel="noopener noreferrer" target="_blank">IEEE Hive</a>. The immersive professional development program is available for students and early career professionals at technical conferences and congresses around the world.</p><p>The ability to make an impact, build a community, and connect with people resonates with her, Bandla says.</p><p>“Volunteering with IEEE gives me so much energy!” she says.</p>]]></description><pubDate>Fri, 30 May 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/cooling-tech-nerve-damage</guid><category>Biomedical engineering</category><category>Health care</category><category>Ieee awards</category><category>Ieee member news</category><category>Ieee young professionals</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/close-up-portrait-of-aishwarya-bandla-speaking-into-microphones-behind-a-podium-on-stage.jpg?id=60339007&amp;width=980"></media:content></item><item><title>Video Friday: Atlas Robot Sees the World</title><link>https://spectrum.ieee.org/video-friday-atlas-robot-sees-world</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-boston-dynamics-robot-with-a-round-camera-equipped-head-being-adjusted-in-a-tech-environment.jpg?id=60342329&width=1245&height=700&coordinates=0%2C55%2C0%2C55"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://smartconf.jp/content/rcar2025/">IEEE RCAR 2025</a>: 1–6 June 2025, TOYAMA, JAPAN</h5><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON, TX</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="http://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote class="rm-anchors" id="oe1dke3cf7i"><em>For a humanoid robot to be successful and generalizable in a factory, warehouse, or even at home requires a comprehensive understanding of the world around it—both the shape and the context of the objects and environments the robot interacts with. To do those tasks with agility and adaptability, Atlas needs an equally agile and adaptable perception system.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b1816bab6f8494b71379b19f2f7ebce2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/oe1dke3Cf7I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://bostondynamics.com/blog/making-atlas-see-the-world/">Boston Dynamics</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="mcayqe7pkog"><em>What happens when a bipedal robot is placed in the back of a moving cargo truck without any support? LimX Dynamics explored this idea in a real-world test. During the test, TRON 1 was positioned in the compartment of a medium-sized truck. The vehicle carried out a series of demanding maneuvers—sudden stops, rapid acceleration, sharp turns, and lane changes. With no external support, TRON 1 had to rely entirely on its onboard control system to stay upright, presenting a real challenge for <a data-linked-post="2650275699" href="https://spectrum.ieee.org/video-friday-robot-dance-teacher-transformer-drone-pneumatic-reel-actuator" target="_blank">dynamic stability</a>.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="009596bae52f3b2025639533d34acf81" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/McAYQE7Pkog?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.limxdynamics.com/en">LimX Dynamics</a>]</p><p>Thanks, Jinyan!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8-pz_8hqe6s"><em>We present a quiet, smooth-walking controller for quadruped guide robots, addressing key challenges for blind and low-vision (BLV) users. Unlike conventional controllers, which produce distracting noise and jerky motion, ours enables slow, stable, and human-speed walking—even on stairs. Through interviews and user studies with BLV individuals, we show that our controller reduces noise by half and significantly improves user acceptance, making quadruped robots a more viable mobility aid.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7a26c9a4605682421ddb6717bc3eeb91" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8-pz_8Hqe6s?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.umass.edu/robotics/daros/research/guide-dog-robot">University of Massachusetts Amherst</a>]</p><p>Thanks, Julia!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="dpfqk0-z-rm"><em>RIVR, the leader in physical AI and robotics, is partnering with Veho to pilot our delivery robots in the heart of Austin, Texas. Designed to solve the “last-100-yard” challenge, our wheeled-legged robots navigate stairs, gates, and real-world terrain to deliver parcels directly to the doorstep—working alongside human drivers, not replacing them.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="da73a51acb294a3abd98a49bb3574c99" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dPFQK0-Z-rM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.rivr.ai/">RIVR</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="6avyea_ewnk">We will have more on this robot shortly, but for now, this is all you need to know.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2ce1b2819ae5e7bdbc2687b0b537cca8" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6AvyeA_Ewnk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://pintobotics.substack.com/">Pintobotics</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ezbm594t3c4">Some pretty awesome <a data-linked-post="2671184284" href="https://spectrum.ieee.org/ai-institute" target="_blank">quadruped parkour</a> here—haven’t seen the wall running before.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="68cbe70a242882376dbdfe7c75045fe7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/EZbM594T3c4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.science.org/doi/10.1126/scirobotics.ads6192">Paper</a>] via [<a href="https://www.science.org/journal/scirobotics" target="_blank">Science Robotics</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="wkcdp41_4m8">This is fun, and also useful, because it’s all about recovering from unpredictable and forceful impacts.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="931d3f4445a0ce56c02dd3e5f1300086" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/WKCDP41_4m8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>What is that move at 0:06, though?! Wow.</p><p>[<a href="https://www.unitree.com/mobile/boxing/">Unitree</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="qwmuo91drji">Maybe an option for all of those <a data-linked-post="2655919083" href="https://spectrum.ieee.org/social-robots-children" target="_blank">social robots</a> that are now not social?</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d550874508203107b36d676f522f944b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/QwmuO91drJI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.robohearts.eu/">RoboHearts</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ylssuetkmha">Oh, good, another robot I want nowhere near me.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d9ad398b2fc26a39029d5e185133bfb7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yLssUETKmHA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.softrobotics.dk/">SDU Biorobotics Lab, University of Southern Denmark</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ergit1fglck">While this “has become the first humanoid robot to skillfully use chopsticks,” I’m pretty skeptical of the implied autonomy. Also, those chopsticks are cheaters.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b8a648ba60752b47748ec4ac07db70af" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ergiT1fglCk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.robotera.com/en/">ROBOTERA</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="xhec98zkiho">Looks like Westwood Robotics had a fun time at <a data-linked-post="2669279706" href="https://spectrum.ieee.org/video-friday-icra40" target="_blank">ICRA</a>!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6e1596cda9a3f8c5d183d670f058fece" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xhEc98ZkIho?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.westwoodrobotics.io/">Westwood Robotics</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="9pq2zg19hgg"><em>Tessa Lau, CEO and co-founder of Dusty Robotics, delivered a plenary session (keynote) at the 2025 IEEE International Conference on Robotics & Automation (ICRA) in May 2025.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7f1e331cc0ac5ea071814aedc85e9fc5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9pq2ZG19hGg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.dustyrobotics.com/">Dusty Robotics</a>]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 30 May 2025 15:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-atlas-robot-sees-world</guid><category>Video friday</category><category>Robotics</category><category>Quadruped robots</category><category>Social robots</category><category>Icra</category><category>Humanoid robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-boston-dynamics-robot-with-a-round-camera-equipped-head-being-adjusted-in-a-tech-environment.jpg?id=60342329&amp;width=980"></media:content></item><item><title>Self-Adapting Drones for Unpredictable Worlds</title><link>https://content.knowledgehub.wiley.com/empowering-drone-security-with-embodied-ai/</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/tii-logo.png?id=31835447&width=980"/><br/><br/><p>As drones evolve into critical agents across defense, disaster response, and infrastructure inspection, they must become more adaptive, secure, and resilient. Traditional AI methods fall short in real-world unpredictability. This whitepaper from the Technology Innovation Institute (TII) explores how Embodied AI – AI that integrates perception, action, memory, and learning in dynamic environments, can revolutionize drone operations. Drawing from innovations in GenAI, Physical AI, and zero-trust frameworks, TII outlines a future where drones can perceive threats, adapt to change, and collaborate safely in real time. The result: smarter, safer, and more secure autonomous aerial systems.</p><p><span><a href="https://content.knowledgehub.wiley.com/empowering-drone-security-with-embodied-ai/" target="_blank">Download this free whitepaper now!</a></span></p>]]></description><pubDate>Thu, 29 May 2025 20:00:11 +0000</pubDate><guid>https://content.knowledgehub.wiley.com/empowering-drone-security-with-embodied-ai/</guid><category>Artificial intelligence</category><category>Disaster response</category><category>Drones</category><category>Type:whitepaper</category><dc:creator>Technology Innovation Institute</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/31835447/origin.png"></media:content></item><item><title>Maine’s Floating Offshore Wind Ambitions in Jeopardy</title><link>https://spectrum.ieee.org/volturnus-floating-offshore-wind-turbine</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/floating-wind-turbine-prototype-with-a-cross-shaped-platform-and-a-three-blade-turbine-floating-in-the-water-next-to-a-dock.jpg?id=60329495&width=1245&height=700&coordinates=0%2C376%2C0%2C376"/><br/><br/><p>When the platform for a prototype floating offshore wind turbine arrived at a dock in Searsport, Maine, on April 11, engineers at the University of Maine were ready to add a tower and a turbine and set it afloat in the Gulf of Maine. The prototype, called the VolturnUS+, was a 1:4 scale model of a 15-megawatt version, and its deployment would mark only the second wind turbine to float in U.S. waters. </p><p>But on the very same day, university officials received a letter from the U.S. Department of Energy’s <span><a href="https://arpa-e.energy.gov/" target="_blank">Advanced Research Projects Agency - Energy</a></span> (ARPA-E) saying it was “suspending all activity” remaining on the project’s $12.6 million grant. The move left the university’s 375-tonne concrete hull tied up dockside and its creators scrambling to resolve the situation.</p><p><span>VolturnUS+ is one of many offshore wind projects that have been delayed or killed in the United States since President Trump’s second inauguration. On his <a href="https://spectrum.ieee.org/trump-tech-policy" target="_blank">first day back in office</a>, Trump signed an executive order freezing all permitting of offshore wind projects, impacting nearly all that were not yet under construction. </span><span>And in an unprecedented move, the President on April 16 froze work on one offshore wind farm that was already being built off of New York’s coast, before withdrawing the order last week. </span></p><p><span><span>In response, wind developers are pulling back on U.S. projects. Multinational </span>wind giant <span><span><a href="https://www.rwe.com/en/" target="_blank">RWE</a> <a href="https://www.rivieramm.com/news-content-hub/rwe-boss-confirms-us-offshore-wind-activity-at-an-end-84650" target="_blank">paused</a> work on its entire 6-gigawatt U.S. portfolio</span></span><span>, citing “the political environment.” </span></span></p><p><span><span>The turmoil</span> may prove particularly devastating for floating wind projects like VolturnUS+. Floating turbines are <a href="https://spectrum.ieee.org/floating-offshore-wind-turbine" target="_blank">designed to function farther offshore</a> in waters too deep to anchor turbine towers to the sea floor, and the fledgling industry has yet to install a single commercial-scale turbine in U.S. waters.</span></p><h2>Trump’s Impact on Floating Offshore Wind</h2><p><span>Tokyo-based <a href="https://www.mitsubishicorp.com/jp/en/index.html" target="_blank">Mitsubishi Corporation</a> in March paused work on what could have been a U.S. first: a 12-turbine, 144-MW floating <strong>”</strong>research array” planned for a spot 50 kilometers east of Portland, Maine. The company cited “recent shifts in the energy landscape that have, in particular, caused uncertainty in the offshore wind industry.”</span></p><p><span>Maine policymakers have been counting on Mitsubishi’s </span><span><a href="https://www.maine.gov/energy/initiatives/offshorewind/researcharray/" target="_blank"><span>research array</span></a></span> to jump-start development in floating wind and thus secure the state’s energy transition and bolster coastal economies. These small floating arrays serve as testbeds to help de-risk gigawatt-scale projects to come and provide an opportunity to engage with stakeholders. “It<span>’s important because the technology is still relatively immature,” says </span><span><a href="https://www.ucs.org/about/people/steve-clemmer" target="_blank">Steve Clemmer</a></span><span>, director of energy research at the Union of Concerned Scientists. “You</span><span>’ve got to start somewhere demonstrating the technology, researching impacts on the fishing industry and wildlife, especially related to the mooring systems,” he says.</span></p><p><span>Indeed,</span> developers of a California floating demonstration project, <a href="https://cademo.net/the-project/" target="_blank">Cademo</a>, had also been closely watching Maine’s progress. F<span>loating turbines are the U.S. Pacific Coast’s only offshore wind option due to its deeper waters.</span></p><p><span>In response to federal opposition to wind development, proponents of floating technology are taking a variety of strategies. For the VolturnUS+ team, leaving their massive </span><span>concrete platform tied to a dock would have been unsafe and financially ruinous, says </span><span><a href="https://civil.umaine.edu/faculty/habib-joseph-dagher/" target="_blank">Habib Dagher</a></span><span>, executive director of the University of Maine</span><span>’s </span><span><a href="https://composites.umaine.edu/" target="_blank">Advanced Structures & Composites Center</a></span> in Orono and VolturnUS+ co-director. “You’re going to destroy the pier if you get weather. And we were paying fees to stay at the pier—fees that we can’t even afford,” he says.</p><p><span>Blocked from accessing more than $3 million remaining in their ARPA-E grant, Dagher’s team cobbled together enough</span> cash from industry partners and state funds to do what needed to be done: mate the tower and turbine to the platform and then tow the package to its planned test site about 600 meters off the coast of Castine, Maine. It was the only viable option, says Dagher. “We had no choice but to find emergency funds to get it out of there,” he says.</p><p class="pull-quote">It was the only viable option, says Dagher. “We had no choice but to find emergency funds to get it out of there.”</p><p><span>For Mitsubishi, the challenges appear more widespread than the U.S. political climate, and the international conglomerate is responding by hitting the pause button. In February it <a href="https://www.mitsubishicorp.com/jp/en/news/release/2025/20250203002.html" target="_blank">paused</a> three conventional offshore wind projects in Japan, citing “material changes in the macroeconomic environment,” including the war in Ukraine, depreciation of the yen, and tight supply chains.</span></p><p><span>California, however, is pressing on. In February, California governor Gavin Newsom proposed a $228 million investment to prepare ports for major offshore wind farm construction expected in the next decade. And in March, the state of California awarded $20 million to the Port of Long Beach and $18 million to the Port of Humboldt to foster public engagement and conduct studies required for permit filings.</span></p><p><span>“They</span><span>’re not pulling back money that was previously allocated for offshore wind. They’re sticking to the course,” says </span><span><a href="https://www.wildcalifornia.org/post/matt-simmons-epic-s-new-climate-attorney" target="_blank">Matt Simmons</a>, climate attorney for the Environmental Protection Information Center, an Arcata, Calif.–based nonprofit.</span></p><p><span>Of course, California can only do so much without federal cooperation. The Cademo demonstration </span>on California’s Central Coast hopes to sell its power to the nearby Vandenberg Space Force Base. They also need a green light from the U.S. National Oceanic and Atmospheric Administration, which <a href="https://sanctuaries.noaa.gov/chumash-heritage/" target="_blank">designated a National Marine Sanctuary</a> in October that spans Cademo’s site.</p><h2>What Are the Next Steps for VolturnUS+?</h2><p><span>Maine’s VolturnUS+ floating turbine is a follow-on to its much smaller VolturnUS test turbine, which was a semi-submersible assemblage of pontoon beams and flotation columns. In making the VolturnUS+, the University of Maine streamlined the design to lower cost—it’s essentially a barge that sits atop the water. On their own, barges are less stable, but Dagher says his team made the design workable by taking inspiration from the mass-dampers that sway in high-rise buildings to counteract earthquakes. “You negate some of the earthquake motions by moving the mass back and forth to oppose the motions of the earthquake,” he explains.</span></p><p><span>In the case of VolturnUS+, mass shifting within each of the </span><span>hull</span><span>’s crossed arms counteracts tilting forces from winds and waves. Based on</span><span><a href="https://patents.google.com/patent/US20240101232A1" target="_blank"> a 2023 patent filing</a></span><span>, that mass could be seawater. The resulting stability reduced the size of the float required. Combined with the relative ease of construction, the design changes cut the platform costs by 20 to 30 percent, Dagher says.</span></p><p><span>ARPA-E’s suspension letter to the University of Maine alleged a </span><span>“failure to comply with one or more” federal policies. When asked for comment on the matter, a spokesperson for ARPA-E referred <em>Spectrum</em> to the U.S. Department of Energy (DOE), and the DOE did not respond to <em>Spectrum</em>‘s inquiry. The university says it is “compliant with all state and federal laws, and the conditions of its federal grants and contracts.” </span></p><p><span>Thanks to the emergency funds, the </span>completed VolturnUS+ test rig is now moored in 21 meters of water. The next step for the project’s leaders is to install a power cable that connects the turbine to the onshore grid—a project they hope to complete in the next two to three months. Dagher says the turbine will operate for 18 months, as planned, to evaluate the platform’s stability. But, in a statement provided to <span><em>Spectrum</em></span>, the university says that, without resumption of the ARPA-E funds, researchers will have less ability to analyze results and to craft a commercialization plan.</p><p><span>The University of Maine </span><span><a href="https://www.wabi.tv/2025/05/15/umaine-lay-off-research-staffers-due-federal-funding-concerns/" target="_blank"><span>announced in mid-May that it would lay off nine people</span></a></span> at Dagher’s Advanced Structures & Composites Center, citing “unexpected pauses and delays in federal funding.”</p>]]></description><pubDate>Thu, 29 May 2025 14:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/volturnus-floating-offshore-wind-turbine</guid><category>Wind energy</category><category>Floating wind turbine</category><category>Offshore wind</category><dc:creator>Peter Fairley</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/floating-wind-turbine-prototype-with-a-cross-shaped-platform-and-a-three-blade-turbine-floating-in-the-water-next-to-a-dock.jpg?id=60329495&amp;width=980"></media:content></item><item><title>How a Harvard Engineer Lost Three Grants in One Day</title><link>https://spectrum.ieee.org/harvard-funding-cuts</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/vijay-janapa-reddi-smiling-and-posing-on-a-staircase-in-an-office-he-is-wearing-glasses-a-dress-shirt-and-jeans.jpg?id=60333525&width=1245&height=700&coordinates=0%2C390%2C0%2C391"/><br/><br/><p>Last week, the federal government <a href="https://www.nature.com/articles/d41586-025-01645-4" rel="noopener noreferrer" target="_blank">terminated</a> hundreds of research grants to Harvard University professors from a broad range of fields of study. This comes on the heels of a conflict between Harvard, among other universities, and the Trump administration. </p><p>To recap: The Trump administration has <a href="https://www.usatoday.com/story/news/education/2025/03/31/trump-administration-harvard-grants-antisemitism/82746775007/" rel="noopener noreferrer" target="_blank">accused</a> Harvard of not doing enough to combat anti-Semitism on its campus and made a series of <a href="https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Letter-Sent-to-Harvard-2025-04-11.pdf" rel="noopener noreferrer" target="_blank">demands</a> to the university. Harvard has <a href="https://www.harvard.edu/research-funding/wp-content/uploads/sites/16/2025/04/Harvard-Response-2025-04-14.pdf" rel="noopener noreferrer" target="_blank">refused</a> to comply, claiming that the demands violate the First Amendment and amount to a government takeover of the institution. The Trump administration retaliated by terminating grants to Harvard from the National Science Foundation, the National Institutes of Health, and others.</p><p>Yesterday, the administration <a href="https://www.reuters.com/world/us/trump-administration-moves-cut-all-remaining-federal-contracts-with-harvard-2025-05-27/#:~:text=The%20government%20has%20already%20terminated,27%25%20of%20Harvard's%20total%20enrollment." rel="noopener noreferrer" target="_blank">foreshadowed</a> cutting all remaining federal funds to Harvard.</p><p><a href="https://edge.seas.harvard.edu/people/vijay-janapa-reddi" target="_blank">Vijay Janapa Reddi</a> is an associate professor of engineering and applied science at Harvard who specializes in computer architecture, specifically edge devices such as smartwatches, smartphones, autonomous vehicles, and more. His team focuses on making edge computing more sustainable by rethinking how these systems are designed and deployed in the real world. He’s also an IEEE member.</p><p>Last week, while his group was working hard to meet the abstract submission deadline for the prestigious <a href="https://neurips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> conference, Janapa Reddi learned that three of his grants had been terminated. <em>IEEE Spectrum</em> caught up with him about his experience and how the Trump administration’s actions will affect his field of study.</p><p><strong>How and when did you find out your grants were getting terminated?</strong></p><p><strong>Vijay Janapa Reddi:</strong> <span>It was around 10 p.m. when internal emails went out listing which grants were being cut. We were deep in submission mode for the </span><a href="https://neurips.cc/" target="_blank">NeurIPS</a><span> deadline, so it felt surreal. At first I tried to stay focused, doing business as usual. But as the news sank in the next day, the scale of the disruption became clear.</span></p><p>What’s most jarring is trying to hold both realities at once: pushing forward with your work, while also watching the foundation beneath it begin to crumble. That cognitive dissonance is hard to carry.</p><p><strong>What work were you doing under those grants?</strong></p><p><strong>Janapa Reddi:</strong> <span>One grant was focused on sustainability at the extreme edge, where computing must operate in settings with strict limits on power, cost, and available materials. These systems are deployed in places like food supply chains, agricultural fields, environmental sensors, and health care diagnostics in underserved areas. In such environments, computing can’t simply be an add-on. It must be reimagined to fit within the constraints of the setting while still delivering meaningful impact.</span></p><p>For instance, monitoring food spoilage is not just about attaching an everyday computer chip to a box of apples to monitor food deterioration. In many cases, the cost of that chip would exceed the value of the food itself. The deeper question is how to fundamentally redesign computing to be practical, scalable, and sustainable in resource-constrained contexts. This challenge led us to explore new types of hardware, including flexible, <a href="https://vimeo.com/1012849664" target="_blank">non-silicon microprocessors</a> based on the open RISC-V instruction set. These systems are programmable, low cost, and suited to real-world applications where traditional computing models fall short. The work is aligned with the <a href="https://sdgs.un.org/goals" target="_blank">UN’s Sustainable Development Goals</a> and seeks to bring technological innovation to places where it’s needed most.<br/></p><p>Another project we were working on was through <a target="_blank"></a><a href="https://mlcommons.org/" target="_blank">MLCommons</a>, a nonprofit organization where I serve as vice president. MLCommons helped establish some of the original industry <a href="https://spectrum.ieee.org/ai-inference" target="_blank">benchmarks for machine learning</a>, promoting shared evaluation standards across the field. One of our recent research initiatives focuses on supporting the development of <a href="https://spectrum.ieee.org/ai-for-science-2" target="_blank">foundation models for scientific applications</a>. We have been working on building an open-source ecosystem that enables contributions from the broader community, while also curating a set of benchmarks tailored to AI for science.</p><p>The other grant was intended to support a community workshop we were organizing to bring researchers together around shared challenges and opportunities. This effort was part of our broader commitment to education and public engagement, which aligns with the <a href="https://www.nsf.gov/" target="_blank">National Science Foundation</a>’s mission to ensure that research advances knowledge and reaches and benefits a wider audience.</p><p><strong>What effect does this have on your research?</strong></p><p><strong>Janapa Reddi:</strong> <span>The immediate impact is c</span><span>lear: I have to pause or scale down without funding. The deeper concern is what happens next. </span><span>Research doesn’t ramp down like a switch; for all of us, it unwinds slowly and takes time to regain the lost momentum. It’s a bit like stopping a freight train. You can’t bring it to a halt instantly, and once it has stopped, getting it moving again takes even more energy and time. Research is the same. It depends on people, planning, and long-term vision, none of which can be restarted overnight.</span></p><p><strong>What do you see as the longer-term effects of these cuts?</strong></p><p><strong>Janapa Reddi: </strong><span>I still believe in the strength of the American higher education and research ecosystem. It has a long history of rising to challenges, of turning constraints into catalysts for innovation. But moments like this test our resilience. The global perception of U.S. research is at risk. Disruptions like these send a concerning message to the next generation of scientists, engineers, and innovators around the world. That is troubling because what makes American research exceptional is not just the level of funding but the steady influx of talent, the diversity of thought, and the culture of open competition and collaboration.</span></p><p>Perhaps the most important thing to realize is that the research itself is almost secondary. It starts with people. If you look at any company with a trillion-dollar market value and ask what drives that long-term technology roadmap, it’s not an AI agent mapping it out. It’s the people behind it, the ones building, questioning, imagining, and creating. If we’re not investing in training those people to the highest caliber, then where is the next wave of innovation going to come from?</p><p><strong>What would you like to see going forward?</strong></p><p><strong>Janapa Reddi: </strong>The silence from those who have benefited from higher education is the most deafening—the people who earned their degrees, built their lives on that foundation, and know just how many doors it can open. If we want our kids to have the same chances we did, we cannot take those opportunities for granted. As beneficiaries of that system, we have a responsibility not just to protect it but to renew it, so that a decade from now, those doors are still open and continue to lead to even greater possibilities.</p><p>That’s especially true in areas like sustainable computing, where the challenges are urgent and the impact is tangible. Whether it is reducing food waste or building energy-efficient AI systems for science, these efforts cannot be paused indefinitely. As we submitted our work to NeurIPS last week, it reminded me why this matters. We are not just writing papers. We are trying to build a future that is smarter, more sustainable, and more just. To do that, we need a system that still believes in investing in the future.</p>]]></description><pubDate>Wed, 28 May 2025 19:51:34 +0000</pubDate><guid>https://spectrum.ieee.org/harvard-funding-cuts</guid><category>Computer architecture</category><category>Trump administration</category><category>National science foundation</category><category>Science funding</category><category>Edge computing</category><dc:creator>Dina Genkina</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/vijay-janapa-reddi-smiling-and-posing-on-a-staircase-in-an-office-he-is-wearing-glasses-a-dress-shirt-and-jeans.jpg?id=60333525&amp;width=980"></media:content></item><item><title>Price Index Could Clarify Opaque GPU Costs for AI</title><link>https://spectrum.ieee.org/gpu-prices</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-blue-shaded-line-chart-on-a-black-background.jpg?id=60333386&width=1245&height=700&coordinates=100%2C0%2C100%2C0"/><br/><br/><p>
	Ask what—if anything—is holding back the AI industry, and the answer you get depends a lot on who you’re talking to. I asked one of 
	<a href="https://www.bloomberg.com/professional/products/data/" target="_blank">Bloomberg’s</a> former chief data wranglers <a href="https://www.linkedin.com/in/carmenrli/" target="_blank">Carmen Li</a>, and her answer was “price transparency.”
</p><p>
	According to Li, the inability of most of the smaller AI companies to predict how much they will need to spend for the privilege of renting time on a GPU to train their models makes their businesses unpredictable and has made financing AI companies unnecessarily expensive. She founded the startup 
	<a href="https://www.silicondata.com/" target="_blank">Silicon Data</a> to create a solution: the first worldwide rental price index for a GPU.<strong></strong>
</p><p>
	That rental price index, called the 
	<a href="https://www.silicondata.com/silicon-index" target="_blank">SDH100RT</a>, launched today. Every day, it will crunch 3.5 million data points from more than 30 sources around the world to deliver an average spot rental price for using an <a href="https://www.nvidia.com/en-us/data-center/h100/" target="_blank">Nvidia H100</a> GPU for an hour. (“Spot price” is what a commodity to be delivered immediately sells for right now.)
</p><p>
	“I really believe compute will be the biggest resource for humanity in the next few years,” says Li. “If my thesis is right, then it will need more sophisticated risk management.”
</p><p>
	According to Li, such an index will lead to cheaper AI tools and more opportunities for a wider set of players to get involved in the AI industry. How do you get from an index to all that? Silicon Data’s origin story helps explain it.
</p><h3>US $1.04</h3><br/><p>Rental price advantage for Nvidia H100 GPUs on the East Coast of the United States versus those on the West Coast.</p><p>
<span>Until early last year, Li was in charge of global data integration at </span><a href="https://www.bloomberg.com/professional/products/data/" target="_blank">Bloomberg</a><span>. In that position she met with several small companies that were trying to deliver AI-fueled data products, and many of them were struggling with the same problem. They could only offer their product at a fixed rate, but the cost of the GPU time they needed was unpredictable. Therefore, so were their profit margins.</span>
</p><p>
	With typical commodities like energy, companies can plan for these swings by knowing historical trends and hedging with financial products like futures contracts. But that didn’t exist for AI’s main commodity: time on a GPU. So Li set out to create the foundation for those products, and the result is the SDH100RT price index.
</p><p>
	She chose to index the Nvidia H100, because it’s the most widely deployed GPU, and it’s used to train new AI models. However, a price index for Nvidia A100s, which tackle a lot of inference tasks, is in the works as well. And she’s developed a method that will determine when it makes sense to index prices for other AI chips, such as those from 
	<a href="https://spectrum.ieee.org/amd-mi300" target="_blank">AMD</a> and <a href="https://spectrum.ieee.org/nvidia-blackwell" target="_blank">Nvidia’s Blackwell</a> series.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="Professional portrait of Carmen Li." class="rm-shortcode" data-rm-shortcode-id="0b43b60fa835d67ad4e81deaa335b71f" data-rm-shortcode-name="rebelmouse-image" id="9e38b" loading="lazy" src="https://spectrum.ieee.org/media-library/professional-portrait-of-carmen-li.jpg?id=60333511&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Carmen Li founded Silicon Data after a stint at Bloomberg.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Silicon Data</small>
</p><p>
	Armed with the data, startups and others building new AI products will be able to understand their potential costs better, so they can set their services at a profitable price. And those building new AI infrastructure will be able to set a benchmark for their own revenue. But just as important, in Li’s opinion, is that new sources of capital can get involved in the AI space.
</p><p>
	Banks, for example, are a relatively inexpensive supplier of capital, notes Li. But because they have strict risk controls and there hasn’t been enough GPU price data, they haven’t been in a position to fund AI projects. Li hopes that the SDH100RT will let banks lend to a wider set of players in the AI industry and allow them to come up with financial products that reduce the risk for those already in it.
</p><h2>Insights and Oddities from the Data</h2><p>
	Although it launched today, Silicon Data has been tracking GPU rental prices for months. As you might expect, having a window into the price of AI training has unveiled some interesting insights. What follows are a few things Li has discovered. (She’s been publishing 
	<a href="https://medium.com/@cli_87015" target="_blank">these analyses</a> on the regular since last September.)
</p><p>
<strong>East Coast rules! West Coast drools:</strong> H100 rental pricing is very stable in the United States, but there’s a <a href="https://medium.com/@cli_87015/gpu-rental-prices-vary-more-than-you-think-even-across-u-s-regions-5fe0ce91f55e" target="_blank">persistent East Coast advantage</a>. In March you could get an hour of work from an H100 on the East Coast for US $5.76. But that same hour would cost you $6.80 on the West Coast.
</p><p>
<strong>Hyperscaler chips help: </strong>Amazon Web Services’ foray into <a href="https://spectrum.ieee.org/amazon-ai" target="_self">designing its own chips</a> and servers has <a href="https://medium.com/@cli_87015/aws-built-its-own-ai-chips-and-what-that-means-for-pricing-749ffcb3bf43" target="_blank">lowered prices</a> for the cloud giant’s customers. According to Silicon Data, at about $4.80 per hour, the average unit price per GPU for AWS’s Trainium2 is less than half the price for using an Nvidia H100. Its first-generation chips Inferentia and Trainium both come in at less than $1.50 per hour, which is less than half the price of today’s inference workhorse, the Nvidia A100. However, H100s are thought to be the only option for cutting-edge model training, so their performance might justify the extra scratch.
</p><p>
<strong>DeepSeek’s modest effect:</strong> January’s DeepSeek shock <a href="https://medium.com/@cli_87015/how-deepseek-r1-shook-up-the-gpu-market-21130ea9e2e1" target="_blank">did little</a> to the spot rental price. You may recall that the performance and <a href="https://spectrum.ieee.org/deepseek" target="_blank">reported low-cost training</a> of Hangzhou-based DeepSeek’s LLMs took many by surprise and sent AI-related stocks into a patch of pearl clutching. “When DeepSeek came out, the [stock] market went nuts,” says Li. “But the spot price didn’t change much.” On DeepSeek’s debut, the H100 price went up mildly to $2.50 per hour, but that was still in the $2.40 per hour to $2.60 per hour range from the months before. It then slid to $2.30 per hour for much of February before it started climbing again.
</p><p>
<strong>Intel is more posh than AMD: </strong>GPUs are always under the control of CPUs, usually in a 4:1 ratio. And the market for that CPU spot is contested between Intel and AMD. (Nvidia also makes its own CPU, called <a href="https://spectrum.ieee.org/nvidia-supercomputing-cpu-puts-intel-under-pressure" target="_self">Grace</a>.) But it seems customers are willing to pay a bit of a premium for Intel-powered systems. For Nvidia A100 systems, those with Intel CPUs fetched about a 40 percent higher price than those with AMD processors. For the H100, the effect depended on the interconnect technology involved. If a computer used SXM or PCIe as its links, Intel fetched a higher price. But for those using Nvidia’s NVLink interconnect scheme, AMD got the premium.
</p><h2>The Commoditization of AI</h2><p>
	Can you really boil the price of AI down to a single number? After all, there are so many factors involved in a computer’s performance and its utility to a particular customer. For example, a customer might be training with data that cannot, for legal reasons, cross international borders. So why should they care about the price in another country? And, as anyone who has examined machine learning’s leading benchmark results, 
	<a href="https://spectrum.ieee.org/tag/mlperf" target="_blank">MLPerf,</a> can see, the performance of the same Nvidia GPU can vary widely depending on the system it’s in and the software it’s running.
</p><p>
	According to Li, the commodity view can work. Silicon Data’s index normalizes all these differences and gives different weights to things like how much a data center participates in the rental market, its location, its data sources, and many, many other things.
</p><p>
	Perhaps the biggest endorsement of the idea of AI as a commodity is from 
	<a href="https://nvidianews.nvidia.com/bios/jensen-huang" target="_blank">Nvidia CEO Jensen Huang</a> himself. At the company’s big developer event, <a href="https://www.nvidia.com/gtc/" target="_blank">GTC</a>, he pushed for thinking of data centers as “AI factories” whose output would be measured in how many tokens, the smallest unit of information an LLM uses, they can produce per second.
</p>]]></description><pubDate>Wed, 28 May 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/gpu-prices</guid><category>Gpus</category><category>Nvidia</category><category>Prices</category><category>Finance</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-blue-shaded-line-chart-on-a-black-background.jpg?id=60333386&amp;width=980"></media:content></item><item><title>Record Number of Members Visit U.S. Congress to Talk Tech Policy</title><link>https://spectrum.ieee.org/ieee-congressional-visits-day-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-small-group-of-professionals-sit-and-speak-with-representative-lloyd-dogget-around-a-table.jpg?id=60332681&width=1245&height=700&coordinates=0%2C176%2C0%2C176"/><br/><br/><p>Standing in the hallowed halls of the U.S. Capitol Complex during <a href="https://ieeeusa.org/" rel="noopener noreferrer" target="_blank">IEEE-USA</a>’s <a href="https://ieeeusa.org/public-policy/cvd/" rel="noopener noreferrer" target="_blank">Congressional Visits Day</a> on 9 April, representing the collective voice of 150,000 dedicated U.S. members, was an experience that resonated with me.</p><p>A daylong event, CVD 2025 drew an impressive turnout of 329 participants from 39 states, culminating in 240 impactful meetings with U.S. legislators and their staff. As part of the Texas team, I participated in four crucial discussions.</p><p>The event was pivotal, with the potential to help shape the future of innovation and technology in the United States. Engaging directly with lawmakers, along with esteemed IEEE leaders and members who are experts in their fields, brought critical issues impacting our field into sharp focus. I left with a renewed sense of urgency and purpose.</p><h2>Funding federal scientific institutions</h2><p>One of the most compelling aspects of our discussions underscored the vital role of sustained funding under the fiscal year 2026 appropriations budget through strategic investments in key federal research institutions such as the <a href="https://www.energy.gov/" rel="noopener noreferrer" target="_blank">Energy Department</a>’s <a href="https://www.energy.gov/science/office-science" rel="noopener noreferrer" target="_blank">Office of Science</a>, the <a href="https://www.defense.gov/" rel="noopener noreferrer" target="_blank">Defense Department</a>’s <a href="https://basicresearch.defense.gov/" rel="noopener noreferrer" target="_blank">Basic and Applied Research</a>, <a href="https://www.nasa.gov/" rel="noopener noreferrer" target="_blank">NASA</a>, the <a href="https://www.nist.gov/" rel="noopener noreferrer" target="_blank">National Institute of Standards and Technology</a>, and the <a href="https://www.nsf.gov/" rel="noopener noreferrer" target="_blank">National Science Foundation</a> (NSF).</p><p>Having witnessed the groundbreaking technology and innovation-driven work emanating from those institutions and the fundamental discoveries that often seed transformative technologies, it’s clear to me that consistent investment would be not just beneficial but also essential for maintaining the country’s scientific and technological edge.</p><p>To consider diminishing the support in the current global landscape would be a detrimental step backward. </p><h2>Funding programs for small businesses</h2><p>Equally crucial was advocating for the reauthorization of the <a href="https://www.dhs.gov/science-and-technology/sbir" rel="noopener noreferrer" target="_blank">Small Business Innovation Research</a><em> </em>and <a href="https://www.sbir.gov/" rel="noopener noreferrer" target="_blank">Small Business Technology Transfer</a><em> </em>programs. The initiatives are more than just funding mechanisms; they are catalysts for ingenuity, empowering small businesses and supporting nimble innovators as they translate cutting-edge ideas into concrete commercial products.</p><p>The SBIR and STTR programs are administered by the U.S. <a href="https://www.sba.gov/" rel="noopener noreferrer" target="_blank">Small Business Administration</a>, and 11 federal agencies participate in them. Congress must reauthorize funding for this year. The programs support approximately 4,000 businesses to the tune of about US $4 billion annually.</p><p>Their continued vitality is paramount because small businesses foster a dynamic ecosystem wherein trickle-down benefits significantly contribute to job creation and economic growth in addition to promoting innovation. </p><h2>Legislation for democratizing artificial intelligence resources</h2><p>One conversation that struck a chord with me was on <a href="https://spectrum.ieee.org/ai-regulation-worldwide" target="_self">democratizing AI</a> resources through the <a href="https://www.heinrich.senate.gov/imo/media/doc/create_ai_act_fact_sheet1.pdf" rel="noopener noreferrer" target="_blank">Creating Resources for Every American to Experiment With Artificial Intelligence (CREATE AI) Act</a>. The proposal would permanently establish the <a href="https://www.nsf.gov/focus-areas/artificial-intelligence/nairr" rel="noopener noreferrer" target="_blank">National AI Research Resource</a> pilot program, led by the NSF in coordination with the <a href="https://spectrum.ieee.org/tech-leader-us-ai-strategy" target="_self">White House Office of Science and Technology Policy</a> in partnership with federal and private organizations. It is a cloud computing resource that leverages the <a href="https://www.energy.gov/" rel="noopener noreferrer" target="_blank">Energy Department</a>’s computational assets including the <a href="https://www.ornl.gov/" rel="noopener noreferrer" target="_blank">Oak Ridge National Laboratory</a>’s <a href="https://spectrum.ieee.org/a-us-machine-recaptures-the-supercomputing-crown" target="_self">Summit supercomputer</a>.</p><p>The CREATE AI Act’s vision of shared national infrastructure for the research community could level the playing field, enabling wider participation in the technology’s innovation and ensuring its benefits reach all sectors of society. It isn’t just about technological advancement; it’s about equitable access to future-defining resources.</p><p>Academic research is a top source for cited AI research, directly benefiting industry and even contributing to life-saving drugs—which highlights a symbiotic relationship that must be nurtured.</p><p>The industry trends we discussed with politicians serve as powerful evidence for our calls to action.</p><p class="pull-quote"><span>“We, as engineers, scientists, and innovators, have a crucial role to play in educating and advocating for policies that will foster a thriving ecosystem of innovation.”</span></p><p>The exponential rise in <a href="https://www.fda.gov/" target="_blank">Food and Drug Administration</a>-approved <a href="https://spectrum.ieee.org/ai-diagnosis-cancer" target="_self">AI-based medical devices</a>—from six in 2015 to more than 220 in 2023—isn’t just a statistic. It represents a revolution in health care, offering the potential for more accurate diagnoses, more personalized treatments, and saved lives. Similarly, the lower <a href="https://spectrum.ieee.org/data-center-cooling-xmems" target="_self">hardware costs</a> fueling broader AI adoption across diverse sectors, including <a href="https://spectrum.ieee.org/driverless-bus" target="_self">autonomous driverless vehicles</a> and supply-chain optimization, signal a profound economic transformation on the horizon.</p><p>The compelling data the IEEE participants shared from <a href="https://www.stanford.edu/" target="_blank">Stanford</a>’s <a href="https://hai.stanford.edu/ai-index/2025-ai-index-report" rel="noopener noreferrer" target="_blank">2025 AI Index Report</a> showed a staggering 280 times drop in AI model inference costs in two years, from November 2022 to October 2024. The findings paint a clear picture: AI is no longer a distant frontier but an increasingly accessible tool.</p><p>A legislative aide, after hearing that statistic, remarked on the need to plan for the energy requirements and the diversified sources that could be leveraged, including <a href="https://spectrum.ieee.org/small-modular-reactor-united-states" target="_self">nuclear energy</a>.</p><p>It would be a missed opportunity of significant scientific, economic, and strategic consequence if the momentum is not leveraged for the nation’s benefit.</p><h2>Strengthening STEM roots</h2><p>We must continue to champion strengthening the science, technology, engineering, and mathematics talent pipeline through robust funding and sustained support for the 2022 <a href="https://spectrum.ieee.org/chips-act-of-2022" target="_self">CHIPS and Science Act</a>. The country’s ability to lead in technology hinges on the next generation of skilled professionals.</p><p>To that end, it is crucial that the United States expand the <a href="https://travel.state.gov/content/travel/en/us-visas/immigrate/employment-based-immigrant-visas.html" rel="noopener noreferrer" target="_blank">employment-based immigrant visa program</a> for highly skilled, qualified individuals to ensure a sustained supply of talent required for industry and academia through legislation such as the <a href="https://www.congress.gov/bill/118th-congress/senate-bill/2384" rel="noopener noreferrer" target="_blank">Keep STEM Talent Act</a>. My own experiences, including my engagement with <a href="https://www.northwestern.edu/" rel="noopener noreferrer" target="_blank">Northwestern University</a> and its <a href="https://www.mccormick.northwestern.edu/engineering-management/overview/advisory-board/" rel="noopener noreferrer" target="_blank">STEM master of engineering management program advisory board</a>, have underscored the critical link between academia and industry.</p><p>Witnessing the bipartisan engagement and the genuine interest from lawmakers and their staff last month, including influential figures serving on key congressional committees, was inspiring. It underscored that the importance of technological innovation transcends political divides. The engagement must translate into concrete action in Congress and by the administration.</p><p>My experience on Capitol Hill was more than just a privilege; it was a stark reminder of our collective responsibility.</p><p>The future of U.S. technological leadership is not a passive outcome. It demands active engagement between multiple stakeholders and unwavering support from our policymakers.</p><p>We, as engineers, scientists, and innovators, have a crucial role to play in educating and advocating for policies that will foster a thriving ecosystem of innovation.</p><p>The time for congressional intervention is now. We cannot afford to be complacent. The global landscape is rapidly evolving, and maintaining U.S. leadership in technology depends on strategic investments and forward-thinking policies.</p><p>For U.S. lawmakers, the importance of current technological changes serves as a call to action. The elected officials have a responsibility to ensure the United States remains at the forefront of technological advancement for generations to come.</p><p>My experience last month on Capitol Hill reinforced the power of our collective voice, and I am grateful to IEEE-USA for the esteemed opportunity to participate.</p>]]></description><pubDate>Wed, 28 May 2025 17:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-congressional-visits-day-2025</guid><category>Artificial intelligence</category><category>Ieee member news</category><category>Ieee-usa</category><category>Semiconductors</category><category>Stem</category><category>Type:ti</category><category>U.s. congress</category><dc:creator>Azgar Ali Noor Ahamed</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-small-group-of-professionals-sit-and-speak-with-representative-lloyd-dogget-around-a-table.jpg?id=60332681&amp;width=980"></media:content></item><item><title>A Simple Solution to Wind Turbine Bird Deaths?</title><link>https://spectrum.ieee.org/bird-deaths-from-wind-turbines</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-in-glasses-with-eagles-and-wind-turbines-overlay-image-with-seasonal-color-labels.png?id=60328359&width=1245&height=700&coordinates=0%2C114%2C0%2C114"/><br/><br/><p><a data-linked-post="2667602894" href="https://spectrum.ieee.org/wind-turbine-tech-acceleration" target="_blank">Wind turbines</a> kill a lot of birds, particularly eagles and other raptors. The exact number is unknown, because many of the world’s wind farms don’t monitor bird deaths. One mitigation idea to reduce these numbers—and assuage a political argument against wind turbines—is gaining traction: Paint one turbine blade black. Ecologist <a href="https://www.nina.no/english/Contact/Employees/Employee-info/AnsattID/12861" rel="noopener noreferrer" target="_blank"><u>Roel May</u></a> spoke with <em><em>IEEE Spectrum</em></em> about his <a href="https://onlinelibrary.wiley.com/doi/10.1002/ece3.6592" rel="noopener noreferrer" target="_blank"><u>11-year study</u></a> to reduce raptor deaths, and his surprise at the lukewarm reactions from wind-turbine engineers.</p><h3>Roel May </h3><br/><p><a href="https://www.nina.no/english/Contact/Employees/Employee-info/AnsattID/12861" rel="noopener noreferrer" target="_blank"><u>Roel May</u></a> is a senior research scientist focusing on renewable energy impacts and mitigation at the Norwegian Institute for Nature Research (NINA) in Trondheim, Norway.</p><p><strong>How bad are wind turbines for birds?</strong></p><p><strong>Roel May: </strong>It depends a lot on where you put the turbines, and whether there’s breeding or foraging areas nearby, and the species of birds. If you put wind turbines smack in the middle of a vulnerable population, the effect can be quite large. That’s what happened in Norway on the island of Smøla, which is a hot spot for white-tailed eagles. That’s where we did our study.</p><p><strong>Why are eagles susceptible to wind turbines?</strong></p><p><strong>May: </strong>Raptor species like eagles are very good at flying, but they don’t look straight ahead; they look down at the ground for prey. They like to use updrafts to soar, but updrafts are common near ridges where wind conditions are good for turbines. So that’s a bad combination.</p><p><strong>Does the wind-turbine type make a difference? </strong></p><p><strong>May: </strong>Larger turbines kill more birds because their blades take up more area. But where wind turbines are smaller, there’s often more of them, so those wind farms may end up killing more birds than farms with fewer, larger turbines, at least onshore. <a data-linked-post="2650271092" href="https://spectrum.ieee.org/floating-wind-turbines-headed-for-offshore-farms" target="_blank">Offshore wind farms</a> are harder to study: You can’t count the exact number of birds that die because you can’t find them; they fall into the sea and they’re gone. Some researchers are trying to record collisions with <a href="https://www.robinradar.com/markets/wind-farm-bird-radar" rel="noopener noreferrer" target="_blank"><u>bird radar</u></a>, cameras, and other systems.</p><p><strong>You found that painting one blade black resulted in a 70 percent decline in bird deaths. Should all wind farms be doing this?</strong></p><p><strong>May: </strong>Our study is something that should be repeated in other places because there are likely site-specific and species-specific effects. A group in the Netherlands painted blades and hasn’t seen a clear effect. So we need more studies. There’s one study going in South Africa where they <a href="https://mg.co.za/the-green-guardian/2023-10-17-wind-farms-to-trial-painted-blades-in-bid-to-prevent-bird-strikes/" rel="noopener noreferrer" target="_blank"><u>painted a blade red</u></a>, and some others that are<a href="https://www.pacificorp.com/about/newsroom/news-releases/bird-safety-wind-turbines.html" rel="noopener noreferrer" target="_blank"> <u>starting in Wyoming</u></a>, Italy, and <a href="https://www.iberdrola.com/documents/20125/622199/IB_NI_Iberdrola_Painting_Wind_Turbine_Birdlife.pdf" rel="noopener noreferrer" target="_blank"><u>Spain</u></a>. The United Kingdom is planning<a href="https://www.telegraph.co.uk/politics/2025/02/26/wind-turbines-paint-black-donald-trump-birds-keir-starmer/?ICID=continue_without_subscribing_reg_first" rel="noopener noreferrer" target="_blank"> <u>a pilot project</u></a> for offshore turbines. But these studies are hard to do because the operator has to get exemptions from regulations to paint the blades and hire certified painters who can rappel up and down. And it takes a lot of time. We collected data on eagles in Smøla for seven years before the blades were painted. After we painted, we spent four more years studying the effects.</p><p><strong>How did engineers respond to your study</strong>?</p><p><strong>May: </strong>A lot of people from turbine manufacturers asked if I had considered the technical implications of this. I hadn’t, because I’m an ecologist, not an engineer. Apparently black blades will heat up more than white blades, which may cause structural effects. And the black paint is made with carbon, which could affect the turbine’s performance when hit by lightning. Engineers don’t like that. So we need to overcome these disciplinary silos and work together to develop functional mitigation measures.<br/></p><p><em>This article appears in the June 2025 print issue as “Roel May.”</em></p>]]></description><pubDate>Wed, 28 May 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/bird-deaths-from-wind-turbines</guid><category>5 questions</category><category>Climate change</category><category>Climate tech</category><category>Type:departments</category><category>Wind turbines</category><dc:creator>Emily Waltz</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-man-in-glasses-with-eagles-and-wind-turbines-overlay-image-with-seasonal-color-labels.png?id=60328359&amp;width=980"></media:content></item><item><title>A Knee Injury Launched This VR Pioneer’s Career</title><link>https://spectrum.ieee.org/ballerina-turned-vr-pioneer</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-woman-standing-by-the-corner-of-a-room-with-circular-blue-images-projected-on-the-walls.png?id=60311059&width=1245&height=700&coordinates=0%2C192%2C0%2C192"/><br/><br/><p>
	Carolina Cruz-Neira is living proof of just how far a “plan B” can take you. Growing up, she was determined to become a professional ballet dancer. But after injury ended her ballet dreams, Cruz-Neira went on to become a pioneer of <a href="https://spectrum.ieee.org/tag/virtual-reality" target="_self">virtual-reality</a> technology.</p><h3>Carolina Cruz-Neira</h3><br/><p>
<strong>Employer:</strong>
</p><p>
	University of Central Florida
</p><p>
<strong>Occupation:</strong>
</p><p>
	Computer-science professor
</p><p><strong>Education:</strong></p><p>Bachelor’s degree in systems engineering, Universidad Metropolitana in Caracas; master’s degree in electrical engineering and computer science, University of Illinois Chicago; Ph.D. in electrical engineering and computer science, University of Illinois Chicago</p><p>
	A computer-science professor at the <a href="https://www.ucf.edu/" rel="noopener noreferrer" target="_blank">University of Central Florida</a> (UCF) and IEEE Fellow, Cruz-Neira is best known for developing the Cave Automatic Virtual Environment, or CAVE, an immersive VR system that turns a small room into an interactive 3D digital world, in the 1990s. Over her nearly 40-year career she has also developed VR tools for fields as varied as medical research and defense.</p><p>Unlike many engineers, though, Cruz-Neira had little interest in technology as a child. When her dreams of a career in ballet ended the year before she graduated from university, she fell back on the technical skills she had been developing and began working as a software engineer. But her heart wasn’t in it—until an introduction to early VR technology set her off on a new trajectory.
</p><p>
	“I found I could work with computer systems in real time in a way that was very visual and in touch with your users, equivalent to how you are in touch with your audience as a dancer,” she says.
</p><h2>Born to Dance</h2><p>
	Cruz-Neira’s childhood was split between Spain, where she was born, and Venezuela, where her parents ran a fashion import business. She started ballet classes at the age of three, and by the time Cruz-Neira was a teenager, she was spending two or three hours every evening at the dance studio. “I always had my ballet shoes in my backpack,” she says. “So even if I was in school I would be dancing around the hallways.”
</p><p>
	Her determination to fill any spare time with ballet, combined with a natural aptitude for math and science, pushed her into studying more technical topics. “I could do math homework in 10 minutes, but if I had to read a book and write an essay, it would take me hours,” she says. “So I went into science and engineering just because I needed more time in the dance studio.”
</p><p>
	Cruz-Neira was determined to make a career in ballet, but her father encouraged her to get a bachelor’s degree as a backup. He told her that computers were the future and encouraged her to study <a href="https://spectrum.ieee.org/tag/systems-engineering" target="_self">systems engineering</a> at the <a href="https://www.unimet.edu.ve/" rel="noopener noreferrer" target="_blank">Universidad Metropolitana</a> in Caracas, where she enrolled in 1982.
</p><h2>Dreams Crushed</h2><p>
	In 1986 at the age of 21, Cruz-Neira broke her knee in a skiing accident, putting an end to her hopes of becoming a professional ballerina. The news was devastating, she says, but the same year she started as an intern at Teleprovenca, a Venezuelan company that provided computing services to large corporations. Cruz-Neira graduated cum laude the following year and transitioned to a full-time position as a software architect.
</p><p>
	She excelled in the role and was promoted from intern to manager in less than two years. But she found little enjoyment in it. “That was a very dark time in my life,” she says. “I was almost like a robot that was just mechanically doing things.”
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="two men stand in a small room with a projected scene on three walls and the floor. The wall in front of them displays a large tree with a red door" class="rm-shortcode" data-rm-shortcode-id="ef9358724f7fac6f1e4bc8324c8391c0" data-rm-shortcode-name="rebelmouse-image" id="1f3cd" loading="lazy" src="https://spectrum.ieee.org/media-library/two-men-stand-in-a-small-room-with-a-projected-scene-on-three-walls-and-the-floor-the-wall-in-front-of-them-displays-a-large-tr.png?id=60311097&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In this application of CAVE, researchers use a digital twin to study the design of a new zoo pavilion. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Emerging Analytics Center/University of Arkansas at Little Rock</small></p><p>
	Still, she continued to develop her technical skills. In 1989 she won a scholarship to study in the United States and enrolled in a master’s program at the <a href="https://www.uic.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Chicago</a> to study electrical engineering and computer science. Cruz-Neira’s first year was spent learning English and taking courses in subjects like databases, networking, and parallel computing.
</p><p>
	But in her second year she discovered the university’s <a href="https://www.evl.uic.edu/" rel="noopener noreferrer" target="_blank">Electronic Visualization Laboratory</a>—at the time a joint program offered by the engineering and art departments that focused on computer graphics and computer animation. There, Cruz-Neira finally found a way to connect her technical skills with her artistic passions.
</p><p>
	“That’s when I started to be motivated again and excited about what I was doing,” she says. “It was a very stimulating environment.”
</p><h2>Building the CAVE</h2><p>
	Cruz-Neira’s master’s thesis focused on using interactive 3D graphics to present financial data. When she graduated in 1991, she briefly held a job with IBM developing data-visualization tools for stockbrokers on Wall Street. But she found the corporate structure too restrictive. “I felt like a racehorse that was tied to the back of the stable, because I had all these ideas but had to stay on point on the project,” she says.
</p><p>
	A few months into the job, her master’s program adviser offered to take her on as a Ph.D. student in the Electronic Visualization Laboratory, an opportunity she couldn’t pass up. Just before she began her Ph.D., in August 1991, Cruz-Neira attended the Special Interest Group on Computer Graphics and Interactive Techniques (<a href="https://www.siggraph.org/" rel="noopener noreferrer" target="_blank">SIGGRAPH</a>) conference, which featured an exhibit of early VR devices. She immediately fell in love with the technology. “Whatever digital world you could imagine in a computer, you could actually put a person in the middle of that world,” she says.
</p><p>
	But she found that the cumbersome early VR headsets limited the kinds of <a href="https://spectrum.ieee.org/vr-games" target="_self">experiences that could be created</a>. The chance discovery of some old industrial projectors in one of the university’s storage rooms gave her the inspiration for an entirely new approach.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="black and white photo of a woman lifting glasses above her forehead and holding up a tag that says \u201cCAVE\u201d" class="rm-shortcode" data-rm-shortcode-id="b97bfbc9c799a869136560da5010acca" data-rm-shortcode-name="rebelmouse-image" id="53901" loading="lazy" src="https://spectrum.ieee.org/media-library/black-and-white-photo-of-a-woman-lifting-glasses-above-her-forehead-and-holding-up-a-tag-that-says-u201ccave-u201d.png?id=60311065&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Cruz-Neira first unveiled CAVE in 1992 at the SIGGRAPH conference while pursuing her Ph.D. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Carolina Cruz-Neira</small></p><p>
	She hooked the machines up to graphics workstations built by Silicon Graphics and used them to project a virtual environment onto bedsheets taped to the walls. Her professors loved the idea, and Cruz-Neira began <a href="https://ieeexplore.ieee.org/abstract/document/237582" rel="noopener noreferrer" target="_blank">developing it into the Cave Automatic Virtual Environment</a>, referred to by the recursive acronym CAVE.</p><p>A year later, <a href="https://dl.acm.org/doi/10.1145/129888.129892" rel="noopener noreferrer" target="_blank">in 1992</a>, she unveiled the first version of CAVE at SIGGRAPH. Users wore stereoscopic glasses synchronized with the projector that turned 2D images displayed on the walls into 3D graphics. A motion-capture system also detects the wearer’s orientation, which makes it possible to continuously adapt the projections to fit the user’s perspective.
</p><h2>From Art to Science</h2><p>
	Cruz-Neira spent the rest of her Ph.D. continuing to develop the system. She says the project was inspired by a desire to allow groups of people to share artistic and creative experiences, but she quickly realized its broader potential. It could also be used as a collaborative space for science or engineering, she says.
</p><p>
	For example, she worked with Argonne National Laboratory to develop a CAVE system that <a href="https://www.sciencedirect.com/science/article/abs/pii/0097848596000095" rel="noopener noreferrer" target="_blank">allowed biologists to interact with molecular dynamics simulations</a>. The project helped researchers accelerate the development of new drugs to treat AIDS, which at the time was a death sentence. This is one of the projects Cruz-Neira is most proud of.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;">
<img alt="woman smiling and wearing a pink jacket and black electronic gloves, holding dark glasses above her forehead" class="rm-shortcode" data-rm-shortcode-id="68dfe9ab3305860478c9f19392c86b4e" data-rm-shortcode-name="rebelmouse-image" id="0e86f" loading="lazy" src="https://spectrum.ieee.org/media-library/woman-smiling-and-wearing-a-pink-jacket-and-black-electronic-gloves-holding-dark-glasses-above-her-forehead.jpg?id=60311087&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In newer versions of the CAVE system, haptic gloves like the ones Cruz-Neira wears here allow users to feel virtual objects.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">VARLab/University of Central Florida</small></p><p>
	After completing her Ph.D. in 1995, she cofounded the <a href="https://www.vrac.iastate.edu/" rel="noopener noreferrer" target="_blank">Virtual Reality Applications Center</a> at <a href="https://www.iastate.edu/" rel="noopener noreferrer" target="_blank">Iowa State University</a>, and since then has held positions at several U.S. universities, including her current professorship at UCF.
</p><p>
	Over the years her work has broadened, Cruz-Neira says. She now creates software for real-time information manipulation in fields as varied as energy, pharmaceuticals, and finance. Her group is “display agnostic,” she says, so they work with all kinds of devices, including CAVE, VR headsets, and standard monitors.
</p><p>
	Currently much of her focus is on “<a href="https://spectrum.ieee.org/tag/digital-twin" target="_self">digital twins</a>”—dynamic virtual copies of real-world objects that can be used for simulation and testing. Despite the excitement around this technology, Cruz-Neira says it’s really just an evolution of the ideas she was using in her molecular dynamics simulations in the ’90s.
</p><p>
	“That was a digital twin of a molecular system,” she says of her past project. “It had simulation, it had interactivity, it had real-time interconnections with many other systems. So, in a sense, we aren’t going into new areas. We are evolving with the times.”
</p><p>
	But she hasn’t forgotten her roots. Cruz-Neira still regularly stages interactive experiences at theaters, museums, and art galleries with CAVE and other systems, and she recently produced a dance performance. “I still keep in touch with my more artistic side,” she says.
</p>]]></description><pubDate>Tue, 27 May 2025 13:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/ballerina-turned-vr-pioneer</guid><category>Type:departments</category><category>Virtual reality</category><category>Computer graphics</category><category>Computer animation</category><category>Software engineer</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/photo-of-woman-standing-by-the-corner-of-a-room-with-circular-blue-images-projected-on-the-walls.png?id=60311059&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1245&height=700&coordinates=0%2C0%2C0%2C474"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Deep learning</category><category>Artificial intelligence</category><category>Andrew ng</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1245&height=700&coordinates=0%2C156%2C0%2C156"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Matlab</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1245&height=700&coordinates=0%2C156%2C0%2C156"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Quantum computing</category><category>2d materials</category><category>Ibm</category><category>Qubits</category><category>Hexagonal boron nitride</category><category>Superconducting qubits</category><category>Mit</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>