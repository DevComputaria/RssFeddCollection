<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Wed, 02 Jul 2025 22:15:37 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2MzA3MTQzOX0.SxRBIud_XE2YWQFaIJD9BPB1w-3JsFhiRkJIIe9Yq-g/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Be Polarizing as a Job Seeker</title><link>https://spectrum.ieee.org/be-a-polarizing-job-seeker</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p><span>There are countless technologies, tools, and frameworks to learn as an engineer. As a </span>job seeker, this makes it extremely difficult to decide where to invest. You don’t want to spend a bunch of time learning something that employers don’t care about. So, how can you determine what to learn with your valuable time?</p><p>The answer to this comes from the realization that your goal is to find exactly one job. Ultimately, you will accept one job offer; you cannot hold multiple full-time jobs concurrently. Instead of appealing to all potential employers, your goal is to be an exceptional fit for one specific job.</p><p><span></span><span>Said differently, </span><strong>you should aim to be polarizing</strong><span>. A hiring manager should look at your resume or LinkedIn profile and have one of two reactions:</span><br/></p><ul><li>This person is a great fit (“<em>I don’t see many developers who know Rust so deeply. We should definitely interview them”</em>)</li><li>This person is clearly not a good fit (<em>“We use Python, which they don’t mention at all”</em>)</li></ul><p>You want to avoid the “maybe” bucket, which is unfortunately where the majority of job seekers end up. These borderline engineers dilute their resumes by mentioning too many buzzwords and technologies. If they followed a 2-hour YouTube tutorial, they’ll list that technology on their resume. These resumes don’t strongly appeal to anyone because they try to appeal to everyone. The logic of the hiring manager is, “They’ve listed Python along with 10 other programming languages, so they’re probably not good at any of them.” In a difficult job market, this results in poor outcomes.</p><p>As you become increasingly senior in your career, you should become increasingly polarizing. When you have clarity about the technologies, people, and companies you want to work with, you dramatically increase your chances of landing a role that meets your criteria.</p><p>So how should we decide which technology to invest in? The answer is that it doesn’t matter, as long as you pick something and commit to it. When we embrace the idea of polarization, we can remove the second-guessing that plagues many job seekers. Too many engineers agonize over which JavaScript framework to learn or which AutoCAD toolset to master. </p><p>Instead of getting stuck in decision paralysis, simply commit to a technology for a few months based on what you or your friends find interesting. You can always change course later, but the learning starts only when you commit. When you go deep enough in one area, you’ll find it significantly easier to translate that expertise into adjacent areas. Your confidence will grow, and you’ll have more fun.<br/></p><p>Depth in one domain will significantly increase your chances of landing a job that fits your background.</p><p>-Rahul</p><h2><a href="https://spectrum.ieee.org/women-in-semiconductors-workforce" target="_blank" title="Why the Semiconductor Industry Can’t Abandon Women">Why the Semiconductor Industry Can’t Abandon Women</a></h2><p>Fewer companies in the semiconductor industry were publicly committed to equal opportunity measures in 2024 than the year prior, despite the fact that major workforce shortages are expected. In this Q&A, Andrea Mohamed, COO and co-founder of the professional development company QuantumBloom, speaks about supporting women in semiconductor jobs, and why a retreat from these initiatives is at odds with the needs of the industry.<br/></p><div><span><a href="https://spectrum.ieee.org/women-in-semiconductors-workforce" target="_blank" title="Read more here.">Read more here.</a></span></div><h2><a href="https://techcrunch.com/2025/06/30/tech-layoffs-2025-list" target="_blank" title="A comprehensive list of 2025 tech layoffs">A comprehensive list of 2025 tech layoffs</a></h2><p>Tech layoffs have continued in 2025, with more than 22,000 workers being cut since January. Updated 17 June, TechCrunch is compiling a comprehensive list of the layoffs, including monthly totals and notable events from individual companies. Microsoft, Airtime, Playtika, and Intel all reduced their workforce in June. </p><div><span><a href="https://techcrunch.com/2025/06/30/tech-layoffs-2025-list" target="_blank" title="Read more here.">Read more here.</a></span></div><h2><a href="https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research" target="_blank" title="Profile: Guatemalan Engineer Grows From Rural Roots to Ph.D.">Profile: Guatemalan Engineer Grows From Rural Roots to Ph.D.</a><span class="redactor-invisible-space"></span></h2><div><span></span></div><p>Mayra Yucely Beb Caal has overcome obstacles to forge a successful career in engineering. Raised in a rural Guatemalan village, Caal received her Ph.D. with support from an IEEE scholarship and now uses AI to improve cancer detection. “I don’t want to create technology just for the sake of it,” she says. “I want it to mean something—to help solve real problems in society, like the ones I faced early on.”<span></span></p><div><span><a href="https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research" target="_blank" title="Read more here.">Read more here.</a><span class="redactor-invisible-space"></span></span></div><div><span><span class="redactor-invisible-space"></span></span></div><div><span></span></div><div><span></span></div>]]></description><pubDate>Wed, 02 Jul 2025 21:05:35 +0000</pubDate><guid>https://spectrum.ieee.org/be-a-polarizing-job-seeker</guid><category>Careers newsletter</category><category>Careers</category><category>Practical strategies</category><category>Tech careers</category><category>Career development</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>Vera Rubin: This Is How Far Engineers Go to Explore the Universe</title><link>https://spectrum.ieee.org/vera-rubin-engineering</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/man-in-yellow-hard-hat-with-a-white-building-housing-a-telescope-in-the-background.png?id=61113802&width=1200&height=800&coordinates=0%2C58%2C0%2C59"/><br/><br/><p>The<a href="https://rubinobservatory.org/about" rel="noopener noreferrer" target="_blank"> Vera C. Rubin Observatory</a>, in Chile, saw its first photon a few months ago, a monumental event 25 years in the making. Senior Editor <a href="https://spectrum.ieee.org/u/evan-ackerman" target="_blank">Evan Ackerman’s</a> journey to Chile, to see the observatory and talk with the team about the multiple and massive engineering challenges they overcame, itself took more than a year to plan.</p><p>The visit was initiated by Italian photographer <a href="https://es-photography.com/" rel="noopener noreferrer" target="_blank">Enrico Sacchetti</a>, who had arranged for exclusive access to the telescope. The story we wanted Ackerman to tell required more than a quick tour. So Sacchetti and Ackerman arranged to spend three nights on the summit of Cerro Pachón, sleeping during the day and then staying up late with the engineers and scientists as they worked to get Rubin “on sky.”</p><p>Ackerman and Sacchetti didn’t know exactly what would happen while they were there. In some ways, they got lucky—the few days before first photon were full of frantic activity. In other ways, they weren’t so fortunate. The dome covering the telescope wasn’t working, the moon (bane of astronomers everywhere) was near full, and Sacchetti came down with an illness that nearly required him to be evacuated down the mountain.</p><p class="pull-quote">“Spotting a viscacha near the observatory is good luck for that night’s seeing.” <strong>—Evan Ackerman</strong></p><p> “For a slightly panicked 24 hours, I enlisted the help of some amateur photographers among the Rubin staff to make sure that we had all the photos that we’d need,” Ackerman says. You can see some of their excellent work in “<a href="https://spectrum.ieee.org/vera-rubin-observatory-first-images" target="_self">How the Rubin Observatory Will Reinvent Astronomy</a>.” And Sacchetti recovered enough to get the crucial shots he wanted.</p><p>The same characteristics that make Cerro Pachón the perfect place for observatories can make it a challenging place to work. For Sacchetti and Ackerman, as well as the Rubin staff, schlepping up to the 2,600-meter summit from sea level took some adjustment. Ackerman didn’t have much of a physical reaction to the altitude. But he learned that mentally, the thin air hits everyone a little differently.</p><p>“I discovered a complete inability to remember schedules,” Ackerman recalls. “William O’Mullane, data-management project manager for Rubin, told me that for him, it’s feeling that he knows the answer to a question, but not what the answer actually is.”</p><p> In addition to scheduled interviews with engineers and astronomers, Ackerman wandered around the control room, joining conversations that seemed interesting. The Rubin staff isn’t superstitious, but he nonetheless heard some rumors involving the local fauna.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A chinchilla-like rodent with big ears in front of a rock." class="rm-shortcode" data-rm-shortcode-id="0214ff0dae5562ee123bf1c597c7db6d" data-rm-shortcode-name="rebelmouse-image" id="87447" loading="lazy" src="https://spectrum.ieee.org/media-library/a-chinchilla-like-rodent-with-big-ears-in-front-of-a-rock.jpg?id=61113803&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Viscachas, which are a type of chinchilla the size of a rabbit, are a good omen for astronomers at the Rubin observatory.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Evan Ackerman</small></p><p>“Spotting a viscacha near the observatory is good luck for that night’s seeing, as it should be. It looks like an aggressively cute cross between a squirrel and a rabbit, but it’s technically a kind of large chinchilla,” he says. Less cute are the Andean condors that live on the cliffs near the Southern Astrophysical Research Telescope, which is also located on Cerro Pachón. Seeing them in the air in the evening is a bad sign, Ackerman was told, which may be somewhat grounded in reality, since the thermals that the condors ride imply turbulent air around the mountain.</p><p>Even the other “unlucky” parts of the visit helped make the story better. The full moon, while overpowering much of the sky, lit up the outside of the observatory and resulted in some fantastic nighttime photos. And the temporarily nonfunctional dome led to several in-depth conversations about how difficult it is to get all of these bespoke systems to work with one another, and helped Ackerman appreciate the complex job of the commissioning engineers.</p>]]></description><pubDate>Wed, 02 Jul 2025 20:26:42 +0000</pubDate><guid>https://spectrum.ieee.org/vera-rubin-engineering</guid><category>Astronomy</category><category>Vera c. rubin observatory</category><category>Telescope</category><category>Observatory</category><category>Cosmology</category><dc:creator>Harry Goldstein</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/man-in-yellow-hard-hat-with-a-white-building-housing-a-telescope-in-the-background.png?id=61113802&amp;width=980"></media:content></item><item><title>LLM Benchmarking Shows Capabilities Doubling Every 7 Months</title><link>https://spectrum.ieee.org/llm-benchmarking-metr</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/spacey-scene-shows-a-network-of-orange-nodes-in-space-over-the-earth.jpg?id=61135467&width=1200&height=800&coordinates=200%2C0%2C200%2C0"/><br/><br/><p>The main purpose of many <a data-linked-post="2668300000" href="https://spectrum.ieee.org/theory-of-mind-ai" target="_blank">large language models</a> (LLMs) is providing compelling text that’s as close as possible to being indistinguishable from human writing. And therein lies a major reason why it’s so hard to gauge the relative performance of LLMs using traditional benchmarks: Quality of writing doesn’t necessarily correlate with metrics traditionally used to measure processor performance, such as instruction execution rate.</p><p class="ieee-inbody-related"><strong>RELATED: </strong><span><a href="https://spectrum.ieee.org/large-language-model-performance" target="_blank">Large Language Models Are Improving Exponentially</a></span><br/></p><p>But researchers at the Berkeley, Calif., think tank METR (for <a href="https://metr.org/" target="_blank">Model Evaluation & Threat Research</a>) have come up with an ingenious idea. First, identify a series of tasks with varying complexity and record the average time it takes for a group of humans to complete each task. Then have various versions of LLMs complete the same tasks, noting cases in which a version of an LLM successfully completes the task with some level of reliability, say 50 percent of the time. Plots of the resulting data confirm that as time goes on, successive generations of an LLM can reliably complete longer and longer (more and more complex) tasks.</p><p>No surprise there. But the shock was that this improvement in the ability of LLMs to reliably complete harder tasks has been <em>exponential</em>, with a doubling period of about seven months.</p><p><em><a href="https://spectrum.ieee.org/" target="_blank">IEEE Spectrum</a></em> reached out to <a href="https://metr.org/team/megan-kinniment/" target="_blank">Megan Kinniment</a>, one of the authors of <a href="https://arxiv.org/abs/2503.14499" target="_blank">an METR research paper</a> describing this work and its surprising implications.</p><h2>Evaluating LLM Performance Metrics</h2><p><strong>Did you suspect that you’d get these results?</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Megan Kinniment:</strong> I, at least personally, didn’t expect us to have quite as clear an exponential as we did. Models have definitely been getting better quickly, though. So some fast rate of progress wasn’t entirely unexpected.</p><p><strong><a rel="noopener noreferrer" target="_blank"></a>As you point out in the paper, it’s always dangerous to look into the future and extrapolate. However, you suggest that there is a likelihood of this continuing, which means that by 2030 we’ll be looking at monthlong tasks being within the capability of the most advanced large language models.</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> Let’s have a look at that. By one month, we mean around 167 working hours, so the number of [human] working hours in a month. <a rel="noopener noreferrer" target="_blank"></a>And that’s at 50 percent reliability. But longer tasks typically seem to require higher reliability to actually be useful. So that’s something that could make the in-practice, real-world, economic impacts not be as intense as what is predicted.</p><p><strong><a rel="noopener noreferrer" target="_blank"></a>There are a number of things that would have to continue for this prediction to come true. Hardware would have to continue improving at roughly the rate it’s improving; software would have to keep improving. You would have to have sufficient training data and availability of that training data to continue training at the breathtaking clip that’s been occurring in recent years.</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> The forecasts and the dates that we’ve found are just extrapolating the trend that we see on our task suite. [The trends are] not taking into account real-world factors or compute-scaling changes. </p><p><strong>If a large language model could somehow achieve the ability to complete 167-hour type tasks with 50 percent reliability, what are the kinds of things that that now puts in the realm of capability for a large language model?</strong><a rel="noopener noreferrer" target="_blank"></a></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> Well, the big one that we often think about is accelerating AI R&D research itself. To the extent that you can make models that accelerate your company’s ability to make better models, you could end up in a situation where AI capabilities develop really quite rapidly.  </p><h2>What Exponential Growth in AI Means for Humanity</h2><p><strong>What you are describing is reminiscent of the idea of <a href="https://spectrum.ieee.org/special-reports/singularity/" target="_blank">the singularity</a>, where you have AIs creating other AIs on their own, not assisted by human beings.</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> I think that you could get acceleration that is quite intense and does make things meaningfully more difficult to control without it necessarily resulting in this massively explosive growth. There are reasons to think that you might have various bottlenecks that slow things down in practice. Even if it were the case that we had very, very clever AIs, this pace of progress could still end up bottlenecked on things like hardware and robotics. But yeah, the singularity is for sure an idea that is relevant to this whole sector of things.<a rel="noopener noreferrer" target="_blank"></a></p><p>Things could go quite quickly, but it’s not like it’s the singularity or nothing. [AI-development rates] that were mild compared to a singularity could still be quite intense for how the world needs to adapt.</p><p><a rel="noopener noreferrer" target="_blank"></a><strong>You indicated in the paper that some large language models seem to be improving in their ability to adapt and improve from mistakes.</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> I think it’s actually been a relatively gradual thing since ChatGPT, and potentially before that. They’re less likely to get stuck. They’re a bit better at changing strategies when things aren’t working, but that’s a bit hit or miss. And they’re definitely a lot better at doing things than they used to be and better at using tools. But it does seem like there’s some fundamental aspects that haven’t changed a great deal. One thing that I like to look at when I get a new model is, on each task, we give the model a number of <a href="https://seantrott.substack.com/p/tokenization-in-large-language-models" target="_blank">tokens</a>, a number of words that it can say. And if you could imagine giving them more and more time or more and more tokens to do a task, how does that affect how likely they are to succeed? And basically, what we see is they plateau quite strongly. There’s a point at which you give them more tokens and it doesn’t really help. And for each new model, that plateau gets a bit higher.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" rel="float: left;" style="float: left;"> <img alt="A woman with brown hair who is wearing a maroon t-shirt." class="rm-shortcode" data-rm-shortcode-id="cdee9d9a8b54b15ac78c5ee650069e9d" data-rm-shortcode-name="rebelmouse-image" id="4ec99" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-with-brown-hair-who-is-wearing-a-maroon-t-shirt.jpg?id=61135475&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Megan Kinniment was on the team at METR that published the results of a study of LLM performance.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Megan Kinniment</small></p><p><a target="_blank"></a> Humans, I imagine, also have diminishing returns. But if you give a human lots and lots of time to do something, they’ll probably do a better job, especially if you have multiple humans. And I think I’d be pretty impressed with a large language model that, even if its absolute score was lower, seemed like it could just keep doing things and improving. That could be a big deal.</p><p><a target="_blank"></a><strong>You found that models performed worse on tasks that had higher “messiness” scores. Was there any signal that you got out of the data that this state of affairs might be changing? In other words, that models might be gaining greater ability to handle tasks that had higher messiness?</strong></p><p><a target="_blank"></a><strong>Kinniment:</strong> Messiness was a measure that I made to try and get a somewhat quantitative measure of how unrealistic our tasks were compared to the real world. And most of our tasks aren’t that messy. It’s a 16-point scale. The mean is about 3, and the most messy tasks are about 8 out of 16.</p><p><strong><a rel="noopener noreferrer" target="_blank"></a>So what would a 16 task be in terms of messiness?</strong></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> Something like espionage, where you have a lot of resource limitations. It’s very punishing. You have agents that are optimizing against you actively. It’s easy to mess up. It’s novel.</p><p><strong>Are you all planning to follow up this study?</strong><a rel="noopener noreferrer" target="_blank"></a></p><p><a rel="noopener noreferrer" target="_blank"></a><strong>Kinniment:</strong> OpenAI published <a href="https://openai.com/index/introducing-o3-and-o4-mini/" target="_blank">o3</a>, and o3 was a little bit more capable than anticipated given the trend. So we are doing some amount of follow-up in terms of measuring other models. We do want to keep focused on informing the world about AI development and catastrophic risks from AI systems.</p><h2>Catastrophic Risks from Advanced AI</h2><p><strong>What are the most likely catastrophic risks from AI? I mean, the ones that come to my mind are massive dislocations in employment if and when AI becomes supremely capable.</strong></p><p><strong></strong><strong>Kinniment:</strong><span> When we’re talking about catastrophic risks, we’re not just talking about mass unemployment. We’re talking about things that are more like this: if everybody became unemployed or you just didn’t need human workers for the vast majority of things, you might not need human workers to maintain your military, or much fewer humans. That could make it easier for somebody to perform a coup, essentially. Or, if you have a vast quantity of geniuses in a data center, then that would make you a very powerful person. If you use that to produce military hardware, it’s possible we could get a concentration of power, and you might not have a democratic state anymore.</span></p><p><strong>All this would happen, obviously, without any form of consciousness. These would be machines that would have the capability to scheme and plot and plan, but without the kind of consciousness that characterizes human ability to do this. Consciousness isn’t necessary for this.</strong></p><p><a target="_blank"></a><strong>Kinniment:</strong> <a href="https://spectrum.ieee.org/the-consciousness-conundrum" target="_blank">Consciousness is a hard problem</a>. I’m not sure if consciousness is necessary for any particular behavior. It feels a bit above my pay grade. I also think it’s not crazy that they could be conscious at this point. They would be very intelligent.</p><p><a target="_blank"></a><strong>So you think it’s possible that they may be conscious at some point in the future?</strong></p><p><a target="_blank"></a><strong>Kinniment:</strong> I mean, if they’re as intelligent as you and I, then it doesn’t seem quite crazy. It doesn’t seem crazy for them to not be, and it doesn’t seem crazy for them to be.</p>]]></description><pubDate>Wed, 02 Jul 2025 15:37:04 +0000</pubDate><guid>https://spectrum.ieee.org/llm-benchmarking-metr</guid><category>Exponential improvement</category><category>Large language models</category><category>The singularity</category><category>Llm benchmarking</category><dc:creator>Glenn Zorpette</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/spacey-scene-shows-a-network-of-orange-nodes-in-space-over-the-earth.jpg?id=61135467&amp;width=980"></media:content></item><item><title>Large Language Models Are Improving Exponentially</title><link>https://spectrum.ieee.org/large-language-model-performance</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/ai-success-rate-graph-from-2019-to-2030-for-tasks-by-model-version-and-time-completion.png?id=61137912&width=1200&height=800&coordinates=0%2C1302%2C0%2C1303"/><br/><br/><p>Benchmarking <a href="https://spectrum.ieee.org/tag/large-language-models/" target="_self">large language models</a> presents some unusual challenges. For one, the main purpose of many LLMs is to provide compelling text that’s indistinguishable from human writing. And success in that task may not correlate with metrics traditionally used to judge processor performance, such as instruction execution rate.</p><p class="ieee-inbody-related"><strong>RELATED: </strong><span><a href="https://spectrum.ieee.org/llm-benchmarking-metr" target="_blank">LLM Benchmarking Shows Capabilities Doubling Every 7 Months</a></span><br/></p><p>But there are solid reasons to persevere in attempting to gauge the performance of LLMs. Otherwise, it’s impossible to know quantitatively how much better LLMs are becoming over time—and to estimate when they might be capable of completing substantial and useful projects by themselves.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Scatter plot showing negative correlation between success rate and task-messiness score." class="rm-shortcode" data-rm-shortcode-id="fe9a8b369a52f92281e85f5f38683ee7" data-rm-shortcode-name="rebelmouse-image" id="ed777" loading="lazy" src="https://spectrum.ieee.org/media-library/scatter-plot-showing-negative-correlation-between-success-rate-and-task-messiness-score.png?id=61125420&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Large Language Models are more challenged by tasks that have a high “messiness” score.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Model Evaluation & Threat Research</small></p><p>That was a key motivation behind work at Model Evaluation & Threat Research <a href="https://metr.org/" target="_blank">(METR</a>). The organization, based in Berkeley, Calif., “researches, develops, and runs evaluations of frontier AI systems’ ability to complete complex tasks without human input.” In March, the group released a paper called <a href="https://arxiv.org/abs/2503.14499" target="_blank"><em>Measuring AI Ability to Complete Long Tasks</em></a>, which reached a startling conclusion: According to a metric it devised, the capabilities of key LLMs are doubling every seven months. This realization leads to a second conclusion, equally stunning: By 2030, the most advanced LLMs should be able to complete, with 50 percent reliability, a software-based task that takes humans <em><em>a full month</em></em> of 40-hour workweeks. And the LLMs would likely be able to do many of these tasks much more quickly than humans, taking only days, or even just hours.</p><h2>An LLM Might Write a Decent Novel by 2030</h2><p>Such tasks might include starting up a company, writing a novel, or greatly improving an existing LLM. The availability of LLMs with that kind of capability “would come with enormous stakes, both in terms of potential benefits and potential risks,” AI researcher Zach Stein-Perlman wrote in a <a href="https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks" target="_blank">blog post</a>.</p><p>At the heart of the METR work is a metric the researchers devised called “<a href="https://www.perplexity.ai/page/ai-task-capacity-doubles-every-F0hK0HIkQVKjH.88ZDYArg" target="_blank">task-completion time horizon.</a>” It’s the amount of time human programmers would take, on average, to do a task that an LLM can complete with some specified degree of reliability, such as 50 percent. A plot of this metric for some general-purpose LLMs going back several years [main illustration at top] shows clear exponential growth, with a doubling period of about seven months. The researchers also considered the “messiness” factor of the tasks, with “messy” tasks being those that more resembled ones in the “real world,” according to METR researcher <a href="https://metr.org/team/megan-kinniment/" target="_blank">Megan Kinniment</a>. Messier tasks were more challenging for LLMs [smaller chart, above].</p><p>If the idea of LLMs improving themselves strikes you as having a certain <a href="https://spectrum.ieee.org/special-reports/singularity/" target="_self">singularity</a>-<a href="https://spectrum.ieee.org/automation-jobs" target="_self">robocalypse</a> quality to it, Kinniment wouldn’t disagree with you. But she does add a caveat: “You could get acceleration that is quite intense and does make things meaningfully more difficult to control without it necessarily resulting in this massively explosive growth,” she says. It’s quite possible, she adds, that various factors could slow things down in practice. “Even if it were the case that we had very, very clever AIs, this pace of progress could still end up bottlenecked on things like hardware and <a href="https://spectrum.ieee.org/topic/robotics/" target="_self">robotics</a>.”</p>]]></description><pubDate>Wed, 02 Jul 2025 14:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/large-language-model-performance</guid><category>2030</category><category>Ai capabilities</category><category>Exponential growth</category><category>Large language models</category><category>Metr</category><category>Task-completion time</category><category>Type:departments</category><dc:creator>Glenn Zorpette</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/ai-success-rate-graph-from-2019-to-2030-for-tasks-by-model-version-and-time-completion.png?id=61137912&amp;width=980"></media:content></item><item><title>Cuba’s Power Grid Nears Total Failure</title><link>https://spectrum.ieee.org/cuba-energy-crisis</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/car-headlights-illuminate-a-residential-street-at-night-during-a-power-outage-in-cuba.jpg?id=61115396&width=1200&height=800&coordinates=0%2C20%2C0%2C20"/><br/><br/><p>For many Cubans, the sudden stop of a fan is more than just an annoyance on a tropical island; it’s a daily reminder of a critical, nationwide problem. On an average day, the Cuban government can meet only 50 to 70 percent of its country’s electricity needs. On top of that, Cuba’s entire grid has collapsed four times in the last six months.</p><p>The problem stems from years of neglect of Cuba’s energy infrastructure, exacerbated by constrained access to foreign capital and a failure to adapt to new energy options. As a result, Cubans are experiencing a significant breakdown in basic services, such as the storage of fresh products, basic food preparation, public lighting, and access to businesses. This has forced citizens to take extraordinary measures, like cooking multiple meals at once and working by flashlight.</p><p>Cuba isn’t just in an energy crisis; the country’s grid sits on the verge of systemic failure. The National Electric System, most of which was built after 1959, hasn’t received the investment and maintenance it needs for 35 years—a consequence of Cuba’s complex political and economic history. If neglect continues, the island nation will pay the high price of further economic decline, and increased social and political instability.</p><p>How did my home country get here?</p><h2>Cuba’s Energy Infrastructure Crisis</h2><p>Cuba’s grid infrastructure is so weak that <a href="https://spectrum.ieee.org/spain-grid-failure" target="_blank">run-of-the-mill problems</a> like transmission line failures and generator trips are causing widespread outages. The unexpected shutdown of the Antonio Guiteras oil-fired power plant started the total blackout in October 2024. <a href="https://spectrum.ieee.org/power-grid-failure-lights-on" target="_blank">Healthy grids</a> should be able to detect and isolate these kinds of issues, and provide backup through built-in redundancies. But Cuba’s old protection systems couldn’t detect the faults, and there weren’t enough spinning reserves to compensate for the generation instability, making recovery impossible.</p><p>Underlying the blackouts are three systemic problems: years of inadequate investment, substandard fuel, and deferred maintenance. Cuba’s aging thermal power plants—the backbone of the system, nearly all of which run on crude oil or fuel oil—are becoming less reliable, and must operate well below capacity because of fuel shortages and corrosion. One of the biggest, the 330-megawatt Antonio Guiteras plant in Matanzas, often breaks down because there aren’t enough replacement parts to repair it. Other facilities have been hit with adverse events, such the 2022 fires at the Lidio Ramón Pérez (Felton) and Máximo Gómez (Mariel) thermal plants. During the first five months of 2025, only 34 percent of the capacity of all of Cuba’s power plants, based on 2023 numbers, was available on an average daily basis.</p><p>Cuba’s energy system also suffers from years of reliance on domestic, poor-quality heavy crude oil, which is corrosive because it’s high in sulfur. This has accelerated the wear and tear on boilers, turbines, and pipes in Cuba’s power plants, shortening their life spans and causing frequent and costly outages. Cuba has secured a substantial amount of oil from Venezuela since 2000 through a favorable agreement. But Venezuela’s continuous economic problems have made this outside oil source less reliable, with shipments dwindling in recent years.</p><p>To help compensate for power deficits, Cuba in 2019 started renting floating thermal power plants from other countries such as Turkey’s <a href="https://karpowership.com/" target="_blank">Karpowership</a>. By 2023, eight of these kinds of ships were floating in Mariel Bay, Havana Bay, and Santiago de Cuba Bay. But their fate is now just another symptom of the crisis. The government had trouble paying the high leasing prices, so the ships’ operators withdrew from Cuba’s waters, taking hundreds of megawatts with them.</p><h2>Cuba’s Renewable Energy Options</h2><p><span>Heavy crude oil isn’t Cuba’s only resource; it has a wealth of untapped renewable energy options, including solar, wind, and potentially sugarcane biomass. But the transition to renewables has progressed slowly and <span data-redactor-="">somewhat haphazardly<span>, despite ambitious goals set by the government.</span></span></span></p><p>Crucial <a href="https://spectrum.ieee.org/puerto-rico-solar-microgrids" target="_blank">renewable energy projects</a> often get delayed due to bureaucratic hurdles and a lack of funding. The instability of the grid makes it more challenging to integrate large-scale renewable energy installations, as they require stable connections to function effectively. And while the country actively seeks solar energy, it’s overlooking its once-thriving sugar sector, and the biomass and ethanol resources that come with it.</p><p>The Cuban government’s puzzling decision to invest heavily in building expensive hotels and enhancing tourism infrastructure while neglecting necessary grid updates has also made the system more vulnerable. From 2010 and 2024, <a href="https://www.onei.gob.cu/sites/default/files/datos-estadisticos/2023-10/12.6-volumen-de-inversiones-por-clase-de-actividad-economica.xls" target="_blank">Cuba spent</a> about 32 percent of total investment on tourism-related infrastructure, and only 12 percent on energy infrastructure, according to Cuba’s National Statistics Office.</p><p>Cuba’s economy is in a poor state due to the well-established inefficiencies of its economic model and U.S. sanctions that worsened under the first Trump administration and remain largely in place. As a result, the government lacks the necessary hard currency to import gasoline, acquire the spare parts it needs, access the latest technology, and attract significant foreign investment that is essential for upgrading its energy infrastructure.</p><h2>Cuba’s Power Outages and Rationing Breeds Resentment</h2><p>The Cuban government reacts to its severe deficit with striking public ceremony. Every morning, usually between 7:00 and 8:00, officials openly declare the anticipated electricity generation shortfall. Then, provincial leaders across the island painstakingly choose which of their communities will lose power and for how long—on average 19 hours, and some over 24 hours.</p><h3></h3><br/><div class="flourish-embed flourish-chart" data-src="visualisation/23927522?607871"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23927522/thumbnail" width="100%"/></noscript></div><p class="caption">Daily electricity deficits in Cuba have averaged around 1600 MW in 2025, according to data compiled from daily press releases from Unión Eléctrica (UNE), the state-owned company responsible for Cuba’s electricity system, and news outlets. Data from Cuba’s four recent grid collapses were excluded.</p><p>Blackouts for these “interruptible circuits,” as provincial electrical companies refer to them, are supposed to rotate based on a schedule. But the actual electricity availability almost always falls short of the plan, so the rotation isn’t carried out as intended. And with certain services and businesses being prioritized for power, a disproportionate share of the outages falls on a portion of customers.</p><p>Havana, the capital, gets partly shielded from this, due to its political and economic importance. It’s a favoritism that understandably breeds resentment in other areas. But even in Havana, residents on interruptible circuits must contend with at least four hours of power outages every day—a situation that has worsened this year.</p><p>In both the city and the countryside, the power outages and prolonged uncertainty have altered basic services, economies, and daily life. Refrigerators lose their cooling power, ruining valuable food that was purchased at a high price. Businesses must close, resulting in lost revenue and productivity. Students struggle to study when their phone batteries are low, in part because they use their phones as flashlights. Some medical facilities have backup generators, but they’re not always operational due to a lack of spare parts. Internet connectivity has grown less reliable.</p><h2>Cuba’s Recovery Plans </h2><p>The Cuban administration has admitted that the situation is severe and has developed specific <a href="https://www.youtube.com/watch?v=7WgfqhsYrhw" target="_blank">recovery</a> <a href="https://www.youtube.com/watch?v=BUVV_RQpmz8&t=1755s" target="_blank">plans</a>. These include investing in thermal plant maintenance, adding new capacity, adding solar energy, and securing fuel supplies from abroad. However, progress is gradual and limited by the same problems that caused the crisis in the first place.</p><p>To fix the thermal units, it’s imperative to bring major electricity providers, such as Guiteras and Cienfuegos, back to a state where they can operate reliably again. However, this method is akin to patching up a dam that’s falling apart. Long-term improvements are more important than just repairs, but they require resources that aren’t currently available.</p><p>There are policies in place that encourage rooftop solar, which people commonly buy from other countries. Small solar parks are also being built; at the end of 2024 solar capacity reached 298 MW. Pilot projects for wind farms are underway.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A man uses a transformer from an old television to charge a battery outside his home in Cuba." class="rm-shortcode" data-rm-shortcode-id="a55715ae780e8ff82360b86258911ff5" data-rm-shortcode-name="rebelmouse-image" id="58560" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-uses-a-transformer-from-an-old-television-to-charge-a-battery-outside-his-home-in-cuba.jpg?id=61115449&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Angel Rodriguez uses a transformer from an old television to charge a battery in preparation for blackouts at his home in the Bahia neighborhood of Havana on 26 May 2025.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Ramon Espinosa/AP</small></p><p>But development lacks scale and speed. Large projects often struggle with financial constraints and inadequate planning. The government set a goal of deriving 37 percent of Cuba’s energy from renewable sources by 2030; so far they’ve reached only 3 percent.</p><p>Cuba is actively looking to partner internationally on energy initiatives. Agreements with Russia primarily focus on modernizing existing thermal facilities and possibly constructing new ones. Mexico and other allies have also helped by sending fuel supplies.</p><p>Talks with potential investors for green projects are moving forward, but the investment climate is quite tricky because of Cuba’s economic model and crisis, its history of defaults on payments to foreign companies, and U.S. sanctions. In partnership with China, Cuba is building up to 2,000 MW of solar capacity over more than 92 solar parks across the country. China already sent Cuba equipment for more than 100 MW of solar capacity through a different program. By January 2026, about 1,100 MW of this new capacity is expected to be operational, according to the Cuban government.</p><p>As the primary power grid fails, Cubans with resources are taking matters into their own hands, often in desperate ways. Businesses, hospitals, and wealthy families are installing gasoline and solar generators; the incessant noise from which has become part of the city’s soundscape. Some communities are working together to install solar and battery systems. For example, a <a href="https://www.fao.org/cuba/noticias/detail-events/ar/c/1737965/" target="_blank">farmers’ cooperative</a> in Artemisa powers its processing and irrigation facilities this way. These examples show a critical, bottom-up change, but they’re few and far between because of prohibitive upfront costs.</p><p>Cuba’s people are suffering because the energy system is being pushed too far. And the comprehensive changes required for genuine, long-term recovery are currently beyond the island’s capabilities. The lights came back on after each major blackout, but the specter of the next one looms constantly.</p>]]></description><pubDate>Tue, 01 Jul 2025 12:00:05 +0000</pubDate><guid>https://spectrum.ieee.org/cuba-energy-crisis</guid><category>Cuba</category><category>Grid failure</category><category>Blackouts</category><category>Energy crisis</category><dc:creator>Ricardo Torres</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/car-headlights-illuminate-a-residential-street-at-night-during-a-power-outage-in-cuba.jpg?id=61115396&amp;width=980"></media:content></item><item><title>Digital Signal Processing Pioneer Jim Boddie Remembered</title><link>https://spectrum.ieee.org/dsp-pioneer-jim-boddie</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-collage-of-jim-boddie-and-microprocessor-pieces.jpg?id=61125544&width=1200&height=800&coordinates=0%2C127%2C0%2C128"/><br/><br/><p><a href="https://www.eejournal.com/article/in-memoriam-james-boddie-dsp-pioneer/" rel="noopener noreferrer" target="_blank">James R. “Jim” Boddie</a>, a pioneer of the programmable, single-chip digital signal processor, died on 2 December at his home in Canton, Ga., following a long illness. The IEEE senior member was 74.</p><p>While working as an architect and designer at <a href="https://en.wikipedia.org/wiki/Bell_Labs" rel="noopener noreferrer" target="_blank">AT&T Bell Laboratories</a> in Holmdel, N.J., Boddie applied his expertise in signal processing algorithms to develop a new type of semiconductor: the DSP. The integrated circuit, which <a href="https://spectrum.ieee.org/bell-labs-100-birthday" target="_blank">Bell Labs</a> called DSP1, was announced at the 1980 <a href="https://www.isscc.org/" rel="noopener noreferrer" target="_blank">International Solid-State Circuits Conference</a> (ISSCC). DSP1 became one of the industry’s first successful DSPs.</p><p>Jim led the development of five subsequent DSP generations, economically enabling numerous applications, many for the first time, from AT&T’s gigantic telephone switching systems to tiny digital hearing aids.</p><p>For his contributions as a team leader as well as his technical innovations, Jim was elevated to Bell Labs Fellow. In 1988 he and IEEE Senior Member <a href="https://ethw.org/Richard_A._Pedersen" rel="noopener noreferrer" target="_blank">Richard A. Pedersen</a>, a codeveloper of the DSP, received the <a href="https://ethw.org/IEEE_Morris_N._Liebmann_Memorial_Award" rel="noopener noreferrer" target="_blank">IEEE Liebmann Award</a><strong>.</strong></p><h2>The first digital signal processor</h2><p>Jim was born in Tallassee, a small city in Alabama. He received a bachelor’s degree in electrical engineering in 1971 from <a href="https://www.auburn.edu/" rel="noopener noreferrer" target="_blank">Auburn University</a>, in Alabama. After earning a master’s degree in EE two years later from <a href="https://www.mit.edu/" rel="noopener noreferrer" target="_blank">MIT</a>, he returned to Auburn to pursue a Ph.D. In 1976 he received his doctorate and began a yearlong postdoctoral fellowship at Bell Labs’ acoustic research department in Holmdel.</p><p>There he programmed a refrigerator-size DSP system to do real-time de-reverberation of speech and eliminate echoes in conference-room speakerphones.</p><p>During his fellowship, he met IEEE Life Fellow <a href="https://www.nokia.com/bell-labs/about/history/innovation-stories/presidents-bell-labs/" rel="noopener noreferrer" target="_blank">Dan Stanzione</a>, a Bell Labs group supervisor who recruited him to join the company’s first DSP design team in 1977. Stanzione, who became the eighth president of Bell Labs, said in a 2015 interview with the<a href="https://www.computerhistory.org/collections/catalog/102740084" rel="noopener noreferrer" target="_blank"> Computer History Museum’s oral history project</a> that his first meeting with Jim was “a lucky day for me.” Jim arrived during the glory days of Bell Labs, when algorithm experts, semiconductor experts, and system designers worked for one company, more often than not down the hall from one another.</p><p>Prior to 1980, most signal processing applications were implemented with analog components. Advanced algorithms such as low-data-rate speech coding and speech recognition were validated in the digital domain using supercomputer simulations and were too complex to be implemented with analog designs. By contrast, digital designs used racks of components well beyond commercial feasibility. The dream of an inexpensive, high-performance digital signal processor seemed far off, as no single DSP architecture would work for all algorithms. Therefore <a href="https://www.ti.com/" rel="noopener noreferrer" target="_blank">Texas Instruments</a> and similar companies sold universal digital building blocks such as multipliers and register files.</p><p>Stanzione’s goal was to create a widely applicable DSP that could replace his department’s custom analog filter designs. In time, the team homed in on a specific filtering application: DTMF (dual-tone multifrequency) signaling, which decoded a phone’s dial tones into the dialed number to route calls. It would be deployed by new digital systems in sufficient volume to justify the cost of developing the DSP.</p><p>The team breadboarded the filters and found that with a clever architecture, and a 4.5-micron full-custom negative-channel metal-oxide semiconductor—the best process technology at the time—they could meet the performance requirement. Other related applications would be icing on the cake.</p><p>Jim made critical contributions to DSP1. Foremost among them was the design of its novel arithmetic unit.</p><p>After DSP1 was announced at the 1980 ISSCC, it was not commercially available outside AT&T. It achieved high-volume production in AT&T systems, though.</p><p>Jim was promoted to manager of the DSP architecture team in 1980 to set the direction for subsequent versions.</p><h2>A leader and entrepreneur</h2><p>DSP1 was soon followed by DSP2, with faster performance and a greater memory capacity to meet high-volume demand and to increase the range of applications where the technology could be used.</p><p>In parallel with the development of DSP2, Jim led a new architecture, DSP32, which used floating-point arithmetic. For digital signal processing, floating-point arithmetic simplifies the job of maintaining precision, a near-impossible burden in the most demanding algorithms.</p><p>Jim and his team announced the DSP32 in a 1985 ISSCC document, which received a Best Paper Award.</p><p>Afterward, his team returned to the drawing board and developed the DSP16, which used fixed-point arithmetic. Announced at the 1987 ISSCC, DSP16 became the world’s fastest DSP, a title it retained for several years.</p><p>In 1998 Jim left AT&T and helped found StarCore, a DSP design center in Atlanta. It was funded by <a href="https://en.wikipedia.org/wiki/Agere_Systems" rel="noopener noreferrer" target="_blank">Agere Systems</a>, which had acquired Bell Labs Microelectronics; <a href="https://www.google.com/aclk?sa=l&ai=DChsSEwjW142Y1eSNAxW7GK0GHXHhAhgYACICCAEQBxoCcHY&co=1&ase=2&gclid=Cj0KCQjwjJrCBhCXARIsAI5x66UKpEBsGUeO7S3vyqzd18xYqRpBXeNCt6hn0zdV44gKFOL_j4z6uLEaAg_qEALw_wcB&category=acrcp_v1_53&sig=AOD64_3MA0oIEJNrp0_TvcMxt12-vvx9-g&q&nis=4&adurl&ved=2ahUKEwjel4iY1eSNAxVWJzQIHf2cNtEQ0Qx6BAgMEAE" rel="noopener noreferrer" target="_blank">Motorola</a>; and, later, <a href="https://www.infineon.com/" rel="noopener noreferrer" target="_blank">Infineon</a>. The startup’s sponsors received exclusive rights to incorporate StarCore’s DSP designs into their own IC products. Jim served as StarCore’s executive director until his retirement in 2006.</p><p>StarCore’s architectures used a long instruction word architecture that encoded multiple, independent operations in a single instruction in order to exploit parallelism in DSP code.</p><p>In 2002, after StarCore’s first designs had been released, Will Strauss, president of market research firm Forward Concepts, said, “The combination of the new company’s business model and world-class technology offerings represent a dramatic change in the<a href="https://www.design-reuse.com/news/202504762-infineon-agere-systems-and-motorola-form-new-company-to-develop-and-license-digital-signal-processor-technology/" rel="noopener noreferrer" target="_blank"> industry landscape</a>.”</p><h2>Life after the DSP</h2><p>Jim was an early adopter of home computers, purchasing nearly every model that <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> sold. Along with a <a href="https://us.novationmusic.com/categories/midi-controllers/keys" rel="noopener noreferrer" target="_blank">MIDI keyboard</a>, he used the computers for music synthesis as well as flight-simulation games. He was an avid flier, not only of simulators but also of private aircraft.</p><p>He was a dedicated <a href="https://spectrum.ieee.org/quansheng-uv-k5-hacking" target="_self">ham radio</a> operator—call sign NG2J—and participated in contests. Field Day was his favorite. The annual weekend event in June challenges ham radio enthusiasts worldwide to operate in remote locations to simulate emergency conditions and exchange simulated emergency messages.</p><p>After retiring, Jim programmed a <a href="https://www.thealamo.org/alamo-trust/pressroom/the-alamo-announces-first-ever-3d-virtual-tour" rel="noopener noreferrer" target="_blank">3D virtual tour of the Alamo</a>, a historic site in San Antonio. The program lets users view the former Spanish mission from any direction in both the present and the past. The tour, which launched in 2022, is available at the <a href="https://www.thealamo.org/" rel="noopener noreferrer" target="_blank">Alamo</a> and online.</p><p>Today digital signal processing is pervasive. Support for its algorithms and applications are a fundamental requirement for virtually all programmable semiconductors. Jim’s contributions to DSP technology will be remembered. Equally so, we and hundreds of others will remember Jim for his kind mentorship and clear vision.</p>]]></description><pubDate>Mon, 30 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/dsp-pioneer-jim-boddie</guid><category>Bell labs</category><category>Digital signal processing</category><category>Ieee member news</category><category>In memoriam</category><category>Obituary</category><category>Semiconductors</category><category>Type:ti</category><dc:creator>Pat Hays</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/photo-collage-of-jim-boddie-and-microprocessor-pieces.jpg?id=61125544&amp;width=980"></media:content></item><item><title>A CarFax for Used PCs</title><link>https://spectrum.ieee.org/carmax-used-pcs</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/2013-hp-laptop-on-a-green-background.jpg?id=61114338&width=1200&height=800&coordinates=137%2C0%2C137%2C0"/><br/><br/><p><span>The United Nations’ Global</span><a href="https://ewastemonitor.info/the-global-e-waste-monitor-2024/" target="_blank"> E-waste Monitor</a><span> estimates that the world generates over 60 million tonnes of </span><a href="https://spectrum.ieee.org/the-cybersecurity-of-e-waste" target="_self">e-waste</a><span> annually. Furthermore, this number is rising five times as fast as e-waste recycling. Much of this waste comes from prematurely discarded electronic devices.</span></p><p>Many enterprises follow a standard three-year replacement cycle, assuming older computers are inefficient. However, many of these devices are still functional and could perform well with minor upgrades or maintenance. The issue is, no one knows what the weak points are for a particular machine, or what the needed maintenance is, and the diagnostics would be too costly and time-consuming. It’s easier to just buy brand new <a data-linked-post="2670648506" href="https://spectrum.ieee.org/risc-v-laptops" target="_blank">laptops</a>.</p><p>When buying a used car, dealerships and individual buyers can access each car’s particular <a href="https://www.carfax.com/" target="_blank">CarFax</a> report, detailing the vehicle’s usage and maintenance history. Armed with this information, dealerships can perform the necessary fixes or upgrades before reselling the car. And individuals can decide whether to trust that vehicle’s performance. We at <a href="https://www.hp.com/us-en/home.html" target="_blank">HP</a> realized that, to prevent unnecessary e-waste, we need to collect and make available usage and maintenance data for each laptop, like a CarFax for used PCs.</p><p>There is a particular challenge to collecting usage data for a PC, however. We need to make sure to protect the user’s privacy and security. So, we set out to design a data-collection protocol for PCs that manages to remain secure.</p><h2>The firmware-level data collector</h2><p> Luckily, the sensors that can collect the necessary data are already installed in each PC. There are thermal sensors that monitor CPU temperature, power-consumption monitors that track energy efficiency, storage health indicators that assess solid state drive (SSD) wear levels, performance counters that measure system utilization, fan-rotation-speed sensors that detect cooling efficiency, and more. The key is to collect and store all that data in a secure yet useful way.</p><p> We decided that the best way to do this is to integrate the life-cycle records into the firmware layer. By embedding telemetry capabilities directly within the firmware, we ensure that device health and usage data is captured the moment it is collected. This data is stored securely on HP SSD drives, leveraging hardware-based security measures to protect against unauthorized access or manipulation. </p><p>The secure telemetry protocol we’ve developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we use is the Endpoint Security Controller, a dedicated piece of hardware embedded in business-class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Flowchart illustrating secure sensor data collection at the firmware level." class="rm-shortcode" data-rm-shortcode-id="4a49473576ee96afdb040a577a8f44cc" data-rm-shortcode-name="rebelmouse-image" id="cc78c" loading="lazy" src="https://spectrum.ieee.org/media-library/flowchart-illustrating-secure-sensor-data-collection-at-the-firmware-level.jpg?id=61114393&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The secure telemetry protocol collects data from sensors into a piece of hardware known as an endpoint security controller, with built-in security protections. The endpoint security controller then writes the data to a dedicated read-only portion of the solid state drive, where authorized operating system applications can access the data.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Mark Montgomery</small></p><p>The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD—where telemetry and sensitive data are stored—by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered.</p><p>Then, the collected data is recorded in a time-stamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs.</p><p>The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.</p><h2>The PCFax</h2><p>The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information.</p><p>The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP’s factory and supply-chain records, digital-services platforms, customer-support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner databases, third-party component manufacturers like Intel, and other original equipment manufacturers. This multisource approach creates a complete picture of the device’s entire life cycle, from manufacturing through all subsequent ownership and service events.<strong><br/></strong></p><p>For IT teams within organizations, we hope the PCFax will bring simplicity and give opportunities for optimization. Having access to fine-grained usage and health information for each device in their fleet can help IT managers decide which devices are sent to which users, as well as when maintenance is scheduled. This data can also help device managers decide which specific devices to replace rather than issuing new computers automatically, enhancing sustainability. And this can help with security: With real-time monitoring and firmware-level protection, IT teams can mitigate risks and respond swiftly to emerging threats. All of this can facilitate more efficient use of PC resources, cutting down on unnecessary waste.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Detailed report on a used 2020 HP Elitebook laptop. Insights include owner history, hardware update records, factory installed components and total power consumption to-date." class="rm-shortcode" data-rm-shortcode-id="3943d11913477b60592a97ed57b8901b" data-rm-shortcode-name="rebelmouse-image" id="0d16e" loading="lazy" src="https://spectrum.ieee.org/media-library/detailed-report-on-a-used-2020-hp-elitebook-laptop-insights-include-owner-history-hardware-update-records-factory-installed-c.jpg?id=61115489&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">A PCFax report, much like a CarFax, will detail crucial usage and maintenance information to help inform potential customers. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Hewlett Packard</small></p><p>We also hope that, much as the CarFax gives people confidence in buying used cars, the PCFax can encourage resale of used PCs. For enterprises and consumers purchasing second-life PCs, it provides detailed visibility into the complete service and support history of each system, including any repairs, upgrades, or performance issues encountered during its initial deployment. By making this comprehensive device history readily available, PCFax enables more PCs to find productive second lives rather than being prematurely discarded, directly addressing the e-waste challenge while providing economic benefits to both sellers and buyers in the secondary PC market.</p><p>While HP’s solutions represent a significant step forward, challenges remain. Standardizing telemetry frameworks across diverse ecosystems is critical for broader adoption. Additionally, educating organizations about the benefits of life-cycle records will be essential to driving uptake. </p><p>We are also working on integrating AI into our dashboards. We hope to use AI models to analyze historical telemetry data and predict failures before they happen, such as detecting increasing SSD write cycles to forecast impending failure and alert IT teams for proactive replacement, or predicting battery degradation and automatically generating a service ticket to ensure a replacement battery is ready before failure, minimizing downtime.</p><p>We plan to start rolling out these features at the beginning of 2026.</p>]]></description><pubDate>Mon, 30 Jun 2025 16:02:07 +0000</pubDate><guid>https://spectrum.ieee.org/carmax-used-pcs</guid><category>E-waste</category><category>Electronic devices</category><category>Power consumption</category><dc:creator>Sal Vasi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/2013-hp-laptop-on-a-green-background.jpg?id=61114338&amp;width=980"></media:content></item><item><title>Next-Gen Brain Implants Offer New Hope for Depression</title><link>https://spectrum.ieee.org/deep-brain-stimulation-depression</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/blue-and-gold-fibrous-texture-in-the-shape-of-a-brain-against-a-dark-background.png?id=61109682&width=1200&height=800&coordinates=300%2C0%2C300%2C0"/><br/><br/><p><strong>Her relapse into depression</strong> felt like defeat—but it offered vital clues to achieving lasting psychiatric relief.</p><p>The 67-year-old woman from Alabama had already endured four major depressive episodes in her decades-long battle with mental illness. After exhausting numerous medications and other therapies, in 2015 she turned to an experimental last resort: <a href="https://spectrum.ieee.org/how-to-treat-psychiatric-disorders-with-brain-stimulation" target="_blank">deep brain stimulation</a>, or DBS.</p><p>Neurosurgeons implanted electrodes a few inches below her skull, targeting a small bundle of neural fibers in a brain region behind the forehead that acts as a crucial hub for mood regulation. Thin wires connected the electrodes to a pulse generator discreetly inserted in her upper chest. Once activated, the device delivered a steady stream of high-frequency electricity, gently buzzing the targeted circuits to disrupt maladaptive patterns and, like a pacemaker for the brain, restore a healthier balance of neural activity.</p><p>At first, the treatment seemed to be working. The woman’s despair lifted, and she edged closer to remission. Watching football for hours with her husband on Sundays started to feel tedious—in a good way. Her desire to get off the couch and pursue other activities had returned.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="An X-ray image of a skull shows several lines slanting inward." class="rm-shortcode" data-rm-shortcode-id="7130143a2885964d3c641c38831f65b7" data-rm-shortcode-name="rebelmouse-image" id="b47c3" loading="lazy" src="https://spectrum.ieee.org/media-library/an-x-ray-image-of-a-skull-shows-several-lines-slanting-inward.png?id=61111164&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">An X-ray image shows two pairs of DBS electrodes implanted for depression treatment. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Department of Neurosurgery, Baylor College of Medicine </small></p><p>But four months on, the darkness crept back in. The woman’s sudden downturn blindsided the medical team that had been closely monitoring her recovery. The doctors had to make three adjustments to the implant’s stimulation parameters, slowly increasing the voltage, before her condition finally stabilized—an agonizing couple of months.</p><p>When the clinicians reviewed the data later, they realized that the electrodes embedded in the woman’s brain had detected trouble brewing before she did. Subtle shifts in the electrical patterns coursing through her neural fibers had flagged the impending relapse weeks before her outward symptoms reappeared. If clinicians had acted on those signals, they might have adjusted the stimulation settings in time to prevent her relapse.</p><p>It’s a thought that weighs on <a href="https://med.emory.edu/directory/profile/?u=PRIVAPO" target="_blank">Patricio Riva Posse</a>, the psychiatrist at Emory University School of Medicine, in Atlanta, who treated the woman. Looking back now, he says, had he known that the brain’s circuits were off-kilter, “I would have taken action earlier.”</p><p>Fortunately, Riva Posse no longer has to dwell on what could have been. Together with colleagues at the Icahn School of Medicine at Mount Sinai, in New York City, and Georgia Tech, in Atlanta, he is now leveraging advances in DBS hardware and artificial intelligence (AI) to design more precise treatments for depression. The team’s goal is to base treatment on objective neural data rather than the subjective measures—patient accounts, clinical hunches, questionnaires, mood scales—that dominate psychiatry today.</p><p>The pioneering neurologist <a href="https://profiles.mountsinai.org/helen-s-mayberg" target="_blank">Helen S. Mayberg</a> co-led the team with Riva Posse and <a href="https://ece.gatech.edu/directory/christopher-john-rozell" target="_blank">Christopher Rozell</a> of Georgia Tech. Ultimately, they hope to enable preemptive interventions rather than regretful, after-the-fact adjustments.</p><p>It’s a new frontier for psychiatry. The field has long been one of the few medical disciplines without objective measures to guide treatment decisions. But with the advent of real-time brain monitoring with AI-driven analytics, that could finally change. “It’s a whole different mindset now,” says <a href="https://profiles.mountsinai.org/martijn-figee" target="_blank">Martijn Figee</a>, a Mount Sinai psychiatrist involved in the research. “My intuition, unfortunately, is not 100 percent [accurate],” he acknowledges. “So ultimately, I would always trust the brain more.”</p><p class="pull-quote">Researchers are developing “an automatic alarm system”—an AI-driven tool designed to continuously monitor device output and flag warning signs of relapse.</p><p>Other research groups are pursuing similar goals, aiming to move beyond the one-size-fits-all approach that has long defined DBS treatment for mental health and replace it with precise stimulation tailored to individual needs. While standardized protocols benefit <a href="https://www.sciencedirect.com/science/article/pii/S1878747923010784" target="_blank">around 60 percent</a> of people with treatment-resistant depression, they still leave a substantial minority without meaningful relief.</p><p>No DBS platform is yet approved for treating depression, although some first-generation devices are getting close. Those are rooted in decades-old technology, however, while the Mount Sinai team and others are breaking new ground. They are investigating analytical frameworks that harness brain data to predict relapses, optimize stimulation parameters, or dynamically adjust device output in a responsive, closed-loop manner.</p><p>“The field is just at a super exciting place,” says <a href="https://can-acn.org/2020-brain-star-award-winner-dr-benjamin-davidson/" target="_blank">Benjamin Davidson</a>, a neurosurgeon at the Sunnybrook Health Sciences Centre, in Toronto. “Things are starting to move at a kind of dizzying pace.”</p><h2>The Origins of DBS for Depression</h2><p>That momentum is a relatively recent phenomenon in a field that, for the past two decades, has progressed through baby steps. Beset by commercial and clinical setbacks, little has changed over the years aside from the adoption of newer surgical techniques. The biggest advance was an imaging-guided surgical approach called tractography that allows for <a href="https://www.tandfonline.com/doi/full/10.1080/14737175.2023.2289573" target="_blank">greater precision in electrode placement</a>, informed by connectivity patterns between bundles of brain fibers rather than anatomical landmarks alone.</p><p>“The story is one of iteration to optimize and refine the targeting using new neuroscience tools,” says Mayberg, who launched the <a href="https://www.cell.com/neuron/fulltext/S0896-6273(05)00156-X" target="_blank">world’s first DBS trial for treatment-resistant depression</a> in the early 2000s at the University of Toronto. “The procedure, as envisioned and published in 2005, is, in essence, what we continue to do today,” she says.</p><div class="ieee-sidebar-medium"><h3>Standard method</h3><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="An illustration shows a person with a device in their chest and a wire leading to a brain implant, with directional arrows going to the implant." class="rm-shortcode" data-rm-shortcode-id="89598ad135cdd41beeff1f43f1c52fdd" data-rm-shortcode-name="rebelmouse-image" id="1445e" loading="lazy" src="https://spectrum.ieee.org/media-library/an-illustration-shows-a-person-with-a-device-in-their-chest-and-a-wire-leading-to-a-brain-implant-with-directional-arrows-going.png?id=61111241&width=980"/><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Images: Chris Philpot</small></p><p>The standard method of deep brain stimulation (DBS) for depression takes a “set-it-and-forget-it” approach. Doctors set the stimulation parameters during initial visits and adjust them based on patients’ reports on their moods.</p><div class="horizontal-rule"></div><h3><br/>Sensing system </h3><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="An illustration shows a person with a device in their chest and a wire leading to a brain implant, with directional arrows going to and from the implant." class="rm-shortcode" data-rm-shortcode-id="2dd3935b4f26c9f54670cb6fab577e35" data-rm-shortcode-name="rebelmouse-image" id="5dffe" loading="lazy" src="https://spectrum.ieee.org/media-library/an-illustration-shows-a-person-with-a-device-in-their-chest-and-a-wire-leading-to-a-brain-implant-with-directional-arrows-going.png?id=61111245&width=980"/></p><p>Researchers are also experimenting with new devices that can both stimulate and record signals from the brain. Doctors can then look at patients’ neural activity and adjust theparameters accordingly, sometimes catching signs of a relapse before a patient is aware of them.</p><div class="horizontal-rule"></div><h3><br/>Auto response </h3><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Diagram showing a brain implant with directional flow arrows to and from a certain brain region." class="rm-shortcode" data-rm-shortcode-id="fcec7057e1b43bbfa60b4e060e024d61" data-rm-shortcode-name="rebelmouse-image" id="995a6" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-showing-a-brain-implant-with-directional-flow-arrows-to-and-from-a-certain-brain-region.png?id=61111252&width=980"/></p><p>Taking the idea of adaptive treatment a step farther, one clinical team is testing a DBS device that records signals from the brain and adjusts the settings automatically. This closed-loop system can respond in real time to fluctuations of mood.</p></div><p>DBS is primarily used to manage movement disorders such as essential tremor and <a href="https://spectrum.ieee.org/parkinsons-disease-pen" target="_blank">Parkinson’s disease</a>. For those ailments, it’s an established and approved therapy that can drastically reduce symptoms such as shaking and muscle rigidity.</p><p>But Mayberg was inspired by the discovery of a brain region called the subgenual cingulate (SGC), which plays a <a href="https://psychiatryonline.org/doi/10.1176/ajp.156.5.675" target="_blank">key role in acute sadness</a> and the <a href="https://www.biologicalpsychiatryjournal.com/article/S0006-3223(00)01036-2/abstract" target="_blank">effects of antidepressant treatments</a>. She theorized that stimulating this area might alleviate severe, treatment-resistant depression. Her patients were people who had typically tried several types of antidepressant medications and more drastic measures, like electroconvulsive therapy, without finding any relief.</p><p>While the treatment didn’t work for everyone, many did feel better. Six months after surgery, 12 of the 20-person cohort experienced a profound lifting of their depressive symptoms, with 7 going into full remission. The effect was lasting, with many of those individuals continuing to report benefits to this day, according to <a href="https://surgery.utoronto.ca/faculty-focus-andres-lozano" target="_blank">Andres Lozano</a>, the University of Toronto neurosurgeon who performed the operations.</p><p>Mayberg’s hypothesis, it would seem, had proved correct.</p><h2>Learning from DBS Failures</h2><p>Yet, for all its early potential, DBS never gained traction as a mainstream psychiatric treatment. It is occasionally used today for people with debilitating obsessive-compulsive disorder, but the technique remains unapproved for depression and is largely confined to research trials—some of which have ended in dispiriting, <a href="https://linkinghub.elsevier.com/retrieve/pii/S0006-3223(14)00968-8" target="_blank">high-profile failure</a>.</p><p>One of the most notable setbacks occurred in 2013. The device company St. Jude Medical set out to replicate the findings of Mayberg’s study in a randomized trial, with plans to enlist 200 participants. But the <a href="https://www.theatlantic.com/science/archive/2018/04/zapping-peoples-brains-didnt-cure-their-depression-until-it-did/558032/" target="_blank">study was halted prematurely</a> after only 90 patients had been enrolled. An interim analysis had found the therapy was no more effective than sham stimulation.</p><p>It was a crushing blow to the field. Mayberg and others struggled to continue their research, as funding agencies and the scientific community at large grew increasingly skeptical about the viability of DBS for depression.</p><p>With the benefit of hindsight, however, many researchers now believe that the St. Jude failure owed more to the study’s design flaws than to any inherent shortcomings of DBS itself. A longer-term follow-up of participants indicated that the treatment’s antidepressant effect steadily strengthened. The trial may simply have measured responses on the wrong timeline. Plus, the neurosurgical placement of the DBS electrodes relied on an outdated understanding of brain connectivity, leading to suboptimal positioning. This may have delayed the therapeutic response past the initial 6- to 12-month assessment window.</p><p>These missteps likely undermined the study’s results, <a href="https://www.thelancet.com/journals/lanpsy/article/PIIS2215-0366(17)30371-1/abstract" target="_blank">the trial investigators later concluded</a>. But with the right trial design, most experts anticipate that future studies will succeed. “That could make a huge difference,” says <a href="https://researchers.mgh.harvard.edu/profile/1711504/Darin-Dougherty" target="_blank">Darin Dougherty</a>, a psychiatrist at Massachusetts General Hospital, in Boston. “Hopefully those lessons learned will be enough to get it over the top.”</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A woman sits in a chair while another woman holds a black device to the first woman's head." class="rm-shortcode" data-rm-shortcode-id="428ecaafae1779054aa7e9b114cad792" data-rm-shortcode-name="rebelmouse-image" id="15d04" loading="lazy" src="https://spectrum.ieee.org/media-library/a-woman-sits-in-a-chair-while-another-woman-holds-a-black-device-to-the-first-woman-s-head.png?id=61111265&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">A patient identified as Sarah participates in a trial at UC San Francisco of the first fully closed-loop DBS system for depression. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Maurice Ramirez </small></p><p>The biomedical company Abbott (which acquired St. Jude in 2017) is now conducting a do-over study at 22 sites across the United States; Dougherty, Figee, Riva Posse, and other leaders in the field are involved in the effort. The <a href="https://clinicaltrials.gov/study/NCT06423430" target="_blank">100-person trial</a>, launched in September 2024, could finally lead to regulatory approval and wider-scale adoption of DBS as a treatment strategy for depression.</p><p>But Abbott’s study takes a “set-it-and-forget-it” approach, in which stimulation parameters are programmed during initial visits and remain largely unchanged over time. The settings are generally standardized across patients, with a common pulse width and frequency fixed at around 90 microseconds and 130 hertz, respectively. Only the amplitude of stimulation, measured in volts, is typically adjusted to accommodate individual tolerances or symptom severity.</p><p>While this treatment approach is simple and scalable, it lacks the adaptability to respond to the dynamic nature of depression and its varying symptoms from one individual to the next. This limitation stems in part from a technological shortcoming of the Abbott platform: It can deliver precisely tuned electricity, but it lacks the ability to sense and record neural activity. Without this feedback mechanism, the device cannot detect shifts in brain states that might signal a relapse or a need for parameter adjustments, leaving clinicians reliant on patients’ reports.</p><p>In contrast, newer DBS devices for epilepsy and movement disorders can both stimulate and record signals. Medtronic’s <a href="https://europe.medtronic.com/xd-en/healthcare-professionals/products/neurological/deep-brain-stimulation-systems/percept-pc.html" target="_blank">Percept</a> system and NeuroPace’s <a href="https://neuropace.com/providers/rns-system-neuromodulation/" target="_blank">Responsive Neurostimulator</a>, for example, offer real-time feedback capabilities, which could allow for more adaptive therapies. Researchers want to bring that flexibility to DBS for depression.</p><h2>How Responsive DBS for Depression Works</h2><p>Consider again the example of Riva Posse’s 67-year-old patient. As <a href="https://www.nature.com/articles/s41586-023-06541-3" target="_blank">described in <em><em>Nature</em></em> two years ago</a>, this woman received a research-grade version of the Percept platform that detected signs of neural instability five weeks before her clinical symptoms reappeared.</p><p>“Before the patient knew anything was wrong—before there was even a hint of behavior that could seem symptomatic of a relapse—the brain signal was headed in the wrong direction,” says Rozell, the neuroengineer at Georgia Tech who developed the AI model used to interpret the woman’s brain activity patterns.</p><p>Rozell’s model combined a neural network classification scheme (for analyzing brain signals) with a generative causal explainer (for identifying key activity patterns). His work uncovered a distinct biomarker that reliably differentiated between states of depression relapse and recovery. Intriguingly, the <a href="https://www.nature.com/articles/s41398-024-02816-z" target="_blank">biomarker also reflected changes in sleep quality</a>, a telling early indicator since poor sleep patterns often precede the return of depression symptoms.</p><p class="pull-quote">Depression can take many forms: Some people experience it as emotional despondency, while others struggle with obsessive thoughts or a loss of pleasure.<strong></strong></p><p><span></span>But the insights provided by Rozell’s model came too late to help the patient in the moment—they were validated only after her relapse had occurred. To address this limitation, the researchers are now refining the approach for real-time use, aiming to develop what Mayberg calls “an automatic alarm system”—an AI-driven tool designed to continuously monitor device output and flag warning signs of relapse.</p><p>Such a system could prompt clinicians to intervene before these brain signals escalate into a full-blown depressive episode. Simultaneously, it could filter out false alerts from patients, providing reassurance to users who might otherwise interpret normal stress or anxiety as signs of an impending relapse. Informed by this neurofeedback, psychiatrists might then choose to fine-tune stimulation settings. Or they might proactively recommend additional support, such as psychotherapy or medication adjustments.</p><h2>Closing the Loop for DBS</h2><p>Going one step further, researchers from the University of California, San Francisco, are exploring a fully closed-loop DBS system for depression that removes some of the need for human decision-making. Their approach empowers the device itself to automatically adjust stimulation parameters in real time based on brain activity.</p><p>Reporting on their first patient—a woman in her 30s named Sarah, who withheld her last name for privacy—the UC San Francisco team <a href="https://www.nature.com/articles/s41591-021-01480-w" target="_blank">documented transformative improvements</a> in her mood, emotional balance, everyday functioning, and overall outlook on life, all in the first week after the implant was switched on.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A person gardening in a vibrant community garden under sunny skies; colorful flowers in bloom." class="rm-shortcode" data-rm-shortcode-id="60251c4047293adfbb79d87b8e0d0723" data-rm-shortcode-name="rebelmouse-image" id="b56fb" loading="lazy" src="https://spectrum.ieee.org/media-library/a-person-gardening-in-a-vibrant-community-garden-under-sunny-skies-colorful-flowers-in-bloom.png?id=61111270&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Sarah reports that the closed-loop DBS system restored pleasure and purpose to her life. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">John Lok</small></p><p>“My life took an immediate upward turn,” Sarah said at a 2021 press conference announcing the study’s early findings. “Hobbies I used to distract myself from suicidal thoughts suddenly became pleasurable again. I was able to make small decisions about what to eat without becoming stuck in a morass of indecision for hours,” she said, adding, “the device has kept my depression at bay, allowing me to return to my best self and rebuild a life worth living.”</p><p>According to <a href="https://profiles.ucsf.edu/andrew.krystal" target="_blank">Andrew Krystal</a>, the UC San Francisco psychiatrist leading the effort, similar benefits have since been seen in at least two other recipients of the closed-loop DBS device.</p><p>In each case, patients first undergo an intensive 10-day exploration of their typical neural activity, with 10 electrodes—targeting five locations on each side of the brain—temporarily implanted. During this period, researchers administer a battery of tests to identify the most effective sites for both stimulation and sensing. Once the optimal locations are determined, a second surgery is performed to implant the permanent DBS system, now simplified to just two electrodes: one dedicated to delivering stimulation and the other to recording neural activity.</p><p>When the recording electrode detects brain activity associated with depression—an event that can happen hundreds of times per day—it prompts the other electrode to deliver a brief burst of electricity lasting a few seconds. This approach stands out not only because it operates automatically in response to real-time brain activity, but also because it employs intermittent, on-demand stimulation rather than the continuous stimulation more commonly employed in DBS for psychiatric conditions.</p><p>This adaptive and dynamic feedback strategy may be especially well suited to addressing the day-to-day fluctuations in mood and emotional strain that can make depression so hard to live with, notes <a href="https://www.med.upenn.edu/apps/faculty/index.php/g275/p9682519" target="_blank">Katherine Scangos</a>, a psychiatrist who participated in the study. Patients have told her that receiving stimulation at key moments—like during a stressful interaction at the checkout line of a grocery store—helped prevent them from spiraling into distress. “They could really tell that they were getting the stimulation when they needed it most,” says Scangos, who joined the staff of the University of Pennsylvania last year.</p><p>Identifying the right sites and parameters is an intricate and labor-intensive process, and it’s not always immediately clear which settings will work best, according to UC San Francisco neurosurgeon <a href="https://profiles.ucsf.edu/kristin.sellers" target="_blank">Kristin Sellers</a>. All the data they collect creates a “curse of bounty,” she says. Yet, in her view, the outcomes demonstrate the effectiveness of taking this personalized approach. “No one has an identical implant,” she says.</p><h2>New Ideas on DBS for Depression</h2><p>Meanwhile, a team at Baylor College of Medicine, in Houston, is pursuing a different approach to customized DBS for depression. The team’s standardized implant consists of two coordinated sets of electrodes: One targets the SGC brain region involved in profound sadness, while the other stimulates a reward-and-motivation hub deep in the brain’s basal ganglia.</p><p>The customization happens on the front end during the initial surgical procedure, when clinicians temporarily place another 10 electrodes into the brain that take recordings via electroencephalography (EEG). This method tracks brain waves and, as patients undergo various tests and activities, allows the Baylor team to map relevant neural networks and connections. At the same time, the doctors can fiddle with the amplitude, pulse width, frequency, and shape of the stimulation field.</p><p>“Then we can basically design bespoke stimulation parameters for that individual that are going to move that person’s network in the right direction,” explains <a href="https://www.bcm.edu/people-search/sameer-sheth-30585" target="_blank">Sameer Sheth</a>, the neurosurgeon leading the project. Sheth and his colleagues have treated seven people, with <a href="https://www.biologicalpsychiatryjournal.com/article/S0006-3223(21)01747-9/fulltext" target="_blank">promising initial results</a>.</p><p>Any of these highly individualized approaches will involve additional surgical procedures and lengthy stays in the hospital. But as Dougherty of Massachusetts General Hospital points out, “We need to do this invasive research first so that we might be able to use noninvasive approaches later.”</p><p>He imagines a future in which electrodes on the scalp or advanced imaging techniques could identify optimal targets and guide treatment adjustments. Even then, however, if DBS requires highly personalized programming, it will be challenging to make it accessible to the millions of people worldwide in the throes of depression.</p><p>“The question will always be about the scalability of things,” says <a href="https://www.neuromod.uni-freiburg.de/people/volker-arnd-coenen/" target="_blank">Volker A. Coenen</a>, a neurosurgeon at the University of Freiburg Medical Center, in Germany. Coenen is therefore focusing his energy on testing a standardized DBS protocol, one that involves implanting the <a href="https://www.bostonscientific.com/en-US/products/deep-brain-stimulation-systems/vercise-gevia-tm-dbs-system.html" target="_blank">Vercise Gevia</a> system from Boston Scientific into an area of the brain known as the medial forebrain bundle.</p><p>In his view, this brain region offers a more direct and efficient pathway to reward systems and emotional-regulation networks. Still, the various brain regions under consideration are all interconnected, which explains why they all seem to offer some degree of therapeutic benefit. “You can perturb the network from different angles,” Coenen says.</p><h2>The Road Ahead for DBS</h2><p>So, which site is best? The answer likely depends on the specific symptoms and underlying brain circuits unique to each individual, says <a href="https://med.umn.edu/bio/alik-widge" target="_blank">Alik Widge</a>, a psychiatrist and biomedical engineer at the University of Minnesota, in Minneapolis.</p><p>“There’s no such thing as DBS for depression. There’s DBS for treating specific cognitive-emotional syndromes,” he argues—and different targets will be suited for accessing different aspects of the disorder. Depression can take many forms: Some people experience it as emotional despondency, while others struggle with obsessive thoughts or a loss of pleasure.</p><p>The optimal stimulation method may also vary. Continuous stimulation may work best for people whose depression follows a steady, persistent course, while intermittent or responsive stimulation may be more appropriate for those whose symptoms fluctuate with daily ups and downs. “It’s like the difference between weather and climate,” says Riva Posse—some people may need an umbrella for passing showers, while others need to reinforce their homes against rising tides.</p><p>Ultimately, whether they’re tweaking stimulation parameters, finding the best brain targets, or making stimulation respond to real-time brain signals, the goal for researchers in the field remains the same: to create a neurologically precise approach to treating depression in people who have found no relief. “There are so many levers we can press here,” says <a href="https://sunnybrook.ca/research/team/member.asp?m=734&page=0" target="_blank">Nir Lipsman</a>, who directs the Harquail Centre for Neuromodulation at Sunnybrook, in Toronto. He’s confident that at least some of these efforts will unlock new therapeutic possibilities.</p><p>“The field is experiencing a kind of reset,” Lipsman adds. Now, with neural activity as a guide, the brains of people undergoing DBS should likewise experience a kind of reset as well. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Mon, 30 Jun 2025 13:00:02 +0000</pubDate><guid>https://spectrum.ieee.org/deep-brain-stimulation-depression</guid><category>Deep brain stimulation</category><category>Dbs</category><category>Depression</category><category>Neurotechnology</category><category>Neurostimulation</category><category>Neuroscience</category><dc:creator>Elie Dolgin</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/blue-and-gold-fibrous-texture-in-the-shape-of-a-brain-against-a-dark-background.png?id=61109682&amp;width=980"></media:content></item><item><title>Transform Complexity into Opportunity with Digital Engineering</title><link>https://www.ansys.com/resource-center/white-paper/strategic-digital-engineering?&amp;plid=7i4e4h&amp;cid=1n2r9f&amp;utm_campaign=product&amp;utm_medium=paid-trade&amp;utm_source=ieee&amp;utm_content=digital_digital-mission-engineering_e-book_download_sponsored-content_NULL_a_en_global&amp;campaignid=701Pf00000T0AzfIAFTopics:%20Digital%20Engineering,%20MBSE,%20SPDM,%20Software,%20Security%20&amp;%20Safety,%20AI-Based%20Optimization</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/ansys-logo.png?id=26851525&width=980"/><br/><br/><p>In a literal sense, digital engineering is as old as the first back-of-the-napkin sketch that was shoved in a pocket before being modeled in a computer-aided design program. Those pencil marks became data points, and they were soon joined by millions more as product complexity exploded. We’re now awash in engineering data coming from all corners of the enterprise, as well as from partners, suppliers, and vendors. Product geometries, multiphysics simulations, materials intelligence, market research, digital twins, artificial intelligence datasets, and cyber-physical systems of all kinds contribute to the data deluge.</p><p>Digital engineering is no longer just about making the physical digital and all the benefits that entails. Today, digital engineering is about making the digital physical — in other words, being able to gather useful insights from all those data points, bring those insights together into a strategy, and turn that strategy into reality.</p><p><span><a href="https://www.ansys.com/resource-center/white-paper/strategic-digital-engineering" target="_blank">Download this free whitepaper now!</a></span></p>]]></description><pubDate>Mon, 30 Jun 2025 12:00:03 +0000</pubDate><guid>https://www.ansys.com/resource-center/white-paper/strategic-digital-engineering?&amp;plid=7i4e4h&amp;cid=1n2r9f&amp;utm_campaign=product&amp;utm_medium=paid-trade&amp;utm_source=ieee&amp;utm_content=digital_digital-mission-engineering_e-book_download_sponsored-content_NULL_a_en_global&amp;campaignid=701Pf00000T0AzfIAFTopics:%20Digital%20Engineering,%20MBSE,%20SPDM,%20Software,%20Security%20&amp;%20Safety,%20AI-Based%20Optimization</guid><category>Artificial intelligence</category><category>Computer-aided design</category><category>Digital engineering</category><category>Type:whitepaper</category><dc:creator>Mike Spector</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/26851525/origin.png"></media:content></item><item><title>Video Friday: This Quadruped Throws With Its Whole Body</title><link>https://spectrum.ieee.org/robot-arm-thrower</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/quadruped-robot-with-manipulator-arm-placed-on-pavement-near-a-table-tennis-setup.png?id=61112174&width=1200&height=800&coordinates=0%2C11%2C0%2C11"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN, CHINA</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote class="rm-anchors" id="3ysgbn6ca8a"><em>Throwing is a fundamental skill that enables robots to manipulate objects in ways that extend beyond the reach of their arms. We present a control framework that combines learning and model-based control for prehensile whole-body throwing with legged mobile manipulators. This work provides an early demonstration of prehensile throwing with quantified accuracy on hardware, contributing to progress in dynamic whole-body manipulation.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c380e7cc77c2ff7df6924bc3589434b5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3ysgbN6Ca8A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://arxiv.org/abs/2506.16986">Paper</a> ] from [ <a href="https://ethz.ch/en/studies/master/degree-programmes/engineering-sciences/robotics-systems-and-control.html" target="_blank">ETH Zurich</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="wgr7iizzfr0">As it turns out, in many situations <a data-linked-post="2666662286" href="https://spectrum.ieee.org/humanoid-robots" target="_blank">humanoid robots</a> don’t necessarily need legs at all.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="47302ecf2f03d6151a2dfd94f1ce29d5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/WgR7IIzzfR0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.robotera.com/en/enq5">ROBOTERA</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="xmbuvofnu5a"><em>Picking-in-Motion is a brand new feature of Autopicker 2.0. Instead of remaining stationary while picking an item, Autopicker begins traveling toward its next destination immediately after retrieving a storage tote, completing the pick while on the move. The robot then drops off the first storage tote at an empty slot near the next pick location before collecting the next tote.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8403fb62dce8c4ce1457ec30a8bb67f5" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xmbuvoFNu5A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://brightpick.ai/">Brightpick</a> ]</p><p>Thanks, Gilmarie!</p><div class="horizontal-rule"></div><p class="rm-anchors" id="rwsb78emfgi">I am pretty sure this is not yet real, but boy is it shiny.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c8476f7f896244eb4a23e8481b9be4b7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/rWSb78EmFGI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.softbank.jp/corp/philosophy/technology/special/ntn-solution/haps/">SoftBank</a> ] via [ <a href="https://robotstart.info/2025/06/26/sb-haps-2026.html">RobotStart</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="lf5fw4qunlk">Why use one thumb when you can use two instead?</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3059dc10a2edb86d9db46fdcd9e0bde6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/LF5fW4qUnlk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tu.berlin/en/robotics">TU Berlin</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="3fwefdibpk0"><em>Kirigami offers unique opportunities for guided morphing by leveraging the geometry of the cuts. This work presents inflatable <a data-linked-post="2650276722" href="https://spectrum.ieee.org/artificial-snakeskin-helps-robots-get-their-slither-on" target="_blank">kirigami crawlers</a> created by introducing cut patterns into heat-sealable textiles to achieve locomotion upon cyclic pneumatic actuation. We found that the kirigami actuators exhibit directional anisotropic friction properties when inflated, having higher friction coefficients against the direction of the movement, enabling them to move across surfaces with varying roughness. We further enhanced the functionality of inflatable kirigami actuators by introducing multiple channels and segments to create functional soft robotic prototypes with versatile locomotion capabilities.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="772fc720afe02dd29601706a74c39c62" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3fWEFDibPK0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/adrr.202500044">Paper</a> ] from [ <a href="https://www.softrobotics.dk/" target="_blank">SDU Soft Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="1gviu3trrps">Lockheed Martin wants to get into the <a data-linked-post="2652904040" href="https://spectrum.ieee.org/mars-sample-return-mission" target="_blank">Mars Sample Return</a> game for a mere US $3 billion.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d6f03aad050bbbe77173bfc37241875c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/1GViU3tRRps?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://lockheedmartin.com/en-us/news/features/2025/bringing-commercial-industry-efficiency-to-exploration-lockheed-martins-plan-for-mars-sample-return.html">Lockheed Martin</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="9hwyniiw4nm">This is pretty gross and exactly what you want a robot to be doing: dealing with municipal solid waste.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3a9e942998b6b6ad57bfd01a18012ab7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9HWYNIiW4NM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.terex.com/zenrobotics/waste-types/municipal-solid-waste">ZenRobotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="h5z32e7uakm"><em>Drag your mouse or move your phone to explore this 360-degree panorama provided by <a data-linked-post="2650269754" href="https://spectrum.ieee.org/curiosity-turns-one-on-mars" target="_blank">NASA’s Curiosity Mars rover.</a> This view shows some of the rover’s first looks at a region that has only been viewed from space until now, and where the surface is crisscrossed with spiderweb-like patterns.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1de2285b55a5583bba358034225f6c47" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/H5z32E7uaKM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://science.nasa.gov/mission/msl-curiosity/">NASA Jet Propulsion Laboratory</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="hpvza8rfiks">In case you were wondering, <a data-linked-post="2667116159" href="https://spectrum.ieee.org/irobot-amazon" target="_blank">iRobot</a> is still around.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="94f11994637afc79d347441f03a021f7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hPVZA8rfiKs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.irobot.com/">iRobot</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="shmgjzkqzdm">Legendary roboticist <a data-linked-post="2650271281" href="https://spectrum.ieee.org/cynthia-breazeal-unveils-jibo-a-social-robot-for-the-home" target="_blank">Cynthia Breazeal</a> talks about the equally legendary Personal Robots Group at the MIT Media Lab.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="96578b094f03b3ec1f7d6396f02f9a1c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/shMGJZkQzDM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.media.mit.edu/groups/personal-robots/overview/">MIT Personal Robots Group</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="kj0mp74v4kq"><em>In the first installment of our Moonshot Podcast Deep Dive video interview series, X’s Captain of Moonshots <a data-linked-post="2650275007" href="https://spectrum.ieee.org/astro-teller-captain-of-moonshots-at-x" target="_blank">Astro Teller</a> sits down with <a data-linked-post="2650257886" href="https://spectrum.ieee.org/sebastian-thrun-will-teach-you-how-to-build-your-own-self-driving-car-for-free" target="_blank">Sebastian Thrun</a>, cofounder of the Moonshot Factory, for a conversation about the history of Waymo and Google X, the ethics of innovation, the future of AI, and more.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="54cee25119337253957257dc6f4bd176" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/kj0mp74V4kQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://x.company/">Google X, The Moonshot Factory</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 27 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/robot-arm-thrower</guid><category>Robotics</category><category>Video friday</category><category>Manipulators</category><category>Crawler</category><category>Industrial robots</category><category>Humanoid robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/quadruped-robot-with-manipulator-arm-placed-on-pavement-near-a-table-tennis-setup.png?id=61112174&amp;width=980"></media:content></item><item><title>Reviving a 1970s Analog HP X-Y Recorder</title><link>https://spectrum.ieee.org/reviving-vintage-x-y-recorder</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-picture-of-a-large-bulky-plotter-like-device-connected-to-a-small-board-hosting-several-printed-circuit-boards-including-a-ra.png?id=61109145&width=1200&height=800&coordinates=233%2C0%2C234%2C0"/><br/><br/><p>Solid construction, elegant design, and high-precision output. Once upon a time, <a href="https://en.wikipedia.org/wiki/Hewlett-Packard" rel="noopener noreferrer" target="_blank">Hewlett-Packard</a> made test-and-measurement equipment that was beloved by working engineers. Sure, drop one of those babies on your foot and you were looking at a broken toe. But that’s a small price to pay for reliability and some character building. So when I recently came across an early 1970s HP 7041A X-Y recorder while clearing out my parents’ attic, I knew that I just had to see if I could get it up and running again.</p><h2>What is the difference between an X-Y recorder and a plotter?</h2><p><a href="https://www.hpmemoryproject.org/wb_pages/wall_b_page_13.htm" rel="noopener noreferrer" target="_blank">X-Y recorders</a> were designed to chart data from analog instruments in real time, as opposed to plotters, which plot the outputs of digital computers. The basics are the same: There’s a mechanism to raise and lower a pen, and motors to move the pen across the surface of a page. The big difference is that a plotter typically uses digital commands to control the pen’s motion, while the motion of a recorder’s pen is controlled by analog voltage inputs. </p><p>So all I needed to do was feed the HP recorder the right voltages, and I could draw anything I wanted! Only a few obstacles stood in my way.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Key components of the recorder interface including a Rapsberry Pi, expansion hat and digital-to-analog converters, QWIIC I2C connectors and a 74LS08 and LMC6484 integrated circuits." class="rm-shortcode" data-rm-shortcode-id="70805e8614f28abf23a1867f0c68606e" data-rm-shortcode-name="rebelmouse-image" id="e5317" loading="lazy" src="https://spectrum.ieee.org/media-library/key-components-of-the-recorder-interface-including-a-rapsberry-pi-expansion-hat-and-digital-to-analog-converters-qwiic-i2c-con.png?id=61109151&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The recorder interface allows Python code written on a Raspberry Pi [bottom right] to create analog signals that are level shifted up and down to meet the different ranges required by the plotter’s x- and y- axis. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Provost</small></p><p>The first obstacle was getting the thing home, because my parents’ attic was in Ireland and fitting a 13-kilogram, 48-by-36-by-17-centimeter behemoth into my suitcase for my flight back to NYC wasn’t happening. About US $300 in packing materials and shipping fees solved that problem, and I was still a little ahead of the game financially compared to obtaining a similar vintage recorder from eBay, and way ahead of buying a new Bantam Tools <a href="https://bantamtools.com/products/bantam-tools-nextdraw-1117" target="_blank">NextDraw plotter</a> with a similar drawing area. (To be fair to Bantam, its plotters are sleek plug-and-play devices that can handle a much wider range of pens).</p><p>The second obstacle was that the recorder didn’t work. Once I got it home, I discovered that the <em><em>y</em></em>-axis could be adjusted manually using a knob on the control panel, but the <em><em>x</em></em>-axis was dead. The mechanism for raising and lowering the pen made a weak clunking sound and barely twitched. </p><p class="pull-quote"><span>“Had I just spent a lot of money to ship home an HP-model boat anchor?”</span></p><p>I opened up the case, which was quick work with a Phillips screwdriver—no weird security screws, no glue, no fragile plastic, no yellow stickers warning that your warranty would be voided and you’d probably get boils if you dared to look within. On the beautifully laid out printed circuit boards inside, I spotted some resistors that were clearly not part of the factory install. Were they modifications or repairs? If the latter, had they been successful or had I just spent a lot of money to ship home an HP-model boat anchor? And I had no idea how the 40-pin interface connector on the back of the recorder was supposed to be hooked up to control signals.</p><p>All these problems were solved when I found the recorder’s manual on eBay. Oh, what a manual. Not just operating instructions, but detailed illustrations for taking the recorder completely apart and putting it back together. It listed every component, with photos of the circuit boards and electronic schematics on gatefolds. And it included directions for modifying the circuitry if you wanted to measure different voltage ranges than the factory settings–extract a resistor <em><em>here</em></em> and <em><em>there</em></em> and solder in some new ones. This explained the resistors I’d spotted. In a world where licenses and software locks <a href="https://spectrum.ieee.org/why-we-must-fight-for-the-right-to-repair-our-electronics" target="_self">forbid folks from simply plugging in a replacement component</a>, the thought of a major company encouraging its customers to break out a soldering iron is mind boggling.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/why-we-must-fight-for-the-right-to-repair-our-electronics" target="_self">Why We Must Fight for the Right to Repair Our Electronics</a></p><p>Soon, the application of some<a href="https://www.starrett.com/details?cat-no=1620" target="_blank"> instrument oil</a>,<a href="https://kezeproducts.shop/product/keze-silicone-grease-waterproof-food-grade-silicone-sealant-B0DK7188RF23" rel="noopener noreferrer" target="_blank"> silicone grease</a>, and<a href="https://www.kcprofessional.com/en-us/products/wiping-and-cleaning/controlled-environments/delicate-task-dry-wipes/kimtech-science-kimwipes-delicate-task-wipes/34120" rel="noopener noreferrer" target="_blank"> Kimwipes</a> had the <em><em>x</em></em>-axis and pen lifter working again. I hooked up a variable power supply to the recorder’s connector and <em><em>slowly</em></em> brought the voltage up as I watched the pen holder move in response. This allowed me to determine the recorder’s input ranges, which turned out to be 0 to 1 volt for the <em><em>y</em></em>-axis, and 0 to 5 V for the <em><em>x</em></em>-axis, covering 25 and 38 cm of motion, respectively, with about 0.2 millimeter accuracy.</p><p>The next step was to build an interface. Although microcontrollers often have digital-to-analog capabilities built-in, there’s often only one true analog output pin. I needed two. A technique like<a href="https://en.wikipedia.org/wiki/Pulse-width_modulation" rel="noopener noreferrer" target="_blank"> pulse-width modulation</a> would let me output an analog-ish voltage on multiple pins, but typically with<a href="https://docs.arduino.cc/learn/microcontrollers/analog-output/" rel="noopener noreferrer" target="_blank"> only 8-bit resolution</a> or 256 distinct voltage levels. I needed at least 1,900 levels to match the recorder’s accuracy.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A block diagram of the path taken by control signals from the Pi to the recorder." class="rm-shortcode" data-rm-shortcode-id="292b7719580ef594eaedb735375f5567" data-rm-shortcode-name="rebelmouse-image" id="cd996" loading="lazy" src="https://spectrum.ieee.org/media-library/a-block-diagram-of-the-path-taken-by-control-signals-from-the-pi-to-the-recorder.png?id=61109155&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The Raspberry Pi uses an expansion “HAT” and two 12-bit digital-to-analog converters commanded via I2C connectors to create 0- to 3.3-volt control signals. These are shifted to 0- to 1-V and 0- to 5-V ranges using a voltage divider and amplifier respectively, as well as a logic gate used to convert a 3.3-V digital signal that raises and lowers the pen to a 5-V level.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">James Provost</small></p><p>So I bought two $5 Adafruit<a href="https://www.adafruit.com/product/935" rel="noopener noreferrer" target="_blank"> MCP4725 breakout boards</a>. These are 12-bit digital-to-analog converters—each providing 4,096 distinct levels—controlled over an<a href="https://en.wikipedia.org/wiki/I%C2%B2C" rel="noopener noreferrer" target="_blank"> I2C serial</a> connection, and two boards can share the same I2C bus. I connected them to a<a href="https://www.raspberrypi.com/products/raspberry-pi-1-model-b-plus/" rel="noopener noreferrer" target="_blank"> Raspberry Pi Model B+</a> I fished out of a drawer via a $6.60<a href="https://www.sparkfun.com/sparkfun-qwiic-hat-for-raspberry-pi.html?gad_source=1&gad_campaignid=17479024039&gbraid=0AAAAADsj4ETIsvzPrWRZILPBJJYuYL4lW&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5EY7W2Dcff6vdxZ05A9JDZ53YuAs0DOLpBhLPfsXnEw6yKQjBOoKhBoCrqUQAvD_BwE" rel="noopener noreferrer" target="_blank"> SparkFun Qwiic HAT</a>.</p><p>The DACs put out a signal in the range of 0 to 3.3 V, so I sent one board’s output through a<a href="https://en.wikipedia.org/wiki/Voltage_divider" rel="noopener noreferrer" target="_blank"> voltage divider</a> to scale it down to 0 to 1 V for the <em><em>y</em></em>-axis. For the <em><em>x</em></em>-axis, I fed the other board’s output through an<a href="https://www.ti.com/lit/ds/symlink/lmc6484.pdf" rel="noopener noreferrer" target="_blank"> LMC6484</a> amplifier, powered by a 5-V pin from the Pi, to bring it up to 0 to 4.8 V—not quite the full range, but it’ll do until I come up with a more sophisticated interface. I brought a signal to raise and lower the pen out from one of the Pi’s GPIO pins, passing it through a<a href="https://www.ti.com/lit/ds/symlink/sn74ls08.pdf?ts=1749054414890" rel="noopener noreferrer" target="_blank"> 74LS08 AND gate IC</a> used as a cheap and cheerful 3.3- to 5-V digital level shifter.</p><p>I then wrote code on the Pi to put the plotter through its paces, using parametric equations written in<a href="https://circuitpython.org/" rel="noopener noreferrer" target="_blank"> CircuitPython</a> to draw swirling<a href="https://en.wikipedia.org/wiki/Hypotrochoid" rel="noopener noreferrer" target="_blank"> hypotrochoids</a> and other geometric curves. Ultimately, it should be possible to have the Pi accept and translate commands written in a plotter-control language such as<a href="https://en.wikipedia.org/wiki/HP-GL" rel="noopener noreferrer" target="_blank"> HP-GL</a>. Then I’ll be able to plot vector graphics and text from drawing software like<a href="https://inkscape.org/" rel="noopener noreferrer" target="_blank"> Inkscape</a>. But for now, I’m happy to just have my recorder humming away beside me, hale and hearty and built to last.</p>]]></description><pubDate>Fri, 27 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/reviving-vintage-x-y-recorder</guid><category>Circuitpython</category><category>Plotter</category><category>Python</category><category>Raspberry pi</category><category>Retrocomputing</category><category>Type:departments</category><category>Vintage electronics</category><dc:creator>Stephen Cass</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-picture-of-a-large-bulky-plotter-like-device-connected-to-a-small-board-hosting-several-printed-circuit-boards-including-a-ra.png?id=61109145&amp;width=980"></media:content></item><item><title>Advancing Quantum Science: Hausi Müller’s Journey</title><link>https://spectrum.ieee.org/ieee-quantum-computing-hausi-muller</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/hausi-a-muller-speaking-on-stage-during-ieee-quantum-week.jpg?id=61108939&width=1200&height=800&coordinates=0%2C136%2C0%2C136"/><br/><br/><p>When <a href="https://webhome.cs.uvic.ca/~hausi/" rel="noopener noreferrer" target="_blank">Hausi A. Müller</a> began conducting research on quantum computing a decade ago, he was excited about the technology’s possibilities for engineering and computer science applications. Quantum computing leverages the principles of quantum mechanics to tackle problems that are beyond the capabilities of classical computers. </p><p>Although it’s been 100 years since the discovery of quantum mechanics, applications for how to use it are just now becoming a reality. The technology is expected to advance the fields of artificial intelligence, cybersecurity, drug discovery, finance, and more. </p><h3>Hausi A. Müller</h3><br/><p><strong>Employer: </strong></p><p><strong></strong>University of Victoria, in British Columbia, Canada</p><p><strong>Title: </strong></p><p><strong></strong>Professor of computer science</p><p><strong>Member grade: </strong></p><p><strong></strong>Life senior member</p><p><strong>Alma maters: </strong></p><p><strong></strong>ETH Zurich and Rice University, in Houston</p><h3></h3><br/><p>“I felt like a kid in the candy store,” says Müller, a software engineering professor at the <a href="https://www.uvic.ca/ecs/computerscience/people/faculty/index.php" target="_blank">University of Victoria</a>, in British Columbia, Canada. “With quantum computing, you can now simulate things quickly that were not possible to do before. In drug discovery, for example, you can play with molecules on your computer and explore new ideas without physically synthesizing compounds in a lab, saving both time and resources.”</p><p>The IEEE life senior member has always been confident that quantum science would take off. An active volunteer for more than 40 years, he persuaded IEEE in 2019 to become more involved in the science and engineering of quantum computing technologies. What began as the <a href="https://quantum.ieee.org/" target="_blank">IEEE Future Directions Quantum Initiative</a>—with support from a handful of IEEE organizational units—evolved into the <a href="https://quantum.ieee.org/about" rel="noopener noreferrer" target="_blank">IEEE Quantum Technical Community</a> last year, with the support of 11 units.</p><p>In 2020 he helped found <a href="https://qce.quantum.ieee.org/2025/" rel="noopener noreferrer" target="_blank">IEEE International Conference on Quantum Computing and Engineering (QCE)</a>, part of the annual <a href="https://qce.quantum.ieee.org/2025/" rel="noopener noreferrer" target="_blank">IEEE Quantum Week</a>, which he also founded, to provide a multidisciplinary and open forum for discussing the technology’s challenges and opportunities.</p><p>Müller’s and IEEE’s efforts to advance quantum computing have not gone unnoticed. In February, Müller, <a href="https://spectrum.ieee.org/ieee-presidents-note-march-2025" target="_self">IEEE President Kathleen Kramer</a>, and other dignitaries attended a <a href="https://www.unesco.org/en" rel="noopener noreferrer" target="_blank">UNESCO </a>ceremony in Paris, where the agency designated 2025 as the <a href="https://www.unesco.org/en/years/quantum-science-technology" rel="noopener noreferrer" target="_blank">International Year of Quantum Science and Technology</a>.</p><h2>Software and quantum computing researcher</h2><p>Müller became interested in software about halfway through his bachelor’s degree program in electrical engineering at <a href="https://ethz.ch/en.html" rel="noopener noreferrer" target="_blank">ETH Zurich</a>, he says. He was inspired by one of his professors, <a href="https://en.wikipedia.org/wiki/Niklaus_Wirth" rel="noopener noreferrer" target="_blank">Niklaus Wirth</a>, a Swiss computer scientist who designed <a href="https://modula.us/" rel="noopener noreferrer" target="_blank">Modula</a>, <a href="https://en.wikipedia.org/wiki/Oberon_(programming_language)" rel="noopener noreferrer" target="_blank">Oberon</a>, <a href="https://en.wikipedia.org/wiki/Pascal_(programming_language)" rel="noopener noreferrer" target="_blank">Pascal</a>, and other programming languages.</p><p>“In electrical engineering, you have these nice little parts to experiment with in the lab. But in software, experiments are done with paper and pencil, and you can build amazing machines quickly,” Müller says. “That’s what really attracted me to software engineering.”</p><h3>Skills Needed for a Career in Quantum Computing</h3><br/><p>There are many opportunities for people seeking a career in the development and operation of quantum computing hardware and software, Müller says. The ease of transitioning into the field depends on the engineer’s background in hardware, software, and applications.</p><p>“Quantum computing is highly interdisciplinary,” he says, “and for it to succeed, we need computer scientists and all kinds of engineers, especially software engineers.”</p><p>Here are some of skills Müller says quantum engineers need:</p>
<p><strong>Linear algebra.</strong> This foundational knowledge is essential for matrix mechanics and quantum computing.</p>
<p><strong>Qubits and quantum states.</strong> Learn how qubits differ from classical bits, including the fundamental quantum concepts of superposition, entanglement, and interference.</p>
<p><strong>Quantum gates, circuits, and measurement.</strong> Understand how to manipulate qubit states using gates and build quantum circuits.</p>
<p><strong>Quantum algorithms and protocols.</strong> Learn the basics to solve problems, dealing with actions and properties governed by the laws of quantum mechanics.</p>
<p><strong>Quantum software platforms and engineering quantum software.</strong> Become proficient in <a href="https://www.python.org/" rel="noopener noreferrer" target="_blank">Python</a> as well as quantum platforms and libraries such as<a href="https://docs.google.com/document/d/1aOG5zY-naqxcrVEACjcPaAlRIbZGw3iTYMRX_AaAKSM/edit?tab=t.0#heading=h.984nt7yojate" rel="noopener noreferrer" target="_blank"> IBM’s Qiskit</a>, Microsoft’s <a href="https://quantum.microsoft.com/" rel="noopener noreferrer" target="_blank">software development kits and Q#</a> programming language, Google’s <a href="https://quantumai.google/cirq" rel="noopener noreferrer" target="_blank">Cirq</a>, Xanadu’s <a href="https://www.xanadu.ai/products/pennylane/" rel="noopener noreferrer" target="_blank">PennyLane</a>, D-Wave’s <a href="https://www.dwavequantum.com/" rel="noopener noreferrer" target="_blank">Leap</a>, Amazon’s <a href="https://aws.amazon.com/braket/" rel="noopener noreferrer" target="_blank">Braket</a>, Nvidia’s <a href="https://developer.nvidia.com/cuda-q" rel="noopener noreferrer" target="_blank">CUDA-Q</a>, and Quantinuum’s <a href="https://www.quantinuum.com/glossary-item/tket" rel="noopener noreferrer" target="_blank">TKET</a>.</p>
<p><strong>Qubit technologies.</strong> Learn how to implement physical qubits such as superconducting transmon qubits, trapped ions, and photonic qubits, as well as their control technologies.</p>
<p><strong>Microwave qubit control.</strong> Precise control of superconducting qubits relies heavily on microwave signals, so learn about microwave generation, manipulation, and measurement.</p>
<p><strong>Pulse shapes for gate design.</strong> Quantum gates are implemented by applying precisely shaped pulses—electrical or optical—to qubits, so understanding pulse design and control is crucial.</p>
<p><strong>Instrumentation and measurement.</strong> Learn how to use tools for generating control signals and measuring qubit states.</p>
<p><strong>Quantum error correction.</strong> Quantum systems are susceptible to noise, which can introduce errors in computations. Understanding quantum error correction at distinct levels of the quantum stack is essential for building fault-tolerant quantum computers.</p><h3></h3><br/><p>After graduating in 1979, Müller joined <a href="https://global.abb/group/en" target="_blank">ABB</a>, a global electrification and automation technology company headquartered in Zurich. He was responsible for building a database management system on <a href="https://en.wikipedia.org/wiki/PDP-11" target="_blank">PDP-11 computers</a> for the central control systems. He was part of a team developing power control and supervision systems for Colombia, Singapore, and Sweden.</p><p>Müller says he believed that an advanced degree would further his career, and he wanted to conduct research into the evolution of software. In 1982 he enrolled at <a href="https://ethz.ch/en.html" target="_blank">Rice University</a>, in Houston, to pursue a master’s degree in computer science, which he earned in 1984. He enjoyed being there so much, he says, that he stayed on to earn a Ph.D. in the same subject in 1986.</p><p>It was while teaching programming courses at Rice that he decided to transition to a career in academia, but he still kept a foot in the industry. He conducted research on high-performance computing and worked extensively with <a href="https://www.ibm.com/us-en" rel="noopener noreferrer" target="_blank">IBM</a>, a relationship he has maintained for more than 30 years. He has served as a principal investigator for several collaborative research projects including the development of hybrid and distributed quantum computing software using IBM’s <a href="https://www.ibm.com/quantum/qiskit" rel="noopener noreferrer" target="_blank">Qiskit</a>.</p><p>He left Rice in 1986 to join the University of Victoria as a professor of computer science. During his nearly 40 years there, he has served as associate dean of research for a decade. He is the founding director of the accredited software engineering bachelor’s degree program.</p><p>His research has mostly focused on <a href="https://spectrum.ieee.org/topic/computing/" target="_self">software engineering</a> for self-adaptive systems, smart software systems, the <a href="https://spectrum.ieee.org/ieee-courses-on-emerging-tech" target="_self">Internet of Things</a>, and intelligent cyber-physical systems. Today he is primarily focused on quantum computing. More than 60 of <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?newsearch=true&queryText=Hausi%20M%C3%BCller" rel="noopener noreferrer" target="_blank">his papers</a> are in the <a href="https://ieeexplore.ieee.org/Xplore/home.jsp" rel="noopener noreferrer" target="_blank">IEEE Xplore Digital Library</a>.</p><p>He is a principal investigator of a large grant from the <a href="https://en.wikipedia.org/wiki/Natural_Sciences_and_Engineering_Research_Council" rel="noopener noreferrer" target="_blank">National Sciences and Engineering Research Council</a> of Canada. The council is assembling a <a href="https://www.qscc.ca/" rel="noopener noreferrer" target="_blank">Quantum Software Consortium</a>, an interdisciplinary research program that aims to develop methods and software for distributed quantum computing in Canada.</p><h2>A quantum forum</h2><p>Müller is most proud of having created <a href="https://qce.quantum.ieee.org/2023/author/hausimullergmail-com/" rel="noopener noreferrer" target="_blank">IEEE Quantum Week</a> and the <a href="https://qce.quantum.ieee.org/2025/" rel="noopener noreferrer" target="_blank">IEEE QCE</a> conference. Now in its fifth year, the forum is more popular than ever. Last year IEEE <a href="https://qce.quantum.ieee.org/2024/" rel="noopener noreferrer" target="_blank">Quantum Week</a> took place in Montréal, drawing 1,600 participants from 52 countries, with 60 percent from industry.</p><p>This year, the event is scheduled for 31 August to 5 September in Albuquerque. Müller is the program board chair. <a href="https://qce.quantum.ieee.org/2025/" rel="noopener noreferrer" target="_blank">IEEE Quantum Week 2025</a> will feature stellar exhibits, he says, with more than 250 technical papers, 150 posters, 35 tutorials, and 40 workshops, including ones for entrepreneurs and venture capitalists. The tutorials are designed to enhance the fundamental quantum computing skill sets needed by scientists and engineers, he says, and the workshops will feature experts from quantum communities.</p><h3>The Applications of Quantum</h3><br/><p>Quantum applications can be classified into three broad areas: optimization, machine learning, and simulating nature.</p><ul><li><strong>Optimization.</strong> Achieving quantum utility will revolutionize industrial applications, Müller says, by optimizing complex computations beyond classical capabilities, thereby improving efficiency, speed, and scalability. For example, the quantum approximate optimization algorithm can optimize power flow distribution across a smart grid. Researchers are exploring the use of quantum technology to enhance power grid efficiency and reduce energy losses.</li><li><strong>Machine learning.</strong> The utility of quantum machine learning lies in its potential to accelerate computations, enhance pattern recognition, and solve complex problems that classical machine learning struggles with, Müller says. Quantum computing has the potential to revolutionize finance, for example, by improving portfolio optimization, risk management, fraud detection, and market simulations.</li><li><strong>Simulation.</strong> Quantum computing has immense potential for simulating nature, Müller says, particularly in areas where classical computers are challenged by exponential complexity. Quantum utility in drug discovery, for example, could accelerate molecular simulations, leading to faster development of new medicines.</li></ul><p><span>“Some of the workshops collaborate throughout the year and then congregate again at Quantum Week,” he says. “I think the impact of these separate little communities that we started at Quantum Week is very powerful.</span></p><p>“Industry is very interested in talking to academia and, of course, the academics really want to talk to industry. From the outset, we designed IEEE Quantum Week to be a forum where this is possible. The activities bridge the gap between the science of quantum computing and the development of the industry surrounding it.”</p><h2>Longtime IEEE Computer Society volunteer</h2><p>Müller joined IEEE in 1979 during a membership drive for students at ETH.</p><p>“The opportunities for volunteering are what interested me, and I could see that it would benefit my career down the line. That has certainly been true,” he says. “I think the kind of volunteering we do within IEEE is super important for the next generation of engineers in universities, because the organization helps universities connect to what’s going on in industry.”</p><p>He is involved with the <a href="https://www.computer.org/" target="_blank">IEEE Computer Society</a> and has served as chair of its <a href="https://tc.computer.org/tcse/" target="_blank">Technical Community on Software Engineering</a>, as well as vice president of its <a href="https://www.computer.org/volunteering/boards-and-committees/technical-activities" rel="noopener noreferrer" target="_blank">technical and conference activities board</a>. He was a member of its <a href="https://www.computer.org/volunteering/board-of-governors" rel="noopener noreferrer" target="_blank">board of governors</a> and IEEE’s <a href="https://www.computer.org/volunteering/volunteer-with-us?source=nav" rel="noopener noreferrer" target="_blank">conferences committee</a>.</p><p>In recognition of his Computer Society volunteerism, he received last year’s <a href="https://www.computer.org/volunteering/boards-and-committees/technical-activities/awards" rel="noopener noreferrer" target="_blank">technical and conference activities board Distinguished Leadership Award</a> for “unwavering commitment and exceptional contributions to software engineering, quantum computing, and IEEE CS’s technical activities.”</p><p>“I think I have had an impact on the profession by volunteering to organize events such as Quantum Week,” he says. “I like the conference business because people get together, exchange ideas, collaborate, and even work together on research grant proposals. I have always been passionate about creating and sustaining communities.”</p>]]></description><pubDate>Thu, 26 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-quantum-computing-hausi-muller</guid><category>Careers</category><category>Ieee computer society</category><category>Ieee member news</category><category>Quantum computing</category><category>Quantum conferences</category><category>Software engineering jobs</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/hausi-a-muller-speaking-on-stage-during-ieee-quantum-week.jpg?id=61108939&amp;width=980"></media:content></item><item><title>The Violinist Who Fell in Love With Machine Learning</title><link>https://spectrum.ieee.org/violinist-to-software-engineer</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/man-in-gray-t-shirt-and-jeans-standing-next-to-a-large-statue-shaped-like-the-linkedin-logo.png?id=61106255&width=1200&height=800&coordinates=0%2C91%2C0%2C92"/><br/><br/><p>Music and engineering might seem like career paths that are almost diametrically opposed. But for <a href="https://www.linkedin.com/in/javierorman/" rel="noopener noreferrer" target="_blank">Javier Orman</a> the transition from professional violinist to a machine learning engineer at LinkedIn was a surprisingly natural one.</p><p>Growing up in Montevideo, Uruguay, Orman excelled at both music and math, and he double-majored in the subjects at college. But music was his true passion and after university he pursued a career as a professional violinist, performing, teaching, and helping to record and produce other artists.</p><h3>Javier Orman</h3><br/><p><strong>Employer:</strong></p><p>LinkedIn</p><p><strong>Occupation:</strong></p><p>Machine learning engineer</p><p><strong>Education:</strong></p><p>Bachelor’s degrees in music and mathematics, College of Charleston; master’s degree in music, University of Michigan</p><p>But during the turmoil of the COVID-19 pandemic, a conversation with a grad-school friend who had given up music for software development piqued his curiosity. After taking some free online courses in Python and machine learning, he quickly became immersed in a fascinating new world of data and algorithms. It didn’t take long for him to realize he wanted to make a career of it.</p><p>Machine learning algorithms were “almost like magic” to Orman. “I became enamored by the methodology, the math behind it.”</p><h2>A Double Prodigy</h2><p>Orman’s unusual career trajectory can be traced back to his early childhood. Both of his parents were software engineers and he grew up with computers around the house from an early age. But they were also a musical family. His mother enjoyed playing the piano, and his father the trumpet.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;"> <img alt="Musician passionately playing a violin on stage." class="rm-shortcode" data-rm-shortcode-id="5a587aa22f84e90342d414f11da739c8" data-rm-shortcode-name="rebelmouse-image" id="e9387" loading="lazy" src="https://spectrum.ieee.org/media-library/musician-passionately-playing-a-violin-on-stage.png?id=61106262&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Orman’s path from music to machine learning shows how those with nontechnical backgrounds can succeed in software.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://www.instagram.com/_desmoney/?hl=en" target="_blank">Desmond “Des Money” Owusu/Instagram</a></small></p><p>His musical journey started at 4 years old when he saw a class of around 100 children playing violin as a group. Orman was captivated and immediately told his mother he wanted to play the violin too. By his teens he had begun touring with Uruguay’s national youth orchestra and entering music competitions. At the same time though, he had discovered a natural aptitude for math and was entering Math Olympiads. But Orman says math was essentially a hobby he did on the side while he focused on his music career.</p><p>After completing two degrees, in music and math, at the <a href="https://charleston.edu/" target="_blank">College of Charleston</a> in South Carolina in 2006, Orman went on to a master’s degree in music at the University of Michigan. By 2009, he was playing in orchestras at Carnegie Hall in New York City and touring South America with a chamber music group.</p><p>Interested in pursuing a more creative avenue, Orman started composing music for short films and taught himself music production, eventually building a small studio where he would record and produce other artists. Over time, he built up a sustainable music career by combining this patchwork of creative projects with teaching violin.</p><h2>A New Direction</h2><p>In early 2020, as COVID-19 was upending the world, Orman found himself reevaluating his future and looking for new challenges. Near the start of the pandemic, he spoke to a friend from graduate school who had recently made the transition from professional violinist to software engineer. She told him about programming languages and what the career was like, and out of curiosity he decided to take an online Python course. Soon after he also began exploring the world of machine learning.</p><p>“I started taking online courses, but I also started looking for data on things that just interested me,” he says. For example, Orman created an <a href="https://www.ormandata.com/projects/project-six-bxe49" target="_blank">animated heat map</a> showing the rate of COVID-19 hospitalizations in each state. “Once I figured out how to make cool plots and investigate the data a bit more, that became actually fun to do.”</p><p>Within six months, Orman realized this was something he wanted to pursue as a career. “I noticed that I was having a hard time stopping for meal breaks or to go to sleep,” he says. “So I began to take it seriously.”</p><p>In April 2021, Orman got a job doing data preparation at New York City–based startup <a href="https://koiosmedical.com/" rel="noopener noreferrer" target="_blank">Koios Medical</a>, which develops <a href="https://spectrum.ieee.org/ai-diagnosis-cancer" target="_self">cancer-detection algorithms</a>. But his big break came just a few months later when he discovered LinkedIn’s <a href="https://careers.linkedin.com/reach" rel="noopener noreferrer" target="_blank">Reach</a> apprenticeship program, which provides a way into the tech industry for people with nontraditional educational or career backgrounds. He applied and started as an apprentice in machine learning software engineering that July.</p><h2>Learning Machine Learning</h2><p>Orman was assigned to LinkedIn’s Feed AI team, which develops the recommendation algorithms that determine what posts a user is shown. There are multiple layers to this system, which gradually filters down millions of potential posts to determine those most of interest to a specific user.</p><p>In 2022, after one year in the Reach program, Orman was promoted to software engineer and now works on a model known as the “second-pass ranker,” the final layer of AI in this system. It decides what posts are most relevant to the user based on factors like their propensity to click or comment on similar kinds of posts.</p><p>Much of his work involves experimenting with new machine learning techniques or making small tweaks to the model to squeeze out extra performance. “It’s a pretty complex system,” says Orman. “It’s also a very mature system, so we measure gains in terms of tenths or hundreds of a percentage.”</p><p>But he relishes the challenge and the push to continually learn new things. That’s something he believes his background in music, which requires constant dedication and practice, has set him up well to do.</p><p>There are also profound <a href="https://www.ams.org/publicoutreach/math-and-music" rel="noopener noreferrer" target="_blank">mathematical underpinnings to music</a>, and Orman thinks those connections have helped in his new career. “Those intersections run deep and they are hard to describe,” he says. “But they do feel like they both tickle my brain in a particular way.”</p><p>And Orman has some advice for others coming to engineering from nontechnical backgrounds: Focus on developing an intuition for how a technology works before diving into the nitty-gritty details. “Spending time understanding and just getting a feel for how things work on an intuitive level just makes everything easier,” he says. “And then you start practicing the nuts and bolts.”</p><p>One day, he hopes to marry his two major passions by working on <a href="https://spectrum.ieee.org/spotify-machine-learning-and-the-business-of-recommendation-engines" target="_blank">recommendation algorithms for music</a>. In the meantime, he’s content for them to play separate but complementary roles in his life. “I like to take breaks from my job and play Bach,” he says. “It just feels like a nice balance to go back and forth between the two.”</p>]]></description><pubDate>Thu, 26 Jun 2025 14:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/violinist-to-software-engineer</guid><category>Linkedin</category><category>Music</category><category>Software engineers</category><category>Type:departments</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/man-in-gray-t-shirt-and-jeans-standing-next-to-a-large-statue-shaped-like-the-linkedin-logo.png?id=61106255&amp;width=980"></media:content></item><item><title>AI Improves at Improving Itself Using an Evolutionary Trick</title><link>https://spectrum.ieee.org/evolutionary-ai-coding-agents</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conceptual-rendering-of-rodin-s-the-thinker-statue-used-to-represent-artificial-intelligence-processes.jpg?id=61098278&width=1200&height=800&coordinates=126%2C0%2C127%2C0"/><br/><br/><p><span>In April, Microsoft’s CEO said that artificial intelligence now wrote close to <a href="https://www.cnbc.com/2025/04/29/satya-nadella-says-as-much-as-30percent-of-microsoft-code-is-written-by-ai.html" target="_blank">a third of the company’s code</a>. Last October, Google’s CEO put their number at <a href="https://arstechnica.com/ai/2024/10/google-ceo-says-over-25-of-new-google-code-is-generated-by-ai/" target="_blank">around a quarter</a>. Other tech companies can’t be far off. Meanwhile, these firms create AI that will presumably be used to help programmers further. </span></p><p><span>Researchers have long hoped to fully close the loop, creating coding agents that recursively improve themselves. New research reveals an impressive demonstration of such a system. Extrapolating, one might see a boon to productivity, or a much darker future for humanity.</span></p><p>“It’s nice work,” said <a href="https://people.idsia.ch/~juergen/" target="_blank">Jürgen Schmidhuber</a>, a computer scientist at the King Abdullah University of Science and Technology (KAUST), in Saudi Arabia, who was not involved in the new research. “I think for many people, the results are surprising. Since I’ve been working on that topic for almost 40 years now, it’s maybe a little bit less surprising to me.” But his work over that time was limited by the tech at hand. One new development is the availability of large language models (LLMs), the engines powering chatbots like ChatGPT. </p><p>In the 1980s and 1990s, Schmidhuber and others explored evolutionary algorithms for improving coding agents, creating programs that write programs. An evolutionary algorithm takes something (such as a program), creates variations, keeps the best ones, and iterates on those. </p><p>But evolution is unpredictable. Modifications don’t always improve performance. So in 2003, Schmidhuber created problem solvers that rewrote their own code only if they could formally prove the updates to be useful. He called them <a href="https://people.idsia.ch/~juergen/goedelmachine.html" target="_blank">Gödel machines</a>, named after <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" target="_blank">Kurt Gödel</a>, a mathematician who’d done work on self-referencing systems. But for complex agents, provable utility doesn’t come easily. Empirical evidence may have to suffice.</p><h2>The Value of Open-Ended Exploration</h2><p>The new systems, described in a recent preprint on arXiv, rely on such evidence. In a nod to Schmidhuber, they’re called <a href="https://arxiv.org/abs/2505.22954" target="_blank">Darwin Gödel Machines</a> (DGMs). A DGM starts with a coding agent that can read, write, and execute code, leveraging an LLM for the reading and writing. Then it applies an evolutionary algorithm to create many new agents. In each iteration, the DGM picks one agent from the population and instructs the LLM to create one change to improve the agent’s coding ability. <a href="https://www.sciencenews.org/article/ai-train-creative-human-intuition" target="_blank">LLMs have something like intuition</a> about what might help, because they’re trained on lots of human code. What results is guided evolution, somewhere between random mutation and provably useful enhancement. The DGM then tests the new agent on a coding benchmark, scoring its ability to solve programming challenges.</p><p>Some evolutionary algorithms keep only the best performers in the population, on the assumption that progress moves endlessly forward. DGMs, however, keep them all, in case an innovation that initially fails actually holds the key to a later breakthrough when further tweaked. It’s a form of “<a href="https://www.quantamagazine.org/computers-evolve-a-new-path-toward-human-intelligence-20191106/" target="_blank">open-ended exploration</a>,” not closing any paths to progress. (DGMs do prioritize higher scorers when selecting progenitors.)</p><p>The researchers ran a DGM for 80 iterations using a coding benchmark called <a href="https://www.swebench.com/" target="_blank">SWE-bench</a>, and ran one for 80 iterations using a benchmark called <a href="https://aider.chat/2024/12/21/polyglot.html#the-polyglot-benchmark" target="_blank">Polyglot</a>. Agents’ scores improved on SWE-bench from 20 percent to 50 percent, and on Polyglot from 14 percent to 31 percent. “We were actually really surprised that the coding agent could write such complicated code by itself,” said <a href="https://www.jennyzhangzt.com/" target="_blank">Jenny Zhang</a>, a computer scientist at the University of British Columbia and the paper’s lead author. “It could edit multiple files, create new files, and create really complicated systems.”</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A family tree style image shows one node at the top branching off into 8 nodes, some of which branch off into more nodes." class="rm-shortcode" data-rm-shortcode-id="9c8c0abec5ca4091302687d3c55eef4c" data-rm-shortcode-name="rebelmouse-image" id="bd7c7" loading="lazy" src="https://spectrum.ieee.org/media-library/a-family-tree-style-image-shows-one-node-at-the-top-branching-off-into-8-nodes-some-of-which-branch-off-into-more-nodes.jpg?id=61104606&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The first coding agent (numbered 0) created a generation of new and slightly different coding agents, some of which were selected to create new versions of themselves. The agents’ performance is indicated by the color inside the circles, and the best performing agent is marked with a star.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Jenny Zhang, Shengran Hu, et al.</small></p><p>Critically, the DGMs outperformed an alternate method that used a fixed external system for improving agents. With DGMs, agents’ improvements compounded as they improved themselves at improving themselves. The DGMs also outperformed a version that didn’t maintain a population of agents and just modified the latest agent. To illustrate the benefit of open-endedness, the researchers created a family tree of the SWE-bench agents. If you look at the best-performing agent and trace its evolution from beginning to end, it made two changes that temporarily reduced performance. So the lineage followed an indirect path to success. Bad ideas can become good ones.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt='On a graph with "SWE-bench score" on the y axis and "iterations" on the x axis, a black line goes up with two dips. ' class="rm-shortcode" data-rm-shortcode-id="61c98db1ae561ef8b2906a4f668299e3" data-rm-shortcode-name="rebelmouse-image" id="fed6e" loading="lazy" src="https://spectrum.ieee.org/media-library/on-a-graph-with-swe-bench-score-on-the-y-axis-and-iterations-on-the-x-axis-a-black-line-goes-up-with-two-dips.jpg?id=61104607&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The black line on this graph shows the scores obtained by agents within the lineage of the final best-performing agent. The line includes two performance dips. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Jenny Zhang, Shengran Hu, et al.</small></p><p>The best SWE-bench agent was not as good as the best agent designed by expert humans, which currently scores about 70 percent, but it was generated automatically, and maybe with enough time and computation an agent could evolve beyond human expertise. The study is a “big step forward” as a proof of concept for recursive self-improvement, said <a href="https://zhengyaojiang.github.io/" target="_blank">Zhengyao Jiang</a>, a cofounder of <a href="https://www.weco.ai/" target="_blank">Weco AI</a>, a platform that automates code improvement. Jiang, who was not involved in the study, said the approach could made further progress if it modified the underlying LLM, or even the chip architecture. (Google DeepMind’s <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/" target="_blank">AlphaEvolve</a> designs better basic algorithms and chips and found a way to accelerate the training of its underlying LLM by 1 percent.)</p><p>DGMs can theoretically score agents simultaneously on coding benchmarks and also specific applications, such as drug design, so they’d get better at getting better at designing drugs. Zhang said she’d like to combine a DGM with AlphaEvolve.</p><p>Could DGMs reduce employment for entry-level programmers? Jiang sees a bigger threat from everyday coding assistants like Cursor. “Evolutionary search is really about building really high-performance software that goes beyond the human expert,” he said, as AlphaEvolve has done on certain tasks.</p><h2>The Risks of Recursive Self-improvement</h2><p>One concern with both evolutionary search and self-improving systems—and especially their combination, as in DGM—is safety. Agents might become uninterpretable or <a href="https://spectrum.ieee.org/the-alignment-problem-openai" target="_blank">misaligned</a> with human directives. So Zhang and her collaborators added guardrails. They kept the DGMs in sandboxes without access to the Internet or an operating system, and they logged and reviewed all code changes. They suggest that in the future, they could even reward AI for making itself more interpretable and aligned. (In the study, they found that agents falsely reported using certain tools, so they created a DGM that rewarded agents for not making things up, partially alleviating the problem. One agent, however, hacked the method that tracked whether it was making things up.)</p><p>In 2017, experts met in Asilomar, Calif., to discuss beneficial AI, and many signed an open letter called the <a href="https://futureoflife.org/open-letter/ai-principles/" target="_blank">Asilomar AI Principles</a>. In part, it called for restrictions on “AI systems designed to recursively self-improve.” One frequently imagined outcome is the so-called <a href="https://www.newyorker.com/science/annals-of-artificial-intelligence/can-we-stop-the-singularity" target="_blank">singularity</a>, in which AIs self-improve beyond our control and threaten human civilization. “I didn’t sign that because it was the bread and butter that I’ve been working on,” Schmidhuber told me. Since the 1970s, he’s predicted that <a href="https://spectrum.ieee.org/tag/superintelligence?__rblms_page_revalidate=1" target="_blank">superhuman AI</a> will come in time for him to retire, but he sees the singularity as the kind of science-fiction dystopia people love to fear. Jiang, likewise, isn’t concerned, at least for the time being. He still places a premium on human creativity.</p><p>Whether digital evolution defeats biological evolution is up for grabs. What’s uncontested is that evolution in any guise has surprises in store.</p>]]></description><pubDate>Thu, 26 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/evolutionary-ai-coding-agents</guid><category>Coding</category><category>Llms</category><category>Agentic ai</category><category>Ai agents</category><category>Singularity</category><category>Evolutionary algorithm</category><dc:creator>Matthew Hutson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conceptual-rendering-of-rodin-s-the-thinker-statue-used-to-represent-artificial-intelligence-processes.jpg?id=61098278&amp;width=980"></media:content></item><item><title>Superconducting Motor Could Propel Electric Aircraft</title><link>https://spectrum.ieee.org/electric-aircraft-motor-hinetics</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-group-of-11-men-watch-a-large-electric-motor-spin-a-propeller.jpg?id=61090759&width=1200&height=800&coordinates=0%2C139%2C0%2C139"/><br/><br/><p>Of the countless technologies invented over the past half century, high-temperature superconductors are among the most promising and yet also the most frustrating. Decades of research has yielded an assortment of materials that superconduct at temperatures as high as -140 °C (133 kelvins) at ambient pressure. And yet commercial applications have been elusive.</p><p>Now, though, a couple of developments could finally push <a href="https://spectrum.ieee.org/high-temperature-superconductors" target="_self">high-temperature superconductors</a> into commercial use. One is the availability, at relatively moderate cost, of copper-oxide-based superconducting tape, which is being produced by a few companies for <a href="https://spectrum.ieee.org/fusion-2662267312" target="_self">startups working on tokamak fusion reactors</a>. The reactors use the superconducting tape, which is typically made of yttrium barium copper oxide, in powerful electromagnets. The other development involves a different group of startups that are using the tape to build electric motors with very high power-to-weight ratios, mainly for use in electric aircraft.</p><p>Among that latter group of startups is <a href="https://hinetics.com/" rel="noopener noreferrer" target="_blank">Hinetics</a>, formed in 2017 to commercialize research led by <a href="https://ece.illinois.edu/about/directory/faculty/kharan" rel="noopener noreferrer" target="_blank">Kiruba Haran</a> at the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois</a> Urbana-Champaign. This past April, the company tested a prototype motor outfitted with superconducting rotor magnets. According to Haran, the tests, which included spinning a propeller in a laboratory setup, validated key components of the company’s designs for superconducting motors that will operate at power levels of 5 and 10 megawatts. Such levels would be high enough to power a regional passenger airliner with multiple motors. The work was funded in part by a grant from the <a href="https://arpa-e.energy.gov/" rel="noopener noreferrer" target="_blank">Advanced Research Projects Agency–Energy</a> (ARPA-E).</p><p>“HTS [high temperature superconductors] are having a moment, because the costs are coming down rapidly, driven by all the work on fusion,” Haran says. “A lot of people are ramping up production, and new startups, and new capabilities, are coming into the market.”</p><p>Hinetics is one of perhaps a dozen companies, large and small, trying to use high-temperature superconductors to build extremely efficient motors with very high power density. These include aerospace giant Airbus, which is working on a <a href="https://spectrum.ieee.org/airbus-electric-aircraft" target="_self">superconducting airliner</a> under a program called <a href="https://www.airbus.com/en/innovation/energy-transition/hydrogen/zeroe-our-hydrogen-powered-aircraft" rel="noopener noreferrer" target="_blank">ZEROe</a>, as well as Toshiba, Raytheon, and U.K. startup <a href="https://www.hyflux.aero/" rel="noopener noreferrer" target="_blank">HyFlux</a>. However, Hinetics is taking an unusual approach.</p><p>Common approaches to building a superconducting machine use the superconducting material for either the rotor or stator coils, or both. Typically, the coils are cooled with a liquid or gas kept at a sufficiently low temperature by an external cryocooling system. The fluid cools the superconducting coils by convection, by physically flowing through heat exchangers in contact with the coils and carrying away heat as it does so. The system has been used successfully in some experimental motors and generators, but it suffers from several fundamental problems. A big one is the need to circulate the cooling fluid through the rotor coils, which are embedded in a rotor assembly that is spinning at perhaps thousands of revolutions per minute. Another problem is that this approach requires a complicated cryocooling system that includes pumps, seals, gaskets, pipes, insulation, a rotary coupling that transfers the cryogen into and out of the rotor, and other components that can fail and that add considerable weight.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="An experimental electric motor is shown in a cutaway view." class="rm-shortcode" data-rm-shortcode-id="0c402a9027bc072395e4ec51432ee22c" data-rm-shortcode-name="rebelmouse-image" id="3601d" loading="lazy" src="https://spectrum.ieee.org/media-library/an-experimental-electric-motor-is-shown-in-a-cutaway-view.jpg?id=61106310&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">The rotor coils in an experimental Hinetics electric motor are made of a high-temperature superconductor. They are cooled by a cryocooler that runs axially down the center of the motor. The rotor assembly and the cryocooler are enclosed within a vacuum vessel.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Hinetics</small></p><h2>Hinetics’s Revolutionary Idea: Spin the Cryocooler</h2><p>Hinetics’s system, on the other hand, uses a self-contained cryocooler that is small enough to be attached to the rotor, and which spins along with it, eliminating the need to pass fluids into and out of a spinning vessel. With this arrangement, “you don’t have to immerse the superconductor into the fluid,” notes Laurent Pilon, an associate director for technology at ARPA-E. Instead, “there’s a cryocooler, and a cold connection, and you pull out the heat from the superconducting magnetic coils to the cryocooler, performing a refrigeration cycle. The beauty here is that it simplifies everything because now you just have the cryocooler that spins with the shaft.”</p><p>In this configuration, the rotor assembly, including the coils, is cooled by conduction rather than convection. The rotor is installed within a vacuum chamber. Heat from the superconducting magnet assembly is transferred through a “thermal bus,” which is basically just a disk-shaped copper structure that conducts the heat to the cryocooler, which is attached to the other side of the copper disk.</p><p>One of the challenges, Haran says, was finding a cryocooler small and light enough to spin at high rates and keep functioning while doing so. For its proof-of-concept unit, the Hinetics team used an off-the-shelf <a href="https://www.sunpowerinc.com/our-technologies/stirlingcycle" target="_blank">Stirling-cycle</a> cooler from <a href="https://www.sunpowerinc.com/" target="_blank">Sunpower</a>. It can remove only 10 watts of heat from the rotor assembly but, in this configuration, that’s all that’s needed to keep the rotor coils superconducting, Haran says.</p><p>One potential drawback of the system is that, because of this relatively low heat-removal capacity, the cryocooler takes a few hours to cool the superconducting magnet sufficiently to start operating. Future versions will reduce the period needed, according to Haran. And on the bright side, the low heat-removal rate means high efficiency, because the cooler has just enough power to maintain the low temperatures needed during operation, and not much excess capacity.</p><p>To provide electric power to the spinning cryostat and rotor magnets the prototype used a slip ring. But future versions of the motor will use a wireless system, possibly based on inductive coupling, Haran says.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="An experimental electric motor, painted black, is shown on a test bench with a three-bladed propeller attached to its shaft." class="rm-shortcode" data-rm-shortcode-id="27a58a415f9466df16ad222ba080f68b" data-rm-shortcode-name="rebelmouse-image" id="0744a" loading="lazy" src="https://spectrum.ieee.org/media-library/an-experimental-electric-motor-painted-black-is-shown-on-a-test-bench-with-a-three-bladed-propeller-attached-to-its-shaft.jpg?id=61104840&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Tests of Hinetics’s superconducting motor this past April validated the basic design and cleared the way for construction of  more powerful units.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Hinetics</small></p><h2>Applications on Ships Are Also Possible</h2><p>He opted not to make the stators superconducting, because in a typical configuration the stator is energized by an alternating-current (AC) waveform. Superconductors are only completely lossless for direct current. So the application of AC to superconducting coils in the stator would result in power losses that would require another cooling system to remove heat from the stator.</p><p>Haran figures it’s not necessary. With superconductors just in the rotor coils, the motor will achieve efficiencies in the range of 98 to 99.5 percent, which is about four or five percentage points higher than what is realistically possible with a permanent-magnet synchronous motor. Haran also insists that the superconducting design would attain this high efficiency without any reduction in power density, a combination that’s hard to achieve in a conventional motor.</p><p>Four or five percentage points might not seem like a lot, but it would matter in typical aviation applications, Pilon says, especially when coupled with higher power density. On its website, Hinetics claims that its motor has a continuous specific power of <a href="https://hinetics.com/" target="_blank">10 kilowatts per kilogram</a>, which would put the machine among the most power-dense units available, on a continuous-power basis. According to Haran, the next generation of the superconducting motor will achieve 40 kW/kg, which would be far higher than anything commercially available.</p><p>Although aviation is the initial target, Haran sees potential applications in ship propulsion, where the motor’s high volumetric power density would be a draw. “What’s really exciting is that we are seeing a transformational new technology become practical,” he says. “Once you get to megawatts and low speed, anywhere you need high torque, this could be very interesting.”</p>]]></description><pubDate>Thu, 26 Jun 2025 12:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/electric-aircraft-motor-hinetics</guid><category>Cryocooling system</category><category>Electric aircraft</category><category>Electric aircraft motors</category><category>Hightemperature superconductors</category><category>Ship propulsion</category><category>Superconducting motor</category><dc:creator>Glenn Zorpette</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-group-of-11-men-watch-a-large-electric-motor-spin-a-propeller.jpg?id=61090759&amp;width=980"></media:content></item><item><title>Estonia Debuts AI Chatbots for High School Classrooms</title><link>https://spectrum.ieee.org/estonia-ai-leap</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/abstract-ai-visualization-with-labeled-nodes-and-directional-arrows.png?id=61103737&width=1200&height=800&coordinates=0%2C24%2C0%2C24"/><br/><br/><p>Estonia has a reputation as one of the most digitally advanced nations in the world, thanks to its efficient <a href="https://spectrum.ieee.org/estonia-manages-legacy-it" target="_self">digital platforms for government services</a> and its startup-friendly culture. Its citizens’ digital prowess is largely due to the government’s decades-long campaign to bring technology into schools. Now, the government is launching <a href="https://www.aileap.ee/en" rel="noopener noreferrer" target="_blank">AI Leap 2025</a>, which will bring AI tools to an initial cohort of 20,000 high school students in September. <a href="https://www.linkedin.com/in/siimsikkut/" rel="noopener noreferrer" target="_blank">Siim Sikkut</a>, a former member of the Estonian government and part of the launch team, says the AI Leap program goes beyond just providing access to new technology. Its goal is to give students the skills they need to use it both ethically and effectively.</p><h3>Slim Sikkut</h3><br/><p><a href="https://www.linkedin.com/in/siimsikkut/" rel="noopener noreferrer" target="_blank">Siim Sikkut</a> served as the Estonian government’s chief information officer from 2017 to 2022, a role in which he created policies regarding digital government operations, cybersecurity, and connectivity. He is currently a managing partner at <a href="https://www.digitalnation.eu/" target="_blank">Digital Nation</a>, an Estonian consulting firm that works with governments around the world.<br/></p><p><strong>What was the Tiger Leap program, and how is it the model for what you’re doing now?</strong></p><p><strong>Siim Sikkut:</strong> Tiger Leap was a program in the ’90s to bring computers and Internet and basic digital skills to all the schools in Estonia. I myself got exposed to all things Internet, because at that time, we didn’t have a chance to use them at home. These guys and girls became the founders of industry and of digital government, so it allowed us to make a leap in building a digital society in Estonia. </p><p><strong>How does the AI Leap program follow that model?</strong></p><p><strong>Sikkut: </strong>Our thinking is now we have to do the same sort of leap and expose our younger generations to this next wave [of technology]. There are differences between the programs. Then it was, We’ll give you the access and the tools to do with what you like. Now, with AI tools, we feel it has to be a bit more curated. You need to learn to use them as opposed to just getting an easier way out of your homework. So it’s more of a skilling effort than just an access effort.</p><p><strong>What will this look like in practice? What tools will the students have access to?</strong></p><p><strong>Sikkut: </strong>We are still negotiating with the partners and vendors, so I won’t be naming companies. But fundamentally, we’re talking about a conversational AI assistant that is trained in the context of Estonian language and Estonian curriculum. It will be built for educational use, so it won’t be, for example, the <a href="https://spectrum.ieee.org/tag/chatgpt" target="_self">ChatGPT</a> that you and I would use in our daily life. It will support the learning more. For example, you don’t just submit your homework and get the answers back. In that scenario, the tool starts to tutor you more than give you an answer. We’re re-creating conversational AI as a learning assistant, and ideally we’ll have a lot of smaller subject-based apps added to that. We will have in place at least one tool, a conversation tool, and then we’ll build on that in the next few years.</p><p><strong>Will the teachers be able to see what the students are doing?</strong></p><p><strong>Sikkut: </strong>We might have to launch it first just with basics. But the idea is that we’ll have two apps, a teacher’s assistant and a student’s one, so teachers get feedback or recommendations on how to guide the particular student better. The idea is to make learning more personalized for better learning outcomes.</p><p><strong>When we hear about AI in education, there’s usually a doom and gloom attitude like, “This is going to ruin the minds of the next generation. There’s no way they’re going to learn anything. They’re just going to have these shortcuts.”</strong></p><p><strong>Sikkut: </strong>These same concerns led us to do more on this front. What’s really driving us are two very pragmatic considerations. A lot of kids use [AI tools] anyway to substitute thinking more than to complement it. We have numbers that 70 percent of kids in high school use them anyway. So the harmful use is already there, and we want to counter that. Secondly, there’s a <a href="https://spectrum.ieee.org/tag/digital-divide" target="_self">divide</a> in use, maybe for reasons of socioeconomic background. But Estonia’s whole education system is built on uniform opportunity. So this is also an attempt to make sure that we don’t increase the divide for the future.</p><p><strong>And what is the opportunity that you see for the students?</strong></p><p><strong>Sikkut: </strong>We’re making a bet that this is a competitiveness factor. If you’re not there, you’re left out. In the labor market, as a country, globally speaking, we’re saying, “Hey, look, you need to know how to get the most out of these tools.”</p><p><strong>The current program will provide tools to 10th and 11th graders, right?</strong></p><p><strong>Sikkut: </strong>Yes, we’re focusing on high school and vocational education now. But there’s still a debate going on: Should we go younger than that? The jury is still out on whether that would make sense. You need to have some independent thinking and study discipline and just be a self-driven learner. That doesn’t start early.</p><p><strong>Are there concerns about </strong><a href="https://spectrum.ieee.org/ai-hallucination" target="_self"><strong>hallucinations</strong></a><strong> and how to teach kids how to check for accuracy?</strong></p><p><strong>Sikkut: </strong>That goes right into the technical skill set of using these things. Unfortunately, hallucinations are a fact of life, and they will be for the time [being]. These AI skills will be taught by applying them in the rest of the curriculum. So in history class, as you use this AI study assistant, that’s where you learn about hallucinations and how to watch out for them.</p><p><strong>Have you talked about this program with teachers? Are they receptive or nervous?</strong></p><p><strong>Sikkut: </strong>It’s all of the above. As you can imagine, you have early adopters who are enthusiastic. Today, they’re already using these tools to plan for their class, or they run essays through an AI tool. On the other end, you have folks who have basic digital literacy, but they don’t want anything more than that. We’ll have a communication effort to make sure that the teachers are okay and calm about it. The main message we’re trying to tell teachers is that they won’t get the full suite of tools yet. They will all be part of an experimentation program.</p><p><em>A version of this article appears in the July 2025 issue as “5 Questions for Siim Sikkut.”</em><br/></p>]]></description><pubDate>Wed, 25 Jun 2025 12:33:36 +0000</pubDate><guid>https://spectrum.ieee.org/estonia-ai-leap</guid><category>5 questions</category><category>Chatbots</category><category>Type:departments</category><category>Estonia</category><category>Artificial intelligence</category><category>Education</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/abstract-ai-visualization-with-labeled-nodes-and-directional-arrows.png?id=61103737&amp;width=980"></media:content></item><item><title>IEEE Presidents’ Scholarship Changes Students’ Lives</title><link>https://spectrum.ieee.org/ieee-presidents-scholarship</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/amon-schumann-smiling-at-his-laboratory-desk.jpg?id=61097963&width=1200&height=800&coordinates=184%2C0%2C184%2C0"/><br/><br/><p>Last year marked the 25th anniversary of the <a href="https://www.ieee.org/education/preuniversity/scholarship.html" rel="noopener noreferrer" target="_blank">IEEE Presidents’ Scholarship</a>. Since its inception, the prestigious US $10,000 award has been given annually to one exceptional high school student participating in the <a href="https://www.societyforscience.org/isef/" rel="noopener noreferrer" target="_blank">Regeneron (formerly Intel) International Science and Engineering Fair</a>. The ISEF is the world’s largest international STEM research competition for high school students.</p><p>Finalists for the scholarship are selected by a team of IEEE volunteer judges. The scholarship is funded by the <a href="https://www.ieeefoundation.org/" rel="noopener noreferrer" target="_blank">IEEE Foundation</a> and administered by <a href="https://ea.ieee.org/ea-programs" rel="noopener noreferrer" target="_blank">IEEE Educational Activities</a>.</p><p>To commemorate the scholarship’s anniversary, I asked past winners how the award impacted their life and career, and what they are doing today.</p><h2>Harvard educator and a film producer</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Person wearing black shirt and turquoise necklace against a red brick wall background." class="rm-shortcode" data-rm-shortcode-id="d4c1c0b11c13c2fb4385c80fb7037201" data-rm-shortcode-name="rebelmouse-image" id="926c8" loading="lazy" src="https://spectrum.ieee.org/media-library/person-wearing-black-shirt-and-turquoise-necklace-against-a-red-brick-wall-background.jpg?id=61097970&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Elena Glassman received the scholarship in 2004 for her Brain-Computer Interface for the Muscularly Disabled project.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Elena Glassman</small></p><p><a href="https://seas.harvard.edu/person/elena-glassman" target="_blank"><strong>Elena Glassman</strong></a> received the scholarship in 2004 for her <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2004_president_scholarship_winner.pdf" target="_blank">Brain-Computer Interface for the Muscularly Disabled</a> project. She wrote code to collect <a href="https://spectrum.ieee.org/search/?q=EEG" target="_self">EEG</a> wavelets that predicted her own right or left arm movement with an accuracy rate of 73 percent.</p><p>Today Glassman is an assistant professor of computer science at <a href="https://seas.harvard.edu/person/elena-glassman" rel="noopener noreferrer" target="_blank">Harvard</a>, where she teaches human-computer interaction. She is also a new mother. The scholarship supported her education at <a href="https://www.mit.edu/" rel="noopener noreferrer" target="_blank">MIT</a>, where she earned a bachelor’s degree in electrical engineering. She says the scholarship was among the most memorable awards she received.</p><p>“When your project is being evaluated by IEEE judges who understand the work,” she says, “that’s what was so meaningful about receiving the award.” With encouragement from her father, a lifelong IEEE member, she submitted a <a href="https://ieeexplore.ieee.org/document/1519594" rel="noopener noreferrer" target="_blank">paper</a> about her project to <a href="https://www.embs.org/tbme/" rel="noopener noreferrer" target="_blank"><em><em>IEEE Transactions on Biomedical Engineering</em></em></a>, which published it.</p><p>In her current work, she says, she enjoys focusing on the “human side of programming.” She adds that her electrical engineering background is useful in tackling all sorts of projects.</p><p><a href="https://www.imdb.com/name/nm2186596/" rel="noopener noreferrer" target="_blank"><strong>Adam Sidman</strong></a><strong> </strong>received the 2005 scholarship for <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2005_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">Camera Stabilization: Take Two</a>. His project centered around the development of a <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2005_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">handheld servo-based device</a>. The film and TV producer in Los Angeles says his invention is a “go-to everyday technology for filmmakers on sets around the world.”</p><p>Sidman is chief executive of <a href="https://en.wikipedia.org/wiki/Timur_Bekmambetov" rel="noopener noreferrer" target="_blank">Timur Bekmambetov</a>’s production company, <a href="https://bazelevs.com/" rel="noopener noreferrer" target="_blank">Bazelevs</a>, where he has overseen a variety of movies including <a href="https://en.wikipedia.org/wiki/The_Current_War" rel="noopener noreferrer" target="_blank"><em>The Current War</em></a>, <a href="https://en.wikipedia.org/wiki/Hardcore_Henry" rel="noopener noreferrer" target="_blank"><em>Hardcore Henry</em></a><em><em>,</em></em> <a href="https://en.wikipedia.org/wiki/Searching_(film)" rel="noopener noreferrer" target="_blank"><em><em>Searching</em></em></a>, and <a href="https://en.wikipedia.org/wiki/Unfriended" rel="noopener noreferrer" target="_blank"><em><em>Unfriended</em></em></a>.</p><p>Receiving the scholarship was “a tremendous honor,” he says, “validating my passion to combine the arts and sciences.” He graduated from <a href="https://www.harvard.edu/" rel="noopener noreferrer" target="_blank">Harvard</a> with a bachelor’s degree in mechanical engineering and visual and environmental studies.</p><p>Last year he collaborated with ISEF organizers to establish a new category of projects, <a href="https://isef.net/viewAll?category=TECA-2024&page=home" rel="noopener noreferrer" target="_blank">Technology Enhances the Arts</a>, and he continues to serve as a judge.</p><h2>An entrepreneur and a gene researcher</h2><p><a href="https://www.linkedin.com/in/rpandey1234/" rel="noopener noreferrer" target="_blank"><strong>Rahul Kumar Pandey</strong></a>, a software engineer-turned-entrepreneur, received the scholarship in 2007. His startup, <a href="https://www.jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a>, helps software engineers navigate the professional world, providing advice on job searching, negotiation, promotions, and leadership. The platform boasts more than 100,000 users. Pandey is a writer for <a href="https://spectrum.ieee.org/tag/careers-newsletter" target="_self">IEEE’s <em><em>Careers</em></em> newsletter</a>.</p><p>The scholarship supported his degree in computer science at <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford</a>.</p><p>He credits his science-fair experience with giving him the confidence to innovate and advance the field.</p><p>His winning project, <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2009_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">A Microwave Metamaterial Lens With Negative Index of Refraction</a>, focused on building a lens array to transmit microwave signals, and it tested their behavior in terms of how the lens affected the propagation of electromagnetic waves.</p><p>“When I heard my name called, I couldn’t stop smiling,” he recalls, “because an organization like IEEE believed in me.”</p><p>Pandey advises high school students that “the world is your oyster, if you have curiosity. You don’t have to wait until you feel ready.”</p><p><a href="https://www.linkedin.com/in/hari-rallapalli/" rel="noopener noreferrer" target="_blank"><strong>Harikrishna “Hari” Rallapalli</strong></a><strong>,</strong> the 2008 scholarship recipient, is a research fellow at the U.S. <a href="https://www.ninds.nih.gov/" rel="noopener noreferrer" target="_blank">National Institute of Neurological Disorders and Stroke</a>, in Bethesda, Md.</p><p>Rallapalli says he plans to research techniques that enable gene expression imaging in humans, a method that allows for the visualization and quantification of the activity of specific genes.</p><p>His winning project, <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2008_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">Low-Cost Total Internal Reflection Fluorescence Microscopy</a>, focused on building a microscope for classrooms, both for demonstrations and student-level research.</p><p>The scholarship helped support his education at the <a href="https://www.ucdavis.edu/" rel="noopener noreferrer" target="_blank">University of California, Davis</a>, where he earned a bachelor’s degree in biomedical engineering.</p><p>“It felt amazing to have my work recognized by anyone, let alone an organization as prestigious as IEEE,” he says. “It was an early indication that I might cut it as a scientist.”</p><h2>Software engineer and a <em><em>Forbes</em></em> 30 Under 30 candidate</h2><p><a href="https://www.wimef.org/members/jessica-richeri" rel="noopener noreferrer" target="_blank"><strong>Jessica Richeri</strong></a>,<strong> </strong>the 2011 recipient, is a software design engineer at <a href="https://www.fluke.com/" rel="noopener noreferrer" target="_blank">Fluke</a> in Everett, Wash. She is designing and developing a supervisory control and data acquisition (SCADA) system for one of the company’s factories. The system collects data from equipment and creates analytics dashboards and reports.</p><p>Richeri’s winning entry, <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2011_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">Autonomous Robotic Vehicle: Saving Lives, Preventing Accidents, One at a Time</a>, centered on building a vehicle and software to support it. Ultimately, she says, her design and its use of sensors and software could be incorporated into vehicles to prevent traffic accidents.</p><p>“The scholarship meant the world [to me]. I felt so honored that I was chosen to receive the award for all the hard work I put into my project,” she says.</p><p>A year after receiving the scholarship, she was invited to the California Capitol, in Sacramento, to present her project and discuss promoting STEM fields with her U.S. representative.</p><p>The money supported her education at <a href="https://calbaptist.edu/" rel="noopener noreferrer" target="_blank">California Baptist University</a>, in Riverside, where she earned a bachelor’s degree in electrical and computer engineering.</p><p class="pull-quote"><span>“Winning the scholarship gave me confidence that my engineering passion could become a career. It was the start of my incredibly fun and exciting professional journey.” —2015 winner Alex Tacescu </span></p><p>She advises aspiring engineers that “the journey might be challenging, but the sense of accomplishment and the impact you’ll make in the world are more than worth the effort.”</p><p>The 2014 scholarship recipient, <a href="https://www.linkedin.com/in/georgemorgan2/?originalSubdomain=uk" target="_blank"><strong>George Morgan</strong></a>,<strong> </strong>presented <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2014_president_scholarship_winner.pdf" target="_blank">A Multi-Architectural Approach to the Development of Embedded Software</a>. His aim, he says, was to make hardware and software development more accessible. He transformed his project into a suite of development tools for embedded-systems engineers to expedite operations.</p><p>“I remember walking on stage and feeling the excitement of being recognized for my project,” Morgan says. “In that instant, all my hard work felt validated, and I knew someone understood the level of difficulty and commitment required to reach that point.”</p><p>The scholarship supported his education at the <a href="https://www.rit.edu/" rel="noopener noreferrer" target="_blank">Rochester Institute of Technology</a>, in New York, where he graduated with a bachelor’s degree in computer engineering in 2017.</p><p>He began working at <a href="https://www.tesla.com/" rel="noopener noreferrer" target="_blank">Tesla</a> in 2018 on the AI team, dealing with the software and hardware that powers the electric vehicles’ <a href="https://www.tesla.com/support/autopilot" rel="noopener noreferrer" target="_blank">autopilot</a>.</p><p>Recently named among<a href="https://www.forbes.com/profile/george-morgan/" rel="noopener noreferrer" target="_blank"> <em><em>Forbes</em></em>’ 30 Under 30</a> in AI, he founded <a href="https://www.symbolica.ai/" rel="noopener noreferrer" target="_blank">Symbolica</a>, a research-focused startup that develops foundational AI models and alternatives to the transformer architecture used in <a href="https://chatgpt.com/" rel="noopener noreferrer" target="_blank">ChatGPT</a>.</p><h2>Spaceflight engineer and a crash-prevention-system designer</h2><p><a href="https://alextac.com/" rel="noopener noreferrer" target="_blank"><strong>Alex Tacescu</strong></a> received the 2015 scholarship for <a href="https://alextac.com/projects/" rel="noopener noreferrer" target="_blank">Project Maverick</a> (now known as Mavdrive). The project let users <a href="https://www.ieee.org/content/dam/ieee-org/ieee/web/org/educ/2015_president_scholarship_winner.pdf" rel="noopener noreferrer" target="_blank">stand upright while moving around on a wheeled, motorized 0.6- by 0.6-meter platform</a>. It is similar to a Segway but more stable, with four wheels instead of two, and each powered by independently controlled motors. He says it serves as a pathway to learn new technologies including control engineering, robot autonomy, simulation, and AI-powered machine vision.</p><p>“Winning the scholarship gave me confidence that my engineering passion could become a career,” Tacescu says. “It was the start of my incredibly fun and exciting professional journey. I will never forget the moment I was called up on that stage.”</p><p>He earned a bachelor’s degree in science and robotics engineering and a master’s degree in robotics engineering, both from <a href="https://www.wpi.edu/" rel="noopener noreferrer" target="_blank">Worcester Polytechnic Institute</a>, in Massachusetts. He joined <a href="https://www.spacex.com/" rel="noopener noreferrer" target="_blank">SpaceX</a> as a Falcon flight software engineer and contributed to two groundbreaking missions. <a href="https://science.nasa.gov/mission/dart/" rel="noopener noreferrer" target="_blank">DART</a> was the first human-made item to measurably move a celestial body, and <a href="https://inspiration4.com/" rel="noopener noreferrer" target="_blank">Inspiration4</a> was the first all-civilian mission to orbit. <a href="https://www.netflix.com/title/81441273" rel="noopener noreferrer" target="_blank"><em>Countdown: Inspiration4 Mission to Space</em></a> is now on Netflix.</p><p>After nearly three years at SpaceX, Tacescu joined <a href="https://www.inversionspace.com/" rel="noopener noreferrer" target="_blank">Inversion Space</a> as a flight software engineer. The startup is focused on developing re-entry vehicles.</p><p>He advises high school seniors to “find their passion and keep at it. It’s going to be hard, and there will be very rough moments, but having it part of your passion makes it so much more fun, especially when those accomplishments start coming in.”</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" rel="float: left;" style="float: left;"> <img alt="Person with curly hair in a red polo shirt." class="rm-shortcode" data-rm-shortcode-id="49749fd3cd2cc7a6a920755efb95eefb" data-rm-shortcode-name="rebelmouse-image" id="0f8c4" loading="lazy" src="https://spectrum.ieee.org/media-library/person-with-curly-hair-in-a-red-polo-shirt.jpg?id=61097971&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">University student Kerem Bayhan, the 2021 scholarship recipient, won for a project focused on a system to help prevent underride car crashes, which occur when a vehicle collides with the rear or side of a large truck and gets stuck under it.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Kerem Bayhan</small></p><p><a href="https://www.linkedin.com/in/kerem-bayhan-13bb13249/" target="_blank"><strong>Kerem Bayhan</strong></a>, the 2021 scholarship recipient, won for a project focused on a <a href="https://spectrum.ieee.org/car-crash-prevention-system-wins-student-10000-ieee-scholarship" target="_self">system to help prevent underride car crashes</a>, which occur when a vehicle collides with the rear or side of a large truck and gets stuck under it.</p><p>Driven by a desire to make an impact on human lives, Bayhan says, he shifted his focus from engineering to medicine. He is currently a student at <a href="https://tip.hacettepe.edu.tr/en/introduction-88" target="_blank">Hacettepe University</a>, in Ankara, Türkiye.</p><p>“Winning the IEEE Presidents’ Scholarship award was an incredible and unexpected honor,” he says. “To have my project recognized by IEEE, one of the largest and most prestigious organizations in engineering, was immensely rewarding.”</p><p>He says he believes engineering skills are invaluable across many fields: “The analytical thinking, problem-solving abilities, and creativity at the core of engineering can set individuals apart, no matter what path they choose to follow.”</p><h2>Weather balloons, a glaucoma-detection device, and drones</h2><p><a href="https://spectrum.ieee.org/2022-ieee-presidents-scholarship-recipients?_gl=1*jgz4gs*_gcl_au*MTQ5MDg0NzMxLjE3MzE2MTU0OTk." target="_self"><strong>Amon Schumann</strong></a> was the 2022 scholarship recipient based on his <a href="https://isef.net/project/etsd005---small-radiosondes-on-a-great-mission" rel="noopener noreferrer" target="_blank">Small Radiosondes on a Great Mission</a> project, a <a href="https://spectrum.ieee.org/2022-ieee-presidents-scholarship-recipients?_gl=1*jgz4gs*_gcl_au*MTQ5MDg0NzMxLjE3MzE2MTU0OTk." target="_self">sustainable weather balloon</a> that is eco-friendly, cost-effective, and stays in the air longer than traditional ones. Now he’s studying electrical engineering at <a href="https://www.tu.berlin/en/" rel="noopener noreferrer" target="_blank">Technische Universität Berlin</a>, where he has discovered an interest in high-frequency technology and circuit design.</p><p>After receiving the scholarship, Schumann enhanced his ISEF project by adding a live-streaming camera and additional sensors.</p><p>“My system initially allowed for flights lasting a few weeks,” he says. “I’ve since refined it to enable the balloons to collect data in the stratosphere for several months.”</p><p>Winning the scholarship was a significant motivator, he says, as it validated the potential of his basement-developed project and showed its potential for broad impact.</p><p>“Receiving the scholarship gave me the opportunity to become deeply involved with IEEE,” he says, “opening doors to connect with key decision-makers in science and technology and leading to my student researcher role” at a Berlin-based research institute specializing in high-frequency electronics.</p><p><a href="https://www.nmoe.org/individual/rohan-kalia" rel="noopener noreferrer" target="_blank"><strong>Rohan Kalia</strong></a>, the 2023 scholarship recipient, is a senior at <a href="https://www.cobbk12.org/wheeler" rel="noopener noreferrer" target="_blank">Wheeler High School</a> in Marietta, Ga. For his winning project, he developed <a href="https://spectrum.ieee.org/ieee-presidents-scholarship-2023?_gl=1*jgz4gs*_gcl_au*MTQ5MDg0NzMxLjE3MzE2MTU0OTk." target="_self">EyePal</a>, an inexpensive tool for early glaucoma detection.</p><p>After receiving the scholarship, he continued to refine his project, enhancing its functionality.</p><p>“I open-sourced the parts, so if anyone wants to build on my device, they can,” Kalia says.</p><p>He was honored to receive the award, he says, knowing that past scholarship winners had made such creative projects.</p><p>“I really enjoyed the conversation with my interviewers,” he says. “We discussed the technical aspects of possible solutions and their trade-offs.”</p><p>He advises high school students to “keep an open mind” about their interests.</p><p>“Be curious about many different things,” he suggests, “as they connect in interesting ways. Once you find a topic you can’t stop thinking about, start a project to explore it.”</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" rel="float: left;" style="float: left;"> <img alt="Woman in black blazer with colorful lanyard and various pins in a park setting." class="rm-shortcode" data-rm-shortcode-id="ff75bf67bc52a4ba4e85db3d35fae1f6" data-rm-shortcode-name="rebelmouse-image" id="74040" loading="lazy" src="https://spectrum.ieee.org/media-library/woman-in-black-blazer-with-colorful-lanyard-and-various-pins-in-a-park-setting.jpg?id=61097985&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">High school senior Angelina Kim was the 2024 scholarship recipient for her Autonomous Scout and Rescue UAVs for Ocean Safety.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Angelina Kim</small></p><p><a href="https://www.linkedin.com/in/angelina-kim-b4508a268/" target="_blank"><strong>Angelina Kim</strong></a> was last year’s scholarship recipient. She is a high school senior at the <a href="https://www.bishops.com/" target="_blank">Bishop’s School</a> in La Jolla, Calif., where she is president of the <a href="https://www.linkedin.com/company/all-girls-stem-society-agss?trk=public_profile_topcard-current-company" rel="noopener noreferrer" target="_blank">All Girls STEM Society</a>, a nonprofit, student-led organization that holds free monthly workshops for girls in grades 3 to 8 across San Diego. She plans to study electrical engineering at <a href="https://www.mit.edu/" rel="noopener noreferrer" target="_blank">MIT</a>.</p><p>Kim won the scholarship for her <a href="https://isef.net/project/etsd037-autonomous-scout-and-rescue-uavs-for-ocean-safety" rel="noopener noreferrer" target="_blank">Autonomous Scout and Rescue UAVs for Ocean Safety</a>. She developed a drone that could survey the shoreline, taking photographs and analyzing them to identify rip currents.</p><p>To continue her work related to the project, she is chief executive of AngelTech, a startup dedicated to enhancing public safety through innovative technologies.</p><p>“Through AngelTech, I’m partnering with local lifeguards to deploy my lifeguard scout and rescue drones on nearby beaches,” she says.</p><p>She also is developing new technologies to enhance public safety, she says, including a synchronized display created through several device screens. She holds a patent for the invention.</p><p>She says she was thrilled to receive the scholarship because she knew it would help her develop valuable contacts within IEEE and provide support for her future research.</p><p>“I hope to use the connections I’ve made through the scholarship to share my research and networking experiences with fellow engineers and companies, and to serve as a mentor for young girls who have limited access to STEM resources,” she says.</p><p>An article about this year’s recipient is scheduled to be published in <em><em>The Institute </em></em>in August.</p>]]></description><pubDate>Tue, 24 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-presidents-scholarship</guid><category>Ieee member news</category><category>Ieee presidents’ scholarship</category><category>Stem</category><category>Students</category><category>Type:ti</category><dc:creator>Lynn Bowlby</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/amon-schumann-smiling-at-his-laboratory-desk.jpg?id=61097963&amp;width=980"></media:content></item><item><title>Toward Trustworthy AI: A Zero-Trust Framework for Foundational Models</title><link>https://content.knowledgehub.wiley.com/toward-trustworthy-ai-a-zero-trust-framework-for-foundational-models/</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/technology-innovation-institute-logo-in-purple-text-on-white-background.png?id=61098254&width=980"/><br/><br/><p>As foundational AI models grow in power and reach, they also expose new attack surfaces, vulnerabilities, and ethical risks. This white paper by the Secure Systems Research Center (SSRC) at the Technology Innovation Institute (TII) outlines a comprehensive framework to ensure security, resilience, and safety in large-scale AI models. By applying Zero-Trust principles, the framework addresses threats across training, deployment, inference, and post-deployment monitoring. It also considers geopolitical risks, model misuse, and data poisoning, offering strategies such as secure compute environments, verifiable datasets, continuous validation, and runtime assurance. The paper proposes a roadmap for governments, enterprises, and developers to collaboratively build trustworthy AI systems for critical applications.</p><p><span><a href="https://content.knowledgehub.wiley.com/toward-trustworthy-ai-a-zero-trust-framework-for-foundational-models/" target="_blank">Download this free whitepaper now!</a></span></p>]]></description><pubDate>Tue, 24 Jun 2025 17:52:19 +0000</pubDate><guid>https://content.knowledgehub.wiley.com/toward-trustworthy-ai-a-zero-trust-framework-for-foundational-models/</guid><category>Artificial intelligence</category><category>Large-scale ai model</category><category>Type:whitepaper</category><category>Vulnerabilities</category><dc:creator>Technology Innovation Institute</dc:creator><media:content medium="image" type="image/png" url="https://assets.rbl.ms/61098254/origin.png"></media:content></item><item><title>Another Plan to Test Satellite Deorbiting Takes Shape</title><link>https://spectrum.ieee.org/electrodynamic-tether-deborbit-satellite</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/gloved-hands-assembling-an-electronic-circuit-board-on-a-workbench.jpg?id=61083088&width=1200&height=800&coordinates=0%2C173%2C0%2C173"/><br/><br/><p>More and more satellites are being added to low Earth orbit (LEO) every month. As that number continues to increase, so do the risks of that <a href="https://spectrum.ieee.org/averting-space-doom-solving-the-orbital-junk-problem" target="_blank">critical area surrounding Earth becoming impassable</a>, trapping us on the planet for the foreseeable future. Ideas from different labs have presented potential solutions to this problem, but one of the most promising, <a href="https://en.wikipedia.org/wiki/Electrodynamic_tether" target="_blank">electrodynamic tethers</a> (EDTs), have only now begun to be tested in space. A new <a data-linked-post="2659652601" href="https://spectrum.ieee.org/cubesat" target="_blank">CubeSat</a> called the <a href="https://www.researchgate.net/publication/391689140_SPARCS_Design_and_Development_of_CubeSats_for_Advanced_Research_and_Cooperative_Studies" target="_blank">Spacecraft for Advanced Research and Cooperative Studies</a> (SPARCS) mission from researchers at the <a href="https://en.wikipedia.org/wiki/Sharif_University_of_Technology" target="_blank">Sharif University of Technology</a> in Tehran hopes to contribute to that effort by testing an EDT and intersatellite communication system as well as collecting real-time data on the radiation environment of its orbital path.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://www.universetoday.com/"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://www.universetoday.com/" target="_blank"><img alt='Universe Today logo; text reads "This post originally appeared on Universe Today."' class="rm-shortcode" data-rm-shortcode-id="087bc0ad57330681cee5d3354f3d2ac7" data-rm-shortcode-name="rebelmouse-image" id="b9dd2" loading="lazy" src="https://spectrum.ieee.org/media-library/universe-today-logo-text-reads-this-post-originally-appeared-on-universe-today.png?id=60568425&width=980"/></a></p><p>SPARCS actually consists of two separate CubeSats. SPARCS-A is a <a href="https://en.wikipedia.org/wiki/CubeSat#Design" target="_blank">1U CubeSat</a> primarily designed as a communications platform, with the mission design requiring it to talk to SPARCS-B, which is a 2U CubeSat that, in addition to the communication system, contains a EDT. That EDT, which can measure up to 12 meters in length, is deployed via a servomotor, with a camera watching to ensure proper deployment.</p><p>EDTs are essentially giant poles with electric current running through them. They use this current, and the tiny magnetic field it produces, to push off of the Earth’s natural magnetic sphere using a property called the Lorentz force. This allows the satellite to adjust its orbit without the use of fuel, simply by orienting its EDT in a specific direction (which the EDT itself can assist with) and then using the Lorentz force to either push it up into a higher orbit, or—more significant for the purposes for technology demonstration—to slow the CubeSat down to a point where it can make a controlled entry into the atmosphere.</p><h2>Why Are EDTs Important for Satellites?</h2><p><span>That controlled-entry feature is why EDTs have garnered so much attention. Previous missions, such as <a href="https://www.sciencedirect.com/science/article/abs/pii/S0094576520301429" target="_blank">KITE</a> from JAXA and <a href="https://mdp.engin.umich.edu/research_teams/mitee-21/" target="_blank">MiTEE</a> from the University of Michigan, have already attempted to use EDTs to change their orbits. Unfortunately neither of those missions successfully utilized their EDT, though a follow-up mission called MiTEE-2 is in the works with an even larger EDT than SPARCS.</span></p><p>The final piece of SPARCS’ kit is its dosimeter, which is intended to monitor the radiation environment of its orbit. As anyone familiar with spacecraft design knows, radiation hardening of electronics is absolutely critical to the success of a mission, but it is also expensive and time consuming, so it is best done at a minimal required level. Understanding the radiation environment of this popular orbital path can help future engineers make better, and hopefully less expensive, design decisions tailored to operation in this specific area.</p><p><span>Engineers have already finalized the design for the mission and have run simulations showing its expected operations. They have now moved on to building an engineering model of the two CubeSats, allowing them to validate their design and test the real-world implementation before it is ready for launch. Given the <a href="https://spectrum.ieee.org/the-real-story-of-stuxnet" target="_blank">current turmoil</a> in that region of the world, there is a chance that conflict could put a halt to development of this system. But, if successfully tested and launched, the very first demonstration of an EDT system could be deployed in the not-too-distant future.</span></p>]]></description><pubDate>Mon, 23 Jun 2025 16:38:43 +0000</pubDate><guid>https://spectrum.ieee.org/electrodynamic-tether-deborbit-satellite</guid><category>Cubesats</category><category>Orbital debris</category><category>Satellites</category><category>Kessler syndrome</category><dc:creator>Andy Tomaswick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/gloved-hands-assembling-an-electronic-circuit-board-on-a-workbench.jpg?id=61083088&amp;width=980"></media:content></item><item><title>Transforming Physical Substation Security</title><link>https://spectrum.ieee.org/meerkat-substation-security</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/aerial-of-a-power-station-with-highlighted-threat-zones-and-bad-actor-location.png?id=61081553&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><em>This is a sponsored article brought to you by <a href="https://meerkat.powereng.com/home?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB4" target="_blank">POWER Engineers, Member of WSP</a>.</em></p><p>Digital transformation is reshaping industries across the globe, and the power delivery sector is no exception. As demand for reliable and efficient energy supply continues to grow, the need to modernize and optimize operations becomes increasingly critical. By leveraging digital tools and technologies, utilities are unlocking unprecedented opportunities to enhance precision, efficiency and resilience throughout the power delivery value chain—from generation to distribution.</p><p>However, while digitalization offers transformative potential, the power delivery industry continues to grapple with substantial technical and operational challenges. Many utilities still operate with legacy or manual security protocols that rely on reactive rather than proactive strategies. The slow pace of technology adoption further compounds these issues, increasing the vulnerability of critical assets to inefficiencies, downtime and physical threats. Overcoming these obstacles requires a strategic shift toward innovative solutions that drive measurable improvements in safety, reliability and operational optimization.</p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="178ec3e93eff2ed8d3f8c00bbdb10be3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/D1Oishpaaps?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">Meerkat takes the guesswork out of substation security by integrating high-fidelity data with real-time 3D mitigation modeling. This sophisticated approach identifies all line-of-sight vulnerabilities, and delivers robust protection for critical infrastructure in an increasingly complex threat landscape.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: POWER Engineers, Member of WSP</small></p><h2>The Need for Digital Transformation in Physical Security</h2><p>Physical attacks on substations are becoming increasingly prevalent and sophisticated. As technology evolves, so do the bad actors that are trying to take down the grid. Many mitigation methods are no longer sufficient against modern methods of attack. These facilities, which are crucial to keeping the grid operational, must be able to comprehensively assess and adapt to new threats. Digital transformation is the key to this goal.</p><h3></h3><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Electric disturbance events by type, 2017-2023" class="rm-shortcode" data-rm-shortcode-id="980d5999fe51e7f2f205d16fe7ac7009" data-rm-shortcode-name="rebelmouse-image" id="abea8" loading="lazy" src="https://spectrum.ieee.org/media-library/electric-disturbance-events-by-type-2017-2023.png?id=60869959&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Physical breach events, defined here as physical attacks, vandalism, theft and suspicious activity, accounted for more than half of all electric disturbance events reported to the United States Department of Energy in 2023. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">POWER Engineers, Member of WSP</small></p><h3>Traditional Methods Fail to Meet Modern Demands</h3><p>Conventional site analysis methods in power delivery are often inefficient and prone to inaccuracies, particularly at substations, where the shortcomings can lead to significant vulnerabilities.</p><p>Physical site walkthroughs to identify areas of vulnerability, for example, are inherently subjective and susceptible to human error. Compounding matters, safety concerns in high-voltage environments, coordination challenges and access restrictions to areas not owned by the substation can result in incomplete assessments and evaluations fraught with delays.</p><p>Static analysis is also limited by outdated or erroneous publicly available data, hindering precise assessments and delaying decision-making processes. For instance, assets captured in publicly available data may misrepresent recent construction near the site, which may create new lines of sight to critical assets.</p><p class="pull-quote">Meerkat, developed by POWER Engineers, Member of WSP, leverages advanced technology to enhance threat assessment accuracy, significantly reducing assessment times, lowering mitigation costs and improving overall protection at substation facilities.</p><p>The Vulnerability of Integrated Security Analysis (VISA) method attempts to address some of these shortcomings by leveraging expert collaboration. Yet, it too has limitations—expertise variability among participants can lead to unrepresented perspectives, and reliance on static drawings and resources hampers effective visualization during sessions.</p><p>In contrast, some utilities opt for no analysis at all, erecting perimeter walls around facilities without pinpointing specific vulnerabilities. This approach often results in overbuilding and overspending while potentially leaving critical assets exposed due to overlooked threats from neighboring structures or terrain features.</p><p>Communication silos between stakeholders can also exacerbate these inefficiencies.</p><h2>It’s Time to Transform: Embrace Digital Solutions</h2><p>Emerging tools and technologies have the ability to address the longstanding inefficiencies in physical substation security.</p><h3>Enhance Precision and Efficiency</h3><p>Integrating cutting-edge technologies such as real-time data analytics and remote sensing, for example, can significantly enhance the precision and efficiency of security assessments. These tools provide dynamic insights into potential vulnerabilities, enabling proactive measures that adapt to emerging threats.</p><h3>Prioritize and Optimize Resources</h3><p>Transitioning from subjective assessments to data-backed evaluations ensures that decisions are grounded in accurate information rather than intuition alone. Robust datasets allow for thorough risk analyses that prioritize high-impact vulnerabilities while optimizing resource allocation.</p><h3>Implement Scalable Solutions</h3><p>Embrace flexible solutions capable of scaling with evolving infrastructure requirements or regulatory changes over time. This adaptability ensures continued relevance amidst shifting industry landscapes driven by technological advancements or policy shifts.</p><h2>Where to Start</h2><p>To solve the insufficiencies found within conventional site assessment methodologies, <a href="https://www.powereng.com/?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB3" target="_blank">POWER Engineers, Member of WSP</a>, designed a transformative threat assessment tool called <a href="https://meerkat.powereng.com/meerkat?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB2" target="_blank">Meerkat</a>. Meerkat harnesses high-quality data and advanced modeling techniques to deliver comprehensive vulnerability assessments customized to each unique facility. It is offered alongside an industry-leading team of experts who can help break down costs, explore alternative mitigations and address operational concerns.</p><p>Meerkat revolutionizes physical substation security by offering a more accurate and thorough analysis compared to conventional approaches. It mitigates the risk of human error inherent in manual inspections and overcomes access limitations through advanced remote sensing capabilities. Additionally, Meerkat facilitates seamless collaboration among stakeholders by providing dynamic, easily interpretable visualizations that enhance communication and decision-making processes. Analyses can even be performed in a secure, online workshop, allowing subject matter experts to skip the travel delays and jump right into the action.</p><p>By using Meerkat in substation security projects, utilities can transition from reactive to proactive strategies that anticipate and counter potential vulnerabilities before they are exploited. This shift not only ensures compliance with regulatory standards but also aligns security enhancements with financial objectives, ultimately safeguarding both assets and investments in a rapidly changing technological landscape.</p><h2>How it Works</h2><p class="shortcode-media shortcode-media-rebelmouse-image" style="margin: 0px;"> <img alt="Electric substation aerial view with security zones marked in red and blue sections." class="rm-shortcode" data-rm-shortcode-id="eea9c0502ee0a4fe21f966d74132f4d5" data-rm-shortcode-name="rebelmouse-image" id="3dc90" loading="lazy" src="https://spectrum.ieee.org/media-library/electric-substation-aerial-view-with-security-zones-marked-in-red-and-blue-sections.png?id=61081558&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">The Meerkat assessment features real-time mitigation modeling, optimizes camera placement, and identifies all vulnerabilities that could be exploited by malicious actors.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">POWER Engineers, Member of WSP</small></p><h3>Step One: Data Collection</h3><p>Meerkat starts with data collection. When pre-existing data of the site is available and of good quality and accuracy, it can be used for this process. However, when there is not sufficient data available, the Meerkat team collects its own high-fidelity data of the study area. This includes the substation facility, property and all surrounding terrain and infrastructure within an established radius of concern.</p><h3>Step Two: Build a Model</h3><p>Next, the high-quality data is transformed into an interactive 3D model in a virtual environment. The model is so accurate that it can facilitate virtual site visits. Users can navigate around the substation environment by clicking and dragging on screen and can visualize the site from any point ranging from a bird’s-eye view to the perspective of a potential bad actor looking into the station.</p><h3>Step Three: Test Mitigations in Real Time</h3><p>This interactive model serves as a virtual sandbox where mitigation strategies can be tested in real time. It can comprehensively and objectively map all line-of-sight vulnerabilities—big and small—that a bad actor might use to attack critical components. Then, existing or proposed mitigation strategies, if available, can be tested and validated within the system. This stage is great for testing what-if scenarios and seeing how multiple mitigations interact if combined before construction even comes into play.</p><h3>Step Four: Find the Best-Cost Solution</h3><p>POWER’s team of industry-leading experts use their knowledge to guide iterative solutions that bring substation owners and operators closer to the best-cost solutions for their substations. Sometimes moving or changing the height of a proposed wall is all it takes to drastically improve protections without drastically changing the price. A built-in cost estimator can also give a rough idea of how material costs change as the design does.</p><h2>The Benefits of Using Meerkat</h2><p>Meerkat is an industry-leading technology that offers unparalleled benefits in conducting thorough vulnerability assessments for critical assets at substations. By leveraging sophisticated algorithms and high-quality data, Meerkat delivers precise evaluations that pinpoint potential weaknesses with exceptional accuracy. This comprehensive approach means that every aspect of a substation’s physical security is meticulously analyzed, leaving no stone unturned.</p><h3>Enhanced Efficiency</h3><p>One of the key advantages of Meerkat is its ability to significantly enhance efficiency in the assessment process. This not only reduces the time and resources required for site assessments but also ensures consistent and reliable results.</p><p>Meerkat also allows an evaluation and design process that can sometimes take months of back-and-forth communication to happen in just a handful of hour-long workshops.</p><h3>Improved Accuracy</h3><p>Accuracy is another hallmark of Meerkat, as it eliminates the guesswork associated with human-based evaluations. By leveraging advanced modeling techniques, Meerkat provides actionable insights that empower utilities to make informed decisions regarding security upgrades and mitigations. This precision facilitates proactive risk management strategies, allowing stakeholders to address vulnerabilities before they manifest into tangible threats.</p><p>Ultimately, by improving both efficiency and accuracy in vulnerability assessments, Meerkat enables better decision-making processes that enhance overall risk management. Utilities can confidently implement targeted security measures tailored to each site’s unique needs, ensuring robust protection against emerging threats while optimizing resource allocation. In a landscape where rapid technological advancements challenge conventional practices, Meerkat stands as a vital tool for safeguarding critical infrastructure with foresight and precision.</p><h3>A Case Study: Strategic Security Optimization with Meerkat</h3><br/><p><em>The following case study has been sanitized of identifying information to maintain the security of the facility.</em></p><p><strong>Background</strong></p><p>A client faced a critical decision regarding the security of their substation, which was surrounded by a chain-link fence spanning 3,523 linear feet. Concerned about potential line-of-sight attacks on their critical assets, they planned to construct a new 15 ft tall concrete masonry unit (CMU) wall around the entire perimeter. Before proceeding with this significant investment, they sought validation from physical security experts at POWER and used the advanced threat assessment capabilities of Meerkat.</p><p><strong>Security Plan Validation</strong></p><p>To assess the effectiveness of the proposed security plan, Meerkat was employed to model the 15 ft wall within a highly accurate digital representation of the facility and its surroundings. The comprehensive data-backed threat assessment revealed lingering vulnerabilities despite the proposed construction. With estimated costs between $12 million and $15 million—and additional expenses for ballistic rated gates—the financial implications were substantial.</p><p><strong>Working Backward</strong></p><p>Recognizing that the original plan might not sufficiently mitigate risks, the client collaborated with Meerkat experts and key personnel across disciplines—including electrical engineers, civil engineers and transmission planners—to explore alternative strategies. Through a series of concise workshops over several days, they reimagined security designs by focusing on protecting critical assets identified as essential to system stability.</p><p>Meerkat enabled real-time modeling and testing of diverse mitigation strategies. Its interactive features allowed stakeholders to dynamically adjust protective measures—such as repositioning or resizing ballistic barriers—with immediate insights into effectiveness against vulnerabilities. This iterative process prioritized achieving the optimal balance between cost efficiency and robust protection.</p><p><strong>The Results</strong></p><p>Through strategic analysis using Meerkat, it became clear that constructing two separate 166 ft long, 25 ft tall walls at targeted locations around critical assets offered superior protection compared to encircling the entire perimeter with a single structure. This solution significantly enhanced security while reducing the estimated implementation costs to approximately $3.4 million—about a quarter of the cost of the initial projections.</p><p>Ultimately, the revised approach not only lowered risk profiles but also prevented unnecessary expenditure on inadequate defenses. By leveraging the advanced technology provided by Meerkat, the client successfully optimized resource allocation, comprehensively safeguarding their vital infrastructure.</p><h2><span>Get Started</span></h2><p>Any entity interested in learning more about Meerkat and its applications can request a free demonstration from our team of experts at <a href="https://meerkat.powereng.com/home?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEIAL" target="_blank">meerkat.powereng.com</a>.</p><div class="ieee-image-small"><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Meerkat Power Engineers logo in black and red font with a shield emblem above." class="rm-shortcode" data-rm-shortcode-id="89a8d2cf23d243a4b12f1249442c9201" data-rm-shortcode-name="rebelmouse-image" id="ed2d2" loading="lazy" src="https://spectrum.ieee.org/media-library/meerkat-power-engineers-logo-in-black-and-red-font-with-a-shield-emblem-above.png?id=61013898&width=980"/></p></div>]]></description><pubDate>Mon, 23 Jun 2025 12:22:12 +0000</pubDate><guid>https://spectrum.ieee.org/meerkat-substation-security</guid><category>Digital transformation</category><category>Substation security</category><category>Vulnerability assessments</category><category>Security analysis</category><category>Security</category><dc:creator>POWER Engineers, Member of WSP</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/aerial-of-a-power-station-with-highlighted-threat-zones-and-bad-actor-location.png?id=61081553&amp;width=980"></media:content></item><item><title>How the Rubin Observatory Will Reinvent Astronomy</title><link>https://spectrum.ieee.org/vera-rubin-observatory-first-images</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-researchers-inspect-a-large-telescope-system-inside-a-scientific-facility.jpg?id=61078903&width=1200&height=800&coordinates=0%2C951%2C0%2C952"/><br/><br/><p><strong>Night is falling</strong> on Cerro Pachón.</p><p class="shortcode-media shortcode-media-rebelmouse-image" style="display:none;"> <img alt="Observatory under Milky Way band at twilight, stars densely scatter across the clear sky." class="rm-shortcode" data-rm-shortcode-id="0c70c434a7ca2d1e8a14f7f386591855" data-rm-shortcode-name="rebelmouse-image" id="864d3" loading="lazy" src="https://spectrum.ieee.org/media-library/observatory-under-milky-way-band-at-twilight-stars-densely-scatter-across-the-clear-sky.jpg?id=61087574&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">A view of NSF-DOE Vera C. Rubin Observatory beneath the Milky Way galaxy.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">NSF-DOE Vera C. Rubin Observatory/H. Stockebrand</small></p><h3></h3><br/><p>Stray clouds reflect the last few rays of golden light as the sun dips below the horizon. I focus my camera across the summit to the westernmost peak of the mountain. Silhouetted within a dying blaze of red and orange light looms the sphinxlike shape of the <a href="https://rubinobservatory.org/" target="_blank">Vera C. Rubin Observatory</a>.</p><p>“Not bad,” says <a href="https://www.lsst.org/about/team/wil-omullane" target="_blank">William O’Mullane</a>, the observatory’s deputy project manager, amateur photographer, and master of understatement. We watch as the sky fades through reds and purples to a deep, velvety black. It’s my first night in Chile. For O’Mullane, and hundreds of other astronomers and engineers, it’s the culmination of years of work, as the Rubin Observatory is finally ready to go “on sky.”</p><p>Rubin is unlike any telescope ever built. Its exceptionally wide field of view, extreme speed, and massive digital camera will soon begin the 10-year Legacy Survey of Space and Time (<a href="https://rubinobservatory.org/explore/how-rubin-works/lsst" target="_blank">LSST</a>) across the entire southern sky. The result will be a high-resolution movie of how our solar system, galaxy, and universe change over time, along with hundreds of petabytes of data representing billions of celestial objects that have never been seen before.</p><p>Stars begin to appear overhead, and O’Mullane and I pack up our cameras. It’s astronomical twilight, and after nearly 30 years, it’s time for Rubin to get to work.</p><h3></h3><br/><img alt="Starry galaxy field with colorful spirals and nebulae in deep space." class="rm-shortcode" data-rm-shortcode-id="87e8a03ee6fa2cd051d2622335071e36" data-rm-shortcode-name="rebelmouse-image" id="73efd" loading="lazy" src="https://spectrum.ieee.org/media-library/starry-galaxy-field-with-colorful-spirals-and-nebulae-in-deep-space.jpg?id=61078937&width=980"/><p class="hide-on-mobile">On 23 June, the Vera C. Rubin Observatory released the first batch of images to the public. One of them, shown here, features a small section of the Virgo cluster of galaxies. Visible are two prominent spiral galaxies (lower right), three merging galaxies (upper right), several groups of distant galaxies, and many stars in the Milky Way galaxy. Created from over 10 hours of observing data, this image represents less than 2 percent of the field of view of a single Rubin image.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><img alt="Colorful nebulae in space with clouds of pink, blue, and dark dust against a starry background." class="rm-shortcode" data-rm-shortcode-id="73bf55d1507cd7faa1b698d5ce5ec867" data-rm-shortcode-name="rebelmouse-image" id="ed39f" loading="lazy" src="https://spectrum.ieee.org/media-library/colorful-nebulae-in-space-with-clouds-of-pink-blue-and-dark-dust-against-a-starry-background.jpg?id=61078975&width=980"/><p class="hide-on-mobile">A second image reveals clouds of gas and dust in the Trifid and Lagoon nebulae, located several thousand light-years from Earth. It combines 678 images taken by the Rubin Observatory over just seven hours, revealing faint details—like nebular gas and dust—that would otherwise be invisible.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><h2>Engineering the Simonyi Survey Telescope</h2><p>The top of Cerro Pachón is not a big place. Spanning about 1.5 kilometers at 2,647 meters of elevation, its three peaks are home to the Southern Astrophysical Research Telescope (<a href="https://noirlab.edu/public/programs/ctio/soar-telescope/" target="_blank">SOAR</a>), the <a href="https://noirlab.edu/public/programs/gemini-observatory/gemini-south/" target="_blank">Gemini South Telescope</a>, and for the last decade, the Vera Rubin Observatory construction site. An hour’s flight north of the Chilean capital of Santiago, these foothills of the Andes offer uniquely stable weather. The Humboldt Current flows just offshore, cooling the surface temperature of the Pacific Ocean enough to minimize atmospheric moisture, resulting in some of the best “seeing,” as astronomers put it, in the world.</p><h3></h3><br/><img alt="Map showing Vera C. Rubin Observatory in Chile, near La Serena and Santiago." class="rm-shortcode" data-rm-shortcode-id="81465e6874b0a2d4ca6b19dd26e60a43" data-rm-shortcode-name="rebelmouse-image" id="d419b" loading="lazy" src="https://spectrum.ieee.org/media-library/map-showing-vera-c-rubin-observatory-in-chile-near-la-serena-and-santiago.png?id=61079069&width=980"/><h3></h3><br/><p>It’s a complicated but exciting time to be visiting. It’s mid-April of 2025, and I’ve arrived just a few days before “first photon,” when light from the night sky will travel through the completed telescope and into its camera for the first time. In the control room on the second floor, engineers and astronomers make plans for the evening’s tests. O’Mullane and I head up into a high bay that contains the silvering chamber for the telescope’s mirrors and a clean room for the camera and its filters. Increasingly exhausting flights of stairs lead to the massive pier on which the telescope sits, and then up again into the dome.</p><p>I suddenly feel very, very small. The Simonyi Survey Telescope towers above us—350 tonnes of steel and glass, nestled within the 30-meter-wide, 650-tonne dome. One final flight of stairs and we’re standing on the telescope platform. In its parked position, the telescope is pointed at horizon, meaning that it’s looking straight at me as I step in front of it and peer inside.</p><h3></h3><br/><img alt="Modern observatory under a starry night sky on a rocky hilltop." class="rm-shortcode" data-rm-shortcode-id="e5d8ac1a3234928fae5a0e9d20b48c9b" data-rm-shortcode-name="rebelmouse-image" id="b09ed" loading="lazy" src="https://spectrum.ieee.org/media-library/modern-observatory-under-a-starry-night-sky-on-a-rocky-hilltop.jpg?id=61079018&width=980"/><h3></h3><br/><p>The telescope’s enormous 8.4-meter primary mirror is so flawlessly reflective that it’s essentially invisible. Made of a single piece of low-expansion borosilicate glass covered in a 120-nanometer-thick layer of pure silver, the huge mirror acts as two different mirrors, with a more pronounced curvature toward the center. Standing this close means that different reflections of the mirrors, the camera, and the structure of the telescope all clash with one another in a way that shifts every time I move. I feel like if I can somehow look at it in just the right way, it will all make sense. But I can’t, and it doesn’t.</p><h3></h3><br/><img alt="Diagram of a telescope with labeled mirrors, lenses, filters, and camera components." class="rm-shortcode" data-rm-shortcode-id="41ac3b3bcd8dff4b1ba021f7efe8cec3" data-rm-shortcode-name="rebelmouse-image" id="a0745" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-a-telescope-with-labeled-mirrors-lenses-filters-and-camera-components.png?id=61079019&width=980"/><p>I’m rescued from madness by O’Mullane snapping photos next to me. “Why?” I ask him. “You see this every day, right?”</p><p>“This has never been seen before,” he tells me. “It’s the first time, ever, that the lens cover has been off the camera since it’s been on the telescope.” Indeed, deep inside the nested reflections I can see a blue circle, the r-band filter within the camera itself. As of today, it’s ready to capture the universe.</p><h3></h3><br/><img alt="Two images show the inner parts of a telescope, with large mirrors and a camera housed inside a metal frame." class="rm-shortcode" data-rm-shortcode-id="f368f2c2e79daf918d78617bac9c9c4f" data-rm-shortcode-name="rebelmouse-image" id="e07ec" loading="lazy" src="https://spectrum.ieee.org/media-library/two-images-show-the-inner-parts-of-a-telescope-with-large-mirrors-and-a-camera-housed-inside-a-metal-frame.png?id=61079027&width=980"/><h3></h3><br/><img alt="Close-up of a large, complex astronomical telescope structure in an observatory." class="rm-shortcode" data-rm-shortcode-id="ebb33fd87f829faf8f8aba8f2fd2213c" data-rm-shortcode-name="rebelmouse-image" id="80ae6" loading="lazy" src="https://spectrum.ieee.org/media-library/close-up-of-a-large-complex-astronomical-telescope-structure-in-an-observatory.jpg?id=61079033&width=980"/><h3></h3><br/><img alt="Large telescope inside observatory dome against a bright starry night sky." class="rm-shortcode" data-rm-shortcode-id="986dd7896096599c527677a99bbfda40" data-rm-shortcode-name="rebelmouse-image" id="4be71" loading="lazy" src="https://spectrum.ieee.org/media-library/large-telescope-inside-observatory-dome-against-a-bright-starry-night-sky.jpg?id=61078913&width=980"/><h3></h3><br/><h2>Rubin’s Wide View Unveils the Universe</h2><p>Back down in the control room, I find director of construction Željko Ivezić. He’s just come up from the summit hotel, which has several dozen rooms for lucky visitors like myself, plus a few even luckier staff members. The rest of the staff commutes daily from the coastal town of La Serena, a 4-hour round trip.</p><p>To me, the summit hotel seems luxurious for lodgings at the top of a remote mountain. But Ivezić has a slightly different perspective. “The European-funded telescopes,” he grumbles, “have swimming pools at their hotels. And they serve wine with lunch! Up here, there’s no alcohol. It’s an American thing.” He’s referring to the fact that <a href="https://nsf-gov-resources.nsf.gov/2023-03/37_fy2024.pdf.pdf" rel="noopener noreferrer" target="_blank">Rubin is primarily funded</a> by the U.S. <a href="https://www.nsf.gov/" target="_blank">National Science Foundation</a> and the U.S. Department of Energy’s <a href="https://www.energy.gov/science/office-science" target="_blank">Office of Science</a>, which have strict safety requirements.</p><h3></h3><br/><img alt="Silhouetted telescope under a starry sky and vibrant, colorful sunset." class="rm-shortcode" data-rm-shortcode-id="b492254ba5a5b929bfc315a49a32eb62" data-rm-shortcode-name="rebelmouse-image" id="69e31" loading="lazy" src="https://spectrum.ieee.org/media-library/silhouetted-telescope-under-a-starry-sky-and-vibrant-colorful-sunset.jpg?id=61079034&width=980"/><h3></h3><br/><p>Originally, Rubin was intended to be a dark-matter survey telescope, to search for the 85 percent of the mass of the universe that we know exists but can’t identify. In the 1970s, astronomer <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5338491/" rel="noopener noreferrer" target="_blank">Vera C. Rubin</a> pioneered a spectroscopic method to measure the speed at which stars orbit around the centers of their galaxies, revealing motion that could be explained only by the presence of a halo of invisible mass at least five times the apparent mass of the galaxies themselves. Dark matter can warp the space around it enough that galaxies act as lenses, bending light from even more distant galaxies as it passes around them. It’s this gravitational lensing that the Rubin observatory was designed to detect on a massive scale. But once astronomers considered what else might be possible with a survey telescope that combined enormous light-collecting ability with a wide field of view, Rubin’s science mission rapidly expanded beyond dark matter.</p><p>Trading the ability to focus on individual objects for a wide field of view that can see tens of thousands of objects at once provides a critical perspective for understanding our universe, says Ivezić. Rubin will complement other observatories like the <a href="https://spectrum.ieee.org/hubble-space-telescope-re-invention" target="_self">Hubble Space Telescope</a> and the <a href="https://spectrum.ieee.org/collections/james-webb-telescope/" target="_self">James Webb Space Telescope</a>. <a href="https://esahubble.org/about/general/instruments/wfc3/" rel="noopener noreferrer" target="_blank">Hubble’s Wide Field Camera 3</a> and <a href="https://jwst-docs.stsci.edu/jwst-near-infrared-camera#gsc.tab=0" rel="noopener noreferrer" target="_blank">Webb’s Near Infrared Camera</a> have fields of view of less than 0.05 square degrees each, equivalent to just a few percent of the size of a full moon. The upcoming <a href="https://spectrum.ieee.org/rogue-planet" target="_self">Nancy Grace Roman Space Telescope</a> will see a bit more, with a field of view of about one full moon. Rubin, by contrast, can image 9.6 square degrees at a time—about 45 full moons’ worth of sky.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/rogue-planet" target="_self">A Trillion Rogue Planets and Not One Sun to Shine on Them</a></p><p>That ultrawide view offers essential context, Ivezić explains. “My wife is American, but I’m from Croatia,” he says. “Whenever we go to Croatia, she meets many people. I asked her, ‘Did you learn more about Croatia by meeting many people very superficially, or because you know me very well?’ And she said, ‘You need both. I learn a lot from you, but you could be a weirdo, so I need a control sample.’ ” Rubin is providing that control sample, so that astronomers know just how weird whatever they’re looking at in more detail might be.</p><h3>Explore Rubin Observatory’s First Images With Skyviewer</h3><br/><iframe allowed="fullscreen; clipboard-write; clipboard-read; web-share" sandbox="allow-downloads allow-scripts allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-forms" src="https://skyviewer.app/embed" style="border: none; aspect-ratio: 16 / 9; min-height: 500px; max-height: 100%"></iframe><p>Rubin Observatory’s <a href="https://skyviewer.app/" target="_blank">Skyviewer</a> app lets you explore its stunning first images by interactively navigating a vast, detailed view of the cosmos — you can zoom in and out and move around to examine the rich tapestry of stars and galaxies in extraordinary detail. The area observed includes the southern region of the Virgo Cluster — approximately 55 million light-years from Earth — as well as closer stars in the Milky Way and much more distant galaxy groups. This image, built from over 3 trillion pixels of data collected in just seven nights, contains millions of galaxies. Eventually, the full <a href="https://rubinobservatory.org/explore/how-rubin-works/lsst" target="_blank">Legacy Survey of Space and Time (LSST)</a> will catalog about 20 billion galaxies of all types, and from all times in the history of the Universe.<br/></p><h3></h3><br><p>Every night, the telescope will take a thousand images, one every 34 seconds. After three or four nights, it’ll have the entire southern sky covered, and then it’ll start all over again. After a decade, Rubin will have taken more than 2 million images, generated 500 petabytes of data, and visited every object it can see at least 825 times. In addition to identifying an estimated 6 million bodies in our solar system, 17 billion stars in our galaxy, and 20 billion galaxies in our universe, Rubin’s rapid cadence means that it will be able to delve into the time domain, tracking how the entire southern sky changes on an almost daily basis.</p><h3></h3><br/><h2>Cutting-Edge Technology Behind Rubin’s Speed</h2><p>Achieving these science goals meant pushing the technical envelope on nearly every aspect of the observatory. But what drove most of the design decisions is the speed at which Rubin needs to move (3.5 degrees per second)—the phrase most commonly used by the Rubin staff is “crazy fast.”</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="4aa52bc458fa1bc86c7aa06457d17e9d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8kF9PrMXqBU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p>Crazy fast movement is why the telescope looks the way it does. The squat arrangement of the mirrors and camera centralizes as much mass as possible. Rubin’s oversize supporting pier is mostly steel rather than mostly concrete so that the movement of the telescope doesn’t twist the entire pier. And then there’s the megawatt of power required to drive this whole thing, which comes from huge banks of capacitors slung under the telescope to prevent a brownout on the summit every 30 seconds all night long.</p><p>Rubin is also unique in that it utilizes the largest digital camera ever built. The size of a small car and weighing 2,800 kilograms, the LSST camera captures 3.2-gigapixel images through six swappable color filters ranging from near infrared to near ultraviolet. The camera’s focal plane consists of 189 4K-by-4K charge-coupled devices grouped into 21 “rafts.” Every CCD is backed by 16 amplifiers that each read 1 million pixels, bringing the readout time for the entire sensor down to 2 seconds flat.</p><h3></h3><br/><img alt="Technician examines a large telescope camera in a clean room environment." class="rm-shortcode" data-rm-shortcode-id="73a69383c750509ec15ecca488fb763d" data-rm-shortcode-name="rebelmouse-image" id="16585" loading="lazy" src="https://spectrum.ieee.org/media-library/technician-examines-a-large-telescope-camera-in-a-clean-room-environment.jpg?id=61079036&width=980"/><h3></h3><br/><h2>Astronomy in the Time Domain</h2><p>As humans with tiny eyeballs and short lifespans who are more or less stranded on Earth, we have only the faintest idea of how dynamic our universe is. To us, the night sky seems mostly static and also mostly empty. This is emphatically not the case.</p><p>In 1995, the Hubble Space Telescope pointed at a small and deliberately unremarkable part of the sky for a cumulative six days. The resulting image, called the <a href="https://science.nasa.gov/mission/hubble/science/universe-uncovered/hubble-deep-fields/" target="_blank">Hubble Deep Field</a>, revealed about 3,000 distant galaxies in an area that represented just one twenty-four-millionth of the sky. To observatories like Hubble, and now Rubin, the sky is crammed full of so many objects that it becomes a problem. As O’Mullane puts it, “There’s almost nothing not touching something.”</p><p>One of Rubin’s biggest challenges will be deblending—­identifying and then separating things like stars and galaxies that appear to overlap. This has to be done carefully by using images taken through different filters to estimate how much of the brightness of a given pixel comes from each object.</p><h3></h3><br/><img alt="Exploded diagram of a large telescope camera, with labeled parts including lens, shutter, filters, and a 3.2-gigapixel CCD." class="rm-shortcode" data-rm-shortcode-id="9b28eeff16dde98dae9b8fa724a5930b" data-rm-shortcode-name="rebelmouse-image" id="402f3" loading="lazy" src="https://spectrum.ieee.org/media-library/exploded-diagram-of-a-large-telescope-camera-with-labeled-parts-including-lens-shutter-filters-and-a-3-2-gigapixel-ccd.png?id=61079041&width=980"/><h3></h3><br/><p>At first, Rubin won’t have this problem. At each location, the camera will capture one 30-second exposure before moving on. As Rubin returns to each location every three or four days, subsequent exposures will be combined in a process called coadding. In a coadded image, each pixel represents all of the data collected from that location in every previous image, which results in a much longer effective exposure time. The camera may record only a few photons from a distant galaxy in each individual image, but a few photons per image added together over 825 images yields much richer data. By the end of Rubin’s 10-year survey, the coadding process will generate images with as much detail as a typical Hubble image, but over the entire southern sky. A few lucky areas called “<a href="https://survey-strategy.lsst.io/baseline/ddf.html" rel="noopener noreferrer" target="_blank">deep drilling fields</a>” will receive even more attention, with each one getting a staggering 23,000 images or more.</p><p>Rubin will add every object that it detects to its catalog, and over time, the catalog will provide a baseline of the night sky, which the observatory can then use to identify changes. Some of these changes will be movement—Rubin may see an object in one place, and then spot it in a different place some time later, which is how objects like <a href="https://spectrum.ieee.org/planetary-defense-killer-asteroids" target="_self">near-Earth asteroids</a> will be detected. But the vast majority of the changes will be in brightness rather than movement.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/planetary-defense-killer-asteroids" target="_self">Three Steps to Stopping Killer Asteroids</a></p><h3></h3><br/><img alt="A circle with grid lines overlaying a night sky background with stars and a full moon." class="rm-shortcode" data-rm-shortcode-id="3da7febf385d5d7e3565ead376c2b3fb" data-rm-shortcode-name="rebelmouse-image" id="94e9a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-circle-with-grid-lines-overlaying-a-night-sky-background-with-stars-and-a-full-moon.png?id=61079047&width=980"/><h3></h3><br/><p>Every image that Rubin collects will be compared with a baseline image, and any change will automatically generate a software alert within 60 seconds of when the image was taken. Rubin’s wide field of view means that there will be a lot of these alerts—on the order of 10,000 per image, or 10 million alerts per night. Other automated systems will manage the alerts. Called alert brokers, they ingest the alert streams and filter them for the scientific community. If you’re an astronomer interested in Type Ia supernovae, for example, you can subscribe to an alert broker and set up a filter so that you’ll get notified when Rubin spots one.</p><p>Many of these alerts will be triggered by variable stars, which cyclically change in brightness. Rubin is also expected to identify somewhere <a href="https://www.lsst.org/science/transient-optical-sky/supernovae" target="_blank">between 3 million and 4 million supernovae</a>—that works out to over a thousand new supernovae for every night of observing. And the rest of the alerts? Nobody knows for sure, and that’s why the alerts have to go out so quickly, so that other telescopes can react to make deeper observations of what Rubin finds.</p><h3></h3><br/><h2>Managing Rubin’s Vast Data Output</h2><p>After the data leaves Rubin’s camera, most of the processing will take place at the <a href="https://www6.slac.stanford.edu/" target="_blank">SLAC National Accelerator Laboratory</a> in Menlo Park, Calif., over 9,000 kilometers from Cerro Pachón. It takes less than 10 seconds for an image to travel from the focal plane of the camera to SLAC, thanks to a 600-gigabit fiber connection from the summit to La Serena, and from there, a dedicated 100-gigabit line and a backup 40-gigabit line that connect to the Department of Energy’s science network in the United States. The 20 terabytes of data that Rubin will produce nightly makes this bandwidth necessary. “There’s a new image every 34 seconds,” O’Mullane tells me. “If I can’t deal with it fast enough, I start to get behind. So everything has to happen on the cadence of half a minute if I want to keep up with the data flow.”</p><p>At SLAC, each image will be calibrated and cleaned up, including the removal of satellite trails. Rubin will see a lot of satellites, but since the satellites are unlikely to appear in the same place in every image, the impact on the data is expected to be minimal when the images are coadded. The processed image is compared with a baseline image and any alerts are sent out, by which time processing of the next image has already begun.</p><h3></h3><br/><img alt="Numerous thick cables hang in an industrial setting, surrounded by blue metal scaffolding." class="rm-shortcode" data-rm-shortcode-id="269376d383325b40dbf8a0d5cb7bbf6a" data-rm-shortcode-name="rebelmouse-image" id="2c0ac" loading="lazy" src="https://spectrum.ieee.org/media-library/numerous-thick-cables-hang-in-an-industrial-setting-surrounded-by-blue-metal-scaffolding.jpg?id=61079049&width=980"/><h3></h3><br/><p>As Rubin’s catalog of objects grows, astronomers <a href="https://dmtn-243.lsst.io/" rel="noopener noreferrer" target="_blank">will be able to query it</a> in all kinds of useful ways. Want every image of a particular patch of sky? No problem. All the galaxies of a certain shape? A little trickier, but sure. Looking for 10,000 objects that are similar in some dimension to 10,000 other objects? That might take a while, but it’s still possible. Astronomers can even run their own code on the raw data.</p><p>“Pretty much everyone in the astronomy community wants something from Rubin,” O’Mullane explains, “and so they want to make sure that we’re treating the data the right way. All of our code is public. It’s on <a href="https://github.com/lsst" target="_blank">GitHub</a>. You can see what we’re doing, and if you’ve got a better solution, we’ll take it.”</p><p>One better solution may involve AI. “I think as a community we’re struggling with how we do this,” says O’Mullane. “But it’s probably something we ought to do—curating the data in such a way that it’s consumable by machine learning, providing foundation models, that sort of thing.”</p><p>The data management system is arguably as much of a critical component of the Rubin observatory as the telescope itself. While most telescopes make targeted observations that get distributed to only a few astronomers at a time, Rubin will make its data available to everyone within just a few days, which is a completely different way of doing astronomy. “We’ve essentially promised that we will take every image of everything that everyone has ever wanted to see,” explains <a href="https://www.lsst.org/content/kevin-reil" rel="noopener noreferrer" target="_blank">Kevin Reil</a>, Rubin observatory scientist. “If there’s data to be collected, we will try to collect it. And if you’re an astronomer somewhere, and you want an image of something, within three or four days we’ll give you one. It’s a colossal challenge to deliver something on this scale.”</p><h3></h3><br/><img alt="Animated image on the left shows an automated mechanism that switches color filters; an image on the right shows how each filter affects the exposures of stars and galaxies." class="rm-shortcode" data-rm-shortcode-id="ac1caf92f248603bd9f1bdf40716d16c" data-rm-shortcode-name="rebelmouse-image" id="21741" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-image-on-the-left-shows-an-automated-mechanism-that-switches-color-filters-an-image-on-the-right-shows-how-each-filter.gif?id=61079050&width=980"/><h3></h3><br/><p>The more time I spend on the summit, the more I start to think that the science that we know Rubin will accomplish may be the least interesting part of its mission. And despite their best efforts, I get the sense that everyone I talk to is wildly understating the impact it will have on astronomy. The sheer volume of objects, the time domain, the 10 years of coadded data—what new science will all of that reveal? Astronomers have no idea, because we’ve never looked at the universe in this way before. To me, that’s the most fascinating part of what’s about to happen.</p><p>Reil agrees. “You’ve been here,” he says. “You’ve seen what we’re doing. It’s a paradigm shift, a whole new way of doing things. It’s still a telescope and a camera, but we’re changing the world of astronomy. I don’t know how to capture—I mean, it’s the people, the intensity, the awesomeness of it. I want the world to understand the beauty of it all.”</p><h3></h3><br/><h2>The Intersection of Science and Engineering</h2><p>Because nobody has built an observatory like Rubin before, there are a lot of things that aren’t working exactly as they should, and a few things that aren’t working at all. The most obvious of these is the dome. The capacitors that drive it blew a fuse the day before I arrived, and the electricians are off the summit for the weekend. The dome shutter can’t open either. Everyone I talk to takes this sort of thing in stride—they have to, because they’ve been troubleshooting issues like these for years.</p><p>I sit down with <a href="https://kipac.stanford.edu/people/yousuke-utsumi" target="_blank">Yousuke Utsumi</a>, a camera operations scientist who exudes the mixture of excitement and exhaustion that I’m getting used to seeing in the younger staff. “Today is amazingly quiet,” he tells me. “I’m happy about that. But I’m also really tired. I just want to sleep.”</p><p>Just yesterday, Utsumi says, they managed to finally solve a problem that the camera team had been struggling with for weeks—an intermittent fault in the camera cooling system that only seemed to happen when the telescope was moving. This was potentially a very serious problem, and Utsumi’s phone would alert him every time the fault occurred, over and over again in the middle of the night. The fault was finally traced to a cable within the telescope’s structure that used pins that were slightly too small, leading to a loose connection.</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="6e7db8e07af90b232c49a6b7a5a89175" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/QRr7wpLXas8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p>Utsumi’s contract started in 2017 and was supposed to last three years, but he’s still here. “I wanted to see first photon,” he says. “I’m an astronomer. I’ve been working on this camera so that it can observe the universe. And I want to see that light, from those photons from distant galaxies.” This is something I’ve also been thinking about—those lonely photons traveling through space for billions of years, and within the coming days, a lucky few of them will land on the sensors Utsumi has been tending, and we’ll get to see them. He nods, smiling. “I don’t want to lose one, you know?”</p><h3></h3><br/><img alt="Illuminated telescope interior with vibrant blue and red hues, showcasing intricate machinery." class="rm-shortcode" data-rm-shortcode-id="c00643cc41a4fbfecec072e87cfff410" data-rm-shortcode-name="rebelmouse-image" id="9d141" loading="lazy" src="https://spectrum.ieee.org/media-library/illuminated-telescope-interior-with-vibrant-blue-and-red-hues-showcasing-intricate-machinery.jpg?id=61079051&width=980"/><h3></h3><br/><p>Rubin’s commissioning scientists have a unique role, working at the intersection of science and engineering to turn a bunch of custom parts into a functioning science instrument. Commissioning scientist <a href="https://www.linkedin.com/in/marina-pavlovic-6489831a2/" target="_blank">Marina Pavlovic</a> is a postdoc from Serbia with a background in the formation of supermassive black holes created by merging galaxies. “I came here last year as a volunteer,” she tells me. “My plan was to stay for three months, and 11 months later I’m a commissioning scientist. It’s crazy!”</p><h3></h3><br/><img alt="Technicians in clean suits handling a large metallic component in a laboratory." class="rm-shortcode" data-rm-shortcode-id="630da2a002eb632655116dbcb4585422" data-rm-shortcode-name="rebelmouse-image" id="8d2f5" loading="lazy" src="https://spectrum.ieee.org/media-library/technicians-in-clean-suits-handling-a-large-metallic-component-in-a-laboratory.jpg?id=61079052&width=980"/><h3></h3><br/><p>Pavlovic’s job is to help diagnose and troubleshoot whatever isn’t working quite right. And since most things aren’t working quite right, she’s been very busy. “I love when things need to be fixed because I am learning about the system more and more every time there’s a problem—every day is a new experience here.”</p><p>I ask her what she’ll do next, once Rubin is up and running. “If you love commissioning instruments, that is something that you can do for the rest of your life, because there are always going to be new instruments,” she says.</p><p>Before that happens, though, Pavlovic has to survive the next few weeks of going on sky. “It’s going to be so emotional. It’s going to be the beginning of a new era in astronomy, and knowing that <em>you</em> did it, that <em>you</em> made it happen, at least a tiny percent of it, that will be a priceless moment.”</p><p>“I had to learn how to calm down to do this job,” she admits, “because sometimes I get too excited about things and I cannot sleep after that. But it’s okay. I started doing yoga, and it’s working.”</p><h3></h3><br/><h2>From First Photon to First Light</h2><p>My stay on the summit comes to an end on 14 April, just a day before first photon, so as soon as I get home I check in with some of the engineers and astronomers that I met to see how things went. <a href="https://www.linkedin.com/in/gmegiashomar/" target="_blank">Guillem Megias Homar</a> manages the adaptive optics system—232 actuators that flex the surfaces of the telescope’s three mirrors a few micrometers at a time to bring the image into perfect focus. Currently working on his Ph.D., he was born in 1997, one year after the Rubin project started.</p><p>First photon, for him, went like this: “I was in the control room, sitting next to the camera team. We have a microphone on the camera, so that we can hear when the shutter is moving. And we hear the first click. And then all of a sudden, the image shows up on the screens in the control room, and it was just an explosion of emotions. All that we have been fighting for is finally a reality. We are on sky!” There were toasts (with sparkling apple juice, of course), and enough speeches that Megias Homar started to get impatient: “I was like, when can we start working? But it was only an hour, and then everything became much more quiet.”</p><h3></h3><br/><img alt="Dense galaxy cluster with diverse stars and galaxies scattered across the dark universe background." class="rm-shortcode" data-rm-shortcode-id="0af096dd8e983460a936fb9998613789" data-rm-shortcode-name="rebelmouse-image" id="f82d6" loading="lazy" src="https://spectrum.ieee.org/media-library/dense-galaxy-cluster-with-diverse-stars-and-galaxies-scattered-across-the-dark-universe-background.jpg?id=61079148&width=980"/><p class="hide-on-mobile">Another newly released image showing a small section of the Rubin Observatory’s total view of the Virgo cluster of galaxies. Visible are bright stars in the Milky Way galaxy shining in the foreground, and many distant galaxies in the background.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><p>“It was satisfying to see that everything that we’d been building was finally working,” <a href="https://www.lsst.org/about/team/lsst-project-manager" target="_blank">Victor Krabbendam</a>, project manager for Rubin construction, tells me a few weeks later. “But some of us have been at this for so long that first photon became just one of many firsts.” Krabbendam has been with the observatory full-time for the last 21 years. “And the very moment you succeed with one thing, it’s time to be doing the next thing.”</p><h3></h3><br/><img alt="Group of people seated in office chairs look at a screen (not shown) and smile slightly, with one person covering their mouth with their hands." class="rm-shortcode" data-rm-shortcode-id="86f08e956bbe0f9860250a08b5ff72fe" data-rm-shortcode-name="rebelmouse-image" id="5e282" loading="lazy" src="https://spectrum.ieee.org/media-library/group-of-people-seated-in-office-chairs-look-at-a-screen-not-shown-and-smile-slightly-with-one-person-covering-their-mouth-wi.jpg?id=61079053&width=980"/><h3></h3><br/><p>Since first photon, Rubin has been undergoing calibrations, collecting data for the first images that it’s now sharing with the world, and preparing to scale up to begin its survey. Operations will soon become routine, the commissioning scientists will move on, and eventually, Rubin will largely run itself, with just a few people at the observatory most nights.</p><p>But for astronomers, the next 10 years will be anything but routine. “It’s going to be wildly different,” says Krabbendam. “Rubin will feed generations of scientists with trillions of data points of billions of objects. Explore the data. Harvest it. Develop your idea, see if it’s there. It’s going to be phenomenal.”<span class="ieee-end-mark"></span></p><p><em>This article appears in the July 2025 print issue as “Unveiling a Dynamic Universe.”</em></p><h3></h3><br/><h3 class="rm-anchors" id="listen-conversation-rubin-observatory">Listen to a Conversation About the Rubin Observatory</h3><p>As part of an experiment with AI storytelling tools, author Evan Ackerman—who visited the Vera C. Rubin Observatory in Chile for four days this past April—fed over 14 hours of raw audio from his interviews and other reporting notes into <a href="https://notebooklm.google.com/" target="_blank">NotebookLM</a>, an AI-powered research assistant developed by Google. The result is a podcast-style audio experience that you can listen to here. While the script and voices are AI-generated, the conversation is grounded in Ackerman’s original reporting, and includes many details that did not appear in the article above. Ackerman reviewed and edited the audio to ensure accuracy, and there are minor corrections in the transcript. <a href="mailto:evan.ackerman@ieee.org">Let us know</a> what you think of this experiment in AI narration.</p><p class="shortcode-media shortcode-media-audio"> <audio class="rm-shortcode" controls="" data-rm-shortcode-id="ffb294f9920a948fea39f581a6e4068e" expand="1" frameborder="0" id="265d5" original_filename="Vera+Rubin+Observatory+Podcast+Short4.mp3" site_id="20265424"> <source src="/files/81935/Vera+Rubin+Observatory+Podcast+Short4.mp3" type="audio/mpeg"/> Your browser does not support the audio tag. </audio></p><div id="ieee-transcript-container"><button aria-controls="ieee-transcript-content" aria-expanded="false" class="ieee-transcript-toggle" id="ieee-transcript-toggle">  See transcript  <span aria-hidden="true" class="ieee-transcript-icon"></span></button><div aria-label="Podcast transcript" hidden="" id="ieee-transcript-content" role="region" style="display: none;"><p>0:01: Today we’re taking a deep dive into the engineering marvel that is the Vera C. Rubin Observatory.</p><p>0:06: And and it really is a marvel.</p><p>0:08: This project pushes the limits, you know, not just for the science itself, like mapping the Milky Way or exploring dark energy, which is amazing, obviously.</p><p>0:16: But it’s also pushing the limits in just building the tools, the technical ingenuity, the, the sheer human collaboration needed to make something this complex actually work.</p><p>0:28: That’s what’s really fascinating to me.</p><p>0:29: Exactly.</p><p>0:30: And our mission for this deep dive is to go beyond the headlines, isn’t it?</p><p>0:33: We want to uncover those specific Kind of hidden technical details, the stuff from the audio interviews, the internal docs that really define this observatory.</p><p>0:41: The clever engineering solutions.</p><p>0:43: Yeah, the nuts and bolts, the answers to challenges nobody’s faced before, stuff that anyone who appreciates, you know, complex systems engineering would find really interesting.</p><p>0:53: Definitely.</p><p>0:54: So let’s start right at the heart of it.</p><p>0:57: The Simonyi survey telescope itself.</p><p>1:00: It’s this 350 ton machine inside a 600 ton dome, 30 m wide, huge. [The dome is closer to 650 tons.]</p><p>1:07: But the really astonishing part is its speed, speed and precision.</p><p>1:11: How do you even engineer something that massive to move that quickly while keeping everything stable down to the submicron level? [Micron level is more accurate.]</p><p>1:18: Well, that’s, that’s the core challenge, right?</p><p>1:20: This telescope, it can hit a top speed of 3.5 degrees per second.</p><p>1:24: Wow.</p><p>1:24: Yeah, and it can, you know, move to basically any point in the sky.</p><p>1:28: In under 20 seconds, 20 seconds, which makes it by far the fastest moving large telescope ever built, and the dome has to keep up.</p><p>1:36: So it’s also the fastest moving dome.</p><p>1:38: So the whole building is essentially racing along with the telescope.</p><p>1:41: Exactly.</p><p>1:41: And achieving that meant pretty much every component had to be custom designed like the pier holding the telescope up.</p><p>1:47: It’s mostly steel, not concrete.</p><p>1:49: Oh, interesting.</p><p>1:50: Why steel?</p><p>1:51: Specifically to stop it from twisting or vibrating when the telescope makes those incredibly fast moves.</p><p>1:56: Concrete just wouldn’t handle the torque the same way. [The pier is more steel than concrete, but it's still substantially concrete.]</p><p>1:59: OK, that makes sense.</p><p>1:59: And the power needed to accelerate and decelerate, you know, 300 tons, that must be absolutely massive.</p><p>2:06: Oh.</p><p>2:06: The instantaneous draw would be enormous.</p><p>2:09: How did they manage that without like dimming the lights on the whole.</p><p>2:12: Mountaintop every 30 seconds.</p><p>2:14: Yeah, that was a real concern, constant brownouts.</p><p>2:17: The solution was actually pretty elegant, involving these onboard capacitor banks.</p><p>2:22: Yep, slung right underneath the telescope structure.</p><p>2:24: They can slowly sip power from the grid, store it up over time, and then bam, discharge it really quickly for those big acceleration surges.</p><p>2:32: like a giant camera flash, but for moving a telescope, of yeah.</p><p>2:36: It smooths out the demand, preventing those grid disruptions.</p><p>2:40: Very clever engineering.</p><p>2:41: And beyond the movement, the mirrors themselves, equally critical, equally impressive, I imagine.</p><p>2:47: How did they tackle designing and making optics that large and precise?</p><p>2:51: Right, so the main mirror, the primary mirror, M1M3.</p><p>2:55: It’s a single piece of glass, 8.4 m across, low expansion borosilicate glass.</p><p>3:01: And that 8.4 m size, was that just like the biggest they could manage?</p><p>3:05: Well, it was a really crucial early decision.</p><p>3:07: The science absolutely required something at least 7 or 8 m wide.</p><p>3:13: But going much bigger, say 10 or 12 m, the logistics became almost impossible.</p><p>3:19: The big one was transport.</p><p>3:21: There’s a tunnel on the mountain road up to the summit, and a mirror, much larger than 8.4 m, physically wouldn’t fit through it.</p><p>3:28: No way.</p><p>3:29: So the tunnel actually set an upper limit on the mirror size.</p><p>3:31: Pretty much, yeah.</p><p>3:32: Building new road or some other complex transport method.</p><p>3:36: It would have added enormous cost and complexity.</p><p>3:38: So 8.4 m was that sweet spot between scientific need.</p><p>3:42: And, well, physical reality.</p><p>3:43: Wow, a real world constraint driving fundamental design.</p><p>3:47: And the mirror itself, you said M1 M3, it’s not just one simple mirror surface.</p><p>3:52: Correct.</p><p>3:52: It’s technically two mirror surfaces ground into that single piece of glass.</p><p>3:57: The central part has a more pronounced curvature.</p><p>3:59: It’s M1 and M3 combined.</p><p>4:00: OK, so fabricating that must have been tricky, especially with what, 10 tons of glass just in the center.</p><p>4:07: Oh, absolutely novel and complicated.</p><p>4:09: And these mirrors, they don’t support their own weight rigidly.</p><p>4:12: So just handling them during manufacturing, polishing, even getting them out of the casting mold, was a huge engineering challenge.</p><p>4:18: You can’t just lift it like a dinner plate.</p><p>4:20: Not quite, and then there’s maintaining it, re-silvering.</p><p>4:24: They hope to do it every 5 years.</p><p>4:26: Well, traditionally, big mirrors like this often need it more, like every 1.5 to 2 years, and it’s a risky weeks-long job.</p><p>4:34: You have to unbolt this priceless, unique piece of equipment, move it.</p><p>4:39: It’s nerve-wracking.</p><p>4:40: I bet.</p><p>4:40: And the silver coating itself is tiny, right?</p><p>4:42: Incredibly thin, just a few nanometers of pure silver.</p><p>4:46: It takes about 24 g for the whole giant surface, bonded with the adhesive layers that are measured in Angstroms. [It's closer to 26 grams of silver.]</p><p>4:52: It’s amazing precision.</p><p>4:54: So tying this together, you have this fast moving telescope, massive mirrors.</p><p>4:59: How do they keep everything perfectly focused, especially with multiple optical elements moving relative to each other?</p><p>5:04: that’s where these things called hexapods come in.</p><p>5:08: Really crucial bits of kit.</p><p>5:09: Hexapods, like six feet?</p><p>5:12: Sort of.</p><p>5:13: They’re mechanical systems with 6 adjustable arms or struts.</p><p>5:17: A simpler telescope might just have one maybe on the camera for basic focusing, but Ruben needs more because it’s got the 3 mirrors plus the camera.</p><p>5:25: Exactly.</p><p>5:26: So there’s a hexapod mounted on the secondary mirror, M2.</p><p>5:29: Its job is to keep M2 perfectly positioned relative to M1 and M3, compensating for tiny shifts or flexures.</p><p>5:36: And then there’s another hexapod on the camera itself.</p><p>5:39: That one adjusts the position and tilt of the entire camera’s sensor plane, the focal plane.</p><p>5:43: To get that perfect focus across the whole field of view.</p><p>5:46: And these hexapods move in 6 ways.</p><p>5:48: Yep, 6 degrees of freedom.</p><p>5:50: They can adjust position along the X, Y, and Z axis, and they can adjust rotation or tilt around those 3 axes as well.</p><p>5:57: It allows for incredibly fine adjustments, microp precision stuff.</p><p>6:00: So they’re constantly making these tiny tweaks as the telescope moves.</p><p>6:04: Constantly.</p><p>6:05: The active optics system uses them.</p><p>6:07: It calculates the needed corrections based on reference stars in the images, figures out how the mirror might be slightly bending.</p><p>6:13: And then tells the hexapods how to compensate.</p><p>6:15: It’s controlling like 26 g of silver coating on the mirror surface down to micron precision, using the mirror’s own natural bending modes.</p><p>6:24: It’s pretty wild.</p><p>6:24: Incredible.</p><p>6:25: OK, let’s pivot to the camera itself.</p><p>6:28: The LSST camera.</p><p>6:29: Big digital camera ever built, right?</p><p>6:31: Size of a small car, 2800 kg, captures 3.2 gigapixel images, just staggering numbers.</p><p>6:38: They really are, and the engineering inside is just as staggering.</p><p>6:41: That Socal plane where the light actually hits.</p><p>6:43: It’s made up of 189 individual CCD sensors.</p><p>6:47: Yep, 4K by 4K CCDs grouped into 21 rafts.</p><p>6:50: They give them like tiles, and each CCD has 16 amplifiers reading it out.</p><p>6:54: Why so many amplifiers?</p><p>6:56: Speed.</p><p>6:56: Each amplifier reads out about a million pixels.</p><p>6:59: By dividing the job up like that, they can read out the entire 3.2 gigapixel sensor in just 2 seconds.</p><p>7:04: 2 seconds for that much data.</p><p>7:05: Wow.</p><p>7:06: It’s essential for the survey’s rapid cadence.</p><p>7:09: Getting all those 189 CCDs perfectly flat must have been, I mean, are they delicate?</p><p>7:15: Unbelievably delicate.</p><p>7:16: They’re silicon wafers only 100 microns thick.</p><p>7:18: How thick is that really?</p><p>7:19: about the thickness of a human hair.</p><p>7:22: You could literally break one by breathing on it wrong, apparently, seriously, yeah.</p><p>7:26: And the challenge was aligning all 189 of them across this 650 millimeter wide focal plane, so the entire surface is flat.</p><p>7:34: To within just 24 microns, peak to valley.</p><p>7:37: 24 microns.</p><p>7:39: That sounds impossibly flat.</p><p>7:40: It’s like, imagine the entire United States.</p><p>7:43: Now imagine the difference between the lowest point and the highest point across the whole country was only 100 ft.</p><p>7:49: That’s the kind of relative flatness they achieved on the camera sensor.</p><p>7:52: OK, that puts it in perspective.</p><p>7:53: And why is that level of flatness so critical?</p><p>7:56: Because the telescope focuses light.</p><p>7:58: terribly.</p><p>7:58: It’s an F1.2 system, which means it has a very shallow depth of field.</p><p>8:02: If the sensors aren’t perfectly in that focal plane, even by a few microns, parts of the image go out of focus.</p><p>8:08: Gotcha.</p><p>8:08: And the pixels themselves, the little light buckets on the CCDs, are they special?</p><p>8:14: They’re custom made, definitely.</p><p>8:16: They settled on 10 micron pixels.</p><p>8:18: They figured anything smaller wouldn’t actually give them more useful scientific information.</p><p>8:23: Because you start hitting the limits of what the atmosphere and the telescope optics themselves can resolve.</p><p>8:28: So 10 microns was the optimal size, right?</p><p>8:31: balancing sensor tech with physical limits.</p><p>8:33: Now, keeping something that sensitive cool, that sounds like a nightmare, especially with all those electronics.</p><p>8:39: Oh, it’s a huge thermal engineering challenge.</p><p>8:42: The camera actually has 3 different cooling zones, 3 distinct temperature levels inside.</p><p>8:46: 3.</p><p>8:47: OK.</p><p>8:47: First, the CCDs themselves.</p><p>8:49: They need to be incredibly cold to minimize noise.</p><p>8:51: They operate at -125 °C.</p><p>8:54: -125C, how do they manage that?</p><p>8:57: With a special evaporator plate connected to the CCD rafts by flexible copper braids, which pulls heat away very effectively.</p><p>9:04: Then you’ve got the cameras, electronics, the readout boards and stuff.</p><p>9:07: They run cooler than room temp, but not that cold, around -50 °C.</p><p>9:12: OK.</p><p>9:12: That requires a separate liquid cooling loop delivered through these special vacuum insulated tubes to prevent heat leaks.</p><p>9:18: And the third zone.</p><p>9:19: That’s for the electronics in the utility trunk at the back of the camera.</p><p>9:23: They generate a fair bit of heat, about 3000 watts, like a few hair dryers running constantly.</p><p>9:27: Exactly.</p><p>9:28: So there’s a third liquid cooling system just for them, keeping them just slightly below the ambient room temperature in the dome.</p><p>9:35: And all this cooling, it’s not just to keep the parts from overheating, right?</p><p>9:39: It affects the images, absolutely critical for image quality.</p><p>9:44: If the outer surface of the camera body itself is even slightly warmer or cooler than the air inside the dome, it creates tiny air currents, turbulence right near the light path.</p><p>9:57: And that shows up as little wavy distortions in the images, messing up the precision.</p><p>10:02: So even the outside temperature of the camera matters.</p><p>10:04: Yep, it’s not just a camera.</p><p>10:06: They even have to monitor the heat generated by the motors that move the massive dome, because that heat could potentially cause enough air turbulence inside the dome to affect the image quality too.</p><p>10:16: That’s incredible attention to detail, and the camera interior is a vacuum you mentioned.</p><p>10:21: Yes, a very strong vacuum.</p><p>10:23: They pump it down about once a year, first using turbopumps spinning at like 80,000 RPM to get it down to about 102 tor.</p><p>10:32: Then they use other methods to get it down much further.</p><p>10:34: The 107 tor, that’s an ultra high vacuum.</p><p>10:37: Why the vacuum?</p><p>10:37: Keep frost off the cold part.</p><p>10:39: Exactly.</p><p>10:40: Prevents condensation and frost on those negatives when it 25 degree CCDs and generally ensures everything works optimally.</p><p>10:47: For normal operation, day to day, they use something called an ion pump.</p><p>10:51: How does that work?</p><p>10:52: It basically uses a strong electric field to ionize any stray gas molecules, mostly hydrogen, and trap them, effectively removing them from the vacuum space, very efficient for maintaining that ultra-high vacuum.</p><p>11:04: OK, so we have this incredible camera taking these massive images every few seconds.</p><p>11:08: Once those photons hit the CCDs and become digital signals, What happens next?</p><p>11:12: How does Ruben handle this absolute flood of data?</p><p>11:15: Yeah, this is where Ruben becomes, you know, almost as much a data processing machine as a telescope.</p><p>11:20: It’s designed for the data output.</p><p>11:22: So photons hit the CCDs, get converted to electrical signals.</p><p>11:27: Then, interestingly, they get converted back into light signals, photonic signals back to light.</p><p>11:32: Why?</p><p>11:33: To send them over fiber optics.</p><p>11:34: They’re about 6 kilometers of fiber optic cable running through the observatory building.</p><p>11:39: These signals go to FPGA boards, field programmable gate arrays in the data acquisition system.</p><p>11:46: OK.</p><p>11:46: And those FPGAs are basically assembling the complete image data packages from all the different CCDs and amplifiers.</p><p>11:53: That sounds like a fire hose of data leaving the camera.</p><p>11:56: How does it get off the mountain and where does it need to go?</p><p>11:58: And what about all the like operational data, temperatures, positions?</p><p>12:02: Good question.</p><p>12:03: There are really two main data streams all that telemetry you mentioned, sensor readings, temperatures, actuator positions, command set, everything about the state of the observatory that all gets collected into something called the Engineering facility database or EFD.</p><p>12:16: They use Kafka for transmitting that data.</p><p>12:18: It’s good for high volume streams, and store it in an influx database, which is great for time series data like sensor readings.</p><p>12:26: And astronomers can access that.</p><p>12:28: Well, there’s actually a duplicate copy of the EFD down at SLAC, the research center in California.</p><p>12:34: So scientists and engineers can query that copy without bogging down the live system running on the mountain.</p><p>12:40: Smart.</p><p>12:41: How much data are we talking about there?</p><p>12:43: For the engineering data, it’s about 20 gigabytes per night, and they plan to keep about a year’s worth online.</p><p>12:49: OK.</p><p>12:49: And the image data, the actual science pixels.</p><p>12:52: That takes a different path. [All of the data from Rubin to SLAC travels over the same network.]</p><p>12:53: It travels over dedicated high-speed network links, part of ESET, the research network, all the way from Chile, usually via Boca Raton, Florida, then Atlanta, before finally landing at SLAC.</p><p>13:05: And how fast does that need to be?</p><p>13:07: The goal is super fast.</p><p>13:09: They aim to get every image from the telescope in Chile to the data center at SLAC within 7 seconds of the shutter closing.</p><p>13:15: 7 seconds for gigabytes of data.</p><p>13:18: Yeah.</p><p>13:18: Sometimes network traffic bumps it up to maybe 30 seconds or so, but the target is 7.</p><p>13:23: It’s crucial for the next step, which is making sense of it all.</p><p>13:27: How do astronomers actually use this, this torrent of images and data?</p><p>13:30: Right.</p><p>13:31: This really changes how astronomy might be done.</p><p>13:33: Because Ruben is designed to generate alerts, real-time notifications about changes in the sky.</p><p>13:39: Alerts like, hey, something just exploded over here.</p><p>13:42: Pretty much.</p><p>13:42: It takes an image compared to the previous images of the same patch of sky and identifies anything that’s changed, appeared, disappeared, moved, gotten brighter, or fainter.</p><p>13:53: It expects to generate about 10,000 such alerts per image.</p><p>13:57: 10,000 per image, and they take an image every every 20 seconds or so on average, including readouts. [Images are taken every 34 seconds: a 30 second exposure, and then about 4 seconds for the telescope to move and settle.]</p><p>14:03: So you’re talking around 10 million alerts every single night.</p><p>14:06: 10 million a night.</p><p>14:07: Yep.</p><p>14:08: And the goal is to get those alerts out to the world within 60 seconds of the image being taken.</p><p>14:13: That’s insane.</p><p>14:14: What’s in an alert?</p><p>14:15: It contains the object’s position, brightness, how it’s changed, and little cut out images, postage stamps in the last 12 months of observations, so astronomers can quickly see the history.</p><p>14:24: But surely not all 10 million are real astronomical events satellites, cosmic rays.</p><p>14:30: Exactly.</p><p>14:31: The observatory itself does a first pass filter, masking out known issues like satellite trails, cosmic ray hits, atmospheric effects, with what they call real bogus stuff.</p><p>14:41: OK.</p><p>14:42: Then, this filtered stream of potentially real alerts goes out to external alert brokers.</p><p>14:49: These are systems run by different scientific groups around the world.</p><p>14:52: Yeah, and what did the brokers do?</p><p>14:53: They ingest the huge stream from Ruben and apply their own filters, based on what their particular community is interested in.</p><p>15:00: So an astronomer studying supernovae can subscribe to a broker that filters just for likely supernova candidates.</p><p>15:06: Another might filter for near Earth asteroids or specific types of variable stars.</p><p>15:12: so it makes the fire hose manageable.</p><p>15:13: You subscribe to the trickle you care about.</p><p>15:15: Precisely.</p><p>15:16: It’s a way to distribute the discovery potential across the whole community.</p><p>15:19: So it’s not just raw images astronomers get, but these alerts and presumably processed data too.</p><p>15:25: Oh yes.</p><p>15:26: Rubin provides the raw images, but also fully processed images, corrected for instrument effects, calibrated called processed visit images.</p><p>15:34: And also template images, deep combinations of previous images used for comparison.</p><p>15:38: And managing all that data, 15 petabytes you mentioned, how do you query that effectively?</p><p>15:44: They use a system called Keyserve. [The system is "QServ."]</p><p>15:46: It’s a distributed relational database, custom built basically, designed to handle these enormous astronomical catalogs.</p><p>15:53: The goal is to let astronomers run complex searches across maybe 15 petabytes of catalog data and get answers back in minutes, not days or weeks.</p><p>16:02: And how do individual astronomers actually interact with it?</p><p>16:04: Do they download petabytes?</p><p>16:06: No, definitely not.</p><p>16:07: For general access, there’s a science platform, the front end of which runs on Google Cloud.</p><p>16:11: Users interact mainly through Jupiter notebooks.</p><p>16:13: Python notebooks, familiar territory for many scientists.</p><p>16:17: Exactly.</p><p>16:18: They can write arbitrary Python code, access the catalogs directly, do analysis for really heavy duty stuff like large scale batch processing.</p><p>16:27: They can submit jobs to the big compute cluster at SLEC, which sits right next to the data storage.</p><p>16:33: That’s much more efficient.</p><p>16:34: Have they tested this?</p><p>16:35: Can it handle thousands of astronomers hitting it at once?</p><p>16:38: They’ve done extensive testing, yeah, scaled it up with hundreds of users already, and they seem confident they can handle up to maybe 3000 simultaneous users without issues.</p><p>16:49: And a key point.</p><p>16:51: After an initial proprietary period for the main survey team, all the data and importantly, all the software algorithms used to process it become public.</p><p>17:00: Open source algorithms too.</p><p>17:01: Yes, the idea is, if the community can improve on their processing pipelines, they’re encouraged to contribute those solutions back.</p><p>17:08: It’s meant to be a community resource.</p><p>17:10: That open approach is fantastic, and even the way the images are presented visually has some deep thought behind it, doesn’t it?</p><p>17:15: You mentioned Robert Leptina’s perspective.</p><p>17:17: Yes, this is fascinating.</p><p>17:19: It’s about how you assign color to astronomical images, which usually combine data from different filters, like red, green, blue.</p><p>17:28: It’s not just about making pretty pictures, though they can be beautiful.</p><p>17:31: Right, it should be scientifically meaningful.</p><p>17:34: Exactly.</p><p>17:35: Lepton’s approach tries to preserve the inherent color information in the data.</p><p>17:40: Many methods saturate bright objects, making their centers just white blobs.</p><p>17:44: Yeah, you see that a lot.</p><p>17:46: His algorithm uses a different mathematical scaling, more like a logarithmic scale, that avoids this saturation.</p><p>17:52: It actually propagates the true color information back into the centers of bright stars and galaxies.</p><p>17:57: So, a galaxy that’s genuinely redder, because it’s red shifted, will actually look redder in the image, even in its bright core.</p><p>18:04: Precisely, in a scientifically meaningful way.</p><p>18:07: Even if our eyes wouldn’t perceive it quite that way directly through a telescope, the image renders the data faithfully.</p><p>18:13: It helps astronomers visually interpret the physics.</p><p>18:15: It’s a subtle but powerful detail in making the data useful.</p><p>18:19: It really is.</p><p>18:20: Beyond just taking pictures, I heard Ruben’s wide view is useful for something else entirely gravitational waves.</p><p>18:26: That’s right.</p><p>18:26: It’s a really cool synergy.</p><p>18:28: Gravitational wave detectors like Lego and Virgo, they detect ripples in space-time, often from emerging black holes or neutron stars, but they usually only narrow down the location to a relatively large patch of sky, maybe 10 square degrees or sometimes much more.</p><p>18:41: Ruben’s camera has a field of view of about 9.6 square degrees.</p><p>18:45: That’s huge for a telescope.</p><p>18:47: It almost perfectly matches the typical LIGO alert area.</p><p>18:51: so when LIGO sends an alert, Ruben can quickly scan that whole error box, maybe taking just a few pointings, looking for any new point of light.</p><p>19:00: The optical counterpart, the Killanova explosion, or whatever light accompany the gravitational wave event.</p><p>19:05: It’s a fantastic follow-up machine.</p><p>19:08: Now, stepping back a bit, this whole thing sounds like a colossal integration challenge.</p><p>19:13: A huge system of systems, many parts custom built, pushed to their limits.</p><p>19:18: What were some of those big integration hurdles, bringing it all together?</p><p>19:22: Yeah, classic system of systems is a good description.</p><p>19:25: And because nobody’s built an observatory quite like this before, a lot of the commissioning phase, getting everything working together involves figuring out the procedures as they go.</p><p>19:34: Learning by doing on a massive scale.</p><p>19:36: Pretty much.</p><p>19:37: They’re essentially, you know, teaching the system how to walk.</p><p>19:40: And there’s this constant tension, this balancing act.</p><p>19:43: Do you push forward, maybe build up some technical debt, things you know you’ll have to fix later, or do you stop and make sure every little issue is 100% perfect before moving on, especially with a huge distributed team?</p><p>19:54: I can imagine.</p><p>19:55: And you mentioned the dome motors earlier.</p><p>19:57: That discovery about heat affecting images sounds like a perfect example of unforeseen integration issues.</p><p>20:03: Exactly.</p><p>20:03: Marina Pavvich described that.</p><p>20:05: They ran the dome motors at full speed, something maybe nobody had done for extended periods in that exact configuration before, and realized, huh.</p><p>20:13: The heat these generate might actually cause enough air turbulence to mess with our image quality.</p><p>20:19: That’s the kind of thing you only find when you push the integrated system.</p><p>20:23: Lots of unexpected learning then.</p><p>20:25: What about interacting with the outside world?</p><p>20:27: Other telescopes, the atmosphere itself?</p><p>20:30: How does Ruben handle atmospheric distortion, for instance?</p><p>20:33: that’s another interesting point.</p><p>20:35: Many modern telescopes use lasers.</p><p>20:37: They shoot a laser up into the sky to create an artificial guide star, right, to measure.</p><p>20:42: Atmospheric turbulence.</p><p>20:43: Exactly.</p><p>20:44: Then they use deformable mirrors to correct for that turbulence in real time.</p><p>20:48: But Ruben cannot use a laser like that.</p><p>20:50: Why?</p><p>20:51: Because its field of view is enormous.</p><p>20:53: It sees such a wide patch of sky at once.</p><p>20:55: A single laser beam, even a pinpoint from another nearby observatory, would contaminate a huge fraction of Ruben’s image.</p><p>21:03: It would look like a giant streak across, you know, a quarter of the sky for Ruben.</p><p>21:06: Oh, wow.</p><p>21:07: OK.</p><p>21:08: Too much interference.</p><p>21:09: So how does it correct for the atmosphere?</p><p>21:11: Software.</p><p>21:12: It uses a really clever approach called forward modeling.</p><p>21:16: It looks at the shapes of hundreds of stars across its wide field of view in each image.</p><p>21:21: It knows what those stars should look like, theoretically.</p><p>21:25: Then it builds a complex mathematical model of the atmosphere’s distorting effect across the entire field of view that would explain the observed star shapes.</p><p>21:33: It iterates this model hundreds of times per image until it finds the best fit. [The model is created by iterating on the image data, but iteration is not necessary for every image.]</p><p>21:38: Then it uses that model to correct the image, removing the atmospheric blurring.</p><p>21:43: So it calculates the distortion instead of measuring it directly with a laser.</p><p>21:46: Essentially, yes.</p><p>21:48: Now, interestingly, there is an auxiliary telescope built alongside Ruben, specifically designed to measure atmospheric properties independently.</p><p>21:55: Oh, so they could use that data.</p><p>21:57: They could, but currently, they’re finding their software modeling approach using the science images themselves, works so well that they aren’t actively incorporating the data from the auxiliary telescope for that correction right now.</p><p>22:08: The software solution is proving powerful enough on its own.</p><p>22:11: Fascinating.</p><p>22:12: And they still have to coordinate with other telescopes about their lasers, right?</p><p>22:15: Oh yeah.</p><p>22:15: They have agreements about when nearby observatories can point their lasers, and sometimes Ruben might have to switch to a specific filter like the Iband, which is less sensitive to the laser.</p><p>22:25: Light if one is active nearby while they’re trying to focus.</p><p>22:28: So many interacting systems.</p><p>22:30: What an incredible journey through the engineering of Ruben.</p><p>22:33: Just the sheer ingenuity from the custom steel pier and the capacitor banks, the hexapods, that incredibly flat camera, the data systems.</p><p>22:43: It’s truly a machine built to push boundaries.</p><p>22:45: It really is.</p><p>22:46: And it’s important to remember, this isn’t just, you know, a bigger version of existing telescopes.</p><p>22:51: It’s a fundamentally different kind of machine.</p><p>22:53: How so?</p><p>22:54: By creating this massive all-purpose data set, imaging the entire southern sky over 800 times, cataloging maybe 40 billion objects, it shifts the paradigm.</p><p>23:07: Astronomy becomes less about individual scientists applying for time to point a telescope at one specific thing and more about statistical analysis, about mining this unprecedented ocean of data that Rubin provides to everyone.</p><p>23:21: So what does this all mean for us, for science?</p><p>23:24: Well, it’s a generational investment in fundamental discovery.</p><p>23:27: They’ve optimized this whole system, the telescope, the camera, the data pipeline.</p><p>23:31: For finding, quote, exactly the stuff we don’t know we’ll find.</p><p>23:34: Optimized for the unknown, I like that.</p><p>23:36: Yeah, we’re basically generating this incredible resource that will feed generations of astronomers and astrophysicists.</p><p>23:42: They’ll explore it, they’ll harvest discoveries from it, they’ll find patterns and objects and phenomena within billions and billions of data points that we can’t even conceive of yet.</p><p>23:50: And that really is the ultimate excitement, isn’t it?</p><p>23:53: Knowing that this monumental feat of engineering isn’t just answering old questions, but it’s poised to open up entirely new questions about the universe, questions we literally don’t know how to ask today.</p><p>24:04: Exactly.</p><p>24:05: So, for you, the listener, just think about that.</p><p>24:08: Consider the immense, the completely unknown discoveries that are waiting out there just waiting to be found when an entire universe of data becomes accessible like this.</p><p>24:16: What might we find?</p><p><a href="#listen-conversation-rubin-observatory">Back to top</a></p></div></div></br>]]></description><pubDate>Mon, 23 Jun 2025 04:01:02 +0000</pubDate><guid>https://spectrum.ieee.org/vera-rubin-observatory-first-images</guid><category>Astronomy</category><category>Dark matter</category><category>Space</category><category>Telescopes</category><category>Type:cover</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-researchers-inspect-a-large-telescope-system-inside-a-scientific-facility.jpg?id=61078903&amp;width=980"></media:content></item><item><title>Video Friday: Jet-Powered Humanoid Robot Lifts Off</title><link>https://spectrum.ieee.org/video-friday-jet-powered-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/close-up-of-a-humanoid-robot-showing-intricate-mechanical-components-and-wiring-with-small-jet-engines-on-its-arms-and-torso.jpg?id=61079219&width=1200&height=800&coordinates=0%2C50%2C0%2C50"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote class="rm-anchors" id="t1bnhot4d5q"><em>This is the first successful vertical takeoff of a jet-powered flying humanoid robot, developed by Artificial and Mechanical Intelligence (AMI) at Istituto Italiano di Tecnologia (IIT). The robot lifted ~50 cm off the ground while maintaining dynamic stability, thanks to advanced AI-based control systems and aerodynamic modeling.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2666111cae7290d9efdf2108af79560f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/t1bNHoT4D5Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>We will have much more on this in the coming weeks!</p><p>[<a href="https://www.nature.com/articles/s44172-025-00447-w">Nature</a>] via [<a href="https://opentalk.iit.it/en/iit-demonstrates-that-a-humanoid-robot-can-fly/">IIT</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="iwcnynpjnm0"><em>As a first step toward our mission of deploying general-purpose robots, we are pushing the frontiers of what end-to-end AI models can achieve in the real world. We’ve been training models and evaluating their capabilities for dexterous sensorimotor policies across different embodiments, environments, and physical interactions. We’re sharing capability demonstrations on tasks stressing different aspects of manipulation: fine motor control, spatial and temporal precision, generalization across robots and settings, and robustness to external disturbances.</em></blockquote><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="3fbcd7173da8b61dcd8567ca2932e4fd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/mhfleCK_IAI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://generalistai.com/blog.html">Generalist AI</a>]</p><p>Thanks, Noah!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="iwcnynpjnm0"><em>Ground Control Robotics is introducing SCUTTLE, our newest elongate multilegged platform for mobility anywhere!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="da081e0bf9207ff62b17fbdc33a083c9" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/IWcNyNPjnM0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://groundcontrolrobotics.com/">Ground Control Robotics</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="udkddqxth5q"><em>Teleoperation has been around for a while, but what hasn’t been is precise, real-time force feedback. That’s where Flexiv steps in to shake things up. Now, whether you’re across the room or across the globe, you can experience seamless, high-fidelity remote manipulation with a sense of touch.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ec233d3290c27dd0cea84b0f17763f3e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/udkddqxth5Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>This sort of thing usually takes some human training, for which you’d be best served by <a data-linked-post="2657676851" href="https://spectrum.ieee.org/video-friday-iss-robot-arms" target="_blank">robot arms</a> with  <a data-linked-post="2650273235" href="https://spectrum.ieee.org/esa-space-teleoperation-tests" target="_blank">precise, real-time force feedback</a>. Hmm, I wonder where you’d find those...?</p><p>[<a href="https://www.flexiv.com/">Flexiv</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="xpx6ddrybv4"><em>The 1X World Model is a data-driven simulator for humanoid robots built with a grounded understanding of physics. It allows us to predict—or “hallucinate”—the outcomes of NEO’s actions before they’re taken in the real world. Using the 1X World Model, we can instantly assess the performance of AI models—compressing development time and providing a clear benchmark for continuous improvement.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="bbf4dbf833d6ae5f2f31d30e3867918e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xPX6dDRYbV4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.1x.tech/">1X</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="atr87nwq3eq"><em>SLAPBOT is an interactive robotic artwork by Hooman Samani and Chandler Cheng, exploring the dynamics of physical interaction, artificial agency, and power. The installation features a robotic arm fitted with a soft, inflatable hand that delivers slaps through pneumatic actuation, transforming a visceral human gesture into a programmed robotic response.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="98480c72f39d07a271776fa0143a9b44" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ATR87nwq3eQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>I asked, of course, whether SLAPBOT slaps people, and it does not: “Despite its provocative concept and evocative design, SLAPBOT does not make physical contact with human participants. It simulates the gesture of slapping without delivering an actual strike. The robotic arm’s movements are precisely choreographed to suggest the act, yet it maintains a safe distance.”</p><p>[<a href="https://hoomansamani.com/slapbot/">SLAPBOT</a>]</p><p>Thanks, Hooman!</p><div class="horizontal-rule"></div><p class="rm-anchors" id="xht3nvc9d-i">Inspecting the bowels of ships is something we’d really like robots to be doing for us, please and thank you.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f8e59dc2b2d1ebc58f4a8fa063562914" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/XhT3nVC9d-I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.ntnu.edu/itk/research/robotics" target="_blank">Norwegian University of Science and Technology</a>] via [<a href="https://github.com/ntnu-arl/predictive_planning_ros">GitHub</a>]</p><p>Thanks, Kostas!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8a46uap367k"><em>H2L Corporation (hereinafter referred to as H2L) has unveiled a new product called “Capsule Interface,” which transmits whole-body movements and strength, enabling new shared experiences with robots and avatars. A product introduction video depicting a synchronization never before experienced by humans was also released.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cd7cebba8a246b0d342bfa262937b917" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8a46Uap367k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://h2l.jp/2025/06/18/%e5%85%a8%e8%ba%ab%e3%83%aa%e3%82%a2%e3%83%ab%e4%bd%93%e9%a8%93%ef%bc%81%e8%a6%8b%e3%82%8b%e8%81%9e%e3%81%8f%e3%81%ae%e5%85%88%e3%82%92%e5%89%b5%e3%82%8b%e3%80%82%e5%8b%95%e3%81%8d%e3%81%a8%e5%8a%9b/">H2L Corp.</a>] via [<a href="https://robotstart.info/2025/06/18/h2l-capsule-interface-launch.html">RobotStart</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="vbugenau3re">How do you keep a robot safe without requiring it to look at you? <a data-linked-post="2668807636" href="https://spectrum.ieee.org/feral-cat-radar-detector" target="_blank">Radar</a>!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6b6cefa1aff74634e83e5435745c35bc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/vbuGenAu3rE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://ieeexplore.ieee.org/document/11037369">Paper</a>] via [<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361" target="_blank">IEEE Sensors Journal</a>]</p><p>Thanks, Bram!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="pcank-5e-qo"><em>We propose Aerial Elephant Trunk, an aerial continuum manipulator inspired by the elephant trunk, featuring a small-scale quadrotor and a dexterous, compliant tendon-driven continuum arm for versatile operation in both indoor and outdoor settings.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4a2829d05541e793bcf454fe8f2c394d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PcanK-5e-qo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://arclab.hku.hk/">Adaptive Robotics Controls Lab</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="w_qloi1pokw"><em>This video demonstrates a heavy weight lifting test using the ARMstrong Dex robot, focusing on a 40 kg bicep curl motion. ARMstrong Dex is a human-sized, dual-arm hydraulic robot currently under development at the Korea Atomic Energy Research Institute (KAERI) for disaster response applications. Designed to perform tasks flexibly like a human while delivering high power output, ARMstrong Dex is capable of handling complex operations in hazardous environments.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ed3765ff0740c1a013e9beeaae04aa6a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/W_QlOi1PoKw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.kaeri.re.kr/eng/">Korea Atomic Energy Research Institute</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lfat803dqmw"><em>Micro-robots that can inspect water pipes, diagnose cracks, and fix them autonomously—reducing leaks and avoiding expensive excavation work—have been developed by a team of engineers led by the University of Sheffield. </em></blockquote><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="204230bab58cc85ff8909e3f3029be79" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Q2loVe5_NcE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.sheffield.ac.uk/news/tiny-robots-could-help-fix-leaky-water-pipes">University of Sheffield</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lfat803dqmw"><em>We’re growing in size, scale, and impact! We’re excited to announce the opening of our serial production facility in the San Francisco Bay Area, the very first purpose-built <a data-linked-post="2650275419" href="https://spectrum.ieee.org/secretive-robotaxi-startup-zoox-prepares-for-realworld-testing" target="_blank">robotaxi</a> assembly facility in the United States. More space means more innovation, production, and opportunities to scale our fleet.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="504fa629f2368f24a9acf389d3e12a66" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lfAt803DQMw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://zoox.com/">Zoox</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8-0d4lyjhqi"><em>Watch multipick in action as our pickle robot rapidly identifies, picks, and places multiple boxes in a single swing of an arm.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="36412a3fdb34a606e69841ccca3c2110" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8-0d4LyJhQI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://picklerobot.com/">Pickle</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="un_7inyazyq">And now, this.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ec349bce0e0fb055b052b7831af4f49b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/uN_7INYaZYQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://info.aibo.sony.jp/info/2024/12/creatorschallenge2025.html">Aibo</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="aqrqusrezhy"><em>Cargill’s Amsterdam Multiseed facility enlists Spot and Orbit to inspect machinery and perform visual checks, enhanced by all-new AI features, as part of their “Plant of the Future” program. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b7c681340545a8e6334c9f64aa9dadd4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AqRquSReZHY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://bostondynamics.com/products/spot/">Boston Dynamics</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="vmsslbgktcu">This ICRA 2025 plenary talk is from Raffaello D’Andrea, entitled “Models are Dead, Long Live Models!”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d1600ac97242ef165c3fc6d7e438f123" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/vMSSlBGKtCU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://2025.ieee-icra.org/program/plenary-sessions/">ICRA 2025</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="pfvctjompk8">Will data solve robotics and automation? Absolutely! Never! Who knows?! Let’s argue about it!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2e5008c9444113b6baa43e3fdeee54ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PfvctjoMPk8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://2025.ieee-icra.org/announcements/event-overview-for-thursday-may-22/">ICRA 2025</a>]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 20 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-jet-powered-robot</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><category>Industrial robots</category><category>Aibo</category><category>Dexterous</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/close-up-of-a-humanoid-robot-showing-intricate-mechanical-components-and-wiring-with-small-jet-engines-on-its-arms-and-torso.jpg?id=61079219&amp;width=980"></media:content></item><item><title>Making the Most of 1:1 Meetings With Your Boss</title><link>https://spectrum.ieee.org/making-most-of-1-1s</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p><em><em></em></em><span>I once had a manager at Meta who kept flip-flopping. We’d have our one-on-one meetings to align on the priorities, and whether I should focus on new features or fix user-reported bugs.</span></p><p>But after a few days, our plans would suddenly change. Certain bugs would become the highest priority, especially if the order came from directors or VPs. I noticed a pattern where my manager would change his mind after speaking with a strong-willed project manager or some engineering leader up the chain.</p><p>I was left feeling confused and unsupported.</p><p>When this happens, how do you tell your manager to shape up? Is it even your responsibility to give feedback to your manager?</p><p>The 1:1 is a critical forum to share this kind of feedback. A 1:1 is a focused meeting between two people within the company, typically lasting 30 or 45 minutes. When done well, these meetings are a valuable tool for building trust and fostering <span>career</span> growth. In my experience, managers will have weekly or biweekly 1:1s with each of their reports. If you don’t have a regularly scheduled 1:1 with your manager, you’re missing out. Ask for one!</p><p>The effectiveness of a 1:1 depends on your preparation before the meeting. Here are a few ground rules I set with my reports and my own manager to make them as valuable as possible: </p><ul><li><strong>Write down the agenda in advance</strong>. This shows that you have put some thought into the meeting and, therefore, it shouldn’t be canceled. Keep a running doc of everything you’ve written down. It can be helpful for both you and your manager to refer back to prior discussions and action items.</li><li><strong>Avoid status updates</strong>. Approach each 1:1 as a valuable opportunity to learn something or gain a new perspective. Feel free to write down status updates ahead of time, but you should minimize the time spent in the 1:1 just reviewing statuses. The conversation should be more focused on emotions and concerns rather than obvious facts.</li><li><strong>Be vulnerable</strong>. One litmus test for the conversation is, “Could this have been shared in the broader team meeting?” If the answer is yes, don’t waste the valuable 1:1 time on that topic. The 1:1 should focus on the sticky human issues that inevitably come up in the workplace: losing motivation, feeling overwhelmed, or delivering difficult feedback, for example.</li></ul><p>At Meta, I used the 1:1 time with my manager to share my concerns about the constantly shifting priorities between new features and user-reported bugs. The problem didn’t get resolved overnight, but at least he was aware of the issue. I felt heard, and we continued to monitor the situation as it improved.</p><p>What if your manager isn’t receptive to your feedback or concerns? In almost all cases, it’s not worth trying to “fix” your manager or your environment. There’s a clear power dynamic between you and your boss, and the energy spent on your manager is better spent on finding a new team or company altogether.</p><p>The 1:1 is a critical pillar for our <span>career</span> growth as engineers. Try out these tactics in your next 1:1 and let me know how it goes.</p><p>—Rahul</p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7xy-cHkoJQVMnmewwaYMOW8VZFKCyfo-0z-1IIYhFMj6rGI0-sFKmQHrjlK-bBrZYNI=" target="_blank" title="IEEE’s 5 New E-Books Provide Onramp to Engineering">IEEE’s 5 New E-Books Provide Onramp to Engineering</a></h3><p>Five new e-books from IEEE’s TryEngineering initiative provide an overview of topics including semiconductors, signal processing, oceanic engineering, and AI. As part of IEEE’s suite of pre-university resources, the free e-books are meant to introduce these complex technical topics to younger readers—the next generation of engineers.</p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7xy-cHkoJQVMnmewwaYMOW8VZFKCyfo-0z-1IIYhFMj6rGI0-sFKmQHrjlK-bBrZYNI=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7vcQBq3UJs2Ja2HNTuItc3zpfFjpQMKwyRpAeLsYi8ST-UmHQZDwfa-YI-bzs9co78w=" rel="noopener noreferrer" target="_blank" title="In Dubai’s AI job market, your passport matters">In Dubai’s AI job market, your passport matters</a></h3><p>More tech workers are moving to the UAE, which is now second only to the United States in attracting top AI talent, according to reporting from Rest of World. But as the country becomes an AI talent magnet, differences are emerging among workers based on where they’re from. While tech specialists from the West take top positions, engineers from developing nations often fill lower positions.</p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7vcQBq3UJs2Ja2HNTuItc3zpfFjpQMKwyRpAeLsYi8ST-UmHQZDwfa-YI-bzs9co78w=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde75qf3MHa8lF6wHgIMrcfPblu1rhPFWwno5RBO9Kk8Rjn5vNYgTxUfSrDGpdilraaw9E=" rel="noopener noreferrer" target="_blank" title="Record Number of IEEE Members Visit U.S. Congress to Talk Tech Policy">Record Number of IEEE Members Visit U.S. Congress to Talk Tech Policy</a></h3><p>In this guest article, a technical program manager at Google reflects on his experience meeting with U.S. legislators this April. More than 300 IEEE representatives participated in the organization’s Congressional Visits Day to discuss federal funding, the STEM talent pipeline, and other policy issues. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde75qf3MHa8lF6wHgIMrcfPblu1rhPFWwno5RBO9Kk8Rjn5vNYgTxUfSrDGpdilraaw9E=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p>]]></description><pubDate>Thu, 19 Jun 2025 19:45:10 +0000</pubDate><guid>https://spectrum.ieee.org/making-most-of-1-1s</guid><category>Careers newsletter</category><category>Tech careers</category><category>Practical strategies</category><category>Careers</category><category>Career development</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>Check Out IEEE’s Revamped Online Presence</title><link>https://spectrum.ieee.org/ieee-revamped-online-presence</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-smartphone-digital-tablet-desktop-computer-and-laptop-arranged-closely-together-all-of-the-devices-screens-display-ieees.jpg?id=61006929&width=1200&height=800&coordinates=126%2C0%2C127%2C0"/><br/><br/><p>The newly designed <a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE website</a> makes it easier than ever to learn about the organization and its offerings. IEEE incorporated feedback from members and site visitors to create its modern look and feel.</p><p>Throughout the site, the work of IEEE and its members is prominently highlighted to show how they are creating a better world and driving engineering forward.</p><p>“The new website is more visual, with video and other media to engage all visitors. It also showcases our global community’s commitment as a public charity advancing technology for the benefit of humanity,” says <a href="https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead" target="_self">Sophia Muirhead</a>, IEEE executive director and chief operating officer.</p><p>The website reflects IEEE’s commitment to delivering an engaging online experience that is more intuitive for its global community. The storytelling theme of the site highlights select quotes, testimonials, and member and volunteer stories from IEEE’s more than 486,000 members and 189,000 student members from 347 sections in 10 geographic regions.</p><p>Whether you’re looking for a humanitarian project to get involved in, finding an upcoming conference to attend, taking a continuing education course, or publishing a research paper, the new design makes resources easier to access.</p><h2>Where to find courses, career resources, and more</h2><p>The first thing you’ll see on the new site is a box with scrolling options. Power What’s Next for Tech describes what IEEE is, and it includes a link to the <a href="https://www.ieee.org/about-ieee" rel="noopener noreferrer" target="_blank">What We Do</a> page, which gives an overview of the organization, including its <a href="https://www.ieee.org/about/vision-mission" rel="noopener noreferrer" target="_blank">mission</a>, <a href="https://spectrum.ieee.org/ieee-2025-2030-strategic-plan" target="_self">strategic plan</a>, <a href="https://www.ieee.org/about/ieee-history" rel="noopener noreferrer" target="_blank">history</a>, and offerings.</p><p>Using the arrows on the right side of the box, you can see the <a href="https://www.ieee.org/advancing-technology/building-better-world" rel="noopener noreferrer" target="_blank">Building a Better World</a> section, where visitors can learn about humanitarian initiatives such as <a href="https://spectrum.ieee.org/ieee-move-hurricane-helene-milton" target="_self">IEEE MOVE</a> and <a href="https://spectrum.ieee.org/epics-in-ieee-15th-anniversary" target="_self">EPICS in IEEE</a>, then <a href="https://www.ieee.org/education-career/advance-your-career" rel="noopener noreferrer" target="_blank">Career Support</a> and, finally, an option to <a href="https://www.ieee.org/membership/join" rel="noopener noreferrer" target="_blank">join IEEE</a> and be part of something bigger.</p><p>Scrolling down the home page, the next module, Happening Across IEEE, features upcoming <a href="https://www.ieee.org/conferences-events" rel="noopener noreferrer" target="_blank">conferences</a>, the latest <a href="https://www.ieee.org/ieee-standards" rel="noopener noreferrer" target="_blank">standards</a>, new <a href="https://www.ieee.org/education-career/continue-your-education" rel="noopener noreferrer" target="_blank">educational courses</a>, <a href="https://www.ieee.org/education-career/advance-your-career" rel="noopener noreferrer" target="_blank">ways to advance your career</a>, and how to get involved with <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">IEEE’s societies, councils, and communities</a>.</p><p class="pull-quote">“The new website is more visual, with video and other media to engage all visitors. It also showcases our global community’s commitment as a public charity advancing technology for the benefit of humanity.”</p><p>The next section, the IEEE Is the Global Community of Technology Professionals module, has options to Find Your Path to learn about resources available for <a href="https://www.ieee.org/ieee-industry-engagement-committee" target="_blank">industry professionals</a>, <a href="https://www.ieee.org/publications-research/publish-ieee" rel="noopener noreferrer" target="_blank">authors and researchers</a>, <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">students and young professionals</a>, <a href="https://www.ieee.org/communities-connection/volunteering" rel="noopener noreferrer" target="_blank">volunteers</a>, <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">new members</a>, and <a href="https://spectrum.ieee.org/ieee-life-members-reconnect" target="_self">retirees</a>.</p><p>The following section, Latest Innovations, features videos and articles from publications including <a href="https://spectrum.ieee.org/" target="_self"><em>IEEE Spectrum</em></a> and <a href="https://spectrum.ieee.org/the-institute/" target="_self"><em><em>The Institute</em></em></a> on cutting-edge technology engineers are working on, such as <a href="https://spectrum.ieee.org/plant-electronic-tattoo?_gl=1*gas32r*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE." target="_self">electronic tattoos</a>.</p><p>Keep scrolling down and you’ll get to know IEEE members and their thoughts on what’s next for technologies such as <a href="https://transmitter.ieee.org/?_gl=1*1hbzxb6*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE." rel="noopener noreferrer" target="_blank">artificial intelligence</a> and <a href="https://transmitter.ieee.org/iot-2025/?_gl=1*1hbzxb6*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE.#story-5" rel="noopener noreferrer" target="_blank">quantum computing</a>.</p><p>“This redesign marks a key milestone in IEEE’s digital transformation,” Muirhead says. “The use of rich media, video content, and dynamic storytelling features allows for deeper engagement with IEEE and understanding its various offerings.</p><p>“However, it is just the beginning. In the months ahead, we will continue to enhance the site with new features, updated content, and richer tools.”</p>]]></description><pubDate>Thu, 19 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-revamped-online-presence</guid><category>Careers</category><category>Educational courses</category><category>Ieee members</category><category>Ieee products and services</category><category>Publications</category><category>Standards</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-smartphone-digital-tablet-desktop-computer-and-laptop-arranged-closely-together-all-of-the-devices-screens-display-ieees.jpg?id=61006929&amp;width=980"></media:content></item><item><title>A New BCI Instantly Synthesizes Speech</title><link>https://spectrum.ieee.org/bci-speech-synthesis</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-women-analyzing-data-on-three-computer-screens-in-a-modern-workspace.jpg?id=61072467&width=1200&height=800&coordinates=120%2C0%2C120%2C0"/><br/><br/><p>By analyzing neural signals, a brain-computer interface (BCI) can now almost instantaneously synthesize the speech of a man who lost use of his voice due to a neurodegenerative disease, a new study finds.</p><p>The researchers caution it will still be a long time before such a device, which could restore speech to paralyzed patients, will find use in everyday communication. Still, the hope is this work “will lead to a pathway for improving these systems further—for example, through technology transfer to industry,” says <a href="https://maitreyeew.github.io/" rel="noopener noreferrer" target="_blank">Maitreyee Wairagkar</a>, a project scientist at the University of California Davis’s Neuroprosthetics Lab.</p><p>A major potential application for <a href="https://spectrum.ieee.org/soft-robotics" target="_self">brain-computer interfaces</a> is restoring the ability to<strong> </strong>communicate to people who can no longer speak due to disease or injury. For instance, scientists have developed a number of BCIs that can help translate <a href="https://spectrum.ieee.org/braincomputer-interface-smashes-previous-record-for-typing-speed" target="_self">neural signals into text</a>.</p><p>However, text alone fails to capture many key aspects of human speech, such as intonation, that help to convey meaning. In addition, text-based communication is slow, Wairagkar says.</p><p>Now, researchers have developed what they call a brain-to-voice neuroprosthesis that can decode neural activity into sounds in real time. They detailed <a href="https://www.nature.com/articles/s41586-025-09127-3" rel="noopener noreferrer" target="_blank">their findings</a> 11 June in the journal <em>Nature</em>.</p><p>“Losing the ability to speak due to neurological disease is devastating,” Wairagkar says. “Developing a technology that can bypass the damaged pathways of the nervous system to restore speech can have a big impact on the lives of people with speech loss.”</p><h2>Neural Mapping for Speech Restoration</h2><p>The new BCI mapped neural activity using four microelectrode arrays. In total, the scientists placed 256 microelectrode arrays in three brain regions, chief among them the ventral precentral gyrus, which plays a key role in controlling the muscles underlying speech.</p><p>“This technology does not ‘read minds’ or ‘read inner thoughts,’” Wairagkar says. “We record from the area of the brain that controls the speech muscles. Hence, the system only produces voice when the participant voluntarily tries to speak.”</p><p>The researchers implanted the BCI in a 45-year-old volunteer with <a href="https://www.mayoclinic.org/diseases-conditions/amyotrophic-lateral-sclerosis/symptoms-causes/syc-20354022" rel="noopener noreferrer" target="_blank">amyotrophic lateral sclerosis</a> (ALS), the neurodegenerative disorder also known as Lou Gehrig’s disease. Although the volunteer could still generate vocal sounds, he was unable to produce intelligible speech on his own for years before the BCI.</p><p>The neuroprosthesis recorded the neural activity that resulted when the patient attempted to read sentences on a screen out loud. The scientists then trained a <a href="https://spectrum.ieee.org/realtime-hologram" target="_self">deep-learning AI model</a> on this data to produce his intended speech.</p><p>The researchers also trained a voice-cloning AI model on recordings made of the patient before his condition so the BCI could synthesize his pre-ALS voice. The patient reported that listening to the synthesized voice “made me feel happy, and it felt like my real voice,” the study notes.</p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="607bfba9c1ebdc9e03874d7ef83bc6a4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fdfL5P4n6vc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">Neuroprosthesis Reproduces a Man’s Speech</small> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">UC Davis</small> </p><p>In experiments, the scientists found that the BCI could detect key aspects of intended vocal intonation. They had the patient attempt to speak sets of sentences as either statements, which had no changes in pitch, or as questions, which involved rising pitches at the ends of the sentences. They also had the patient emphasize one of the seven words in the sentence “<a href="https://www.wired.com/story/one-sentence-with-7-meanings-unlocks-a-mystery-of-human-speech/" rel="noopener noreferrer" target="_blank">I never said she stole my money</a>” by changing its pitch. (The sentence has seven different meanings, depending on which word is emphasized.) These tests revealed increased neural activity toward the ends of the questions and before emphasized words. In turn, this let the patient control his BCI voice enough to ask a question, emphasize specific words in a sentence, or sing three-pitch melodies.</p><p>“Not only what we say but also how we say it is equally important,” Wairagkar says. “Intonation of our speech helps us to communicate effectively.”</p><p>All in all, the new BCI could acquire neural signals and produce sounds with a delay of 25 milliseconds, enabling near-instantaneous speech synthesis, Wairagkar says. The BCI also proved flexible enough to speak made-up pseudo-words, as well as interjections such as “ahh,” “eww,” “ohh,” and “hmm.”</p><p>The resulting voice was often intelligible, but not consistently so. In tests where human listeners had to transcribe the BCI’s words, they understood what the patient said about 56 percent of the time, up from about 3 percent from when he did not use the BCI.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Computer screen displaying neural signal data in multiple graph plots." class="rm-shortcode" data-rm-shortcode-id="a6610d42c204fca95be9c543fa52613f" data-rm-shortcode-name="rebelmouse-image" id="37c4f" loading="lazy" src="https://spectrum.ieee.org/media-library/computer-screen-displaying-neural-signal-data-in-multiple-graph-plots.jpg?id=61072472&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Neural recordings of the BCI participant shown on screen.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">UC Davis</small></p><p>“We do not claim that this system is ready to be used to speak and have conversations by someone who has lost the ability to speak,” Wairagkar says. “Rather, we have shown a proof of concept of what is possible with the current BCI technology.”</p><p>In the future, the scientists plan to improve the accuracy of the device—for instance, with more electrodes and better AI models. They also hope that BCI companies might start clinical trials incorporating this technology. “It is yet unknown whether this BCI will work with people who are fully locked in”—that is, nearly completely paralyzed, save for eye motions and blinking, Wairagkar adds.</p><p>Another interesting research direction is to study whether such speech BCIs could be useful for people with language disorders, such as <a href="https://www.mayoclinic.org/diseases-conditions/aphasia/symptoms-causes/syc-20369518" target="_blank">aphasia</a>. “Our current target patient population cannot speak due to muscle paralysis,” Wairagkar says. “However, their ability to produce language and cognition remains intact.” In contrast, she notes, future work might investigate restoring speech to people with damage to brain areas that produce speech, or with disabilities that have prevented them from learning to speak since childhood.</p>]]></description><pubDate>Thu, 19 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/bci-speech-synthesis</guid><category>Neuroprosthetic</category><category>Bci</category><category>Deep learning</category><category>Artificial intelligence</category><category>Brain computer interface</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-women-analyzing-data-on-three-computer-screens-in-a-modern-workspace.jpg?id=61072467&amp;width=980"></media:content></item><item><title>Guatemalan Engineer Grew From Rural Roots to Ph.D.</title><link>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&width=1200&height=800&coordinates=0%2C242%2C0%2C243"/><br/><br/><p>Although she is just now starting her career as a tech professional,<a href="https://fr.linkedin.com/in/mayra-yucely-beb-caal-b1a1b969" rel="noopener noreferrer" target="_blank"> Mayra Yucely Beb Caal</a> has already overcome towering obstacles. The IEEE member sees her life as an example for other young people, demonstrating that they can succeed despite disadvantages they face due to their gender, ethnicity, language, or economic background.</p><p>Born in Cobán, the capital of Alta Verapaz in northern Guatemala, she grew up in a community far removed from the world of technology. But she attributes her success to having been steeped in the region’s cultural richness and her people’s unshakable resilience. The daughter of a single mother who was a schoolteacher, Caal says she spent her early years living with her aunts while her mother worked in distant towns for weeks at a time to provide for the family. In her community—mostly descendants of the indigenous<a href="https://en.wikipedia.org/wiki/Q%CA%BCeqchi%CA%BC" rel="noopener noreferrer" target="_blank"> Maya-Kekchi</a> people—technology was rarely discussed. Pursuing a degree meant studying to become a physician, the most prestigious occupation anyone there was aware of.</p><p>No one imagined that a girl from Cobán would one day hold a doctorate in engineering or conduct cancer research in France.</p><p>On the path to her ambitious goals, Caal got a big assist from IEEE. She received a <a href="https://ieeesystemscouncil.org/awards/james-o-gray-scholarship" rel="noopener noreferrer" target="_blank">Gray scholarship</a>, awarded by the IEEE Systems Council to students pursuing graduate studies in process control systems engineering, plant automation, or instrumentation measurement. The US $5,000 award supplemented other scholarships which helped her to study for her Ph.D. </p><h2>Discovering robotics and mechatronics in high school</h2><p>Caal was introduced to technology when, at age 14, she received a government scholarship to attend the<a href="https://au.linkedin.com/school/intecapoficial/" rel="noopener noreferrer" target="_blank"> Instituto Técnico de Capacitación y Productividad</a>, a high school in Guatemala City. It was her first exposure to electronics, <a href="https://spectrum.ieee.org/topic/robotics/" target="_self">robotics</a>, and <a href="https://spectrum.ieee.org/houston-mechatronics-raises-20m-to-bring-nasa-expertise-to-transforming-robot-submersibles" target="_self">mechatronics</a> (an interdisciplinary field that combines mechanical engineering, electronics, computer science, and control systems)—subjects that weren’t taught in her local school. Caal was fascinated by the ability to study the fields, though her family couldn’t afford the tuition to the private universities where she could earn a degree. But that didn’t dissuade her.</p><h2>Pursuing a mechatronics career despite gender barriers</h2><p>She applied for a scholarship from<a href="https://fundacionjbg.org/" rel="noopener noreferrer" target="_blank"> the Gutiérrez Foundation</a>, named for the founder of <a href="https://somoscmi.com/en/" rel="noopener noreferrer" target="_blank">CMI</a>, a Guatemala-based multinational company. The foundation’s scholarship covers full tuition, fees, and the cost of books for the duration of a recipient’s undergraduate studies.</p><p>In 2016 Caal earned a bachelor’s degree in mechatronics engineering at the<a href="https://www.uvg.edu.gt/" rel="noopener noreferrer" target="_blank"> Universidad del Valle de Guatemala</a>, also in Guatemala City. There were few women in her class.</p><p>The job market was unwelcoming, however, she says. Despite her credentials, employers often required five years of experience for entry-level positions, and they expressed a preference for male employees, she says. It took six months to land her first job as a mechanical maintenance supervisor near her hometown.</p><p>She held that job for six months before moving back to Guatemala City in search of better opportunities. She took a position as head of mechanical maintenance at <a href="https://mayaprin.com/" rel="noopener noreferrer" target="_blank">Mayaprin</a>, a company specializing in commercial printing services, but she wasn’t satisfied with her career trajectory.</p><h2>Earning an engineering education abroad</h2><p>Caal decided to return to school in 2018 to pursue a master’s degree in mechatronics and micromechatronics engineering. She received a scholarship from the<a href="https://erasmus-plus.ec.europa.eu/opportunities/opportunities-for-individuals/students/erasmus-mundus-joint-masters" rel="noopener noreferrer" target="_blank"> Mundus Joint Master</a> program, part of a European Commission–sponsored initiative that provides funding for education, training, and youth in sports. Because the Mundus scholarship requires recipients to study at several universities, she took classes at schools in Europe and Africa, including <a href="https://www.supmicrotech.fr/en" rel="noopener noreferrer" target="_blank">École Nationale Supérieure de Mécanique et des Microtechniques</a>, <a href="https://nu.edu.eg/" rel="noopener noreferrer" target="_blank">Nile University</a>, and <a href="https://www.uniovi.es/en/" rel="noopener noreferrer" target="_blank">Universidad de Oviedo</a>. Her studies focused on mechatronics and microelectronics, and the courses were taught in French, English, and Spanish.</p><p>The multilingual challenge was immense, she says. She recently had learned English, and French was completely new to her. Yet she persevered, driven by her goal of working on technology that could serve humanity.</p><p>She received a master’s degree from Universidad de Oviedo in 2020 and was accepted into a Ph.D. program at <a href="https://www.univ-fcomte.fr/universite-bourgogne-franche-comte" rel="noopener noreferrer" target="_blank">Université de Bourgogne Franche-Comté</a>, in Besançon, France. Her doctoral studies  were mainly funded by the French Ministry of Higher Education and Research (MESRI), with additional support from the Gray scholarship..</p><p>Her research led to a full-time job last year as an R&D engineer focused on mechatronics and robotics at<a href="https://www.hyprview.com/" rel="noopener noreferrer" target="_blank"> HyprView</a> in Caen, France. The startup, founded in 2022, develops AI-enabled analysis tools to assist with medical data analysis.</p><p>Caal says she is part of a team that uses AI and automated systems to improve cancer detection. Although she has held the position for less than a year, she says she already feels she is contributing to public health through applied technology.</p><h2>IEEE support and STEM mentorship</h2><p>Through much of Caal’s journey, IEEE has played a critical role. As an undergraduate, she was vice president and then president of her university’s <a href="https://www.facebook.com/IEEEUVG/?locale=pt_PT" rel="noopener noreferrer" target="_blank">IEEE student branch</a>. Her first international conference experience came from attending<a href="https://r9.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Region 9</a> conferences, which she says opened her eyes to the world of research, publishing, and the global engineering community.</p><p>She organized outreach efforts to local schools, conducting simple experiments to encourage girls to consider STEM careers. Her efforts were in direct opposition to longstanding gender norms in Guatemala. Caal was also an active member of the IEEE <a href="https://www.femto-st.fr/en/femto-st-student-chapter-0" rel="noopener noreferrer" target="_blank">student branch at FEMTO-ST /Université de Bourgogne Franche-Comté</a>.</p><p>Today, Caal continues to advise these student branches while advancing her career in France.</p><p>Language issues and gender bias remain obstacles: “As a young woman leading male engineers, I have repeatedly had to prove my competence in ways my male peers haven’t,” she says. But the challenges have only strengthened her resolve, she adds.</p><p>Eventually, she says, she hopes to return to Guatemala to help build a stronger research infrastructure there with sufficient career opportunities for tech professionals in industry and academia. She says she also wants to ensure that children in even the most rural, poverty-stricken schools have access to food, electricity, and the Internet.</p><p>Her mission is clear: “To use technology to serve a purpose, always aimed at improving lives.”</p><p>“I don’t want to create technology just for the sake of it,” she says. “I want it to mean something—to help solve real problems in society, like the ones I faced early on.” </p>]]></description><pubDate>Tue, 17 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</guid><category>Cancer detection</category><category>Guatemala</category><category>Ieee member news</category><category>Ieee systems council</category><category>James o. gray scholarship</category><category>Mechatronics</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&amp;width=980"></media:content></item><item><title>Why JPEGs Still Rule the Web</title><link>https://spectrum.ieee.org/jpeg-image-format-history</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&width=1200&height=800&coordinates=0%2C250%2C0%2C250"/><br/><br/><p><span>For roughly three decades, the JPEG has been the World Wide Web’s primary image format. But it wasn’t the one the Web started with. In fact, the first mainstream graphical browser, NCSA Mosaic, didn’t initially support inline JPEG files—</span><a href="https://ftp.jurassic.nl/pub/irix/mosaic/Mac/FAQ/FAQ.HTML" target="_blank">just inline GIFs</a><span>, along with a couple of other </span><a href="https://spectrum.ieee.org/carnegie-mellon-is-saving-old-software-from-oblivion" target="_blank">formats forgotten to history</a><span>. However, the JPEG had many advantages over the format it quickly usurped.</span></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://tedium.co/" target="_blank"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://tedium.co/" target="_blank"><img alt='Tedium logo, a red rectangle with the word Tedium in white, above the text "This post originally appeared on Tedium."' class="rm-shortcode" data-rm-shortcode-id="c603546dab9e1dd1612b1364d3107471" data-rm-shortcode-name="rebelmouse-image" id="6aeb5" loading="lazy" src="https://spectrum.ieee.org/media-library/tedium-logo-a-red-rectangle-with-the-word-tedium-in-white-above-the-text-this-post-originally-appeared-on-tedium.png?id=60568211&width=980"/></a><small class="image-media media-photo-credit" placeholder="add photo credit..."><a href="https://spectrum.ieee.org/media-library/eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjpbwfnzsi6imh0dhbzoi8vyxnzzxrzlnjibc5tcy82mdu2odixms9vcmlnaw4ucg5niiwizxhwaxjlc19hdci6mtc1nzmynzi1mn0._gbglxpbsmfwoobs84_whxbl_vnslwx1geovlhgvwku/image.png?width=980" target="_blank"> </a></small></p><p>Despite not appearing together right away—the JPEG first appeared in Netscape in 1994, two years after the image standard was officially published—the JPEG and Web browser fit together naturally. JPEG files degraded more gracefully than GIFs, retaining more of the picture’s initial form—and that allowed the format to scale to greater levels of success. While it wasn’t capable of animation, it progressively expanded from something a modem could pokily render to a format that was good enough for high-end professional photography.</p><p>For the Internet’s purposes, the degradation was the important part. But it wasn’t the only thing that made the JPEG immensely valuable to the digital world. An essential part was that it was a documented standard built by numerous stakeholders.</p><h2>The GIF was a de facto standard. The JPEG was an actual one</h2><p>How important is it that JPEG was a standard? Let me tell you a story.</p><p>During <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2013/05/21/an-honor-for-the-creator-of-the-gif/?smid=tw-nytimes" target="_blank">a 2013 <em><em>New York Times</em></em> interview</a> conducted just before he received an award honoring his creation, GIF creator Steve Wilhite stepped into a debate he unwittingly created. <span>Simply put, nobody knew how to pronounce the acronym for the image format he had fostered, the Graphics Interchange Format. He used the moment to attempt to set the record straight—it was pronounced like the peanut butter brand: “It is a soft ‘G,’ pronounced ‘jif.’ End of story,” he said.</span></p><p>I <a href="https://shortformblog.com/post/51026114908/steve-wilhite-gif-award" target="_blank">posted a quote from Wilhite</a> on my popular Tumblr around that time, a period when the social media site was the center of the GIF universe. And soon afterward, my post got thousands of reblogs—nearly all of them disagreeing with Wilhite. Soon, <a href="https://knowyourmeme.com/memes/gif-vs-jif-pronunciation-debate/" target="_blank">Wilhite’s quote became a meme</a>.</p><p>The situation paints how Wilhite, who died in 2022, did not develop his format by committee. He could say it sounded like “JIF” because he built it himself. He was handed the project as a CompuServe employee in 1987; he produced the object, and that was that. The initial document describing how it works? <a href="https://www.w3.org/Graphics/GIF/spec-gif87.txt" target="_blank">Dead simple</a>. Thirty-eight years later, we’re still using the GIF—but it never rose to the same prevalence of JPEG.</p><p>The JPEG, which formally emerged about five years later, was very much <em><em>not</em></em> that situation. Far from it, in fact—it’s the difference between a de facto standard and an actual one. And that proved essential to its eventual ubiquity.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Full-resolution photo of a sunlit pine forest with a narrow trail winding through the trees and grassy undergrowth." class="rm-shortcode" data-rm-shortcode-id="d5f40fd52d60e8ec21c06940db3febc7" data-rm-shortcode-name="rebelmouse-image" id="2999a" loading="lazy" src="https://spectrum.ieee.org/media-library/full-resolution-photo-of-a-sunlit-pine-forest-with-a-narrow-trail-winding-through-the-trees-and-grassy-undergrowth.jpg?id=61013768&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">We’re going to degrade the quality of this image throughout this article. At its full image size, it’s 13.7 megabytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Irina Iriser</small></p><h2>How the JPEG format came to life</h2><p>Built with input from dozens of stakeholders, the Joint Photographic Experts Group ultimately aimed to create a format that fit everyone’s needs. (Reflecting its committee-led roots, there would be no confusion about the format’s name—an acronym of the organization that designed it.) And when the format was finally unleashed on the world, it was the subject of a book that was more than 600 pages.</p><p><em><em>JPEG: Still Image Data Compression Standard</em></em>, written by IBM employees and JPEG organization stakeholders William B. Pennebaker and Joan L. Mitchell, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA1&printsec=frontcover" target="_blank">describes</a> a landscape of multimedia imagery, held back without a way to balance the need for photorealistic images and immediacy. Standardization, they believed, could fix this.</p><p>“The problem was not so much the lack of algorithms for image compression (as there is a long history of technical work in this area),” the authors wrote, “but, rather, the lack of a standard algorithm—one which would allow an interchange of images between diverse applications.”</p><p>And they were absolutely right. For more than 30 years, JPEG has made high-quality, high-resolution photography accessible in operating systems far and wide. Although we no longer need to compress JPEGs to within an inch of their life, having that capability helped enable the modern Internet.</p><p><a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&dq=ibm+jpeg&pg=PA278&printsec=frontcover" target="_blank">As the book notes</a>, Mitchell and Pennebaker were given IBM’s support to follow through this research and work with the JPEG committee, and that support led them to develop many of the JPEG format’s foundational patents. Described in <a href="https://patents.google.com/patent/US4905297" target="_blank">patents</a> filed by Mitchell and Pennebaker in 1988, IBM and other members of the JPEG standards committee, such as AT&T and Canon, were developing ways to use compression to make high-quality images easier to deliver in confined settings.</p><p>Each member brought their own needs to the process. Canon, obviously, was more focused on printers and photography, while AT&T’s interests were tied to data transmission. Together, the companies left behind a standard that has stood the test of time.</p><p>All this means, funnily enough, that the first place that a program capable of using JPEG compression appeared was not MacOS or Windows, but OS/2—a fascinating-but-failed graphical operating system created by Pennebaker and Mitchell’s employer, IBM. As early as 1990, OS/2 supported the format through the <a href="https://www.edm2.com/index.php/OS/2_Image_Support" target="_blank">OS/2 Image Support</a> application.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Nearly identical photo of a sunlit pine forest." class="rm-shortcode" data-rm-shortcode-id="b810c6423ebd0b3e07f4d42c4c7162ac" data-rm-shortcode-name="rebelmouse-image" id="ef951" loading="lazy" src="https://spectrum.ieee.org/media-library/nearly-identical-photo-of-a-sunlit-pine-forest.jpg?id=61015732&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">At 50 percent of its initial quality, the image is down to about 2.6 MB. By dropping half of the image’s quality, we brought it down to one-fifth of the original file size. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><h2>What a JPEG does when you heavily compress it</h2><p>The thing that differentiates a JPEG file from a PNG or a GIF is how the data degrades as you compress it. The goal for a JPEG image is to still look like a photo when all is said and done, even if some compression is necessary to make it all work at a reasonable size. That way, you can display something that looks close to the original image in fewer bytes.</p><p>Or, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA4&printsec=frontcover" target="_blank">as Pennebaker and Mitchell put it</a>, “the most effective compression is achieved by approximating the original image (rather than reproducing it exactly).”</p><p>Central to this is a compression process called <a href="https://spectrum.ieee.org/compression-algorithms" target="_blank">discrete cosine transform</a> (DCT), a lossy form of compression encoding heavily used in all sorts of compressed formats, most notably in digital audio and signal processing. Essentially, it delivers a lower-quality product by removing details, while still keeping the heart of the original product through approximation. The stronger the cosine transformation, the more compressed the final result.</p><p>The algorithm, <a href="https://ieeexplore.ieee.org/abstract/document/1672377" target="_blank">developed by researchers</a> in the 1970s, essentially takes a grid of data and treats it as if you’re controlling its frequency with a knob. The data rate is controlled like water from a faucet: The more data you want, the higher the setting. DCT allows a trickle of data to still come out in highly compressed situations, even if it means a slightly compromised result. In other words, you may not keep all the data when you compress it, but DCT allows you to keep the heart of it.</p><p>(See <a href="https://www.youtube.com/watch?v=Q2aEzeMDHMA" target="_blank">this video</a> for a more technical but still somewhat easy-to-follow description of DCT.)</p><p>DCT is everywhere. If you <a href="https://ottverse.com/discrete-cosine-transform-dct-video-compression/" target="_blank">have ever seen a streaming video</a> or an online radio stream that degraded in quality because your bandwidth suddenly declined, you’ve witnessed DCT being utilized in real time.</p><p>A JPEG file doesn’t have to leverage the DCT with just one method, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA81&printsec=frontcover" target="_blank">as <em><em>JPEG: Still Image Data Compression Standard</em></em> explains</a>:</p><blockquote>The JPEG standard describes a family of large image compression techniques, rather than a single compression technique. It provides a “tool kit” of compression techniques from which applications can select elements that satisfy their particular requirements.</blockquote><p>The toolkit has four modes:</p><ul><li><strong>Sequential DCT,</strong> which displays the compressed image in order, like a window shade slowly being rolled down</li><li><strong>Progressive DCT,</strong> which displays the full image in the lowest-resolution format, then adds detail as more information rolls in</li><li><strong>Sequential lossless,</strong> which uses the window-shade format but doesn’t compress the image</li><li><strong>Hierarchical mode,</strong> which combines the prior three modes—so maybe it starts with a progressive mode, then loads sequential lossless compression slowly, but then reaches a lossless final result</li></ul><p>At the time the JPEG was being created, modems were extremely common. That meant images loaded slowly, making Progressive DCT the most fitting format for the early Internet. Over time, the progressive DCT mode has become less common, as many computers can simply load the sequential DCT in one fell swoop.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with very slight degradation visible." class="rm-shortcode" data-rm-shortcode-id="44d7f73d86c46d8ae4653970ebbffbdd" data-rm-shortcode-name="rebelmouse-image" id="fd0cd" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-very-slight-degradation-visible.jpg?id=61029700&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">That same forest, saved at 5 percent quality, now down to about 419 kilobytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>When an image is compressed with DCT, the change tends to be less noticeable in busier, more textured areas of the picture, like hair or foliage. Those areas are harder to compress, which means they keep their integrity longer. It tends to be more noticeable, however, with solid colors or in areas where the image sharply changes from one color to another—like text on a page. Ever screenshot a social media post, only for it to look noisy? Congratulations, you just made a JPEG file.</p><p>Other formats, like PNG, do better with text, because their compression format is intended to be non-lossy. (Side note: PNG’s compression format, DEFLATE, <a href="https://www.ietf.org/rfc/rfc1951" target="_blank">was designed</a> by Phil Katz, who also created the ZIP format. The PNG format uses it in part because it was a license-free compression format. So it turns out the brilliant coder with the <a href="https://www.wsj.com/articles/SB961363319756539141" target="_blank">sad life story</a> improved the Internet in multiple ways before his <a href="https://tedium.co/2015/02/17/early-internet-history-tales/" target="_blank">untimely passing</a>.)</p><p>In many ways, the JPEG is one tool in our image-making toolkit. Despite its age and maturity, it remains one of our best options for sharing photos on the Internet. But it is not a tool for every setting—despite the fact that, like a wrench sometimes used as a hammer, we often leverage it that way.</p><h2>Forgent Networks claimed to own the JPEG’s defining algorithm</h2><p>The JPEG format gained popularity in the ’90s for reasons beyond the quality of the format. Patents also played a role: Starting in 1994, the tech company Unisys <a href="https://www.theregister.com/1999/09/01/unisys_demands_5k_licence_fee/" target="_blank">attempted to bill individual users</a> who relied on GIF files, which used a patent the company owned. This made the free-to-use JPEG more popular. (This situation also led to the creation of the patent-free PNG format.)</p><p>While the JPEG was standards-based, it could still have faced the same fate as the GIF, thanks to the quirks of the patent system. A few years before the file format came to life, a pair of Compression Labs employees <a href="https://patents.google.com/patent/US4698672A/en" target="_blank">filed a patent application</a> that dealt with the compression of motion graphics. By the time anyone noticed its similarity to JPEG compression, the format was ubiquitous.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with more noticeable color degradation visible. Areas with previously subtle color gradients now appear more like blocks of color." class="rm-shortcode" data-rm-shortcode-id="5fb227e9168811101372ad575e42dc89" data-rm-shortcode-name="rebelmouse-image" id="e1296" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-more-noticeable-color-degradation-visible-areas-with-previously-subtle-color-gradie.jpg?id=61016218&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Our forest, saved at 1 percent quality. This image is only about 239 KB in size, yet it’s still easily recognizable as the same photo. That’s the power of the JPEG.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Then in 1997, a company named Forgent Networks acquired Compression Labs. The company eventually spotted the patent and began filing lawsuits over it, a series of events it saw as a stroke of good luck.</p><p>“The patent, in some respects, is a lottery ticket,” Forgent CEO Jay Peterson <a href="https://www.cnet.com/tech/tech-industry/staking-a-claim-in-the-patent-gold-mine/" target="_blank">told <em><em>CNET</em></em> in 2005</a>. “If you told me five years ago that ‘You have the patent for JPEG,’ I wouldn’t have believed it.”</p><p>While Forgent’s claim of ownership of the JPEG compression algorithm was tenuous, it ultimately saw more success with its legal battles than Unisys did. The company earned more than US $100 million from digital-camera makers before the patent finally ran out of steam around 2007. The company also attempted to extract licensing fees from the PC industry. Eventually, Forgent agreed <a href="https://www.cnet.com/tech/tech-industry/forgent-settles-jpeg-patent-cases/" target="_blank">to a modest $8 million</a> settlement.</p><p>As the company took an increasingly aggressive approach to its acquired patent, it began to lose battles both in the court of public opinion and in actual courtrooms. <a href="https://arstechnica.com/uncategorized/2006/05/6930-2/" target="_blank">Critics pounced on examples of prior art</a>, while courts limited the patent’s use to motion-based uses like video.</p><p>By 2006, Forgent’s compression patent expired—and its litigation-heavy approach to business went away. That year, the company became <a href="https://www.asuresoftware.com" target="_blank">Asure Software</a>, which now specializes in payroll and HR solutions. Talk about a reboot.</p><h2>Why the JPEG won’t die</h2><p>The JPEG file format has served us well. It’s been difficult to remove the format from its perch. The JPEG 2000 format, for example, was intended to supplant it by offering more lossless options and better performance. The format is <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000143.shtml" target="_blank">widely used by the Library of Congress</a> and specialized sites like the Internet Archive; however, it is less popular as an end-user format.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Animated GIF of the forest images, starting at full resolution and progressing through increasingly degraded version of the iamge." class="rm-shortcode" data-rm-shortcode-id="ea7ce7f87d0b7e0afed75e4a9a57e2a7" data-rm-shortcode-name="rebelmouse-image" id="171a2" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-gif-of-the-forest-images-starting-at-full-resolution-and-progressing-through-increasingly-degraded-version-of-the-iamg.gif?id=61016209&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">See the forest JPEG degrade from its full resolution to 1 percent quality in this GIF. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Other image technologies have had somewhat more luck getting past the JPEG format. The Google-supported <a href="https://developers.google.com/speed/webp" target="_blank">WebP</a> is popular with website developers (<a href="https://www.pcgamer.com/heres-why-you-have-to-deal-with-so-many-annoying-webps-now/" target="_blank">and controversial</a> with end users). Meanwhile, the formats <a href="https://aomediacodec.github.io/av1-avif/" target="_blank">AVIF</a> and <a href="https://www.iso.org/standard/83650.html" target="_blank">HEIC</a>, each developed by standards bodies, have largely outpaced both JPEG and JPEG 2000.</p><p>Still, the JPEG will be difficult to kill at this juncture. These days, the format is similar to MP3 or ZIP files—two legacy formats too popular and widely used to kill. Other formats that compress the files better and do the same things more efficiently are out there, but it’s difficult to topple a format with a 30-year head start.</p><p>Shaking off the JPEG is easier said than done. I think most people will be fine to keep it around.</p><p><em><em>Ernie Smith is the editor of </em></em><a href="https://tedium.co/" target="_blank"><em><em>Tedium</em></em></a><em><em>, a long-running newsletter that hunts for the end of the long tail.</em></em></p><em><em><strong></strong><span><em>This article was updated on 01 July 2025.</em></span><br/></em></em>]]></description><pubDate>Tue, 17 Jun 2025 14:45:46 +0000</pubDate><guid>https://spectrum.ieee.org/jpeg-image-format-history</guid><category>Compression algorithms</category><category>Digital photography</category><category>Discrete cosine transform</category><category>Image compression</category><category>Jpeg</category><dc:creator>Ernie Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1200&height=800&coordinates=0%2C0%2C0%2C210"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Deep learning</category><category>Artificial intelligence</category><category>Andrew ng</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Matlab</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Quantum computing</category><category>2d materials</category><category>Ibm</category><category>Qubits</category><category>Hexagonal boron nitride</category><category>Superconducting qubits</category><category>Mit</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>