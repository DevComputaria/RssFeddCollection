<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/"><channel><title>IEEE Spectrum</title><link>https://spectrum.ieee.org/</link><description>IEEE Spectrum</description><atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self"></atom:link><language>en-us</language><lastBuildDate>Mon, 23 Jun 2025 19:27:11 -0000</lastBuildDate><image><url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2MzA3MTQzOX0.SxRBIud_XE2YWQFaIJD9BPB1w-3JsFhiRkJIIe9Yq-g/image.png?width=210</url><link>https://spectrum.ieee.org/</link><title>IEEE Spectrum</title></image><item><title>Another Plan to Test Satellite Deorbiting Takes Shape</title><link>https://spectrum.ieee.org/electrodynamic-tether-deborbit-satellite</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/gloved-hands-assembling-an-electronic-circuit-board-on-a-workbench.jpg?id=61083088&width=1200&height=800&coordinates=0%2C173%2C0%2C173"/><br/><br/><p>More and more satellites are being added to low Earth orbit (LEO) every month. As that number continues to increase, so do the risks of that <a href="https://spectrum.ieee.org/averting-space-doom-solving-the-orbital-junk-problem" target="_blank">critical area surrounding Earth becoming impassable</a>, trapping us on the planet for the foreseeable future. Ideas from different labs have presented potential solutions to this problem, but one of the most promising, <a href="https://en.wikipedia.org/wiki/Electrodynamic_tether" target="_blank">electrodynamic tethers</a> (EDTs), have only now begun to be tested in space. A new <a data-linked-post="2659652601" href="https://spectrum.ieee.org/cubesat" target="_blank">CubeSat</a> called the <a href="https://www.researchgate.net/publication/391689140_SPARCS_Design_and_Development_of_CubeSats_for_Advanced_Research_and_Cooperative_Studies" target="_blank">Spacecraft for Advanced Research and Cooperative Studies</a> (SPARCS) mission from researchers at the <a href="https://en.wikipedia.org/wiki/Sharif_University_of_Technology" target="_blank">Sharif University of Technology</a> in Tehran hopes to contribute to that effort by testing an EDT and intersatellite communication system as well as collecting real-time data on the radiation environment of its orbital path.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://www.universetoday.com/"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" style="background: gray;" title="Select for lightbox">aspect_ratio</a><a href="https://www.universetoday.com/" target="_blank"><img alt='Universe Today logo; text reads "This post originally appeared on Universe Today."' class="rm-shortcode" data-rm-shortcode-id="087bc0ad57330681cee5d3354f3d2ac7" data-rm-shortcode-name="rebelmouse-image" id="b9dd2" loading="lazy" src="https://spectrum.ieee.org/media-library/universe-today-logo-text-reads-this-post-originally-appeared-on-universe-today.png?id=60568425&width=980"/></a></p><p>SPARCS actually consists of two separate CubeSats. SPARCS-A is a <a href="https://en.wikipedia.org/wiki/CubeSat#Design" target="_blank">1U CubeSat</a> primarily designed as a communications platform, with the mission design requiring it to talk to SPARCS-B, which is a 2U CubeSat that, in addition to the communication system, contains a EDT. That EDT, which can measure up to 12 meters in length, is deployed via a servomotor, with a camera watching to ensure proper deployment.</p><p>EDTs are essentially giant poles with electric current running through them. They use this current, and the tiny magnetic field it produces, to push off of the Earth’s natural magnetic sphere using a property called the Lorentz force. This allows the satellite to adjust its orbit without the use of fuel, simply by orienting its EDT in a specific direction (which the EDT itself can assist with) and then using the Lorentz force to either push it up into a higher orbit, or—more significant for the purposes for technology demonstration—to slow the CubeSat down to a point where it can make a controlled entry into the atmosphere.</p><p><span>That controlled-entry feature is why EDTs have garnered so much attention. Previous missions, such as <a href="https://www.sciencedirect.com/science/article/abs/pii/S0094576520301429" target="_blank">KITE</a> from JAXA and <a href="https://mdp.engin.umich.edu/research_teams/mitee-21/" target="_blank">MiTEE</a> from the University of Michigan, have already attempted to use EDTs to change their orbits. Unfortunately neither of those missions successfully utilized their EDT, though a follow-up mission called MiTEE-2 is in the works with an even larger EDT than SPARCS.</span></p><p>The final piece of SPARCS’ kit is its dosimeter, which is intended to monitor the radiation environment of its orbit. As anyone familiar with spacecraft design knows, radiation hardening of electronics is absolutely critical to the success of a mission, but it is also expensive and time consuming, so it is best done at a minimal required level. Understanding the radiation environment of this popular orbital path can help future engineers make better, and hopefully less expensive, design decisions tailored to operation in this specific area.</p><p><span>Engineers have already finalized the design for the mission and have run simulations showing its expected operations. They have now moved on to building an engineering model of the two CubeSats, allowing them to validate their design and test the real-world implementation before it is ready for launch. Given the <a href="https://spectrum.ieee.org/the-real-story-of-stuxnet" target="_blank">current turmoil</a> in that region of the world, there is a chance that conflict could put a halt to development of this system. But, if successfully tested and launched, the very first demonstration of an EDT system could be deployed in the not-too-distant future.</span></p>]]></description><pubDate>Mon, 23 Jun 2025 16:38:43 +0000</pubDate><guid>https://spectrum.ieee.org/electrodynamic-tether-deborbit-satellite</guid><category>Cubesats</category><category>Orbital debris</category><category>Satellites</category><category>Kessler syndrome</category><dc:creator>Andy Tomaswick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/gloved-hands-assembling-an-electronic-circuit-board-on-a-workbench.jpg?id=61083088&amp;width=980"></media:content></item><item><title>Transforming Physical Substation Security</title><link>https://spectrum.ieee.org/meerkat-substation-security</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/aerial-of-a-power-station-with-highlighted-threat-zones-and-bad-actor-location.png?id=61081553&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p><em>This is a sponsored article brought to you by <a href="https://meerkat.powereng.com/home?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB4" target="_blank">POWER Engineers, Member of WSP</a>.</em></p><p>Digital transformation is reshaping industries across the globe, and the power delivery sector is no exception. As demand for reliable and efficient energy supply continues to grow, the need to modernize and optimize operations becomes increasingly critical. By leveraging digital tools and technologies, utilities are unlocking unprecedented opportunities to enhance precision, efficiency and resilience throughout the power delivery value chain—from generation to distribution.</p><p>However, while digitalization offers transformative potential, the power delivery industry continues to grapple with substantial technical and operational challenges. Many utilities still operate with legacy or manual security protocols that rely on reactive rather than proactive strategies. The slow pace of technology adoption further compounds these issues, increasing the vulnerability of critical assets to inefficiencies, downtime and physical threats. Overcoming these obstacles requires a strategic shift toward innovative solutions that drive measurable improvements in safety, reliability and operational optimization.</p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="178ec3e93eff2ed8d3f8c00bbdb10be3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/D1Oishpaaps?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">Meerkat takes the guesswork out of substation security by integrating high-fidelity data with real-time 3D mitigation modeling. This sophisticated approach identifies all line-of-sight vulnerabilities, and delivers robust protection for critical infrastructure in an increasingly complex threat landscape.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Video: POWER Engineers, Member of WSP</small></p><h2>The Need for Digital Transformation in Physical Security</h2><p>Physical attacks on substations are becoming increasingly prevalent and sophisticated. As technology evolves, so do the bad actors that are trying to take down the grid. Many mitigation methods are no longer sufficient against modern methods of attack. These facilities, which are crucial to keeping the grid operational, must be able to comprehensively assess and adapt to new threats. Digital transformation is the key to this goal.</p><h3></h3><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Electric disturbance events by type, 2017-2023" class="rm-shortcode" data-rm-shortcode-id="980d5999fe51e7f2f205d16fe7ac7009" data-rm-shortcode-name="rebelmouse-image" id="abea8" loading="lazy" src="https://spectrum.ieee.org/media-library/electric-disturbance-events-by-type-2017-2023.png?id=60869959&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Physical breach events, defined here as physical attacks, vandalism, theft and suspicious activity, accounted for more than half of all electric disturbance events reported to the United States Department of Energy in 2023. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">POWER Engineers, Member of WSP</small></p><h3>Traditional Methods Fail to Meet Modern Demands</h3><p>Conventional site analysis methods in power delivery are often inefficient and prone to inaccuracies, particularly at substations, where the shortcomings can lead to significant vulnerabilities.</p><p>Physical site walkthroughs to identify areas of vulnerability, for example, are inherently subjective and susceptible to human error. Compounding matters, safety concerns in high-voltage environments, coordination challenges and access restrictions to areas not owned by the substation can result in incomplete assessments and evaluations fraught with delays.</p><p>Static analysis is also limited by outdated or erroneous publicly available data, hindering precise assessments and delaying decision-making processes. For instance, assets captured in publicly available data may misrepresent recent construction near the site, which may create new lines of sight to critical assets.</p><p class="pull-quote">Meerkat, developed by POWER Engineers, Member of WSP, leverages advanced technology to enhance threat assessment accuracy, significantly reducing assessment times, lowering mitigation costs and improving overall protection at substation facilities.</p><p>The Vulnerability of Integrated Security Analysis (VISA) method attempts to address some of these shortcomings by leveraging expert collaboration. Yet, it too has limitations—expertise variability among participants can lead to unrepresented perspectives, and reliance on static drawings and resources hampers effective visualization during sessions.</p><p>In contrast, some utilities opt for no analysis at all, erecting perimeter walls around facilities without pinpointing specific vulnerabilities. This approach often results in overbuilding and overspending while potentially leaving critical assets exposed due to overlooked threats from neighboring structures or terrain features.</p><p>Communication silos between stakeholders can also exacerbate these inefficiencies.</p><h2>It’s Time to Transform: Embrace Digital Solutions</h2><p>Emerging tools and technologies have the ability to address the longstanding inefficiencies in physical substation security.</p><h3>Enhance Precision and Efficiency</h3><p>Integrating cutting-edge technologies such as real-time data analytics and remote sensing, for example, can significantly enhance the precision and efficiency of security assessments. These tools provide dynamic insights into potential vulnerabilities, enabling proactive measures that adapt to emerging threats.</p><h3>Prioritize and Optimize Resources</h3><p>Transitioning from subjective assessments to data-backed evaluations ensures that decisions are grounded in accurate information rather than intuition alone. Robust datasets allow for thorough risk analyses that prioritize high-impact vulnerabilities while optimizing resource allocation.</p><h3>Implement Scalable Solutions</h3><p>Embrace flexible solutions capable of scaling with evolving infrastructure requirements or regulatory changes over time. This adaptability ensures continued relevance amidst shifting industry landscapes driven by technological advancements or policy shifts.</p><h2>Where to Start</h2><p>To solve the insufficiencies found within conventional site assessment methodologies, <a href="https://www.powereng.com/?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB3" target="_blank">POWER Engineers, Member of WSP</a>, designed a transformative threat assessment tool called <a href="https://meerkat.powereng.com/meerkat?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEAB2" target="_blank">Meerkat</a>. Meerkat harnesses high-quality data and advanced modeling techniques to deliver comprehensive vulnerability assessments customized to each unique facility. It is offered alongside an industry-leading team of experts who can help break down costs, explore alternative mitigations and address operational concerns.</p><p>Meerkat revolutionizes physical substation security by offering a more accurate and thorough analysis compared to conventional approaches. It mitigates the risk of human error inherent in manual inspections and overcomes access limitations through advanced remote sensing capabilities. Additionally, Meerkat facilitates seamless collaboration among stakeholders by providing dynamic, easily interpretable visualizations that enhance communication and decision-making processes. Analyses can even be performed in a secure, online workshop, allowing subject matter experts to skip the travel delays and jump right into the action.</p><p>By using Meerkat in substation security projects, utilities can transition from reactive to proactive strategies that anticipate and counter potential vulnerabilities before they are exploited. This shift not only ensures compliance with regulatory standards but also aligns security enhancements with financial objectives, ultimately safeguarding both assets and investments in a rapidly changing technological landscape.</p><h2>How it Works</h2><p class="shortcode-media shortcode-media-rebelmouse-image" style="margin: 0px;"> <img alt="Electric substation aerial view with security zones marked in red and blue sections." class="rm-shortcode" data-rm-shortcode-id="eea9c0502ee0a4fe21f966d74132f4d5" data-rm-shortcode-name="rebelmouse-image" id="3dc90" loading="lazy" src="https://spectrum.ieee.org/media-library/electric-substation-aerial-view-with-security-zones-marked-in-red-and-blue-sections.png?id=61081558&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">The Meerkat assessment features real-time mitigation modeling, optimizes camera placement, and identifies all vulnerabilities that could be exploited by malicious actors.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">POWER Engineers, Member of WSP</small></p><h3>Step One: Data Collection</h3><p>Meerkat starts with data collection. When pre-existing data of the site is available and of good quality and accuracy, it can be used for this process. However, when there is not sufficient data available, the Meerkat team collects its own high-fidelity data of the study area. This includes the substation facility, property and all surrounding terrain and infrastructure within an established radius of concern.</p><h3>Step Two: Build a Model</h3><p>Next, the high-quality data is transformed into an interactive 3D model in a virtual environment. The model is so accurate that it can facilitate virtual site visits. Users can navigate around the substation environment by clicking and dragging on screen and can visualize the site from any point ranging from a bird’s-eye view to the perspective of a potential bad actor looking into the station.</p><h3>Step Three: Test Mitigations in Real Time</h3><p>This interactive model serves as a virtual sandbox where mitigation strategies can be tested in real time. It can comprehensively and objectively map all line-of-sight vulnerabilities—big and small—that a bad actor might use to attack critical components. Then, existing or proposed mitigation strategies, if available, can be tested and validated within the system. This stage is great for testing what-if scenarios and seeing how multiple mitigations interact if combined before construction even comes into play.</p><h3>Step Four: Find the Best-Cost Solution</h3><p>POWER’s team of industry-leading experts use their knowledge to guide iterative solutions that bring substation owners and operators closer to the best-cost solutions for their substations. Sometimes moving or changing the height of a proposed wall is all it takes to drastically improve protections without drastically changing the price. A built-in cost estimator can also give a rough idea of how material costs change as the design does.</p><h2>The Benefits of Using Meerkat</h2><p>Meerkat is an industry-leading technology that offers unparalleled benefits in conducting thorough vulnerability assessments for critical assets at substations. By leveraging sophisticated algorithms and high-quality data, Meerkat delivers precise evaluations that pinpoint potential weaknesses with exceptional accuracy. This comprehensive approach means that every aspect of a substation’s physical security is meticulously analyzed, leaving no stone unturned.</p><h3>Enhanced Efficiency</h3><p>One of the key advantages of Meerkat is its ability to significantly enhance efficiency in the assessment process. This not only reduces the time and resources required for site assessments but also ensures consistent and reliable results.</p><p>Meerkat also allows an evaluation and design process that can sometimes take months of back-and-forth communication to happen in just a handful of hour-long workshops.</p><h3>Improved Accuracy</h3><p>Accuracy is another hallmark of Meerkat, as it eliminates the guesswork associated with human-based evaluations. By leveraging advanced modeling techniques, Meerkat provides actionable insights that empower utilities to make informed decisions regarding security upgrades and mitigations. This precision facilitates proactive risk management strategies, allowing stakeholders to address vulnerabilities before they manifest into tangible threats.</p><p>Ultimately, by improving both efficiency and accuracy in vulnerability assessments, Meerkat enables better decision-making processes that enhance overall risk management. Utilities can confidently implement targeted security measures tailored to each site’s unique needs, ensuring robust protection against emerging threats while optimizing resource allocation. In a landscape where rapid technological advancements challenge conventional practices, Meerkat stands as a vital tool for safeguarding critical infrastructure with foresight and precision.</p><h3>A Case Study: Strategic Security Optimization with Meerkat</h3><br/><p><em>The following case study has been sanitized of identifying information to maintain the security of the facility.</em></p><p><strong>Background</strong></p><p>A client faced a critical decision regarding the security of their substation, which was surrounded by a chain-link fence spanning 3,523 linear feet. Concerned about potential line-of-sight attacks on their critical assets, they planned to construct a new 15 ft tall concrete masonry unit (CMU) wall around the entire perimeter. Before proceeding with this significant investment, they sought validation from physical security experts at POWER and used the advanced threat assessment capabilities of Meerkat.</p><p><strong>Security Plan Validation</strong></p><p>To assess the effectiveness of the proposed security plan, Meerkat was employed to model the 15 ft wall within a highly accurate digital representation of the facility and its surroundings. The comprehensive data-backed threat assessment revealed lingering vulnerabilities despite the proposed construction. With estimated costs between $12 million and $15 million—and additional expenses for ballistic rated gates—the financial implications were substantial.</p><p><strong>Working Backward</strong></p><p>Recognizing that the original plan might not sufficiently mitigate risks, the client collaborated with Meerkat experts and key personnel across disciplines—including electrical engineers, civil engineers and transmission planners—to explore alternative strategies. Through a series of concise workshops over several days, they reimagined security designs by focusing on protecting critical assets identified as essential to system stability.</p><p>Meerkat enabled real-time modeling and testing of diverse mitigation strategies. Its interactive features allowed stakeholders to dynamically adjust protective measures—such as repositioning or resizing ballistic barriers—with immediate insights into effectiveness against vulnerabilities. This iterative process prioritized achieving the optimal balance between cost efficiency and robust protection.</p><p><strong>The Results</strong></p><p>Through strategic analysis using Meerkat, it became clear that constructing two separate 166 ft long, 25 ft tall walls at targeted locations around critical assets offered superior protection compared to encircling the entire perimeter with a single structure. This solution significantly enhanced security while reducing the estimated implementation costs to approximately $3.4 million—about a quarter of the cost of the initial projections.</p><p>Ultimately, the revised approach not only lowered risk profiles but also prevented unnecessary expenditure on inadequate defenses. By leveraging the advanced technology provided by Meerkat, the client successfully optimized resource allocation, comprehensively safeguarding their vital infrastructure.</p><h2><span>Get Started</span></h2><p>Any entity interested in learning more about Meerkat and its applications can request a free demonstration from our team of experts at <a href="https://meerkat.powereng.com/home?utm_source=industrypublication&utm_medium=native&utm_campaign=PE2025_IP_MBU-BM-EDS-FP-MeerkatIEEESNC_TRF&utm_content=MBU-BM-EDS-FP-MeerkatIEEESNC_Multiple_MIEEEIAL" target="_blank">meerkat.powereng.com</a>.</p><div class="ieee-image-small"><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Meerkat Power Engineers logo in black and red font with a shield emblem above." class="rm-shortcode" data-rm-shortcode-id="89a8d2cf23d243a4b12f1249442c9201" data-rm-shortcode-name="rebelmouse-image" id="ed2d2" loading="lazy" src="https://spectrum.ieee.org/media-library/meerkat-power-engineers-logo-in-black-and-red-font-with-a-shield-emblem-above.png?id=61013898&width=980"/></p></div>]]></description><pubDate>Mon, 23 Jun 2025 12:22:12 +0000</pubDate><guid>https://spectrum.ieee.org/meerkat-substation-security</guid><category>Digital transformation</category><category>Substation security</category><category>Vulnerability assessments</category><category>Security analysis</category><category>Security</category><dc:creator>POWER Engineers, Member of WSP</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/aerial-of-a-power-station-with-highlighted-threat-zones-and-bad-actor-location.png?id=61081553&amp;width=980"></media:content></item><item><title>How the Rubin Observatory Will Reinvent Astronomy</title><link>https://spectrum.ieee.org/vera-rubin-observatory-first-images</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-researchers-inspect-a-large-telescope-system-inside-a-scientific-facility.jpg?id=61078903&width=1200&height=800&coordinates=0%2C951%2C0%2C952"/><br/><br/><p><strong>Night is falling</strong> on Cerro Pachón.</p><p class="shortcode-media shortcode-media-rebelmouse-image" style="display:none;"> <img alt="Observatory under Milky Way band at twilight, stars densely scatter across the clear sky." class="rm-shortcode" data-rm-shortcode-id="0c70c434a7ca2d1e8a14f7f386591855" data-rm-shortcode-name="rebelmouse-image" id="864d3" loading="lazy" src="https://spectrum.ieee.org/media-library/observatory-under-milky-way-band-at-twilight-stars-densely-scatter-across-the-clear-sky.jpg?id=61087574&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">A view of NSF-DOE Vera C. Rubin Observatory beneath the Milky Way galaxy.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">NSF-DOE Vera C. Rubin Observatory/H. Stockebrand</small></p><h3></h3><br/><p>Stray clouds reflect the last few rays of golden light as the sun dips below the horizon. I focus my camera across the summit to the westernmost peak of the mountain. Silhouetted within a dying blaze of red and orange light looms the sphinxlike shape of the <a href="https://rubinobservatory.org/" target="_blank">Vera C. Rubin Observatory</a>.</p><p>“Not bad,” says <a href="https://www.lsst.org/about/team/wil-omullane" target="_blank">William O’Mullane</a>, the observatory’s deputy project manager, amateur photographer, and master of understatement. We watch as the sky fades through reds and purples to a deep, velvety black. It’s my first night in Chile. For O’Mullane, and hundreds of other astronomers and engineers, it’s the culmination of years of work, as the Rubin Observatory is finally ready to go “on sky.”</p><p>Rubin is unlike any telescope ever built. Its exceptionally wide field of view, extreme speed, and massive digital camera will soon begin the 10-year Legacy Survey of Space and Time (<a href="https://rubinobservatory.org/explore/how-rubin-works/lsst" target="_blank">LSST</a>) across the entire southern sky. The result will be a high-resolution movie of how our solar system, galaxy, and universe change over time, along with hundreds of petabytes of data representing billions of celestial objects that have never been seen before.</p><p>Stars begin to appear overhead, and O’Mullane and I pack up our cameras. It’s astronomical twilight, and after nearly 30 years, it’s time for Rubin to get to work.</p><h3></h3><br/><img alt="Starry galaxy field with colorful spirals and nebulae in deep space." class="rm-shortcode" data-rm-shortcode-id="87e8a03ee6fa2cd051d2622335071e36" data-rm-shortcode-name="rebelmouse-image" id="73efd" loading="lazy" src="https://spectrum.ieee.org/media-library/starry-galaxy-field-with-colorful-spirals-and-nebulae-in-deep-space.jpg?id=61078937&width=980"/><p class="hide-on-mobile">On 23 June, the Vera C. Rubin Observatory released the first batch of images to the public. One of them, shown here, features a small section of the Virgo cluster of galaxies. Visible are two prominent spiral galaxies (lower right), three merging galaxies (upper right), several groups of distant galaxies, and many stars in the Milky Way galaxy. Created from over 10 hours of observing data, this image represents less than 2 percent of the field of view of a single Rubin image.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><img alt="Colorful nebulae in space with clouds of pink, blue, and dark dust against a starry background." class="rm-shortcode" data-rm-shortcode-id="73bf55d1507cd7faa1b698d5ce5ec867" data-rm-shortcode-name="rebelmouse-image" id="ed39f" loading="lazy" src="https://spectrum.ieee.org/media-library/colorful-nebulae-in-space-with-clouds-of-pink-blue-and-dark-dust-against-a-starry-background.jpg?id=61078975&width=980"/><p class="hide-on-mobile">A second image reveals clouds of gas and dust in the Trifid and Lagoon nebulae, located several thousand light-years from Earth. It combines 678 images taken by the Rubin Observatory over just seven hours, revealing faint details—like nebular gas and dust—that would otherwise be invisible.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><h2>Engineering the Simonyi Survey Telescope</h2><p>The top of Cerro Pachón is not a big place. Spanning about 1.5 kilometers at 2,647 meters of elevation, its three peaks are home to the Southern Astrophysical Research Telescope (<a href="https://noirlab.edu/public/programs/ctio/soar-telescope/" target="_blank">SOAR</a>), the <a href="https://noirlab.edu/public/programs/gemini-observatory/gemini-south/" target="_blank">Gemini South Telescope</a>, and for the last decade, the Vera Rubin Observatory construction site. An hour’s flight north of the Chilean capital of Santiago, these foothills of the Andes offer uniquely stable weather. The Humboldt Current flows just offshore, cooling the surface temperature of the Pacific Ocean enough to minimize atmospheric moisture, resulting in some of the best “seeing,” as astronomers put it, in the world.</p><h3></h3><br/><img alt="Map showing Vera C. Rubin Observatory in Chile, near La Serena and Santiago." class="rm-shortcode" data-rm-shortcode-id="81465e6874b0a2d4ca6b19dd26e60a43" data-rm-shortcode-name="rebelmouse-image" id="d419b" loading="lazy" src="https://spectrum.ieee.org/media-library/map-showing-vera-c-rubin-observatory-in-chile-near-la-serena-and-santiago.png?id=61079069&width=980"/><h3></h3><br/><p>It’s a complicated but exciting time to be visiting. It’s mid-April of 2025, and I’ve arrived just a few days before “first photon,” when light from the night sky will travel through the completed telescope and into its camera for the first time. In the control room on the second floor, engineers and astronomers make plans for the evening’s tests. O’Mullane and I head up into a high bay that contains the silvering chamber for the telescope’s mirrors and a clean room for the camera and its filters. Increasingly exhausting flights of stairs lead to the massive pier on which the telescope sits, and then up again into the dome.</p><p>I suddenly feel very, very small. The Simonyi Survey Telescope towers above us—350 tonnes of steel and glass, nestled within the 30-meter-wide, 650-tonne dome. One final flight of stairs and we’re standing on the telescope platform. In its parked position, the telescope is pointed at horizon, meaning that it’s looking straight at me as I step in front of it and peer inside.</p><h3></h3><br/><img alt="Modern observatory under a starry night sky on a rocky hilltop." class="rm-shortcode" data-rm-shortcode-id="e5d8ac1a3234928fae5a0e9d20b48c9b" data-rm-shortcode-name="rebelmouse-image" id="b09ed" loading="lazy" src="https://spectrum.ieee.org/media-library/modern-observatory-under-a-starry-night-sky-on-a-rocky-hilltop.jpg?id=61079018&width=980"/><h3></h3><br/><p>The telescope’s enormous 8.4-meter primary mirror is so flawlessly reflective that it’s essentially invisible. Made of a single piece of low-expansion borosilicate glass covered in a 120-nanometer-thick layer of pure silver, the huge mirror acts as two different mirrors, with a more pronounced curvature toward the center. Standing this close means that different reflections of the mirrors, the camera, and the structure of the telescope all clash with one another in a way that shifts every time I move. I feel like if I can somehow look at it in just the right way, it will all make sense. But I can’t, and it doesn’t.</p><h3></h3><br/><img alt="Diagram of a telescope with labeled mirrors, lenses, filters, and camera components." class="rm-shortcode" data-rm-shortcode-id="41ac3b3bcd8dff4b1ba021f7efe8cec3" data-rm-shortcode-name="rebelmouse-image" id="a0745" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-a-telescope-with-labeled-mirrors-lenses-filters-and-camera-components.png?id=61079019&width=980"/><p>I’m rescued from madness by O’Mullane snapping photos next to me. “Why?” I ask him. “You see this every day, right?”</p><p>“This has never been seen before,” he tells me. “It’s the first time, ever, that the lens cover has been off the camera since it’s been on the telescope.” Indeed, deep inside the nested reflections I can see a blue circle, the r-band filter within the camera itself. As of today, it’s ready to capture the universe.</p><h3></h3><br/><img alt="Two images show the inner parts of a telescope, with large mirrors and a camera housed inside a metal frame." class="rm-shortcode" data-rm-shortcode-id="f368f2c2e79daf918d78617bac9c9c4f" data-rm-shortcode-name="rebelmouse-image" id="e07ec" loading="lazy" src="https://spectrum.ieee.org/media-library/two-images-show-the-inner-parts-of-a-telescope-with-large-mirrors-and-a-camera-housed-inside-a-metal-frame.png?id=61079027&width=980"/><h3></h3><br/><img alt="Close-up of a large, complex astronomical telescope structure in an observatory." class="rm-shortcode" data-rm-shortcode-id="ebb33fd87f829faf8f8aba8f2fd2213c" data-rm-shortcode-name="rebelmouse-image" id="80ae6" loading="lazy" src="https://spectrum.ieee.org/media-library/close-up-of-a-large-complex-astronomical-telescope-structure-in-an-observatory.jpg?id=61079033&width=980"/><h3></h3><br/><img alt="Large telescope inside observatory dome against a bright starry night sky." class="rm-shortcode" data-rm-shortcode-id="986dd7896096599c527677a99bbfda40" data-rm-shortcode-name="rebelmouse-image" id="4be71" loading="lazy" src="https://spectrum.ieee.org/media-library/large-telescope-inside-observatory-dome-against-a-bright-starry-night-sky.jpg?id=61078913&width=980"/><h3></h3><br/><h2>Rubin’s Wide View Unveils the Universe</h2><p>Back down in the control room, I find director of construction Željko Ivezić. He’s just come up from the summit hotel, which has several dozen rooms for lucky visitors like myself, plus a few even luckier staff members. The rest of the staff commutes daily from the coastal town of La Serena, a 4-hour round trip.</p><p>To me, the summit hotel seems luxurious for lodgings at the top of a remote mountain. But Ivezić has a slightly different perspective. “The European-funded telescopes,” he grumbles, “have swimming pools at their hotels. And they serve wine with lunch! Up here, there’s no alcohol. It’s an American thing.” He’s referring to the fact that <a href="https://nsf-gov-resources.nsf.gov/2023-03/37_fy2024.pdf.pdf" rel="noopener noreferrer" target="_blank">Rubin is primarily funded</a> by the U.S. <a href="https://www.nsf.gov/" target="_blank">National Science Foundation</a> and the U.S. Department of Energy’s <a href="https://www.energy.gov/science/office-science" target="_blank">Office of Science</a>, which have strict safety requirements.</p><h3></h3><br/><img alt="Silhouetted telescope under a starry sky and vibrant, colorful sunset." class="rm-shortcode" data-rm-shortcode-id="b492254ba5a5b929bfc315a49a32eb62" data-rm-shortcode-name="rebelmouse-image" id="69e31" loading="lazy" src="https://spectrum.ieee.org/media-library/silhouetted-telescope-under-a-starry-sky-and-vibrant-colorful-sunset.jpg?id=61079034&width=980"/><h3></h3><br/><p>Originally, Rubin was intended to be a dark-matter survey telescope, to search for the 85 percent of the mass of the universe that we know exists but can’t identify. In the 1970s, astronomer <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC5338491/" rel="noopener noreferrer" target="_blank">Vera C. Rubin</a> pioneered a spectroscopic method to measure the speed at which stars orbit around the centers of their galaxies, revealing motion that could be explained only by the presence of a halo of invisible mass at least five times the apparent mass of the galaxies themselves. Dark matter can warp the space around it enough that galaxies act as lenses, bending light from even more distant galaxies as it passes around them. It’s this gravitational lensing that the Rubin observatory was designed to detect on a massive scale. But once astronomers considered what else might be possible with a survey telescope that combined enormous light-collecting ability with a wide field of view, Rubin’s science mission rapidly expanded beyond dark matter.</p><p>Trading the ability to focus on individual objects for a wide field of view that can see tens of thousands of objects at once provides a critical perspective for understanding our universe, says Ivezić. Rubin will complement other observatories like the <a href="https://spectrum.ieee.org/hubble-space-telescope-re-invention" target="_self">Hubble Space Telescope</a> and the <a href="https://spectrum.ieee.org/collections/james-webb-telescope/" target="_self">James Webb Space Telescope</a>. <a href="https://esahubble.org/about/general/instruments/wfc3/" rel="noopener noreferrer" target="_blank">Hubble’s Wide Field Camera 3</a> and <a href="https://jwst-docs.stsci.edu/jwst-near-infrared-camera#gsc.tab=0" rel="noopener noreferrer" target="_blank">Webb’s Near Infrared Camera</a> have fields of view of less than 0.05 square degrees each, equivalent to just a few percent of the size of a full moon. The upcoming <a href="https://spectrum.ieee.org/rogue-planet" target="_self">Nancy Grace Roman Space Telescope</a> will see a bit more, with a field of view of about one full moon. Rubin, by contrast, can image 9.6 square degrees at a time—about 45 full moons’ worth of sky.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/rogue-planet" target="_self">A Trillion Rogue Planets and Not One Sun to Shine on Them</a></p><p>That ultrawide view offers essential context, Ivezić explains. “My wife is American, but I’m from Croatia,” he says. “Whenever we go to Croatia, she meets many people. I asked her, ‘Did you learn more about Croatia by meeting many people very superficially, or because you know me very well?’ And she said, ‘You need both. I learn a lot from you, but you could be a weirdo, so I need a control sample.’ ” Rubin is providing that control sample, so that astronomers know just how weird whatever they’re looking at in more detail might be.</p><h3>Explore Rubin Observatory’s First Images With Skyviewer</h3><br/><iframe allowed="fullscreen; clipboard-write; clipboard-read; web-share" sandbox="allow-downloads allow-scripts allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-forms" src="https://skyviewer.app/embed" style="border: none; aspect-ratio: 16 / 9; min-height: 500px; max-height: 100%"></iframe><p>Rubin Observatory’s <a href="https://skyviewer.app/" target="_blank">Skyviewer</a> app lets you explore its stunning first images by interactively navigating a vast, detailed view of the cosmos — you can zoom in and out and move around to examine the rich tapestry of stars and galaxies in extraordinary detail. The area observed includes the southern region of the Virgo Cluster — approximately 55 million light-years from Earth — as well as closer stars in the Milky Way and much more distant galaxy groups. This image, built from over 3 trillion pixels of data collected in just seven nights, contains millions of galaxies. Eventually, the full <a href="https://rubinobservatory.org/explore/how-rubin-works/lsst" target="_blank">Legacy Survey of Space and Time (LSST)</a> will catalog about 20 billion galaxies of all types, and from all times in the history of the Universe.<br/></p><h3></h3><br><p>Every night, the telescope will take a thousand images, one every 34 seconds. After three or four nights, it’ll have the entire southern sky covered, and then it’ll start all over again. After a decade, Rubin will have taken more than 2 million images, generated 500 petabytes of data, and visited every object it can see at least 825 times. In addition to identifying an estimated 6 million bodies in our solar system, 17 billion stars in our galaxy, and 20 billion galaxies in our universe, Rubin’s rapid cadence means that it will be able to delve into the time domain, tracking how the entire southern sky changes on an almost daily basis.</p><h3></h3><br/><h2>Cutting-Edge Technology Behind Rubin’s Speed</h2><p>Achieving these science goals meant pushing the technical envelope on nearly every aspect of the observatory. But what drove most of the design decisions is the speed at which Rubin needs to move (3.5 degrees per second)—the phrase most commonly used by the Rubin staff is “crazy fast.”</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="4aa52bc458fa1bc86c7aa06457d17e9d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8kF9PrMXqBU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p>Crazy fast movement is why the telescope looks the way it does. The squat arrangement of the mirrors and camera centralizes as much mass as possible. Rubin’s oversize supporting pier is mostly steel rather than mostly concrete so that the movement of the telescope doesn’t twist the entire pier. And then there’s the megawatt of power required to drive this whole thing, which comes from huge banks of capacitors slung under the telescope to prevent a brownout on the summit every 30 seconds all night long.</p><p>Rubin is also unique in that it utilizes the largest digital camera ever built. The size of a small car and weighing 2,800 kilograms, the LSST camera captures 3.2-gigapixel images through six swappable color filters ranging from near infrared to near ultraviolet. The camera’s focal plane consists of 189 4K-by-4K charge-coupled devices grouped into 21 “rafts.” Every CCD is backed by 16 amplifiers that each read 1 million pixels, bringing the readout time for the entire sensor down to 2 seconds flat.</p><h3></h3><br/><img alt="Technician examines a large telescope camera in a clean room environment." class="rm-shortcode" data-rm-shortcode-id="73a69383c750509ec15ecca488fb763d" data-rm-shortcode-name="rebelmouse-image" id="16585" loading="lazy" src="https://spectrum.ieee.org/media-library/technician-examines-a-large-telescope-camera-in-a-clean-room-environment.jpg?id=61079036&width=980"/><h3></h3><br/><h2>Astronomy in the Time Domain</h2><p>As humans with tiny eyeballs and short lifespans who are more or less stranded on Earth, we have only the faintest idea of how dynamic our universe is. To us, the night sky seems mostly static and also mostly empty. This is emphatically not the case.</p><p>In 1995, the Hubble Space Telescope pointed at a small and deliberately unremarkable part of the sky for a cumulative six days. The resulting image, called the <a href="https://science.nasa.gov/mission/hubble/science/universe-uncovered/hubble-deep-fields/" target="_blank">Hubble Deep Field</a>, revealed about 3,000 distant galaxies in an area that represented just one twenty-four-millionth of the sky. To observatories like Hubble, and now Rubin, the sky is crammed full of so many objects that it becomes a problem. As O’Mullane puts it, “There’s almost nothing not touching something.”</p><p>One of Rubin’s biggest challenges will be deblending—­identifying and then separating things like stars and galaxies that appear to overlap. This has to be done carefully by using images taken through different filters to estimate how much of the brightness of a given pixel comes from each object.</p><h3></h3><br/><img alt="Exploded diagram of a large telescope camera, with labeled parts including lens, shutter, filters, and a 3.2-gigapixel CCD." class="rm-shortcode" data-rm-shortcode-id="9b28eeff16dde98dae9b8fa724a5930b" data-rm-shortcode-name="rebelmouse-image" id="402f3" loading="lazy" src="https://spectrum.ieee.org/media-library/exploded-diagram-of-a-large-telescope-camera-with-labeled-parts-including-lens-shutter-filters-and-a-3-2-gigapixel-ccd.png?id=61079041&width=980"/><h3></h3><br/><p>At first, Rubin won’t have this problem. At each location, the camera will capture one 30-second exposure before moving on. As Rubin returns to each location every three or four days, subsequent exposures will be combined in a process called coadding. In a coadded image, each pixel represents all of the data collected from that location in every previous image, which results in a much longer effective exposure time. The camera may record only a few photons from a distant galaxy in each individual image, but a few photons per image added together over 825 images yields much richer data. By the end of Rubin’s 10-year survey, the coadding process will generate images with as much detail as a typical Hubble image, but over the entire southern sky. A few lucky areas called “<a href="https://survey-strategy.lsst.io/baseline/ddf.html" rel="noopener noreferrer" target="_blank">deep drilling fields</a>” will receive even more attention, with each one getting a staggering 23,000 images or more.</p><p>Rubin will add every object that it detects to its catalog, and over time, the catalog will provide a baseline of the night sky, which the observatory can then use to identify changes. Some of these changes will be movement—Rubin may see an object in one place, and then spot it in a different place some time later, which is how objects like <a href="https://spectrum.ieee.org/planetary-defense-killer-asteroids" target="_self">near-Earth asteroids</a> will be detected. But the vast majority of the changes will be in brightness rather than movement.</p><p class="ieee-inbody-related">RELATED: <a href="https://spectrum.ieee.org/planetary-defense-killer-asteroids" target="_self">Three Steps to Stopping Killer Asteroids</a></p><h3></h3><br/><img alt="A circle with grid lines overlaying a night sky background with stars and a full moon." class="rm-shortcode" data-rm-shortcode-id="3da7febf385d5d7e3565ead376c2b3fb" data-rm-shortcode-name="rebelmouse-image" id="94e9a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-circle-with-grid-lines-overlaying-a-night-sky-background-with-stars-and-a-full-moon.png?id=61079047&width=980"/><h3></h3><br/><p>Every image that Rubin collects will be compared with a baseline image, and any change will automatically generate a software alert within 60 seconds of when the image was taken. Rubin’s wide field of view means that there will be a lot of these alerts—on the order of 10,000 per image, or 10 million alerts per night. Other automated systems will manage the alerts. Called alert brokers, they ingest the alert streams and filter them for the scientific community. If you’re an astronomer interested in Type Ia supernovae, for example, you can subscribe to an alert broker and set up a filter so that you’ll get notified when Rubin spots one.</p><p>Many of these alerts will be triggered by variable stars, which cyclically change in brightness. Rubin is also expected to identify somewhere <a href="https://www.lsst.org/science/transient-optical-sky/supernovae" target="_blank">between 3 million and 4 million supernovae</a>—that works out to over a thousand new supernovae for every night of observing. And the rest of the alerts? Nobody knows for sure, and that’s why the alerts have to go out so quickly, so that other telescopes can react to make deeper observations of what Rubin finds.</p><h3></h3><br/><h2>Managing Rubin’s Vast Data Output</h2><p>After the data leaves Rubin’s camera, most of the processing will take place at the <a href="https://www6.slac.stanford.edu/" target="_blank">SLAC National Accelerator Laboratory</a> in Menlo Park, Calif., over 9,000 kilometers from Cerro Pachón. It takes less than 10 seconds for an image to travel from the focal plane of the camera to SLAC, thanks to a 600-gigabit fiber connection from the summit to La Serena, and from there, a dedicated 100-gigabit line and a backup 40-gigabit line that connect to the Department of Energy’s science network in the United States. The 20 terabytes of data that Rubin will produce nightly makes this bandwidth necessary. “There’s a new image every 34 seconds,” O’Mullane tells me. “If I can’t deal with it fast enough, I start to get behind. So everything has to happen on the cadence of half a minute if I want to keep up with the data flow.”</p><p>At SLAC, each image will be calibrated and cleaned up, including the removal of satellite trails. Rubin will see a lot of satellites, but since the satellites are unlikely to appear in the same place in every image, the impact on the data is expected to be minimal when the images are coadded. The processed image is compared with a baseline image and any alerts are sent out, by which time processing of the next image has already begun.</p><h3></h3><br/><img alt="Numerous thick cables hang in an industrial setting, surrounded by blue metal scaffolding." class="rm-shortcode" data-rm-shortcode-id="269376d383325b40dbf8a0d5cb7bbf6a" data-rm-shortcode-name="rebelmouse-image" id="2c0ac" loading="lazy" src="https://spectrum.ieee.org/media-library/numerous-thick-cables-hang-in-an-industrial-setting-surrounded-by-blue-metal-scaffolding.jpg?id=61079049&width=980"/><h3></h3><br/><p>As Rubin’s catalog of objects grows, astronomers <a href="https://dmtn-243.lsst.io/" rel="noopener noreferrer" target="_blank">will be able to query it</a> in all kinds of useful ways. Want every image of a particular patch of sky? No problem. All the galaxies of a certain shape? A little trickier, but sure. Looking for 10,000 objects that are similar in some dimension to 10,000 other objects? That might take a while, but it’s still possible. Astronomers can even run their own code on the raw data.</p><p>“Pretty much everyone in the astronomy community wants something from Rubin,” O’Mullane explains, “and so they want to make sure that we’re treating the data the right way. All of our code is public. It’s on <a href="https://github.com/lsst" target="_blank">GitHub</a>. You can see what we’re doing, and if you’ve got a better solution, we’ll take it.”</p><p>One better solution may involve AI. “I think as a community we’re struggling with how we do this,” says O’Mullane. “But it’s probably something we ought to do—curating the data in such a way that it’s consumable by machine learning, providing foundation models, that sort of thing.”</p><p>The data management system is arguably as much of a critical component of the Rubin observatory as the telescope itself. While most telescopes make targeted observations that get distributed to only a few astronomers at a time, Rubin will make its data available to everyone within just a few days, which is a completely different way of doing astronomy. “We’ve essentially promised that we will take every image of everything that everyone has ever wanted to see,” explains <a href="https://www.lsst.org/content/kevin-reil" rel="noopener noreferrer" target="_blank">Kevin Reil</a>, Rubin observatory scientist. “If there’s data to be collected, we will try to collect it. And if you’re an astronomer somewhere, and you want an image of something, within three or four days we’ll give you one. It’s a colossal challenge to deliver something on this scale.”</p><h3></h3><br/><img alt="Animated image on the left shows an automated mechanism that switches color filters; an image on the right shows how each filter affects the exposures of stars and galaxies." class="rm-shortcode" data-rm-shortcode-id="ac1caf92f248603bd9f1bdf40716d16c" data-rm-shortcode-name="rebelmouse-image" id="21741" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-image-on-the-left-shows-an-automated-mechanism-that-switches-color-filters-an-image-on-the-right-shows-how-each-filter.gif?id=61079050&width=980"/><h3></h3><br/><p>The more time I spend on the summit, the more I start to think that the science that we know Rubin will accomplish may be the least interesting part of its mission. And despite their best efforts, I get the sense that everyone I talk to is wildly understating the impact it will have on astronomy. The sheer volume of objects, the time domain, the 10 years of coadded data—what new science will all of that reveal? Astronomers have no idea, because we’ve never looked at the universe in this way before. To me, that’s the most fascinating part of what’s about to happen.</p><p>Reil agrees. “You’ve been here,” he says. “You’ve seen what we’re doing. It’s a paradigm shift, a whole new way of doing things. It’s still a telescope and a camera, but we’re changing the world of astronomy. I don’t know how to capture—I mean, it’s the people, the intensity, the awesomeness of it. I want the world to understand the beauty of it all.”</p><h3></h3><br/><h2>The Intersection of Science and Engineering</h2><p>Because nobody has built an observatory like Rubin before, there are a lot of things that aren’t working exactly as they should, and a few things that aren’t working at all. The most obvious of these is the dome. The capacitors that drive it blew a fuse the day before I arrived, and the electricians are off the summit for the weekend. The dome shutter can’t open either. Everyone I talk to takes this sort of thing in stride—they have to, because they’ve been troubleshooting issues like these for years.</p><p>I sit down with <a href="https://kipac.stanford.edu/people/yousuke-utsumi" target="_blank">Yousuke Utsumi</a>, a camera operations scientist who exudes the mixture of excitement and exhaustion that I’m getting used to seeing in the younger staff. “Today is amazingly quiet,” he tells me. “I’m happy about that. But I’m also really tired. I just want to sleep.”</p><p>Just yesterday, Utsumi says, they managed to finally solve a problem that the camera team had been struggling with for weeks—an intermittent fault in the camera cooling system that only seemed to happen when the telescope was moving. This was potentially a very serious problem, and Utsumi’s phone would alert him every time the fault occurred, over and over again in the middle of the night. The fault was finally traced to a cable within the telescope’s structure that used pins that were slightly too small, leading to a loose connection.</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="6e7db8e07af90b232c49a6b7a5a89175" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/QRr7wpLXas8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p>Utsumi’s contract started in 2017 and was supposed to last three years, but he’s still here. “I wanted to see first photon,” he says. “I’m an astronomer. I’ve been working on this camera so that it can observe the universe. And I want to see that light, from those photons from distant galaxies.” This is something I’ve also been thinking about—those lonely photons traveling through space for billions of years, and within the coming days, a lucky few of them will land on the sensors Utsumi has been tending, and we’ll get to see them. He nods, smiling. “I don’t want to lose one, you know?”</p><h3></h3><br/><img alt="Illuminated telescope interior with vibrant blue and red hues, showcasing intricate machinery." class="rm-shortcode" data-rm-shortcode-id="c00643cc41a4fbfecec072e87cfff410" data-rm-shortcode-name="rebelmouse-image" id="9d141" loading="lazy" src="https://spectrum.ieee.org/media-library/illuminated-telescope-interior-with-vibrant-blue-and-red-hues-showcasing-intricate-machinery.jpg?id=61079051&width=980"/><h3></h3><br/><p>Rubin’s commissioning scientists have a unique role, working at the intersection of science and engineering to turn a bunch of custom parts into a functioning science instrument. Commissioning scientist <a href="https://www.linkedin.com/in/marina-pavlovic-6489831a2/" target="_blank">Marina Pavlovic</a> is a postdoc from Serbia with a background in the formation of supermassive black holes created by merging galaxies. “I came here last year as a volunteer,” she tells me. “My plan was to stay for three months, and 11 months later I’m a commissioning scientist. It’s crazy!”</p><h3></h3><br/><img alt="Technicians in clean suits handling a large metallic component in a laboratory." class="rm-shortcode" data-rm-shortcode-id="630da2a002eb632655116dbcb4585422" data-rm-shortcode-name="rebelmouse-image" id="8d2f5" loading="lazy" src="https://spectrum.ieee.org/media-library/technicians-in-clean-suits-handling-a-large-metallic-component-in-a-laboratory.jpg?id=61079052&width=980"/><h3></h3><br/><p>Pavlovic’s job is to help diagnose and troubleshoot whatever isn’t working quite right. And since most things aren’t working quite right, she’s been very busy. “I love when things need to be fixed because I am learning about the system more and more every time there’s a problem—every day is a new experience here.”</p><p>I ask her what she’ll do next, once Rubin is up and running. “If you love commissioning instruments, that is something that you can do for the rest of your life, because there are always going to be new instruments,” she says.</p><p>Before that happens, though, Pavlovic has to survive the next few weeks of going on sky. “It’s going to be so emotional. It’s going to be the beginning of a new era in astronomy, and knowing that <em>you</em> did it, that <em>you</em> made it happen, at least a tiny percent of it, that will be a priceless moment.”</p><p>“I had to learn how to calm down to do this job,” she admits, “because sometimes I get too excited about things and I cannot sleep after that. But it’s okay. I started doing yoga, and it’s working.”</p><h3></h3><br/><h2>From First Photon to First Light</h2><p>My stay on the summit comes to an end on 14 April, just a day before first photon, so as soon as I get home I check in with some of the engineers and astronomers that I met to see how things went. <a href="https://www.linkedin.com/in/gmegiashomar/" target="_blank">Guillem Megias Homar</a> manages the adaptive optics system—232 actuators that flex the surfaces of the telescope’s three mirrors a few micrometers at a time to bring the image into perfect focus. Currently working on his Ph.D., he was born in 1997, one year after the Rubin project started.</p><p>First photon, for him, went like this: “I was in the control room, sitting next to the camera team. We have a microphone on the camera, so that we can hear when the shutter is moving. And we hear the first click. And then all of a sudden, the image shows up on the screens in the control room, and it was just an explosion of emotions. All that we have been fighting for is finally a reality. We are on sky!” There were toasts (with sparkling apple juice, of course), and enough speeches that Megias Homar started to get impatient: “I was like, when can we start working? But it was only an hour, and then everything became much more quiet.”</p><h3></h3><br/><img alt="Dense galaxy cluster with diverse stars and galaxies scattered across the dark universe background." class="rm-shortcode" data-rm-shortcode-id="0af096dd8e983460a936fb9998613789" data-rm-shortcode-name="rebelmouse-image" id="f82d6" loading="lazy" src="https://spectrum.ieee.org/media-library/dense-galaxy-cluster-with-diverse-stars-and-galaxies-scattered-across-the-dark-universe-background.jpg?id=61079148&width=980"/><p class="hide-on-mobile">Another newly released image showing a small section of the Rubin Observatory’s total view of the Virgo cluster of galaxies. Visible are bright stars in the Milky Way galaxy shining in the foreground, and many distant galaxies in the background.</p><p class="caption hide-on-mobile">NSF-DOE Rubin Observatory</p><h3></h3><br/><p>“It was satisfying to see that everything that we’d been building was finally working,” <a href="https://www.lsst.org/about/team/lsst-project-manager" target="_blank">Victor Krabbendam</a>, project manager for Rubin construction, tells me a few weeks later. “But some of us have been at this for so long that first photon became just one of many firsts.” Krabbendam has been with the observatory full-time for the last 21 years. “And the very moment you succeed with one thing, it’s time to be doing the next thing.”</p><h3></h3><br/><img alt="Group of people seated in office chairs look at a screen (not shown) and smile slightly, with one person covering their mouth with their hands." class="rm-shortcode" data-rm-shortcode-id="86f08e956bbe0f9860250a08b5ff72fe" data-rm-shortcode-name="rebelmouse-image" id="5e282" loading="lazy" src="https://spectrum.ieee.org/media-library/group-of-people-seated-in-office-chairs-look-at-a-screen-not-shown-and-smile-slightly-with-one-person-covering-their-mouth-wi.jpg?id=61079053&width=980"/><h3></h3><br/><p>Since first photon, Rubin has been undergoing calibrations, collecting data for the first images that it’s now sharing with the world, and preparing to scale up to begin its survey. Operations will soon become routine, the commissioning scientists will move on, and eventually, Rubin will largely run itself, with just a few people at the observatory most nights.</p><p>But for astronomers, the next 10 years will be anything but routine. “It’s going to be wildly different,” says Krabbendam. “Rubin will feed generations of scientists with trillions of data points of billions of objects. Explore the data. Harvest it. Develop your idea, see if it’s there. It’s going to be phenomenal.”<span class="ieee-end-mark"></span></p><h3></h3><br/><h3 class="rm-anchors" id="listen-conversation-rubin-observatory">Listen to a Conversation About the Rubin Observatory</h3><p>As part of an experiment with AI storytelling tools, author Evan Ackerman—who visited the Vera C. Rubin Observatory in Chile for four days this past April—fed over 14 hours of raw audio from his interviews and other reporting notes into <a href="https://notebooklm.google.com/" target="_blank">NotebookLM</a>, an AI-powered research assistant developed by Google. The result is a podcast-style audio experience that you can listen to here. While the script and voices are AI-generated, the conversation is grounded in Ackerman’s original reporting, and includes many details that did not appear in the article above. Ackerman reviewed and edited the audio to ensure accuracy, and there are minor corrections in the transcript. <a href="mailto:evan.ackerman@ieee.org">Let us know</a> what you think of this experiment in AI narration.</p><p class="shortcode-media shortcode-media-audio"> <audio class="rm-shortcode" controls="" data-rm-shortcode-id="ffb294f9920a948fea39f581a6e4068e" expand="1" frameborder="0" id="265d5" original_filename="Vera+Rubin+Observatory+Podcast+Short4.mp3" site_id="20265424"> <source src="/files/81935/Vera+Rubin+Observatory+Podcast+Short4.mp3" type="audio/mpeg"/> Your browser does not support the audio tag. </audio></p><div id="ieee-transcript-container"><button aria-controls="ieee-transcript-content" aria-expanded="false" class="ieee-transcript-toggle" id="ieee-transcript-toggle">  See transcript  <span aria-hidden="true" class="ieee-transcript-icon"></span></button><div aria-label="Podcast transcript" hidden="" id="ieee-transcript-content" role="region" style="display: none;"><p>0:01: Today we’re taking a deep dive into the engineering marvel that is the Vera C. Rubin Observatory.</p><p>0:06: And and it really is a marvel.</p><p>0:08: This project pushes the limits, you know, not just for the science itself, like mapping the Milky Way or exploring dark energy, which is amazing, obviously.</p><p>0:16: But it’s also pushing the limits in just building the tools, the technical ingenuity, the, the sheer human collaboration needed to make something this complex actually work.</p><p>0:28: That’s what’s really fascinating to me.</p><p>0:29: Exactly.</p><p>0:30: And our mission for this deep dive is to go beyond the headlines, isn’t it?</p><p>0:33: We want to uncover those specific Kind of hidden technical details, the stuff from the audio interviews, the internal docs that really define this observatory.</p><p>0:41: The clever engineering solutions.</p><p>0:43: Yeah, the nuts and bolts, the answers to challenges nobody’s faced before, stuff that anyone who appreciates, you know, complex systems engineering would find really interesting.</p><p>0:53: Definitely.</p><p>0:54: So let’s start right at the heart of it.</p><p>0:57: The Simonyi survey telescope itself.</p><p>1:00: It’s this 350 ton machine inside a 600 ton dome, 30 m wide, huge. [The dome is closer to 650 tons.]</p><p>1:07: But the really astonishing part is its speed, speed and precision.</p><p>1:11: How do you even engineer something that massive to move that quickly while keeping everything stable down to the submicron level? [Micron level is more accurate.]</p><p>1:18: Well, that’s, that’s the core challenge, right?</p><p>1:20: This telescope, it can hit a top speed of 3.5 degrees per second.</p><p>1:24: Wow.</p><p>1:24: Yeah, and it can, you know, move to basically any point in the sky.</p><p>1:28: In under 20 seconds, 20 seconds, which makes it by far the fastest moving large telescope ever built, and the dome has to keep up.</p><p>1:36: So it’s also the fastest moving dome.</p><p>1:38: So the whole building is essentially racing along with the telescope.</p><p>1:41: Exactly.</p><p>1:41: And achieving that meant pretty much every component had to be custom designed like the pier holding the telescope up.</p><p>1:47: It’s mostly steel, not concrete.</p><p>1:49: Oh, interesting.</p><p>1:50: Why steel?</p><p>1:51: Specifically to stop it from twisting or vibrating when the telescope makes those incredibly fast moves.</p><p>1:56: Concrete just wouldn’t handle the torque the same way. [The pier is more steel than concrete, but it's still substantially concrete.]</p><p>1:59: OK, that makes sense.</p><p>1:59: And the power needed to accelerate and decelerate, you know, 300 tons, that must be absolutely massive.</p><p>2:06: Oh.</p><p>2:06: The instantaneous draw would be enormous.</p><p>2:09: How did they manage that without like dimming the lights on the whole.</p><p>2:12: Mountaintop every 30 seconds.</p><p>2:14: Yeah, that was a real concern, constant brownouts.</p><p>2:17: The solution was actually pretty elegant, involving these onboard capacitor banks.</p><p>2:22: Yep, slung right underneath the telescope structure.</p><p>2:24: They can slowly sip power from the grid, store it up over time, and then bam, discharge it really quickly for those big acceleration surges.</p><p>2:32: like a giant camera flash, but for moving a telescope, of yeah.</p><p>2:36: It smooths out the demand, preventing those grid disruptions.</p><p>2:40: Very clever engineering.</p><p>2:41: And beyond the movement, the mirrors themselves, equally critical, equally impressive, I imagine.</p><p>2:47: How did they tackle designing and making optics that large and precise?</p><p>2:51: Right, so the main mirror, the primary mirror, M1M3.</p><p>2:55: It’s a single piece of glass, 8.4 m across, low expansion borosilicate glass.</p><p>3:01: And that 8.4 m size, was that just like the biggest they could manage?</p><p>3:05: Well, it was a really crucial early decision.</p><p>3:07: The science absolutely required something at least 7 or 8 m wide.</p><p>3:13: But going much bigger, say 10 or 12 m, the logistics became almost impossible.</p><p>3:19: The big one was transport.</p><p>3:21: There’s a tunnel on the mountain road up to the summit, and a mirror, much larger than 8.4 m, physically wouldn’t fit through it.</p><p>3:28: No way.</p><p>3:29: So the tunnel actually set an upper limit on the mirror size.</p><p>3:31: Pretty much, yeah.</p><p>3:32: Building new road or some other complex transport method.</p><p>3:36: It would have added enormous cost and complexity.</p><p>3:38: So 8.4 m was that sweet spot between scientific need.</p><p>3:42: And, well, physical reality.</p><p>3:43: Wow, a real world constraint driving fundamental design.</p><p>3:47: And the mirror itself, you said M1 M3, it’s not just one simple mirror surface.</p><p>3:52: Correct.</p><p>3:52: It’s technically two mirror surfaces ground into that single piece of glass.</p><p>3:57: The central part has a more pronounced curvature.</p><p>3:59: It’s M1 and M3 combined.</p><p>4:00: OK, so fabricating that must have been tricky, especially with what, 10 tons of glass just in the center.</p><p>4:07: Oh, absolutely novel and complicated.</p><p>4:09: And these mirrors, they don’t support their own weight rigidly.</p><p>4:12: So just handling them during manufacturing, polishing, even getting them out of the casting mold, was a huge engineering challenge.</p><p>4:18: You can’t just lift it like a dinner plate.</p><p>4:20: Not quite, and then there’s maintaining it, re-silvering.</p><p>4:24: They hope to do it every 5 years.</p><p>4:26: Well, traditionally, big mirrors like this often need it more, like every 1.5 to 2 years, and it’s a risky weeks-long job.</p><p>4:34: You have to unbolt this priceless, unique piece of equipment, move it.</p><p>4:39: It’s nerve-wracking.</p><p>4:40: I bet.</p><p>4:40: And the silver coating itself is tiny, right?</p><p>4:42: Incredibly thin, just a few nanometers of pure silver.</p><p>4:46: It takes about 24 g for the whole giant surface, bonded with the adhesive layers that are measured in Angstroms. [It's closer to 26 grams of silver.]</p><p>4:52: It’s amazing precision.</p><p>4:54: So tying this together, you have this fast moving telescope, massive mirrors.</p><p>4:59: How do they keep everything perfectly focused, especially with multiple optical elements moving relative to each other?</p><p>5:04: that’s where these things called hexapods come in.</p><p>5:08: Really crucial bits of kit.</p><p>5:09: Hexapods, like six feet?</p><p>5:12: Sort of.</p><p>5:13: They’re mechanical systems with 6 adjustable arms or struts.</p><p>5:17: A simpler telescope might just have one maybe on the camera for basic focusing, but Ruben needs more because it’s got the 3 mirrors plus the camera.</p><p>5:25: Exactly.</p><p>5:26: So there’s a hexapod mounted on the secondary mirror, M2.</p><p>5:29: Its job is to keep M2 perfectly positioned relative to M1 and M3, compensating for tiny shifts or flexures.</p><p>5:36: And then there’s another hexapod on the camera itself.</p><p>5:39: That one adjusts the position and tilt of the entire camera’s sensor plane, the focal plane.</p><p>5:43: To get that perfect focus across the whole field of view.</p><p>5:46: And these hexapods move in 6 ways.</p><p>5:48: Yep, 6 degrees of freedom.</p><p>5:50: They can adjust position along the X, Y, and Z axis, and they can adjust rotation or tilt around those 3 axes as well.</p><p>5:57: It allows for incredibly fine adjustments, microp precision stuff.</p><p>6:00: So they’re constantly making these tiny tweaks as the telescope moves.</p><p>6:04: Constantly.</p><p>6:05: The active optics system uses them.</p><p>6:07: It calculates the needed corrections based on reference stars in the images, figures out how the mirror might be slightly bending.</p><p>6:13: And then tells the hexapods how to compensate.</p><p>6:15: It’s controlling like 26 g of silver coating on the mirror surface down to micron precision, using the mirror’s own natural bending modes.</p><p>6:24: It’s pretty wild.</p><p>6:24: Incredible.</p><p>6:25: OK, let’s pivot to the camera itself.</p><p>6:28: The LSST camera.</p><p>6:29: Big digital camera ever built, right?</p><p>6:31: Size of a small car, 2800 kg, captures 3.2 gigapixel images, just staggering numbers.</p><p>6:38: They really are, and the engineering inside is just as staggering.</p><p>6:41: That Socal plane where the light actually hits.</p><p>6:43: It’s made up of 189 individual CCD sensors.</p><p>6:47: Yep, 4K by 4K CCDs grouped into 21 rafts.</p><p>6:50: They give them like tiles, and each CCD has 16 amplifiers reading it out.</p><p>6:54: Why so many amplifiers?</p><p>6:56: Speed.</p><p>6:56: Each amplifier reads out about a million pixels.</p><p>6:59: By dividing the job up like that, they can read out the entire 3.2 gigapixel sensor in just 2 seconds.</p><p>7:04: 2 seconds for that much data.</p><p>7:05: Wow.</p><p>7:06: It’s essential for the survey’s rapid cadence.</p><p>7:09: Getting all those 189 CCDs perfectly flat must have been, I mean, are they delicate?</p><p>7:15: Unbelievably delicate.</p><p>7:16: They’re silicon wafers only 100 microns thick.</p><p>7:18: How thick is that really?</p><p>7:19: about the thickness of a human hair.</p><p>7:22: You could literally break one by breathing on it wrong, apparently, seriously, yeah.</p><p>7:26: And the challenge was aligning all 189 of them across this 650 millimeter wide focal plane, so the entire surface is flat.</p><p>7:34: To within just 24 microns, peak to valley.</p><p>7:37: 24 microns.</p><p>7:39: That sounds impossibly flat.</p><p>7:40: It’s like, imagine the entire United States.</p><p>7:43: Now imagine the difference between the lowest point and the highest point across the whole country was only 100 ft.</p><p>7:49: That’s the kind of relative flatness they achieved on the camera sensor.</p><p>7:52: OK, that puts it in perspective.</p><p>7:53: And why is that level of flatness so critical?</p><p>7:56: Because the telescope focuses light.</p><p>7:58: terribly.</p><p>7:58: It’s an F1.2 system, which means it has a very shallow depth of field.</p><p>8:02: If the sensors aren’t perfectly in that focal plane, even by a few microns, parts of the image go out of focus.</p><p>8:08: Gotcha.</p><p>8:08: And the pixels themselves, the little light buckets on the CCDs, are they special?</p><p>8:14: They’re custom made, definitely.</p><p>8:16: They settled on 10 micron pixels.</p><p>8:18: They figured anything smaller wouldn’t actually give them more useful scientific information.</p><p>8:23: Because you start hitting the limits of what the atmosphere and the telescope optics themselves can resolve.</p><p>8:28: So 10 microns was the optimal size, right?</p><p>8:31: balancing sensor tech with physical limits.</p><p>8:33: Now, keeping something that sensitive cool, that sounds like a nightmare, especially with all those electronics.</p><p>8:39: Oh, it’s a huge thermal engineering challenge.</p><p>8:42: The camera actually has 3 different cooling zones, 3 distinct temperature levels inside.</p><p>8:46: 3.</p><p>8:47: OK.</p><p>8:47: First, the CCDs themselves.</p><p>8:49: They need to be incredibly cold to minimize noise.</p><p>8:51: They operate at -125 °C.</p><p>8:54: -125C, how do they manage that?</p><p>8:57: With a special evaporator plate connected to the CCD rafts by flexible copper braids, which pulls heat away very effectively.</p><p>9:04: Then you’ve got the cameras, electronics, the readout boards and stuff.</p><p>9:07: They run cooler than room temp, but not that cold, around -50 °C.</p><p>9:12: OK.</p><p>9:12: That requires a separate liquid cooling loop delivered through these special vacuum insulated tubes to prevent heat leaks.</p><p>9:18: And the third zone.</p><p>9:19: That’s for the electronics in the utility trunk at the back of the camera.</p><p>9:23: They generate a fair bit of heat, about 3000 watts, like a few hair dryers running constantly.</p><p>9:27: Exactly.</p><p>9:28: So there’s a third liquid cooling system just for them, keeping them just slightly below the ambient room temperature in the dome.</p><p>9:35: And all this cooling, it’s not just to keep the parts from overheating, right?</p><p>9:39: It affects the images, absolutely critical for image quality.</p><p>9:44: If the outer surface of the camera body itself is even slightly warmer or cooler than the air inside the dome, it creates tiny air currents, turbulence right near the light path.</p><p>9:57: And that shows up as little wavy distortions in the images, messing up the precision.</p><p>10:02: So even the outside temperature of the camera matters.</p><p>10:04: Yep, it’s not just a camera.</p><p>10:06: They even have to monitor the heat generated by the motors that move the massive dome, because that heat could potentially cause enough air turbulence inside the dome to affect the image quality too.</p><p>10:16: That’s incredible attention to detail, and the camera interior is a vacuum you mentioned.</p><p>10:21: Yes, a very strong vacuum.</p><p>10:23: They pump it down about once a year, first using turbopumps spinning at like 80,000 RPM to get it down to about 102 tor.</p><p>10:32: Then they use other methods to get it down much further.</p><p>10:34: The 107 tor, that’s an ultra high vacuum.</p><p>10:37: Why the vacuum?</p><p>10:37: Keep frost off the cold part.</p><p>10:39: Exactly.</p><p>10:40: Prevents condensation and frost on those negatives when it 25 degree CCDs and generally ensures everything works optimally.</p><p>10:47: For normal operation, day to day, they use something called an ion pump.</p><p>10:51: How does that work?</p><p>10:52: It basically uses a strong electric field to ionize any stray gas molecules, mostly hydrogen, and trap them, effectively removing them from the vacuum space, very efficient for maintaining that ultra-high vacuum.</p><p>11:04: OK, so we have this incredible camera taking these massive images every few seconds.</p><p>11:08: Once those photons hit the CCDs and become digital signals, What happens next?</p><p>11:12: How does Ruben handle this absolute flood of data?</p><p>11:15: Yeah, this is where Ruben becomes, you know, almost as much a data processing machine as a telescope.</p><p>11:20: It’s designed for the data output.</p><p>11:22: So photons hit the CCDs, get converted to electrical signals.</p><p>11:27: Then, interestingly, they get converted back into light signals, photonic signals back to light.</p><p>11:32: Why?</p><p>11:33: To send them over fiber optics.</p><p>11:34: They’re about 6 kilometers of fiber optic cable running through the observatory building.</p><p>11:39: These signals go to FPGA boards, field programmable gate arrays in the data acquisition system.</p><p>11:46: OK.</p><p>11:46: And those FPGAs are basically assembling the complete image data packages from all the different CCDs and amplifiers.</p><p>11:53: That sounds like a fire hose of data leaving the camera.</p><p>11:56: How does it get off the mountain and where does it need to go?</p><p>11:58: And what about all the like operational data, temperatures, positions?</p><p>12:02: Good question.</p><p>12:03: There are really two main data streams all that telemetry you mentioned, sensor readings, temperatures, actuator positions, command set, everything about the state of the observatory that all gets collected into something called the Engineering facility database or EFD.</p><p>12:16: They use Kafka for transmitting that data.</p><p>12:18: It’s good for high volume streams, and store it in an influx database, which is great for time series data like sensor readings.</p><p>12:26: And astronomers can access that.</p><p>12:28: Well, there’s actually a duplicate copy of the EFD down at SLAC, the research center in California.</p><p>12:34: So scientists and engineers can query that copy without bogging down the live system running on the mountain.</p><p>12:40: Smart.</p><p>12:41: How much data are we talking about there?</p><p>12:43: For the engineering data, it’s about 20 gigabytes per night, and they plan to keep about a year’s worth online.</p><p>12:49: OK.</p><p>12:49: And the image data, the actual science pixels.</p><p>12:52: That takes a different path. [All of the data from Rubin to SLAC travels over the same network.]</p><p>12:53: It travels over dedicated high-speed network links, part of ESET, the research network, all the way from Chile, usually via Boca Raton, Florida, then Atlanta, before finally landing at SLAC.</p><p>13:05: And how fast does that need to be?</p><p>13:07: The goal is super fast.</p><p>13:09: They aim to get every image from the telescope in Chile to the data center at SLAC within 7 seconds of the shutter closing.</p><p>13:15: 7 seconds for gigabytes of data.</p><p>13:18: Yeah.</p><p>13:18: Sometimes network traffic bumps it up to maybe 30 seconds or so, but the target is 7.</p><p>13:23: It’s crucial for the next step, which is making sense of it all.</p><p>13:27: How do astronomers actually use this, this torrent of images and data?</p><p>13:30: Right.</p><p>13:31: This really changes how astronomy might be done.</p><p>13:33: Because Ruben is designed to generate alerts, real-time notifications about changes in the sky.</p><p>13:39: Alerts like, hey, something just exploded over here.</p><p>13:42: Pretty much.</p><p>13:42: It takes an image compared to the previous images of the same patch of sky and identifies anything that’s changed, appeared, disappeared, moved, gotten brighter, or fainter.</p><p>13:53: It expects to generate about 10,000 such alerts per image.</p><p>13:57: 10,000 per image, and they take an image every every 20 seconds or so on average, including readouts. [Images are taken every 34 seconds: a 30 second exposure, and then about 4 seconds for the telescope to move and settle.]</p><p>14:03: So you’re talking around 10 million alerts every single night.</p><p>14:06: 10 million a night.</p><p>14:07: Yep.</p><p>14:08: And the goal is to get those alerts out to the world within 60 seconds of the image being taken.</p><p>14:13: That’s insane.</p><p>14:14: What’s in an alert?</p><p>14:15: It contains the object’s position, brightness, how it’s changed, and little cut out images, postage stamps in the last 12 months of observations, so astronomers can quickly see the history.</p><p>14:24: But surely not all 10 million are real astronomical events satellites, cosmic rays.</p><p>14:30: Exactly.</p><p>14:31: The observatory itself does a first pass filter, masking out known issues like satellite trails, cosmic ray hits, atmospheric effects, with what they call real bogus stuff.</p><p>14:41: OK.</p><p>14:42: Then, this filtered stream of potentially real alerts goes out to external alert brokers.</p><p>14:49: These are systems run by different scientific groups around the world.</p><p>14:52: Yeah, and what did the brokers do?</p><p>14:53: They ingest the huge stream from Ruben and apply their own filters, based on what their particular community is interested in.</p><p>15:00: So an astronomer studying supernovae can subscribe to a broker that filters just for likely supernova candidates.</p><p>15:06: Another might filter for near Earth asteroids or specific types of variable stars.</p><p>15:12: so it makes the fire hose manageable.</p><p>15:13: You subscribe to the trickle you care about.</p><p>15:15: Precisely.</p><p>15:16: It’s a way to distribute the discovery potential across the whole community.</p><p>15:19: So it’s not just raw images astronomers get, but these alerts and presumably processed data too.</p><p>15:25: Oh yes.</p><p>15:26: Rubin provides the raw images, but also fully processed images, corrected for instrument effects, calibrated called processed visit images.</p><p>15:34: And also template images, deep combinations of previous images used for comparison.</p><p>15:38: And managing all that data, 15 petabytes you mentioned, how do you query that effectively?</p><p>15:44: They use a system called Keyserve. [The system is "QServ."]</p><p>15:46: It’s a distributed relational database, custom built basically, designed to handle these enormous astronomical catalogs.</p><p>15:53: The goal is to let astronomers run complex searches across maybe 15 petabytes of catalog data and get answers back in minutes, not days or weeks.</p><p>16:02: And how do individual astronomers actually interact with it?</p><p>16:04: Do they download petabytes?</p><p>16:06: No, definitely not.</p><p>16:07: For general access, there’s a science platform, the front end of which runs on Google Cloud.</p><p>16:11: Users interact mainly through Jupiter notebooks.</p><p>16:13: Python notebooks, familiar territory for many scientists.</p><p>16:17: Exactly.</p><p>16:18: They can write arbitrary Python code, access the catalogs directly, do analysis for really heavy duty stuff like large scale batch processing.</p><p>16:27: They can submit jobs to the big compute cluster at SLEC, which sits right next to the data storage.</p><p>16:33: That’s much more efficient.</p><p>16:34: Have they tested this?</p><p>16:35: Can it handle thousands of astronomers hitting it at once?</p><p>16:38: They’ve done extensive testing, yeah, scaled it up with hundreds of users already, and they seem confident they can handle up to maybe 3000 simultaneous users without issues.</p><p>16:49: And a key point.</p><p>16:51: After an initial proprietary period for the main survey team, all the data and importantly, all the software algorithms used to process it become public.</p><p>17:00: Open source algorithms too.</p><p>17:01: Yes, the idea is, if the community can improve on their processing pipelines, they’re encouraged to contribute those solutions back.</p><p>17:08: It’s meant to be a community resource.</p><p>17:10: That open approach is fantastic, and even the way the images are presented visually has some deep thought behind it, doesn’t it?</p><p>17:15: You mentioned Robert Leptina’s perspective.</p><p>17:17: Yes, this is fascinating.</p><p>17:19: It’s about how you assign color to astronomical images, which usually combine data from different filters, like red, green, blue.</p><p>17:28: It’s not just about making pretty pictures, though they can be beautiful.</p><p>17:31: Right, it should be scientifically meaningful.</p><p>17:34: Exactly.</p><p>17:35: Lepton’s approach tries to preserve the inherent color information in the data.</p><p>17:40: Many methods saturate bright objects, making their centers just white blobs.</p><p>17:44: Yeah, you see that a lot.</p><p>17:46: His algorithm uses a different mathematical scaling, more like a logarithmic scale, that avoids this saturation.</p><p>17:52: It actually propagates the true color information back into the centers of bright stars and galaxies.</p><p>17:57: So, a galaxy that’s genuinely redder, because it’s red shifted, will actually look redder in the image, even in its bright core.</p><p>18:04: Precisely, in a scientifically meaningful way.</p><p>18:07: Even if our eyes wouldn’t perceive it quite that way directly through a telescope, the image renders the data faithfully.</p><p>18:13: It helps astronomers visually interpret the physics.</p><p>18:15: It’s a subtle but powerful detail in making the data useful.</p><p>18:19: It really is.</p><p>18:20: Beyond just taking pictures, I heard Ruben’s wide view is useful for something else entirely gravitational waves.</p><p>18:26: That’s right.</p><p>18:26: It’s a really cool synergy.</p><p>18:28: Gravitational wave detectors like Lego and Virgo, they detect ripples in space-time, often from emerging black holes or neutron stars, but they usually only narrow down the location to a relatively large patch of sky, maybe 10 square degrees or sometimes much more.</p><p>18:41: Ruben’s camera has a field of view of about 9.6 square degrees.</p><p>18:45: That’s huge for a telescope.</p><p>18:47: It almost perfectly matches the typical LIGO alert area.</p><p>18:51: so when LIGO sends an alert, Ruben can quickly scan that whole error box, maybe taking just a few pointings, looking for any new point of light.</p><p>19:00: The optical counterpart, the Killanova explosion, or whatever light accompany the gravitational wave event.</p><p>19:05: It’s a fantastic follow-up machine.</p><p>19:08: Now, stepping back a bit, this whole thing sounds like a colossal integration challenge.</p><p>19:13: A huge system of systems, many parts custom built, pushed to their limits.</p><p>19:18: What were some of those big integration hurdles, bringing it all together?</p><p>19:22: Yeah, classic system of systems is a good description.</p><p>19:25: And because nobody’s built an observatory quite like this before, a lot of the commissioning phase, getting everything working together involves figuring out the procedures as they go.</p><p>19:34: Learning by doing on a massive scale.</p><p>19:36: Pretty much.</p><p>19:37: They’re essentially, you know, teaching the system how to walk.</p><p>19:40: And there’s this constant tension, this balancing act.</p><p>19:43: Do you push forward, maybe build up some technical debt, things you know you’ll have to fix later, or do you stop and make sure every little issue is 100% perfect before moving on, especially with a huge distributed team?</p><p>19:54: I can imagine.</p><p>19:55: And you mentioned the dome motors earlier.</p><p>19:57: That discovery about heat affecting images sounds like a perfect example of unforeseen integration issues.</p><p>20:03: Exactly.</p><p>20:03: Marina Pavvich described that.</p><p>20:05: They ran the dome motors at full speed, something maybe nobody had done for extended periods in that exact configuration before, and realized, huh.</p><p>20:13: The heat these generate might actually cause enough air turbulence to mess with our image quality.</p><p>20:19: That’s the kind of thing you only find when you push the integrated system.</p><p>20:23: Lots of unexpected learning then.</p><p>20:25: What about interacting with the outside world?</p><p>20:27: Other telescopes, the atmosphere itself?</p><p>20:30: How does Ruben handle atmospheric distortion, for instance?</p><p>20:33: that’s another interesting point.</p><p>20:35: Many modern telescopes use lasers.</p><p>20:37: They shoot a laser up into the sky to create an artificial guide star, right, to measure.</p><p>20:42: Atmospheric turbulence.</p><p>20:43: Exactly.</p><p>20:44: Then they use deformable mirrors to correct for that turbulence in real time.</p><p>20:48: But Ruben cannot use a laser like that.</p><p>20:50: Why?</p><p>20:51: Because its field of view is enormous.</p><p>20:53: It sees such a wide patch of sky at once.</p><p>20:55: A single laser beam, even a pinpoint from another nearby observatory, would contaminate a huge fraction of Ruben’s image.</p><p>21:03: It would look like a giant streak across, you know, a quarter of the sky for Ruben.</p><p>21:06: Oh, wow.</p><p>21:07: OK.</p><p>21:08: Too much interference.</p><p>21:09: So how does it correct for the atmosphere?</p><p>21:11: Software.</p><p>21:12: It uses a really clever approach called forward modeling.</p><p>21:16: It looks at the shapes of hundreds of stars across its wide field of view in each image.</p><p>21:21: It knows what those stars should look like, theoretically.</p><p>21:25: Then it builds a complex mathematical model of the atmosphere’s distorting effect across the entire field of view that would explain the observed star shapes.</p><p>21:33: It iterates this model hundreds of times per image until it finds the best fit. [The model is created by iterating on the image data, but iteration is not necessary for every image.]</p><p>21:38: Then it uses that model to correct the image, removing the atmospheric blurring.</p><p>21:43: So it calculates the distortion instead of measuring it directly with a laser.</p><p>21:46: Essentially, yes.</p><p>21:48: Now, interestingly, there is an auxiliary telescope built alongside Ruben, specifically designed to measure atmospheric properties independently.</p><p>21:55: Oh, so they could use that data.</p><p>21:57: They could, but currently, they’re finding their software modeling approach using the science images themselves, works so well that they aren’t actively incorporating the data from the auxiliary telescope for that correction right now.</p><p>22:08: The software solution is proving powerful enough on its own.</p><p>22:11: Fascinating.</p><p>22:12: And they still have to coordinate with other telescopes about their lasers, right?</p><p>22:15: Oh yeah.</p><p>22:15: They have agreements about when nearby observatories can point their lasers, and sometimes Ruben might have to switch to a specific filter like the Iband, which is less sensitive to the laser.</p><p>22:25: Light if one is active nearby while they’re trying to focus.</p><p>22:28: So many interacting systems.</p><p>22:30: What an incredible journey through the engineering of Ruben.</p><p>22:33: Just the sheer ingenuity from the custom steel pier and the capacitor banks, the hexapods, that incredibly flat camera, the data systems.</p><p>22:43: It’s truly a machine built to push boundaries.</p><p>22:45: It really is.</p><p>22:46: And it’s important to remember, this isn’t just, you know, a bigger version of existing telescopes.</p><p>22:51: It’s a fundamentally different kind of machine.</p><p>22:53: How so?</p><p>22:54: By creating this massive all-purpose data set, imaging the entire southern sky over 800 times, cataloging maybe 40 billion objects, it shifts the paradigm.</p><p>23:07: Astronomy becomes less about individual scientists applying for time to point a telescope at one specific thing and more about statistical analysis, about mining this unprecedented ocean of data that Rubin provides to everyone.</p><p>23:21: So what does this all mean for us, for science?</p><p>23:24: Well, it’s a generational investment in fundamental discovery.</p><p>23:27: They’ve optimized this whole system, the telescope, the camera, the data pipeline.</p><p>23:31: For finding, quote, exactly the stuff we don’t know we’ll find.</p><p>23:34: Optimized for the unknown, I like that.</p><p>23:36: Yeah, we’re basically generating this incredible resource that will feed generations of astronomers and astrophysicists.</p><p>23:42: They’ll explore it, they’ll harvest discoveries from it, they’ll find patterns and objects and phenomena within billions and billions of data points that we can’t even conceive of yet.</p><p>23:50: And that really is the ultimate excitement, isn’t it?</p><p>23:53: Knowing that this monumental feat of engineering isn’t just answering old questions, but it’s poised to open up entirely new questions about the universe, questions we literally don’t know how to ask today.</p><p>24:04: Exactly.</p><p>24:05: So, for you, the listener, just think about that.</p><p>24:08: Consider the immense, the completely unknown discoveries that are waiting out there just waiting to be found when an entire universe of data becomes accessible like this.</p><p>24:16: What might we find?</p><p><a href="#listen-conversation-rubin-observatory">Back to top</a></p></div></div></br>]]></description><pubDate>Mon, 23 Jun 2025 04:01:02 +0000</pubDate><guid>https://spectrum.ieee.org/vera-rubin-observatory-first-images</guid><category>Astronomy</category><category>Space</category><category>Telescopes</category><category>Dark matter</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-researchers-inspect-a-large-telescope-system-inside-a-scientific-facility.jpg?id=61078903&amp;width=980"></media:content></item><item><title>Video Friday: Jet-Powered Humanoid Robot Lifts Off</title><link>https://spectrum.ieee.org/video-friday-jet-powered-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/close-up-of-a-humanoid-robot-showing-intricate-mechanical-components-and-wiring-with-small-jet-engines-on-its-arms-and-torso.jpg?id=61079219&width=1200&height=800&coordinates=0%2C50%2C0%2C50"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><blockquote class="rm-anchors" id="t1bnhot4d5q"><em>This is the first successful vertical takeoff of a jet-powered flying humanoid robot, developed by Artificial and Mechanical Intelligence (AMI) at Istituto Italiano di Tecnologia (IIT). The robot lifted ~50 cm off the ground while maintaining dynamic stability, thanks to advanced AI-based control systems and aerodynamic modeling.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2666111cae7290d9efdf2108af79560f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/t1bNHoT4D5Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>We will have much more on this in the coming weeks!</p><p>[<a href="https://www.nature.com/articles/s44172-025-00447-w">Nature</a>] via [<a href="https://opentalk.iit.it/en/iit-demonstrates-that-a-humanoid-robot-can-fly/">IIT</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="iwcnynpjnm0"><em>As a first step toward our mission of deploying general-purpose robots, we are pushing the frontiers of what end-to-end AI models can achieve in the real world. We’ve been training models and evaluating their capabilities for dexterous sensorimotor policies across different embodiments, environments, and physical interactions. We’re sharing capability demonstrations on tasks stressing different aspects of manipulation: fine motor control, spatial and temporal precision, generalization across robots and settings, and robustness to external disturbances.</em></blockquote><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="3fbcd7173da8b61dcd8567ca2932e4fd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/mhfleCK_IAI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://generalistai.com/blog.html">Generalist AI</a>]</p><p>Thanks, Noah!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="iwcnynpjnm0"><em>Ground Control Robotics is introducing SCUTTLE, our newest elongate multilegged platform for mobility anywhere!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="da081e0bf9207ff62b17fbdc33a083c9" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/IWcNyNPjnM0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://groundcontrolrobotics.com/">Ground Control Robotics</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="udkddqxth5q"><em>Teleoperation has been around for a while, but what hasn’t been is precise, real-time force feedback. That’s where Flexiv steps in to shake things up. Now, whether you’re across the room or across the globe, you can experience seamless, high-fidelity remote manipulation with a sense of touch.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ec233d3290c27dd0cea84b0f17763f3e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/udkddqxth5Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>This sort of thing usually takes some human training, for which you’d be best served by <a data-linked-post="2657676851" href="https://spectrum.ieee.org/video-friday-iss-robot-arms" target="_blank">robot arms</a> with  <a data-linked-post="2650273235" href="https://spectrum.ieee.org/esa-space-teleoperation-tests" target="_blank">precise, real-time force feedback</a>. Hmm, I wonder where you’d find those...?</p><p>[<a href="https://www.flexiv.com/">Flexiv</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="xpx6ddrybv4"><em>The 1X World Model is a data-driven simulator for humanoid robots built with a grounded understanding of physics. It allows us to predict—or “hallucinate”—the outcomes of NEO’s actions before they’re taken in the real world. Using the 1X World Model, we can instantly assess the performance of AI models—compressing development time and providing a clear benchmark for continuous improvement.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="bbf4dbf833d6ae5f2f31d30e3867918e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xPX6dDRYbV4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.1x.tech/">1X</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="atr87nwq3eq"><em>SLAPBOT is an interactive robotic artwork by Hooman Samani and Chandler Cheng, exploring the dynamics of physical interaction, artificial agency, and power. The installation features a robotic arm fitted with a soft, inflatable hand that delivers slaps through pneumatic actuation, transforming a visceral human gesture into a programmed robotic response.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="98480c72f39d07a271776fa0143a9b44" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ATR87nwq3eQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>I asked, of course, whether SLAPBOT slaps people, and it does not: “Despite its provocative concept and evocative design, SLAPBOT does not make physical contact with human participants. It simulates the gesture of slapping without delivering an actual strike. The robotic arm’s movements are precisely choreographed to suggest the act, yet it maintains a safe distance.”</p><p>[<a href="https://hoomansamani.com/slapbot/">SLAPBOT</a>]</p><p>Thanks, Hooman!</p><div class="horizontal-rule"></div><p class="rm-anchors" id="xht3nvc9d-i">Inspecting the bowels of ships is something we’d really like robots to be doing for us, please and thank you.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f8e59dc2b2d1ebc58f4a8fa063562914" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/XhT3nVC9d-I?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.ntnu.edu/itk/research/robotics" target="_blank">Norwegian University of Science and Technology</a>] via [<a href="https://github.com/ntnu-arl/predictive_planning_ros">GitHub</a>]</p><p>Thanks, Kostas!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8a46uap367k"><em>H2L Corporation (hereinafter referred to as H2L) has unveiled a new product called “Capsule Interface,” which transmits whole-body movements and strength, enabling new shared experiences with robots and avatars. A product introduction video depicting a synchronization never before experienced by humans was also released.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="cd7cebba8a246b0d342bfa262937b917" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8a46Uap367k?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://h2l.jp/2025/06/18/%e5%85%a8%e8%ba%ab%e3%83%aa%e3%82%a2%e3%83%ab%e4%bd%93%e9%a8%93%ef%bc%81%e8%a6%8b%e3%82%8b%e8%81%9e%e3%81%8f%e3%81%ae%e5%85%88%e3%82%92%e5%89%b5%e3%82%8b%e3%80%82%e5%8b%95%e3%81%8d%e3%81%a8%e5%8a%9b/">H2L Corp.</a>] via [<a href="https://robotstart.info/2025/06/18/h2l-capsule-interface-launch.html">RobotStart</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="vbugenau3re">How do you keep a robot safe without requiring it to look at you? <a data-linked-post="2668807636" href="https://spectrum.ieee.org/feral-cat-radar-detector" target="_blank">Radar</a>!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6b6cefa1aff74634e83e5435745c35bc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/vbuGenAu3rE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://ieeexplore.ieee.org/document/11037369">Paper</a>] via [<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361" target="_blank">IEEE Sensors Journal</a>]</p><p>Thanks, Bram!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="pcank-5e-qo"><em>We propose Aerial Elephant Trunk, an aerial continuum manipulator inspired by the elephant trunk, featuring a small-scale quadrotor and a dexterous, compliant tendon-driven continuum arm for versatile operation in both indoor and outdoor settings.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="4a2829d05541e793bcf454fe8f2c394d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PcanK-5e-qo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://arclab.hku.hk/">Adaptive Robotics Controls Lab</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="w_qloi1pokw"><em>This video demonstrates a heavy weight lifting test using the ARMstrong Dex robot, focusing on a 40 kg bicep curl motion. ARMstrong Dex is a human-sized, dual-arm hydraulic robot currently under development at the Korea Atomic Energy Research Institute (KAERI) for disaster response applications. Designed to perform tasks flexibly like a human while delivering high power output, ARMstrong Dex is capable of handling complex operations in hazardous environments.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ed3765ff0740c1a013e9beeaae04aa6a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/W_QlOi1PoKw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.kaeri.re.kr/eng/">Korea Atomic Energy Research Institute</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lfat803dqmw"><em>Micro-robots that can inspect water pipes, diagnose cracks, and fix them autonomously—reducing leaks and avoiding expensive excavation work—have been developed by a team of engineers led by the University of Sheffield. </em></blockquote><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="204230bab58cc85ff8909e3f3029be79" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Q2loVe5_NcE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://www.sheffield.ac.uk/news/tiny-robots-could-help-fix-leaky-water-pipes">University of Sheffield</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lfat803dqmw"><em>We’re growing in size, scale, and impact! We’re excited to announce the opening of our serial production facility in the San Francisco Bay Area, the very first purpose-built <a data-linked-post="2650275419" href="https://spectrum.ieee.org/secretive-robotaxi-startup-zoox-prepares-for-realworld-testing" target="_blank">robotaxi</a> assembly facility in the United States. More space means more innovation, production, and opportunities to scale our fleet.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="504fa629f2368f24a9acf389d3e12a66" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lfAt803DQMw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://zoox.com/">Zoox</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="8-0d4lyjhqi"><em>Watch multipick in action as our pickle robot rapidly identifies, picks, and places multiple boxes in a single swing of an arm.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="36412a3fdb34a606e69841ccca3c2110" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8-0d4LyJhQI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://picklerobot.com/">Pickle</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="un_7inyazyq">And now, this.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ec349bce0e0fb055b052b7831af4f49b" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/uN_7INYaZYQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://info.aibo.sony.jp/info/2024/12/creatorschallenge2025.html">Aibo</a>]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="aqrqusrezhy"><em>Cargill’s Amsterdam Multiseed facility enlists Spot and Orbit to inspect machinery and perform visual checks, enhanced by all-new AI features, as part of their “Plant of the Future” program. </em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="b7c681340545a8e6334c9f64aa9dadd4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AqRquSReZHY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://bostondynamics.com/products/spot/">Boston Dynamics</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="vmsslbgktcu">This ICRA 2025 plenary talk is from Raffaello D’Andrea, entitled “Models are Dead, Long Live Models!”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d1600ac97242ef165c3fc6d7e438f123" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/vMSSlBGKtCU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://2025.ieee-icra.org/program/plenary-sessions/">ICRA 2025</a>]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="pfvctjompk8">Will data solve robotics and automation? Absolutely! Never! Who knows?! Let’s argue about it!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2e5008c9444113b6baa43e3fdeee54ed" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PfvctjoMPk8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[<a href="https://2025.ieee-icra.org/announcements/event-overview-for-thursday-may-22/">ICRA 2025</a>]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 20 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-jet-powered-robot</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><category>Industrial robots</category><category>Aibo</category><category>Dexterous</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/close-up-of-a-humanoid-robot-showing-intricate-mechanical-components-and-wiring-with-small-jet-engines-on-its-arms-and-torso.jpg?id=61079219&amp;width=980"></media:content></item><item><title>Making the Most of 1:1 Meetings With Your Boss</title><link>https://spectrum.ieee.org/making-most-of-1-1s</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p><em><em></em></em><span>I once had a manager at Meta who kept flip-flopping. We’d have our one-on-one meetings to align on the priorities, and whether I should focus on new features or fix user-reported bugs.</span></p><p>But after a few days, our plans would suddenly change. Certain bugs would become the highest priority, especially if the order came from directors or VPs. I noticed a pattern where my manager would change his mind after speaking with a strong-willed project manager or some engineering leader up the chain.</p><p>I was left feeling confused and unsupported.</p><p>When this happens, how do you tell your manager to shape up? Is it even your responsibility to give feedback to your manager?</p><p>The 1:1 is a critical forum to share this kind of feedback. A 1:1 is a focused meeting between two people within the company, typically lasting 30 or 45 minutes. When done well, these meetings are a valuable tool for building trust and fostering <span>career</span> growth. In my experience, managers will have weekly or biweekly 1:1s with each of their reports. If you don’t have a regularly scheduled 1:1 with your manager, you’re missing out. Ask for one!</p><p>The effectiveness of a 1:1 depends on your preparation before the meeting. Here are a few ground rules I set with my reports and my own manager to make them as valuable as possible: </p><ul><li><strong>Write down the agenda in advance</strong>. This shows that you have put some thought into the meeting and, therefore, it shouldn’t be canceled. Keep a running doc of everything you’ve written down. It can be helpful for both you and your manager to refer back to prior discussions and action items.</li><li><strong>Avoid status updates</strong>. Approach each 1:1 as a valuable opportunity to learn something or gain a new perspective. Feel free to write down status updates ahead of time, but you should minimize the time spent in the 1:1 just reviewing statuses. The conversation should be more focused on emotions and concerns rather than obvious facts.</li><li><strong>Be vulnerable</strong>. One litmus test for the conversation is, “Could this have been shared in the broader team meeting?” If the answer is yes, don’t waste the valuable 1:1 time on that topic. The 1:1 should focus on the sticky human issues that inevitably come up in the workplace: losing motivation, feeling overwhelmed, or delivering difficult feedback, for example.</li></ul><p>At Meta, I used the 1:1 time with my manager to share my concerns about the constantly shifting priorities between new features and user-reported bugs. The problem didn’t get resolved overnight, but at least he was aware of the issue. I felt heard, and we continued to monitor the situation as it improved.</p><p>What if your manager isn’t receptive to your feedback or concerns? In almost all cases, it’s not worth trying to “fix” your manager or your environment. There’s a clear power dynamic between you and your boss, and the energy spent on your manager is better spent on finding a new team or company altogether.</p><p>The 1:1 is a critical pillar for our <span>career</span> growth as engineers. Try out these tactics in your next 1:1 and let me know how it goes.</p><p>—Rahul</p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7xy-cHkoJQVMnmewwaYMOW8VZFKCyfo-0z-1IIYhFMj6rGI0-sFKmQHrjlK-bBrZYNI=" target="_blank" title="IEEE’s 5 New E-Books Provide Onramp to Engineering">IEEE’s 5 New E-Books Provide Onramp to Engineering</a></h3><p>Five new e-books from IEEE’s TryEngineering initiative provide an overview of topics including semiconductors, signal processing, oceanic engineering, and AI. As part of IEEE’s suite of pre-university resources, the free e-books are meant to introduce these complex technical topics to younger readers—the next generation of engineers.</p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7xy-cHkoJQVMnmewwaYMOW8VZFKCyfo-0z-1IIYhFMj6rGI0-sFKmQHrjlK-bBrZYNI=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7vcQBq3UJs2Ja2HNTuItc3zpfFjpQMKwyRpAeLsYi8ST-UmHQZDwfa-YI-bzs9co78w=" rel="noopener noreferrer" target="_blank" title="In Dubai’s AI job market, your passport matters">In Dubai’s AI job market, your passport matters</a></h3><p>More tech workers are moving to the UAE, which is now second only to the United States in attracting top AI talent, according to reporting from Rest of World. But as the country becomes an AI talent magnet, differences are emerging among workers based on where they’re from. While tech specialists from the West take top positions, engineers from developing nations often fill lower positions.</p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde7vcQBq3UJs2Ja2HNTuItc3zpfFjpQMKwyRpAeLsYi8ST-UmHQZDwfa-YI-bzs9co78w=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p><h3><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde75qf3MHa8lF6wHgIMrcfPblu1rhPFWwno5RBO9Kk8Rjn5vNYgTxUfSrDGpdilraaw9E=" rel="noopener noreferrer" target="_blank" title="Record Number of IEEE Members Visit U.S. Congress to Talk Tech Policy">Record Number of IEEE Members Visit U.S. Congress to Talk Tech Policy</a></h3><p>In this guest article, a technical program manager at Google reflects on his experience meeting with U.S. legislators this April. More than 300 IEEE representatives participated in the organization’s Congressional Visits Day to discuss federal funding, the STEM talent pipeline, and other policy issues. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGbGAde75qf3MHa8lF6wHgIMrcfPblu1rhPFWwno5RBO9Kk8Rjn5vNYgTxUfSrDGpdilraaw9E=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p>]]></description><pubDate>Thu, 19 Jun 2025 19:45:10 +0000</pubDate><guid>https://spectrum.ieee.org/making-most-of-1-1s</guid><category>Careers newsletter</category><category>Tech careers</category><category>Practical strategies</category><category>Careers</category><category>Career development</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>Check Out IEEE’s Revamped Online Presence</title><link>https://spectrum.ieee.org/ieee-revamped-online-presence</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-smartphone-digital-tablet-desktop-computer-and-laptop-arranged-closely-together-all-of-the-devices-screens-display-ieees.jpg?id=61006929&width=1200&height=800&coordinates=126%2C0%2C127%2C0"/><br/><br/><p>The newly designed <a href="https://www.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE website</a> makes it easier than ever to learn about the organization and its offerings. IEEE incorporated feedback from members and site visitors to create its modern look and feel.</p><p>Throughout the site, the work of IEEE and its members is prominently highlighted to show how they are creating a better world and driving engineering forward.</p><p>“The new website is more visual, with video and other media to engage all visitors. It also showcases our global community’s commitment as a public charity advancing technology for the benefit of humanity,” says <a href="https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead" target="_self">Sophia Muirhead</a>, IEEE executive director and chief operating officer.</p><p>The website reflects IEEE’s commitment to delivering an engaging online experience that is more intuitive for its global community. The storytelling theme of the site highlights select quotes, testimonials, and member and volunteer stories from IEEE’s more than 486,000 members and 189,000 student members from 347 sections in 10 geographic regions.</p><p>Whether you’re looking for a humanitarian project to get involved in, finding an upcoming conference to attend, taking a continuing education course, or publishing a research paper, the new design makes resources easier to access.</p><h2>Where to find courses, career resources, and more</h2><p>The first thing you’ll see on the new site is a box with scrolling options. Power What’s Next for Tech describes what IEEE is, and it includes a link to the <a href="https://www.ieee.org/about-ieee" rel="noopener noreferrer" target="_blank">What We Do</a> page, which gives an overview of the organization, including its <a href="https://www.ieee.org/about/vision-mission" rel="noopener noreferrer" target="_blank">mission</a>, <a href="https://spectrum.ieee.org/ieee-2025-2030-strategic-plan" target="_self">strategic plan</a>, <a href="https://www.ieee.org/about/ieee-history" rel="noopener noreferrer" target="_blank">history</a>, and offerings.</p><p>Using the arrows on the right side of the box, you can see the <a href="https://www.ieee.org/advancing-technology/building-better-world" rel="noopener noreferrer" target="_blank">Building a Better World</a> section, where visitors can learn about humanitarian initiatives such as <a href="https://spectrum.ieee.org/ieee-move-hurricane-helene-milton" target="_self">IEEE MOVE</a> and <a href="https://spectrum.ieee.org/epics-in-ieee-15th-anniversary" target="_self">EPICS in IEEE</a>, then <a href="https://www.ieee.org/education-career/advance-your-career" rel="noopener noreferrer" target="_blank">Career Support</a> and, finally, an option to <a href="https://www.ieee.org/membership/join" rel="noopener noreferrer" target="_blank">join IEEE</a> and be part of something bigger.</p><p>Scrolling down the home page, the next module, Happening Across IEEE, features upcoming <a href="https://www.ieee.org/conferences-events" rel="noopener noreferrer" target="_blank">conferences</a>, the latest <a href="https://www.ieee.org/ieee-standards" rel="noopener noreferrer" target="_blank">standards</a>, new <a href="https://www.ieee.org/education-career/continue-your-education" rel="noopener noreferrer" target="_blank">educational courses</a>, <a href="https://www.ieee.org/education-career/advance-your-career" rel="noopener noreferrer" target="_blank">ways to advance your career</a>, and how to get involved with <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">IEEE’s societies, councils, and communities</a>.</p><p class="pull-quote">“The new website is more visual, with video and other media to engage all visitors. It also showcases our global community’s commitment as a public charity advancing technology for the benefit of humanity.”</p><p>The next section, the IEEE Is the Global Community of Technology Professionals module, has options to Find Your Path to learn about resources available for <a href="https://www.ieee.org/ieee-industry-engagement-committee" target="_blank">industry professionals</a>, <a href="https://www.ieee.org/publications-research/publish-ieee" rel="noopener noreferrer" target="_blank">authors and researchers</a>, <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">students and young professionals</a>, <a href="https://www.ieee.org/communities-connection/volunteering" rel="noopener noreferrer" target="_blank">volunteers</a>, <a href="https://www.ieee.org/communities-connection/societies-councils-and-communities" rel="noopener noreferrer" target="_blank">new members</a>, and <a href="https://spectrum.ieee.org/ieee-life-members-reconnect" target="_self">retirees</a>.</p><p>The following section, Latest Innovations, features videos and articles from publications including <a href="https://spectrum.ieee.org/" target="_self"><em>IEEE Spectrum</em></a> and <a href="https://spectrum.ieee.org/the-institute/" target="_self"><em><em>The Institute</em></em></a> on cutting-edge technology engineers are working on, such as <a href="https://spectrum.ieee.org/plant-electronic-tattoo?_gl=1*gas32r*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE." target="_self">electronic tattoos</a>.</p><p>Keep scrolling down and you’ll get to know IEEE members and their thoughts on what’s next for technologies such as <a href="https://transmitter.ieee.org/?_gl=1*1hbzxb6*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE." rel="noopener noreferrer" target="_blank">artificial intelligence</a> and <a href="https://transmitter.ieee.org/iot-2025/?_gl=1*1hbzxb6*_gcl_au*OTQ1MzIwMDg3LjE3NDczMzI3NDE.#story-5" rel="noopener noreferrer" target="_blank">quantum computing</a>.</p><p>“This redesign marks a key milestone in IEEE’s digital transformation,” Muirhead says. “The use of rich media, video content, and dynamic storytelling features allows for deeper engagement with IEEE and understanding its various offerings.</p><p>“However, it is just the beginning. In the months ahead, we will continue to enhance the site with new features, updated content, and richer tools.”</p>]]></description><pubDate>Thu, 19 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-revamped-online-presence</guid><category>Careers</category><category>Educational courses</category><category>Ieee members</category><category>Ieee products and services</category><category>Publications</category><category>Standards</category><category>Type:ti</category><dc:creator>Kathy Pretz</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-smartphone-digital-tablet-desktop-computer-and-laptop-arranged-closely-together-all-of-the-devices-screens-display-ieees.jpg?id=61006929&amp;width=980"></media:content></item><item><title>A New BCI Instantly Synthesizes Speech</title><link>https://spectrum.ieee.org/bci-speech-synthesis</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-women-analyzing-data-on-three-computer-screens-in-a-modern-workspace.jpg?id=61072467&width=1200&height=800&coordinates=120%2C0%2C120%2C0"/><br/><br/><p>By analyzing neural signals, a brain-computer interface (BCI) can now almost instantaneously synthesize the speech of a man who lost use of his voice due to a neurodegenerative disease, a new study finds.</p><p>The researchers caution it will still be a long time before such a device, which could restore speech to paralyzed patients, will find use in everyday communication. Still, the hope is this work “will lead to a pathway for improving these systems further—for example, through technology transfer to industry,” says <a href="https://maitreyeew.github.io/" rel="noopener noreferrer" target="_blank">Maitreyee Wairagkar</a>, a project scientist at the University of California Davis’s Neuroprosthetics Lab.</p><p>A major potential application for <a href="https://spectrum.ieee.org/soft-robotics" target="_self">brain-computer interfaces</a> is restoring the ability to<strong> </strong>communicate to people who can no longer speak due to disease or injury. For instance, scientists have developed a number of BCIs that can help translate <a href="https://spectrum.ieee.org/braincomputer-interface-smashes-previous-record-for-typing-speed" target="_self">neural signals into text</a>.</p><p>However, text alone fails to capture many key aspects of human speech, such as intonation, that help to convey meaning. In addition, text-based communication is slow, Wairagkar says.</p><p>Now, researchers have developed what they call a brain-to-voice neuroprosthesis that can decode neural activity into sounds in real time. They detailed <a href="https://www.nature.com/articles/s41586-025-09127-3" rel="noopener noreferrer" target="_blank">their findings</a> 11 June in the journal <em>Nature</em>.</p><p>“Losing the ability to speak due to neurological disease is devastating,” Wairagkar says. “Developing a technology that can bypass the damaged pathways of the nervous system to restore speech can have a big impact on the lives of people with speech loss.”</p><h2>Neural Mapping for Speech Restoration</h2><p>The new BCI mapped neural activity using four microelectrode arrays. In total, the scientists placed 256 microelectrode arrays in three brain regions, chief among them the ventral precentral gyrus, which plays a key role in controlling the muscles underlying speech.</p><p>“This technology does not ‘read minds’ or ‘read inner thoughts,’” Wairagkar says. “We record from the area of the brain that controls the speech muscles. Hence, the system only produces voice when the participant voluntarily tries to speak.”</p><p>The researchers implanted the BCI in a 45-year-old volunteer with <a href="https://www.mayoclinic.org/diseases-conditions/amyotrophic-lateral-sclerosis/symptoms-causes/syc-20354022" rel="noopener noreferrer" target="_blank">amyotrophic lateral sclerosis</a> (ALS), the neurodegenerative disorder also known as Lou Gehrig’s disease. Although the volunteer could still generate vocal sounds, he was unable to produce intelligible speech on his own for years before the BCI.</p><p>The neuroprosthesis recorded the neural activity that resulted when the patient attempted to read sentences on a screen out loud. The scientists then trained a <a href="https://spectrum.ieee.org/realtime-hologram" target="_self">deep-learning AI model</a> on this data to produce his intended speech.</p><p>The researchers also trained a voice-cloning AI model on recordings made of the patient before his condition so the BCI could synthesize his pre-ALS voice. The patient reported that listening to the synthesized voice “made me feel happy, and it felt like my real voice,” the study notes.</p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="607bfba9c1ebdc9e03874d7ef83bc6a4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/fdfL5P4n6vc?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">Neuroprosthesis Reproduces a Man’s Speech</small> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">UC Davis</small> </p><p>In experiments, the scientists found that the BCI could detect key aspects of intended vocal intonation. They had the patient attempt to speak sets of sentences as either statements, which had no changes in pitch, or as questions, which involved rising pitches at the ends of the sentences. They also had the patient emphasize one of the seven words in the sentence “<a href="https://www.wired.com/story/one-sentence-with-7-meanings-unlocks-a-mystery-of-human-speech/" rel="noopener noreferrer" target="_blank">I never said she stole my money</a>” by changing its pitch. (The sentence has seven different meanings, depending on which word is emphasized.) These tests revealed increased neural activity toward the ends of the questions and before emphasized words. In turn, this let the patient control his BCI voice enough to ask a question, emphasize specific words in a sentence, or sing three-pitch melodies.</p><p>“Not only what we say but also how we say it is equally important,” Wairagkar says. “Intonation of our speech helps us to communicate effectively.”</p><p>All in all, the new BCI could acquire neural signals and produce sounds with a delay of 25 milliseconds, enabling near-instantaneous speech synthesis, Wairagkar says. The BCI also proved flexible enough to speak made-up pseudo-words, as well as interjections such as “ahh,” “eww,” “ohh,” and “hmm.”</p><p>The resulting voice was often intelligible, but not consistently so. In tests where human listeners had to transcribe the BCI’s words, they understood what the patient said about 56 percent of the time, up from about 3 percent from when he did not use the BCI.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Computer screen displaying neural signal data in multiple graph plots." class="rm-shortcode" data-rm-shortcode-id="a6610d42c204fca95be9c543fa52613f" data-rm-shortcode-name="rebelmouse-image" id="37c4f" loading="lazy" src="https://spectrum.ieee.org/media-library/computer-screen-displaying-neural-signal-data-in-multiple-graph-plots.jpg?id=61072472&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Neural recordings of the BCI participant shown on screen.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">UC Davis</small></p><p>“We do not claim that this system is ready to be used to speak and have conversations by someone who has lost the ability to speak,” Wairagkar says. “Rather, we have shown a proof of concept of what is possible with the current BCI technology.”</p><p>In the future, the scientists plan to improve the accuracy of the device—for instance, with more electrodes and better AI models. They also hope that BCI companies might start clinical trials incorporating this technology. “It is yet unknown whether this BCI will work with people who are fully locked in”—that is, nearly completely paralyzed, save for eye motions and blinking, Wairagkar adds.</p><p>Another interesting research direction is to study whether such speech BCIs could be useful for people with language disorders, such as <a href="https://www.mayoclinic.org/diseases-conditions/aphasia/symptoms-causes/syc-20369518" target="_blank">aphasia</a>. “Our current target patient population cannot speak due to muscle paralysis,” Wairagkar says. “However, their ability to produce language and cognition remains intact.” In contrast, she notes, future work might investigate restoring speech to people with damage to brain areas that produce speech, or with disabilities that have prevented them from learning to speak since childhood.</p>]]></description><pubDate>Thu, 19 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/bci-speech-synthesis</guid><category>Neuroprosthetic</category><category>Bci</category><category>Deep learning</category><category>Artificial intelligence</category><category>Brain computer interface</category><dc:creator>Charles Q. Choi</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-women-analyzing-data-on-three-computer-screens-in-a-modern-workspace.jpg?id=61072467&amp;width=980"></media:content></item><item><title>Guatemalan Engineer Ascends From Rural Roots to Ph.D.</title><link>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&width=1200&height=800&coordinates=0%2C242%2C0%2C243"/><br/><br/><p>Although she is just now starting her career as a tech professional,<a href="https://fr.linkedin.com/in/mayra-yucely-beb-caal-b1a1b969" rel="noopener noreferrer" target="_blank"> Mayra Yucely Beb Caal</a> has already overcome towering obstacles. The IEEE member sees her life as an example for other young people, demonstrating that they can succeed despite disadvantages they face due to their gender, ethnicity, language, or economic background.</p><p>Born in Cobán, the capital of Alta Verapaz in northern Guatemala, she grew up in a community far removed from the world of technology. But she attributes her success to having been steeped in the region’s cultural richness and her people’s unshakable resilience. The daughter of a single mother who was a schoolteacher, Caal says she spent her early years living with her aunts while her mother worked in distant towns for weeks at a time to provide for the family. In her community—mostly descendants of the indigenous<a href="https://en.wikipedia.org/wiki/Q%CA%BCeqchi%CA%BC" rel="noopener noreferrer" target="_blank"> Maya-Kekchi</a> people—technology was rarely discussed. Pursuing a degree meant studying to become a physician, the most prestigious occupation anyone there was aware of.</p><p>No one imagined that a girl from Cobán would one day hold a doctorate in engineering or conduct cancer research in France.</p><p>On the path to her ambitious goals, Caal got a big assist from IEEE. She received a <a href="https://ieeesystemscouncil.org/awards/james-o-gray-scholarship" rel="noopener noreferrer" target="_blank">Gray scholarship</a>, awarded by the IEEE Systems Council to students pursuing graduate studies in process control systems engineering, plant automation, or instrumentation measurement. The US $5,000 award supplemented other scholarships which helped her to study for her Ph.D. </p><h2>Discovering robotics and mechatronics in high school</h2><p>Caal was introduced to technology when, at age 14, she received a government scholarship to attend the<a href="https://au.linkedin.com/school/intecapoficial/" rel="noopener noreferrer" target="_blank"> Instituto Técnico de Capacitación y Productividad</a>, a high school in Guatemala City. It was her first exposure to electronics, <a href="https://spectrum.ieee.org/topic/robotics/" target="_self">robotics</a>, and <a href="https://spectrum.ieee.org/houston-mechatronics-raises-20m-to-bring-nasa-expertise-to-transforming-robot-submersibles" target="_self">mechatronics</a> (an interdisciplinary field that combines mechanical engineering, electronics, computer science, and control systems)—subjects that weren’t taught in her local school. Caal was fascinated by the ability to study the fields, though her family couldn’t afford the tuition to the private universities where she could earn a degree. But that didn’t dissuade her.</p><h2>Pursuing a mechatronics career despite gender barriers</h2><p>She applied for a scholarship from<a href="https://fundacionjbg.org/" rel="noopener noreferrer" target="_blank"> the Gutiérrez Foundation</a>, named for the founder of <a href="https://somoscmi.com/en/" rel="noopener noreferrer" target="_blank">CMI</a>, a Guatemala-based multinational company. The foundation’s scholarship covers full tuition, fees, and the cost of books for the duration of a recipient’s undergraduate studies.</p><p>In 2016 Caal earned a bachelor’s degree in mechatronics engineering at the<a href="https://www.uvg.edu.gt/" rel="noopener noreferrer" target="_blank"> Universidad del Valle de Guatemala</a>, also in Guatemala City. There were few women in her class.</p><p>The job market was unwelcoming, however, she says. Despite her credentials, employers often required five years of experience for entry-level positions, and they expressed a preference for male employees, she says. It took six months to land her first job as a mechanical maintenance supervisor near her hometown.</p><p>She held that job for six months before moving back to Guatemala City in search of better opportunities. She took a position as head of mechanical maintenance at <a href="https://mayaprin.com/" rel="noopener noreferrer" target="_blank">Mayaprin</a>, a company specializing in commercial printing services, but she wasn’t satisfied with her career trajectory.</p><h2>Earning an engineering education abroad</h2><p>Caal decided to return to school in 2018 to pursue a master’s degree in mechatronics and micromechatronics engineering. She received a scholarship from the<a href="https://erasmus-plus.ec.europa.eu/opportunities/opportunities-for-individuals/students/erasmus-mundus-joint-masters" rel="noopener noreferrer" target="_blank"> Mundus Joint Master</a> program, part of a European Commission–sponsored initiative that provides funding for education, training, and youth in sports. Because the Mundus scholarship requires recipients to study at several universities, she took classes at schools in Europe and Africa, including <a href="https://www.supmicrotech.fr/en" rel="noopener noreferrer" target="_blank">École Nationale Supérieure de Mécanique et des Microtechniques</a>, <a href="https://nu.edu.eg/" rel="noopener noreferrer" target="_blank">Nile University</a>, and <a href="https://www.uniovi.es/en/" rel="noopener noreferrer" target="_blank">Universidad de Oviedo</a>. Her studies focused on mechatronics and microelectronics, and the courses were taught in French, English, and Spanish.</p><p>The multilingual challenge was immense, she says. She recently had learned English, and French was completely new to her. Yet she persevered, driven by her goal of working on technology that could serve humanity.</p><p>She received a master’s degree from Universidad de Oviedo in 2020 and was accepted into a Ph.D. program at <a href="https://www.univ-fcomte.fr/universite-bourgogne-franche-comte" rel="noopener noreferrer" target="_blank">Université de Bourgogne Franche-Comté</a>, in Besançon, France. Her doctoral studies  were mainly funded by the French Ministry of Higher Education and Research (MESRI), with additional support from the Gray scholarship..</p><p>Her research led to a full-time job last year as an R&D engineer focused on mechatronics and robotics at<a href="https://www.hyprview.com/" rel="noopener noreferrer" target="_blank"> HyprView</a> in Caen, France. The startup, founded in 2021, develops software to assist with medical data analysis and boost the performance of imaging tools.</p><p>Caal says she is part of a team that uses AI and automated systems to improve cancer detection. Although she has held the position for less than a year, she says she already feels she is contributing to public health through applied technology.</p><h2>IEEE support and STEM mentorship</h2><p>Through much of Caal’s journey, IEEE has played a critical role. As an undergraduate, she was vice president and then president of her university’s <a href="https://www.facebook.com/IEEEUVG/?locale=pt_PT" rel="noopener noreferrer" target="_blank">IEEE student branch</a>. Her first international conference experience came from attending<a href="https://r9.ieee.org/" rel="noopener noreferrer" target="_blank"> IEEE Region 9</a> conferences, which she says opened her eyes to the world of research, publishing, and the global engineering community.</p><p>She organized outreach efforts to local schools, conducting simple experiments to encourage girls to consider STEM careers. Her efforts were in direct opposition to longstanding gender norms in Guatemala. Caal was also an active member of the IEEE <a href="https://www.femto-st.fr/en/femto-st-student-chapter-0" rel="noopener noreferrer" target="_blank">student branch at FEMTO-ST /Université de Bourgogne Franche-Comté</a>.</p><p>Today, Caal continues to advise these student branches while advancing her career in France.</p><p>Language issues and gender bias remain obstacles: “As a young woman leading male engineers, I have repeatedly had to prove my competence in ways my male peers haven’t,” she says. But the challenges have only strengthened her resolve, she adds.</p><p>Eventually, she says, she hopes to return to Guatemala to help build a stronger research infrastructure there with sufficient career opportunities for tech professionals in industry and academia. She says she also wants to ensure that children in even the most rural, poverty-stricken schools have access to food, electricity, and the Internet.</p><p>Her mission is clear: “To use technology to serve a purpose, always aimed at improving lives.”</p><p>“I don’t want to create technology just for the sake of it,” she says. “I want it to mean something—to help solve real problems in society, like the ones I faced early on.” </p>]]></description><pubDate>Tue, 17 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/guatemalan-ai-engineer-cancer-research</guid><category>Cancer detection</category><category>Guatemala</category><category>Ieee member news</category><category>Ieee systems council</category><category>James o. gray scholarship</category><category>Mechatronics</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/mayra-yucely-beb-caal-smiling-in-a-dress-shirt-and-blazer.jpg?id=61005981&amp;width=980"></media:content></item><item><title>Why JPEGs Still Rule the Web</title><link>https://spectrum.ieee.org/jpeg-image-format-history</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&width=1200&height=800&coordinates=0%2C250%2C0%2C250"/><br/><br/><p><span>For roughly three decades, the JPEG has been the World Wide Web’s primary image format. But it wasn’t the one the Web started with. In fact, the first mainstream graphical browser, NCSA Mosaic, didn’t initially support inline JPEG files—</span><a href="https://ftp.jurassic.nl/pub/irix/mosaic/Mac/FAQ/FAQ.HTML" target="_blank">just inline GIFs</a><span>, along with a couple of other </span><a href="https://spectrum.ieee.org/carnegie-mellon-is-saving-old-software-from-oblivion" target="_blank">formats forgotten to history</a><span>. However, the JPEG had many advantages over the format it quickly usurped.</span></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://tedium.co/" target="_blank"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://tedium.co/" target="_blank"><img alt='Tedium logo, a red rectangle with the word Tedium in white, above the text "This post originally appeared on Tedium."' class="rm-shortcode" data-rm-shortcode-id="c603546dab9e1dd1612b1364d3107471" data-rm-shortcode-name="rebelmouse-image" id="6aeb5" loading="lazy" src="https://spectrum.ieee.org/media-library/tedium-logo-a-red-rectangle-with-the-word-tedium-in-white-above-the-text-this-post-originally-appeared-on-tedium.png?id=60568211&width=980"/></a><small class="image-media media-photo-credit" placeholder="add photo credit..."><a href="https://spectrum.ieee.org/media-library/eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjpbwfnzsi6imh0dhbzoi8vyxnzzxrzlnjibc5tcy82mdu2odixms9vcmlnaw4ucg5niiwizxhwaxjlc19hdci6mtc1nzmynzi1mn0._gbglxpbsmfwoobs84_whxbl_vnslwx1geovlhgvwku/image.png?width=980" target="_blank"> </a></small></p><p>Despite not appearing together right away—the JPEG first appeared in Netscape in 1995, three years after the image standard was officially published—the JPEG and Web browser fit together naturally. JPEG files degraded more gracefully than GIFs, retaining more of the picture’s initial form—and that allowed the format to scale to greater levels of success. While it wasn’t capable of animation, it progressively expanded from something a modem could pokily render to a format that was good enough for high-end professional photography.</p><p>For the Internet’s purposes, the degradation was the important part. But it wasn’t the only thing that made the JPEG immensely valuable to the digital world. An essential part was that it was a documented standard built by numerous stakeholders.</p><h2>The GIF was a de facto standard. The JPEG was an actual one</h2><p>How important is it that JPEG was a standard? Let me tell you a story.</p><p>During <a href="https://archive.nytimes.com/bits.blogs.nytimes.com/2013/05/21/an-honor-for-the-creator-of-the-gif/?smid=tw-nytimes" target="_blank">a 2013 <em><em>New York Times</em></em> interview</a> conducted just before he received an award honoring his creation, GIF creator Steve Wilhite stepped into a debate he unwittingly created. <span>Simply put, nobody knew how to pronounce the acronym for the image format he had fostered, the Graphics Interchange Format. He used the moment to attempt to set the record straight—it was pronounced like the peanut butter brand: “It is a soft ‘G,’ pronounced ‘jif.’ End of story,” he said.</span></p><p>I <a href="https://shortformblog.com/post/51026114908/steve-wilhite-gif-award" target="_blank">posted a quote from Wilhite</a> on my popular Tumblr around that time, a period when the social media site was the center of the GIF universe. And soon afterward, my post got thousands of reblogs—nearly all of them disagreeing with Wilhite. Soon, <a href="https://knowyourmeme.com/memes/gif-vs-jif-pronunciation-debate/" target="_blank">Wilhite’s quote became a meme</a>.</p><p>The situation paints how Wilhite, who died in 2022, did not develop his format by committee. He could say it sounded like “JIF” because he built it himself. He was handed the project as a CompuServe employee in 1987; he produced the object, and that was that. The initial document describing how it works? <a href="https://www.w3.org/Graphics/GIF/spec-gif87.txt" target="_blank">Dead simple</a>. Thirty-eight years later, we’re still using the GIF—but it never rose to the same prevalence of JPEG.</p><p>The JPEG, which formally emerged about five years later, was very much <em><em>not</em></em> that situation. Far from it, in fact—it’s the difference between a de facto standard and an actual one. And that proved essential to its eventual ubiquity.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Full-resolution photo of a sunlit pine forest with a narrow trail winding through the trees and grassy undergrowth." class="rm-shortcode" data-rm-shortcode-id="d5f40fd52d60e8ec21c06940db3febc7" data-rm-shortcode-name="rebelmouse-image" id="2999a" loading="lazy" src="https://spectrum.ieee.org/media-library/full-resolution-photo-of-a-sunlit-pine-forest-with-a-narrow-trail-winding-through-the-trees-and-grassy-undergrowth.jpg?id=61013768&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">We’re going to degrade the quality of this image throughout this article. At its full image size, it’s 13.7 megabytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Irina Iriser</small></p><h2>How the JPEG format came to life</h2><p>Built with input from dozens of stakeholders, the Joint Photographic Experts Group ultimately aimed to create a format that fit everyone’s needs. (Reflecting its committee-led roots, there would be no confusion about the format’s name—an acronym of the organization that designed it.) And when the format was finally unleashed on the world, it was the subject of a book that was more than 600 pages.</p><p><em><em>JPEG: Still Image Data Compression Standard</em></em>, written by IBM employees and JPEG organization stakeholders William B. Pennebaker and Joan L. Mitchell, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA1&printsec=frontcover" target="_blank">describes</a> a landscape of multimedia imagery, held back without a way to balance the need for photorealistic images and immediacy. Standardization, they believed, could fix this.</p><p>“The problem was not so much the lack of algorithms for image compression (as there is a long history of technical work in this area),” the authors wrote, “but, rather, the lack of a standard algorithm—one which would allow an interchange of images between diverse applications.”</p><p>And they were absolutely right. For more than 30 years, JPEG has made high-quality, high-resolution photography accessible in operating systems far and wide. Although we no longer need to compress JPEGs to within an inch of their life, having that capability helped enable the modern Internet.</p><p><a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&dq=ibm+jpeg&pg=PA278&printsec=frontcover" target="_blank">As the book notes</a>, Mitchell and Pennebaker were given IBM’s support to follow through this research and work with the JPEG committee, and that support led them to develop many of the JPEG format’s foundational patents. Described in <a href="https://patents.google.com/patent/US4905297" target="_blank">patents</a> filed by Mitchell and Pennebaker in 1988, IBM and other members of the JPEG standards committee, such as AT&T and Canon, were developing ways to use compression to make high-quality images easier to deliver in confined settings.</p><p>Each member brought their own needs to the process. Canon, obviously, was more focused on printers and photography, while AT&T’s interests were tied to data transmission. Together, the companies left behind a standard that has stood the test of time.</p><p>All this means, funnily enough, that the first place that a program capable of using JPEG compression appeared was not MacOS or Windows, but OS/2—a fascinating-but-failed graphical operating system created by Pennebaker and Mitchell’s employer, IBM. As early as 1990, OS/2 supported the format through the <a href="https://www.edm2.com/index.php/OS/2_Image_Support" target="_blank">OS/2 Image Support</a> application.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Nearly identical photo of a sunlit pine forest." class="rm-shortcode" data-rm-shortcode-id="b810c6423ebd0b3e07f4d42c4c7162ac" data-rm-shortcode-name="rebelmouse-image" id="ef951" loading="lazy" src="https://spectrum.ieee.org/media-library/nearly-identical-photo-of-a-sunlit-pine-forest.jpg?id=61015732&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">At 50 percent of its initial quality, the image is down to about 2.6 MB. By dropping half of the image’s quality, we brought it down to one-fifth of the original file size. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><h2>What a JPEG does when you heavily compress it</h2><p>The thing that differentiates a JPEG file from a PNG or a GIF is how the data degrades as you compress it. The goal for a JPEG image is to still look like a photo when all is said and done, even if some compression is necessary to make it all work at a reasonable size. That way, you can display something that looks close to the original image in fewer bytes.</p><p>Or, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA4&printsec=frontcover" target="_blank">as Pennebaker and Mitchell put it</a>, “the most effective compression is achieved by approximating the original image (rather than reproducing it exactly).”</p><p>Central to this is a compression process called <a href="https://spectrum.ieee.org/compression-algorithms" target="_blank">discrete cosine transform</a> (DCT), a lossy form of compression encoding heavily used in all sorts of compressed formats, most notably in digital audio and signal processing. Essentially, it delivers a lower-quality product by removing details, while still keeping the heart of the original product through approximation. The stronger the cosine transformation, the more compressed the final result.</p><p>The algorithm, <a href="https://ieeexplore.ieee.org/abstract/document/1672377" target="_blank">developed by researchers</a> in the 1970s, essentially takes a grid of data and treats it as if you’re controlling its frequency with a knob. The data rate is controlled like water from a faucet: The more data you want, the higher the setting. DCT allows a trickle of data to still come out in highly compressed situations, even if it means a slightly compromised result. In other words, you may not keep all the data when you compress it, but DCT allows you to keep the heart of it.</p><p>(See <a href="https://www.youtube.com/watch?v=Q2aEzeMDHMA" target="_blank">this video</a> for a more technical but still somewhat easy-to-follow description of DCT.)</p><p>DCT is everywhere. If you <a href="https://ottverse.com/discrete-cosine-transform-dct-video-compression/" target="_blank">have ever seen a streaming video</a> or an online radio stream that degraded in quality because your bandwidth suddenly declined, you’ve witnessed DCT being utilized in real time.</p><p>A JPEG file doesn’t have to leverage the DCT with just one method, <a href="https://www.google.com/books/edition/JPEG/AepB_PZ_WMkC?hl=en&gbpv=1&pg=PA81&printsec=frontcover" target="_blank">as <em><em>JPEG: Still Image Data Compression Standard</em></em> explains</a>:</p><p>The JPEG standard describes a family of large image compression techniques, rather than a single compression technique. It provides a “tool kit” of compression techniques from which applications can select elements that satisfy their particular requirements.</p><p>The toolkit has four modes:</p><ul><li><strong>Sequential DCT,</strong> which displays the compressed image in order, like a window shade slowly being rolled down</li><li><strong>Progressive DCT,</strong> which displays the full image in the lowest-resolution format, then adds detail as more information rolls in</li><li><strong>Sequential lossless,</strong> which uses the window-shade format but doesn’t compress the image</li><li><strong>Hierarchical mode,</strong> which combines the prior three modes—so maybe it starts with a progressive mode, then loads DCT compression slowly, but then reaches a lossless final result</li></ul><p>At the time the JPEG was being created, modems were extremely common. That meant images loaded slowly, making Progressive DCT the most fitting format for the early Internet. Over time, the progressive DCT mode has become less common, as many computers can simply load the sequential DCT in one fell swoop.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with very slight degradation visible." class="rm-shortcode" data-rm-shortcode-id="44d7f73d86c46d8ae4653970ebbffbdd" data-rm-shortcode-name="rebelmouse-image" id="fd0cd" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-very-slight-degradation-visible.jpg?id=61029700&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">That same forest, saved at 5 percent quality, now down to about 419 kilobytes.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>When an image is compressed with DCT, the change tends to be less noticeable in busier, more textured areas of the picture, like hair or foliage. Those areas are harder to compress, which means they keep their integrity longer. It tends to be more noticeable, however, with solid colors or in areas where the image sharply changes from one color to another—like text on a page. Ever screenshot a social media post, only for it to look noisy? Congratulations, you just made a JPEG file.</p><p>Other formats, like PNG, do better with text, because their compression format is intended to be non-lossy. (Side note: PNG’s compression format, DEFLATE, <a href="https://www.ietf.org/rfc/rfc1951" target="_blank">was designed</a> by Phil Katz, who also created the ZIP format. The PNG format uses it in part because it was a license-free compression format. So it turns out the brilliant coder with the <a href="https://www.wsj.com/articles/SB961363319756539141" target="_blank">sad life story</a> improved the Internet in multiple ways before his <a href="https://tedium.co/2015/02/17/early-internet-history-tales/" target="_blank">untimely passing</a>.)</p><p>In many ways, the JPEG is one tool in our image-making toolkit. Despite its age and maturity, it remains one of our best options for sharing photos on the Internet. But it is not a tool for every setting—despite the fact that, like a wrench sometimes used as a hammer, we often leverage it that way.</p><h2>Forgent Networks claimed to own the JPEG’s defining algorithm</h2><p>The JPEG format gained popularity in the ’90s for reasons beyond the quality of the format. Patents also played a role: Starting in 1994, the tech company Unisys <a href="https://www.theregister.com/1999/09/01/unisys_demands_5k_licence_fee/" target="_blank">attempted to bill individual users</a> who relied on GIF files, which used a patent the company owned. This made the free-to-use JPEG more popular. (This situation also led to the creation of the patent-free PNG format.)</p><p>While the JPEG was standards-based, it could still have faced the same fate as the GIF, thanks to the quirks of the patent system. A few years before the file format came to life, a pair of Compression Labs employees <a href="https://patents.google.com/patent/US4698672A/en" target="_blank">filed a patent application</a> that dealt with the compression of motion graphics. By the time anyone noticed its similarity to JPEG compression, the format was ubiquitous.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="The same photo of a sunlit pine forest with more noticeable color degradation visible. Areas with previously subtle color gradients now appear more like blocks of color." class="rm-shortcode" data-rm-shortcode-id="5fb227e9168811101372ad575e42dc89" data-rm-shortcode-name="rebelmouse-image" id="e1296" loading="lazy" src="https://spectrum.ieee.org/media-library/the-same-photo-of-a-sunlit-pine-forest-with-more-noticeable-color-degradation-visible-areas-with-previously-subtle-color-gradie.jpg?id=61016218&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Our forest, saved at 1 percent quality. This image is only about 239 KB in size, yet it’s still easily recognizable as the same photo. That’s the power of the JPEG.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Then in 1997, a company named Forgent Networks acquired Compression Labs. The company eventually spotted the patent and began filing lawsuits over it, a series of events it saw as a stroke of good luck.</p><p>“The patent, in some respects, is a lottery ticket,” Forgent CEO Jay Peterson <a href="https://www.cnet.com/tech/tech-industry/staking-a-claim-in-the-patent-gold-mine/" target="_blank">told <em><em>CNET</em></em> in 2005</a>. “If you told me five years ago that ‘You have the patent for JPEG,’ I wouldn’t have believed it.”</p><p>While Forgent’s claim of ownership of the JPEG compression algorithm was tenuous, it ultimately saw more success with its legal battles than Unisys did. The company earned more than US $100 million from digital-camera makers before the patent finally ran out of steam around 2007. The company also attempted to extract licensing fees from the PC industry. Eventually, Forgent agreed <a href="https://www.cnet.com/tech/tech-industry/forgent-settles-jpeg-patent-cases/" target="_blank">to a modest $8 million</a> settlement.</p><p>As the company took an increasingly aggressive approach to its acquired patent, it began to lose battles both in the court of public opinion and in actual courtrooms. <a href="https://arstechnica.com/uncategorized/2006/05/6930-2/" target="_blank">Critics pounced on examples of prior art</a>, while courts limited the patent’s use to motion-based uses like video.</p><p>By 2007, Forgent’s compression patent expired—and its litigation-heavy approach to business went away. That year, the company became <a href="https://www.asuresoftware.com" target="_blank">Asure Software</a>, which now specializes in payroll and HR solutions. Talk about a reboot.</p><h2>Why the JPEG won’t die</h2><p>The JPEG file format has served us well. It’s been difficult to remove the format from its perch. The JPEG 2000 format, for example, was intended to supplant it by offering more lossless options and better performance. The format is <a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000143.shtml" target="_blank">widely used by the Library of Congress</a> and specialized sites like the Internet Archive; however, it is less popular as an end-user format.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Animated GIF of the forest images, starting at full resolution and progressing through increasingly degraded version of the iamge." class="rm-shortcode" data-rm-shortcode-id="ea7ce7f87d0b7e0afed75e4a9a57e2a7" data-rm-shortcode-name="rebelmouse-image" id="171a2" loading="lazy" src="https://spectrum.ieee.org/media-library/animated-gif-of-the-forest-images-starting-at-full-resolution-and-progressing-through-increasingly-degraded-version-of-the-iamg.gif?id=61016209&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">See the forest JPEG degrade from its full resolution to 1 percent quality in this GIF. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Original image: Irina Iriser</small></p><p>Other image technologies have had somewhat more luck getting past the JPEG format. The Google-supported <a href="https://developers.google.com/speed/webp" target="_blank">WebP</a> is popular with website developers (<a href="https://www.pcgamer.com/heres-why-you-have-to-deal-with-so-many-annoying-webps-now/" target="_blank">and controversial</a> with end users). Meanwhile, the formats <a href="https://aomediacodec.github.io/av1-avif/" target="_blank">AVIF</a> and <a href="https://www.iso.org/standard/83650.html" target="_blank">HEIC</a>, each developed by standards bodies, have largely outpaced both JPEG and JPEG 2000.</p><p>Still, the JPEG will be difficult to kill at this juncture. These days, the format is similar to MP3 or ZIP files—two legacy formats too popular and widely used to kill. Other formats that compress the files better and do the same things more efficiently are out there, but it’s difficult to topple a format with a 30-year head start.</p><p>Shaking off the JPEG is easier said than done. I think most people will be fine to keep it around.</p><p><em><em>Ernie Smith is the editor of </em></em><a href="https://tedium.co/" target="_blank"><em><em>Tedium</em></em></a><em><em>, a long-running newsletter that hunts for the end of the long tail.</em></em></p><em><em><br/></em></em>]]></description><pubDate>Tue, 17 Jun 2025 14:45:46 +0000</pubDate><guid>https://spectrum.ieee.org/jpeg-image-format-history</guid><category>Compression algorithms</category><category>Digital photography</category><category>Discrete cosine transform</category><category>Image compression</category><category>Jpeg</category><dc:creator>Ernie Smith</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/forest-scene-on-a-vintage-computer-screen-displaying-a-calm-wooded-area-at-daylight.jpg?id=61013743&amp;width=980"></media:content></item><item><title>Why Pilots Will Matter in the Age of Autonomous Planes</title><link>https://spectrum.ieee.org/autonomous-planes-certification</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-electric-vertical-takeoff-aircraft-lifts-off-into-a-cloudy-sky-with-an-office-building-in-the-background.jpg?id=60489393&width=1200&height=800&coordinates=118%2C0%2C119%2C0"/><br/><br/><p><strong> In August 2001,</strong> an anonymous guest <a href="https://www.airliners.net/forum/viewtopic.php?t=101783" rel="noopener noreferrer" target="_blank">posted on the forum</a> at Airliners.net, a popular aviation website. “How Long Will Pilots Be Needed?” they wondered, observing that “20 years or so down the road” technology could be so advanced that planes would fly themselves. “So would it really be useful for a person to go to college now and be an airline pilot if a few years down the road they will be phased out by technology?”</p><p>Twenty-four years later, the basic technology required to make aircraft fly themselves exists, as evidenced by the fact that most commercial flights are flown largely on autopilot. Yet, the fundamental model of flying commercial <a href="https://spectrum.ieee.org/tag/aircraft" target="_self">aircraft</a> hasn’t really changed. Passengers are still flown on large jetliners by two or more highly trained human pilots functioning as a team.</p><p>The main reason why airlines are still decades away from pilotless planes boils down to the strict regulatory framework for aviation. At the heart of this regulation is certification—the process by which governmental authorities determine that an aircraft design is safe for flight. Even for conventional aircraft based on proven technologies, taking a concept from design through certification can require hundreds of millions of dollars and the better part of a decade. Tack on any novel technologies, such as the <a href="https://spectrum.ieee.org/tag/autonomy" target="_self">autonomy</a> necessary to remove the pilot from the cockpit, and that process just gets longer and more expensive, with no guarantee of success.</p><p>Nevertheless, and despite the daunting odds against them, a new generation of startups is making a run at certifying autonomous passenger and cargo aircraft, in the process laying the groundwork for the next chapter of aviation. Instead of airliners, these companies are starting with small aircraft: electric air taxis and single-engine planes that typically seat fewer than a dozen people. Not only are the associated capital costs more manageable on a startup’s budget, there’s also a persuasive safety case to be made: Small aircraft are still prone to the types of accidents that have been largely eliminated from commercial airline operations. According to <a href="https://www.aopa.org/training-and-safety/air-safety-institute/accident-analysis/richard-g-mcspadden-report/mcspadden-report-figure-view" rel="noopener noreferrer" target="_blank">statistics</a> compiled by the Aircraft Owners and Pilots Association, around 300 people die each year in small plane and helicopter crashes in the United States alone.</p><p>“Loss of control—mishandling the plane, usually as a result of disorientation or excessive workload—and controlled flight into terrain, [those] are the leading causes of accidents in small aircraft,” says <a href="https://reliable.co/company" rel="noopener noreferrer" target="_blank">Robert Rose</a>, cofounder and CEO of <a href="https://reliable.co/" rel="noopener noreferrer" target="_blank">Reliable Robotics</a>, one of a few startups now working on retrofits that could enable Cessna Caravan planes to fly autonomously. A veteran of SpaceX and Tesla, Rose is adamant that “we, as a nation, possess the technology to prevent these accidents. If we can [autonomously] land a rocket on a small barge in the middle of the ocean, clearly we can find the centerline at an airport.”</p><h2>The economic case for autonomy in aviation</h2><p>While the safety argument for making small aircraft autonomous is a compelling one, the move is fundamentally rooted in economics. California-based Reliable Robotics and Massachusetts-based Merlin Labs are developing the commercial versions of their <a href="https://www.businesswire.com/news/home/20231206413888/en/Reliable-Robotics-Flies-Large-Cargo-Aircraft-with-No-One-On-Board" rel="noopener noreferrer" target="_blank">autonomous Caravans</a> initially for the cargo feeder industry, which uses small airplanes to move packages to and from rural markets on behalf of carriers like FedEx and UPS. (Both companies also have <a href="https://www.businesswire.com/news/home/20240916015386/en/U.S.-Air-Force-Awards-Reliable-Robotics-Multi-Year-Contract-for-Dual-Use-Advanced-Aircraft-Automation" rel="noopener noreferrer" target="_blank">military funding</a> to develop autonomous aircraft.) Pilots for these feeder networks are typically flying alone, often at night and in bad weather, and <a href="https://www.ainonline.com/aviation-news/business-aviation/2023-11-01/deeper-look-part-135-cargo-ops-accidents" rel="noopener noreferrer" target="_blank">their safety record is poor</a>. This is a comparatively low-volume segment of the aviation industry, and there’s no money for second pilots and other risk mitigations typical of airline operations.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A white airplane is parked on an airport tarmac close to a hangar. Two men are carrying a large box towards the airplane." class="rm-shortcode" data-rm-shortcode-id="ba50de53ab179841fa15be01da30008d" data-rm-shortcode-name="rebelmouse-image" id="21280" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-airplane-is-parked-on-an-airport-tarmac-close-to-a-hangar-two-men-are-carrying-a-large-box-towards-the-airplane.jpg?id=60489399&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Reliable Robotics is one of a couple of companies that are outfitting Cessna Caravan airplanes with advanced software to provide a high level of autonomy, for applications that include cargo transportation. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Reliable Robotics</small></p><p>The economic argument for autonomy is even more compelling in the emerging <a href="https://spectrum.ieee.org/evtol-aircraft" target="_self">air-taxi industry</a>, where hundreds of hopefuls—including a dozen or so <a href="https://spectrum.ieee.org/evtol-air-taxi-industry" target="_self">serious contenders</a>—are racing to develop electric vertical takeoff and landing aircraft to ferry passengers around crowded urban areas. Most of these <a href="https://spectrum.ieee.org/tag/evtol" target="_self">eVTOLs</a> are the size of helicopters, with space for just four or five passengers, and their proponents envision scores or even hundreds of them in the air over major cities, collectively moving millions of passengers annually. The concept is called <a href="https://spectrum.ieee.org/evtol-policy-strategy" target="_self">urban air mobility</a>, and in the speculative math that underpins it, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale.</p><p>China has already certified a pilotless air taxi: the EH216-S, a two-seat multicopter developed by Guangzhou-based <a href="https://www.ehang.com/" target="_blank">EHang</a> that in March <a href="https://www.ehang.com/news/1198.html" target="_blank">obtained initial approval</a> from the <a href="https://www.caac.gov.cn/English/" target="_blank">Civil Aviation Administration of China</a> for limited commercial sightseeing operations. However, many Western observers doubt that EHang’s design would pass muster by the U.S. <a href="https://www.faa.gov/" target="_blank">Federal Aviation Administration</a> (FAA) or the <a href="https://www.easa.europa.eu/en" rel="noopener noreferrer" target="_blank">European Union Aviation Safety Agency</a> (EASA), both of which have an especially conservative approach to safety. For that reason, most Western eVTOL makers have opted to develop piloted aircraft first and plan to introduce autonomous versions at some later date. They figure that seeking certification of novel electric aircraft designs, even without autonomy, is already a big ask of these regulators.</p><p>A notable exception to this strategy is <a href="https://wisk.aero/" rel="noopener noreferrer" target="_blank">Wisk Aero</a>, which began as a project funded by Google cofounder Larry Page and is now a wholly owned subsidiary of Boeing. In January 2022, the company declared that it would obtain FAA certification for its self-flying air taxi by the end of the decade and be operating close to 14 million flights annually within five years after that—a staggering ambition, given that the entire U.S. air traffic system currently manages around 16 million flights per year. While overheated expectations around urban air mobility have cooled considerably in the three years since that announcement, Wisk continues to forge ahead with its autonomous <a href="https://wisk.aero/aircraft/" rel="noopener noreferrer" target="_blank">Generation 6 eVTOL</a>, the company’s sixth aircraft design and the first it plans to certify for passenger-carrying operations.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="a futuristic, bright yellow aircraft  sits on a large concrete pad with a blue sky in the background. " class="rm-shortcode" data-rm-shortcode-id="5b316b2f31dd3cd77f483ae99f98bd26" data-rm-shortcode-name="rebelmouse-image" id="7ea5a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-futuristic-bright-yellow-aircraft-sits-on-a-large-concrete-pad-with-a-blue-sky-in-the-background.jpg?id=60489618&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">A mockup of Wisk’s sixth generation of electric vertical takeoff and landing aircraft was unveiled in October 2022. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Wisk</small></p><p>Importantly, Wisk, Reliable Robotics, and Merlin Labs aren’t just developing autonomous aircraft—they have already launched formal certification programs with the FAA. That means they’re working closely with the agency to define the rules and standards by which autonomous aircraft will be approved for commercial operations, blazing a trail for others to follow. The task is a daunting one, but the regulators and industry are not starting from scratch. Rather, they’re building on decades of certification experience and best practices that have helped to dramatically improve the safety of the aviation industry over its history.</p><h2>Aviation safety starts with certification</h2><p>Although the fatal January 2025 midair collision of an Army Black Hawk helicopter and an American Eagle CRJ700 near Washington, D.C.’s Reagan National Airport <a href="https://www.ipsos.com/en-us/many-americans-are-losing-faith-safety-air-travel" target="_blank">shook public confidence</a> in the safety of the U.S. air transport system, commercial aviation remains a remarkably safe way to get around. According to researchers at MIT, the risk of a fatality from commercial air travel was just <a href="https://www.sciencedirect.com/science/article/pii/S0969699724001066" target="_blank">one per 13.7 million passenger boardings</a> worldwide between 2018 and 2022. Fifty years earlier, the risk was an order of magnitude higher: one per 350,000 boardings between 1968 and 1977.</p><p>There are many reasons for this great leap in safety, and the certification process is an important one. Today, a majority of aviation accidents are attributed to human error, but that’s not because people are inherently less reliable than aircraft. It’s because a systematic approach to design and testing has over the past several decades eliminated many of the mechanical problems that used to cause accidents routinely. In this context, the argument for enhancing safety through autonomy can be thought of as transferring even more responsibilities from highly variable humans to engineered systems that can be subjected to greater scrutiny.</p><p>The overarching principle of certification is that the equipment and systems on an aircraft must be designed and installed so that they perform as intended during any foreseeable circumstances that they might encounter. “Perform as intended” includes <em><em>not</em></em> performing any <em><em>unintended</em></em> functions. An example of an unintended function is pushing the nose of an aircraft down past the level that a pilot can recover—that was the fatal result of a hidden software flaw that caused two <a href="https://spectrum.ieee.org/how-the-boeing-737-max-disaster-looks-to-a-software-developer" target="_self">crashes of the Boeing 737 Max</a> and led to an extended grounding of the fleet while that oversight was remedied.</p><p>Another key principle of certification is that the probability of a failure condition must be inversely proportional to its consequences. In other words, the more serious the impact of a failure, the more remote its chances of occurrence need to be. Aircraft are complex machines with millions of components that can and do fail, but many of these components can fail with no serious effects. For example, it’s no big deal if a lightbulb in the cabin burns out on a regular basis. Certifying authorities like the FAA generally accept a high probability of failure conditions that have a negligible impact on safety. However, failure conditions that are potentially catastrophic are required to be “extremely improbable.”</p><p>Whether a failure condition is extremely improbable is fundamentally a qualitative evaluation that relies on the best judgments of engineers about how a system is likely to fail, supported by numerical assessments of the likelihood of failure. The critical systems on large commercial airliners are held to a numerical safety level of 10<span><sup>-9</sup></span>, meaning that catastrophic failures are expected no more than once in a billion flight hours (the equivalent of once in about 114,000 years of continuous operation).</p><p>Achieving such vanishingly low probabilities may require expensive, heavy, and redundant systems, so regulators typically relax the safety expectations for small aircraft that carry fewer people. For example, a four-seat airplane like a Cessna 172 may only be held to a numerical safety level of 10<span><sup>-6</sup></span>, meaning that catastrophic failures are expected no more than once in a million flight hours. That said, aircraft manufacturers are free to design to higher standards, and Wisk is targeting the highest numerical safety level, 10<span><sup>-9</sup></span>, for its Gen 6 eVTOL.</p><p>These basic principles of certification apply regardless of whether or not there’s a human pilot sitting in the cockpit, which is why developers of autonomous aircraft are confident they don’t need to completely reinvent the certification framework.</p><p>“Everybody thinks that you need to think about the autonomy a different way than you would think about a piloted aircraft,” says <a href="https://wisk.aero/about/" target="_blank">Cindy Comer</a>, Wisk’s vice president of certification, safety management systems and quality. “But really we just don’t get to pass off these failure conditions to a pilot. We still do our safety assessment the same way. We still may design our aircraft in a very similar way, but it may be to higher levels, it may be with more redundancy, or maybe we add equipment, because we no longer have that person that can sit there and see the things, grab the things, to pull the breakers.</p><p>“So it drives our safety assessments to say, ‘Okay, we can’t put this on the pilot now. So what do we put it on?’”</p><h3>Key Aircraft Autonomy Projects</h3><br/><div class="flourish-embed flourish-table" data-src="visualisation/23472514?820658"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="table visualization" src="https://public.flourish.studio/visualisation/23472514/thumbnail" width="100%"/></noscript></div>
<h2>Making autonomy certifiable presents unique challenges</h2><p>Answering that question—What do we put it on?—for every foreseeable failure condition is where the real work of certifying an autonomous aircraft comes in. Conventionally piloted aircraft may use the same overarching framework for certification, but they have the advantage of decades of certification history and precedent to fill in all of the details, down to requirements for such things as the actuation of the landing gear and the markings of instruments and placards. For the new systems on autonomous aircraft, many of those details must be negotiated with the FAA or some other certifying authority, which must be convinced in each instance that the proposed solution is at least as safe as the approach used on conventional aircraft.</p><p>In the United States, applicants for type certificates have considerable flexibility in proposing how to meet the FAA’s safety goals. For each project incorporating novel technologies, the applicant and the agency agree on a set of requirements and standards, which becomes the “regulatory basis” for that aircraft. Theoretically, each autonomous-aircraft developer could have a very different regulatory basis, although in practice, the FAA looks for common ground. Nevertheless, the flexibility in this approach allows industry to explore a variety of possible ways to comply with a certification requirement before a solution is codified in regulation.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A white single-engine airplane with a high wing is seen flying over scrubby brown terrain." class="rm-shortcode" data-rm-shortcode-id="996780adebb70d2a4a94c3b6f98569c5" data-rm-shortcode-name="rebelmouse-image" id="73873" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-single-engine-airplane-with-a-high-wing-is-seen-flying-over-scrubby-brown-terrain.jpg?id=60489677&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">Merlin Labs launched the flight test campaign for its certification-ready autonomy system in June 2024. The Merlin Pilot system is integrated directly onto the aircraft and is intended in the near term to reduce crew workload rather than fully replace pilots.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Merlin Labs</small></p><p>“Once you have the regulatory basis in place, then you need to come to agreement on how you’re going to demonstrate compliance to all of those regulations,” says Rose. “You can pull from existing standards, you can modify existing standards, or you can, in some cases, even just propose your own standards.” After agreeing upon the means of compliance, the applicant and regulator develop a detailed project plan that outlines the tests that will be performed and the reports—known as artifacts—that will be submitted to the regulator to support certification.</p><p>For conventional piloted aircraft with a history of real-world operations, much of how those aircraft will function in the national airspace system is assumed. “Large commercial airplanes operate from airports around the world with relatively known and static equipment that helps them navigate and approach and land,” says <a href="https://www.linkedin.com/in/brian-yutko/" target="_blank">Brian Yutko</a>, until recently Wisk’s CEO (he now heads commercial airplane product development at Boeing). This infrastructure, he adds, has been established over decades and is reflected in the design of aircraft in ways that are often taken for granted.</p><p>The existing system relies heavily on human pilots communicating with air traffic controllers over radio. Autonomous aircraft will require new concepts of operations, or “ConOps,” for how they will function, which could include using ground supervisors to handle radio calls, for example. In turn, the specifics of each ConOps will influence aircraft design requirements. According to Comer, crystallizing the ConOps at the beginning of the certification process “helps drive a common understanding of what you’re actually doing, and that may be different for every applicant with the FAA.”</p><p>Basically, Wisk intends for its autonomous air taxi, which Yutko has likened to “a tram in the sky,” to fly along very specific and limited routes with predetermined emergency landing locations. Such a narrow set of tasks is an easier thing to automate than the varied and flexible operations performed by most small piloted aircraft today (or, for that matter, most self-driving cars). Meanwhile, human supervisors on the ground will monitor flights and communicate with air traffic control as required.</p><p>Reliable Robotics’ automated Cessna Caravans will also have remote operators to handle communications with air traffic control, but they will fly over a much larger and more variable operating area. Because of this added complexity, Reliable has opted to split up the work of certifying its autonomous aircraft into chunks, beginning with certification of an advanced, always-on autopilot. This will assist but not replace the onboard pilot during all phases of flight, including landing as well as taxi and takeoff—which traditional autopilots are not capable of. Taking the pilot out of the cockpit will come as a follow-on certification project.</p><h2>Autonomous aircraft will do what autopilots can’t</h2><p>Proponents of autonomy like to point out that most commercial airline flights today are flown on autopilot from shortly after takeoff until touchdown or just before. It may therefore seem surprising that Europe’s aviation regulator, EASA, <a href="https://www.easa.europa.eu/en/document-library/general-publications/easa-artificial-intelligence-roadmap-20" target="_blank">does not expect</a> to see fully autonomous airliners until after 2050, while other regulators haven’t even speculated on a timeline for the shift.</p><p>There are several reasons why “solving” autonomy in aircraft is not just a matter of expanding the functionality of existing autopilots. Basic flight control—moving flight-control surfaces and power inputs to make an aircraft fly how and where you want—is a relatively simple thing to automate, and most of the time, when everything goes as expected, autopilot works just fine. However, most existing autopilot systems assume there’s a human pilot, and for that reason they aren’t reliable enough to enable full autonomy.</p><p>“There are autopilot actuators that go into aircraft today,” notes Reliable cofounder Rose. “But there’s a person sitting there monitoring them, and if [the actuators] do anything funny, then you click the off switch or actually, in many cases, you can just physically overpower the actuator. That’s not the case with ours—our actuators need to work all the time.”</p><p>More challenging is solving for situations when everything does <em><em>not</em></em> go as expected, such as when another aircraft conflicts with the programmed flight path or a stray vehicle blocks the assigned runway. Autonomous-aircraft developers can’t count on a remote operator to manage these types of urgent, sudden conflicts, because the command-and-control (C2) link between the ground and the aircraft could also fail.</p><p>“The aircraft, without having a [pilot] on board, needs to know where it is, and how to get where it’s going and how to avoid things along the way, over the length of its concept of operations,” says Yutko. Wisk’s Gen 6 flier will have the ability to safely complete a flight even if it loses both its C2 link and GPS signal immediately after takeoff, he says. “It turns out that if you don’t do that, then you start to impose really difficult technical requirements on the C2 link, or on your ability to maintain GPS.”</p><p class="pull-quote">In the speculative math that underpins urban air mobility, eliminating the expense of a pilot and freeing up another seat for a paying passenger are seen as key to maximizing profits and scale.</p><p>Neither Wisk nor Reliable Robotics is using machine learning algorithms in its technical solutions, in large part because there’s no consensus on how to assure, to aviation’s exacting standards, the safety of such algorithms. These algorithms are frequently characterized as “nondeterministic,” meaning that their outputs can’t be reliably predicted from their inputs.</p><p>Some autonomous-aircraft developers are incorporating artificial intelligence into their designs. Merlin Labs, for example, is developing natural-language-processing algorithms to communicate with air traffic control. For the most part, however, autonomous-aircraft developers aren’t counting on technology alone to solve the innumerable contingencies that can arise in flight—that’s where the ground operators come in.</p><p>“We basically have taken everything that can be [automated] deterministically, and we’re making it deterministic,” Rose explains. “And all of the things that are…very hard to automate, that a human can do easily, then let the human do it.”</p><p>Which raises the question: If humans are required to supervise autonomous aircraft, does the business case for them still hold up? Their developers say it does, but in ways that aren’t as simple as just striking “pilots” from the balance sheet. For example, those remote supervisors will need training, but that’s likely to be far less extensive and costly than the training required to competently fly an aircraft. For Reliable Robotics and other companies targeting cargo delivery, autonomy also promises to improve the efficiency of the existing cargo feeder network.</p><p>“The reality is, in cargo aircraft, especially small cargo aircraft, pilots are super underutilized,” says Rose. Pilots at the feeder airlines may spend most of their day hanging out in a hotel room between their morning and evening flights. If people were instead managing autonomous cargo aircraft remotely, they could conceivably oversee additional flights across multiple time zones. “Our analysis has shown you can easily double the productivity of a pilot by putting them into our control center, potentially triple or quadruple the productivity [depending] on the mission set,” Rose says.</p><h2>Autonomous tech might eventually trickle up</h2><p>Even if companies like Wisk and Reliable Robotics succeed in certifying and commercializing their autonomous aircraft, human pilots still won’t face imminent extinction. Solving autonomy for one aircraft type and concept of operations doesn’t mean it’s solved for all types and concepts of operations. The technical, regulatory, and social barriers standing in the way of autonomous passenger jets are formidable.</p><p>“I think for as long as we’re all alive, there will be piloted large commercial aircraft,” Yutko says. “If you solve Gen 6, you don’t get uncrewed large airplanes. You just don’t, and I’m not certain that we will in our lifetimes.” However, he does think it likely that some of the technologies now being developed at Wisk—such as navigating in the absence of GPS or techniques for automating emergency checklists—will find their way into conventionally piloted aircraft in ways that enhance safety.</p><p>“I think those will be the types of things that we see in our lifetime benefiting big commercial transport applications, and I think it’s phenomenal,” adds Comer.</p><p>As for whether it makes sense for anyone to embark upon a career as an airline pilot under the looming shadow of autonomy, it probably still does, at least for now. But check back in another 20 years. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Tue, 17 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/autonomous-planes-certification</guid><category>Autonomous systems</category><category>Aircraft certification</category><category>Autonomous aircraft</category><category>Autonomous passenger jets</category><category>Electric air taxis</category><category>Evtols</category><dc:creator>Elan Head</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-electric-vertical-takeoff-aircraft-lifts-off-into-a-cloudy-sky-with-an-office-building-in-the-background.jpg?id=60489393&amp;width=980"></media:content></item><item><title>Airbnb’s Dying Software Gets a Second Life</title><link>https://spectrum.ieee.org/apache-airflow-3-programmatic-workflows</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/pixel-art-of-four-pinwheels.jpg?id=60930318&width=1200&height=800&coordinates=140%2C0%2C140%2C0"/><br/><br/><p><a href="https://www.linkedin.com/in/vikramkoka/" rel="noopener noreferrer" target="_blank" title="Link: https://www.linkedin.com/in/vikramkoka/">Vikram Koka</a> stumbled upon <a href="https://airflow.apache.org/" rel="noopener noreferrer" target="_blank">Apache Airflow</a> in late 2019. He was working in the Internet of Things industry and searching for a solution to orchestrate sensor data using software. Airflow seemed to be a perfect fit, but Koka noticed the <a href="https://spectrum.ieee.org/tag/open-source" target="_self">open-source</a> project’s stagnant state. Thus began a journey to breathe a second life into this dying software.</p><p>Airflow was the brainchild of <a href="https://spectrum.ieee.org/the-secret-of-airbnbs-pricing-algorithm" target="_self">Airbnb</a>. The company created the system to <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" rel="noopener noreferrer" target="_blank" title="Link: https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">automate and manage its data-related workflows</a>, such as cleaning and organizing datasets in its data warehouse and calculating metrics around host and guest engagement. In 2015, <a href="https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8" rel="noopener noreferrer" target="_blank" title="Link: https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8">Airbnb released the software as open source</a>. Then, four years later, Airflow transitioned into a <a href="https://news.apache.org/foundation/entry/the-apache-software-foundation-announces44" rel="noopener noreferrer" target="_blank">top-level project</a> at the <a href="https://apache.org/" rel="noopener noreferrer" target="_blank">Apache Software Foundation</a>, a leading developer and steward of open-source software.</p><p class="ieee-inbody-related"><strong>RELATED: <a href="https://spectrum.ieee.org/the-secret-of-airbnbs-pricing-algorithm" target="_blank">The Secret of Airbnb’s Pricing Algorithm</a></strong><br/></p><p>What was once a thriving project had stalled, however, with flat downloads and a lack of version updates. Leadership was divided, with some maintainers focusing on other endeavors.</p><p>Yet Koka believed in the software’s potential. Unlike static configuration files, Airflow follows the principle of “configuration as code.” Workflows are represented as <a href="https://networkx.org/nx-guides/content/algorithms/dag/index.html" rel="noopener noreferrer" target="_blank">directed acyclic graphs</a> of tasks—a graph with directed edges and no loops. Developers can code these tasks in the <a href="https://spectrum.ieee.org/tag/python" target="_self">Python</a> <a href="https://spectrum.ieee.org/tag/programming-languages" target="_self">programming language</a>, allowing them to import libraries and other dependencies that can help them better define tasks. Akin to a musical conductor, Airflow orchestrates the symphony of tasks and manages the scheduling, execution, and monitoring of workflows.</p><p>This flexibility is what caught Koka’s eye. “I fell in love with the concept of code-first pipelines—pipelines which could actually be deployed in code,” he says. “The whole notion of programmatic workflows really appealed to me.”</p><p>Koka started work righting the Airflow ship. As an open-source contributor with decades of experience in the data and <a href="https://spectrum.ieee.org/tag/software-engineering" target="_self" title="Link: https://spectrum.ieee.org/tag/software-engineering">software-engineering</a> space, he connected with people in the community to fix bugs around reliability and craft other enhancements. It took a year, but Airflow 2.0 was released in December 2020.</p><h2>Airflow’s Growth and Community Expansion</h2><p>The release served as a crucial turning point for the project. Downloads from its <a href="https://github.com/apache/airflow" rel="noopener noreferrer" target="_blank">GitHub repository</a> increased, and more enterprises adopted the software. Encouraged by this growth, the team envisioned the next generation of Airflow: a modular architecture, a more modern user interface, and a “run anywhere, anytime” feature, enabling it to operate on premises, in the <a href="https://spectrum.ieee.org/tag/cloud-computing" target="_self">cloud</a>, or on <a href="https://spectrum.ieee.org/tag/edge" target="_self">edge</a> devices and handle event-driven and ad hoc scenarios in addition to scheduled tasks. The team delivered on this vision with the launch of Airflow 3.0 last April.</p><p>“It was amazing that we managed to ‘rebuild the plane while flying it’ when we worked on Airflow 3—even if we had some temporary issues and glitches,” says <a href="https://www.linkedin.com/in/jarekpotiuk/" rel="noopener noreferrer" target="_blank">Jarek Potiuk</a>, one of the foremost contributors to Airflow and now a member of its project-management committee. “We had to refactor and move a lot of pieces of the software while keeping Airflow 2 running and providing some bug fixes for it.”</p><p>Compared with Airflow’s second version, which Koka says had only a few hundred to a thousand downloads per month on GitHub, “now we’re averaging somewhere between 35 to 40 million downloads a month,” he says. The project’s community also soared, with more than 3,000 developers of all skill levels from around the world contributing to Airflow.</p><p><a href="https://www.linkedin.com/in/jens-scheffler/" rel="noopener noreferrer" target="_blank">Jens Scheffler</a> is an active part of that community. As a technical architect of digital testing automation at <a href="https://www.bosch.com/" rel="noopener noreferrer" target="_blank">Bosch</a>, his team was one of the early adopters of Airflow, using the software to orchestrate tests for the company’s automated driving systems.</p><p>Scheffler was inspired by the openness and responsiveness of Airflow members to his requests for guidance and support, so he considered “giving back something to the community—a contribution of code.” He submitted a few patches at first, then implemented an idea for a feature that would benefit not only his team but other Airflow users as well. Scheffler also discovered other departments within Bosch employing Airflow, so they’ve formed a small in-house community “so we can exchange knowledge and keep in touch.”</p><p>Koka, who is also a member of Airflow’s project-management committee and a chief strategy officer at the data-operations platform <a href="https://www.astronomer.io/" rel="noopener noreferrer" target="_blank">Astronomer</a>, notes that managing a huge group of contributors is challenging, but nurturing that network is as essential as improving the software. The Airflow team has established a system that enables developers to contribute gradually, starting with documentation and then progressing to small issues and bug fixes before tackling larger features. The team also makes it a point to swiftly respond and provide constructive feedback.</p><p>“For many of us in the community, [Airflow] is an adopted child. None of us were the original creators, but we want more people feeling they’ve also adopted it,” says Koka. “We’re in different organizations, in different countries, speak different languages, but we’re still able to come together toward a certain mission. I love being able to do that.”</p><h2>Future of Airflow in AI and Machine Learning</h2><p>The Airflow team is already planning future features. This includes tools to write tasks in programming languages other than Python, human-in-the-loop capabilities to review and approve tasks at certain checkpoints, and support for <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">artificial intelligence (AI)</a> and <a href="https://spectrum.ieee.org/tag/machine-learning" target="_self">machine learning</a> workflows. According to <a href="https://airflow.apache.org/blog/airflow-survey-2024/" rel="noopener noreferrer" target="_blank">Airflow’s 2024 survey</a>, the software has a rising number of use cases in machine learning operations (MLOps) and <a href="https://spectrum.ieee.org/tag/generative-ai" target="_self">generative AI</a>.</p><p>“We are at a pivotal moment where AI and ML workloads are the most important things in the IT industry, and there is a great need to make all those workloads—from training to inference and <a href="https://spectrum.ieee.org/ai-agents" target="_self">agentic</a> processing—robust, reliable, scalable, and generally have a rock-solid foundation they can run on,” Potiuk says. “I see Airflow as such a foundation.”</p>]]></description><pubDate>Tue, 17 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/apache-airflow-3-programmatic-workflows</guid><category>Software</category><category>Open source</category><category>Ai</category><category>Airbnb</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/pixel-art-of-four-pinwheels.jpg?id=60930318&amp;width=980"></media:content></item><item><title>Why the Semiconductor Industry Can’t Abandon Women</title><link>https://spectrum.ieee.org/women-in-semiconductors-workforce</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-a-gloved-hand-holding-a-computer-chip-against-a-gradient-background.jpg?id=61009400&width=1200&height=800&coordinates=0%2C60%2C0%2C60"/><br/><br/><p>The percentage of women in the semiconductor industry is stubbornly low. According to a report released in April, <a href="https://www.accenture.com/content/dam/accenture/final/accenture-com/document-3/Women-in-Semiconductor-2024-Insights-and-Trends-from-Accenture-and-GSA.pdf" rel="noopener noreferrer" target="_blank">51 percent of companies report</a> having less than 20 percent of their technical roles filled by women. At the same time, fewer of these companies were publicly committed to equal opportunity measures in 2024 than the year prior, the same report found.</p><p>This lack of support comes at the same time that <a href="https://spectrum.ieee.org/workforce-shortage" target="_blank">major workforce shortages</a> are expected, says <a href="https://www.linkedin.com/in/andreamohamed/" rel="noopener noreferrer" target="_blank">Andrea Mohamed</a><span>, COO and co-founder of </span><a href="https://quantumbloom.com/" target="_blank">QuantumBloom</a><span>, which helps companies attract, retain, and advance early career women in STEM. The company</span><span style="background-color: initial;"> focuses on the transition from higher education to the workforce, a critical point during which many women leave STEM.</span></p><p><span><em><span><em>IEEE Spectrum</em></span></em> spoke to Mohamed about supporting women in semiconductor jobs, and why a retreat from these initiatives is at odds with the needs of the industry. </span></p><p class="rm-anchors" id="top"><span>Andrea Mohamed on: </span></p><ul><li><a href="#perspective">The current state of the semiconductor industry</a></li><li><a href="#support">How a lack of support for women will impact the industry</a></li><li><a href="#regress">Whether the industry is regressing in its hiring practices</a></li><li><a href="#lesson">What the semiconductor industry can learn from other industries</a></li></ul><p><strong>Tell me about your perspective as a returning veteran of the semiconductor industry.</strong></p><p class="rm-anchors" id="perspective"><strong>Andrea Mohamed: </strong>I worked for a semiconductor startup company over 20 years ago, and it was very male dominated. Now, it’s still very male dominated. Seeing the semiconductor industry with fresh eyes, what I see is an industry that hasn’t evolved as quickly as other STEM-intensive industries. I’ve worked for science and research-oriented organizations, and the progress that’s been made in other sectors just hasn’t been made in this particular sector. </p><p><a href="#top">Return to top</a></p><p class="rm-anchors" id="support"><strong>How might the lack of support for women in the U.S. semiconductor industry create additional problems?</strong></p><p><strong>Mohamed: </strong>On a macro scale, you have an industry that is facing a lot of geopolitical and economic forces that are disrupting the whole supply chain ecosystem around semiconductors, and there’s a push to reshore and <a href="https://spectrum.ieee.org/tsmc-arizona" target="_blank">onshore</a>. There are a lot of infrastructure gaps in doing that, one of them being <a href="https://spectrum.ieee.org/chips-act-workforce-development" target="_blank">the workforce component</a>. It’s not just semiconductors that are poised to be reshored and onshored to the United States; it’s also pharmaceuticals and automotive. And all of that is going to continue to put pressure on the supply and demand curve, if you will, around labor.</p><p>There’s been an enormous amount of attention on the STEM <a href="https://spectrum.ieee.org/chip-design-enrollment" target="_blank">education</a> pipeline, and rightfully so. China and India are producing STEM graduates at a rate that we are not keeping pace with. While we’ve had that focus on the STEM education pipeline, there’s been very little focused attention on what industry is doing inside companies to address the workforce challenges. </p><p>There is a lot of additional concern around corporate cultures, burn-and-churn cyclical nature, policies that seem out of date relative to other industries, including as it relates to child care. Industry is very clearly articulating to education what it needs the next generation to have <a href="https://spectrum.ieee.org/ieee-microcredential-program" target="_blank">from a skills perspective</a>. But we don’t see the voice of the next generation worker influencing how industry is attracting them. We’ve got to start to see the industry recognize how it’s in its own way when it comes to workforce development.</p><p><strong>It sounds like the problem goes beyond the “leaky pipeline” that’s often discussed. </strong></p><p><strong>Mohamed: </strong>Right. We keep talking about the leaky pipeline for all these stages of women dropping out. It starts in middle school, when girls’ interest and confidence in STEM start to wane. At every stage there’s a leak. And then you get to this early career stage, which QuantumBloom is focused on, and that bucket is gushing. We’re losing a ton, and we’re all thinking about just putting more water in the bucket, when really, we need to fix the holes. There’s a lot of discussion about what it’s going to take to attract women, people of color, other communities into the semiconductor workforce, and very little on fixing the holes.</p><p><span>Oftentimes the early career experience is pretty much sink or swim for everybody, regardless of gender. We know with women, it’s more likely that they leave.</span></p><p><span><a href="#top">Return to top</a></span></p><p class="rm-anchors" id="regress"><strong>I understand that the semiconductor industry may actually be regressing in these areas. Can you talk about that? </strong></p><p><strong>Mohamed: </strong>The latest report that came out from <a href="https://www.gsaglobal.org/about-gsa/" target="_blank">Global Semiconductor Alliance</a> and Accenture <a href="https://www.accenture.com/us-en/insights/high-tech/women-semiconductor-leadership" target="_blank">on the state of women and semiconductors</a>, to me, is like a canary in a coal mine. We’re seeing a decrease in public commitments for diversity and the progress that we’ve made around programs that support women. It’s counterintuitive that we are decreasing support at exactly the time we need to be attracting this audience into the industry. </p><p>I understand the pressures that companies are facing around anything that’s related to DEI. We need to change the conversation from DEI to talent management. This is retention and avoiding turnover costs. This is about needing every available brilliant mind in the United States that wants to be in semiconductors. We have offshored this industry for so long. Other countries have existing talent bases. We have to build it.</p><p><strong>So the industry should work on these initiatives to build better workplaces, regardless of whether they’re labeled as promoting diversity?</strong></p><p><strong>Mohamed: </strong>I think a lot of DEI activity was performative. A lot of companies were really not committed to creating great workplaces for everybody. I think that’s part of the reason DEI has gotten politicized. There’s this notion that people were given opportunities that weren’t based on merit. What I’m saying is that this is not a merit conversation, right? <span>Women are graduating with bachelor’s degrees <a href="https://www.pewresearch.org/short-reads/2024/11/18/us-women-are-outpacing-men-in-college-completion-including-in-every-major-racial-and-ethnic-group/" target="_blank">at a rate higher than men</a> and increasing. </span><span>Really, this is about human capital development. You have women who are opting out of your industry, </span><span>a</span><span>nd you have to recognize and pay attention to the unique lived experience of women in these environments in order to solve the problem.</span></p><p>So there are semantics in all of this, but it’s not just relabeling. This is about business. You are not going to be able to compete on a global stage in the United States if you are not finding ways to attract and retain new communities of workers, and women are one of those communities. That means understanding what women need from their employer, because if you do not provide it, they will go somewhere else that does. The concern by companies about, if they run a program like QuantumBloom, does that create a risk? It’s the wrong question about risk. Your big risk is that your fab is empty, because you can’t find workers and retain them. </p><p><a href="#top">Return to top</a></p><p class="rm-anchors" id="lesson"><strong>What have you observed in other industries, and what can semiconductor leaders learn from them?</strong></p><p><strong>Mohamed: </strong>Many women whose roots are in engineering end up working potentially in a technical organization, but not in a technical role. You see them also pivot into completely different industries. They go to business school, they become a consultant, they go to law school. </p><p>In other industries, there are organizations that are very intentional about attracting and retaining their youngest talent. They are dedicating resources to investing in them, which is very rare—most organizations invest more the higher up you go. Really, we need to be thinking about flipping that script and investing more sooner. </p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Woman with long blonde hair in a blue blouse against a solid blue background." class="rm-shortcode" data-rm-shortcode-id="05250695ff2c19a019192f1b48040ead" data-rm-shortcode-name="rebelmouse-image" id="ac967" loading="lazy" src="https://spectrum.ieee.org/media-library/woman-with-long-blonde-hair-in-a-blue-blouse-against-a-solid-blue-background.jpg?id=61009402&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Andrea Mohamed is COO and co-founder of QuantumBloom, a professional development company focused on women in STEM.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Andrea Mohamed</small></p><p>When I think about employer-led solutions around early career talent, what comes to mind are apprenticeships, rotational programs, and <a href="https://spectrum.ieee.org/leadership-skills-ieee-courses" target="_blank">leadership skill development</a>—all the things you’re not taught in school but that are really important to your success. <span>These are skills that you take with you for an entire career. </span><span>When you invest in the top, most of the time people say, “I wish I had this in my 20s.” </span><span>I don’t see m</span><span>any</span><span> of th</span><span>ese solutions</span><span> being used</span><span> in this industry. I heard recently one of the big semiconductor giants in this country used to have an engineering rotational program and stopped it five years ago. I was talking to a person who had been in that program and how pivotal it was in their early career experience. </span></p><p><strong>Are there other steps that you think are important for semiconductor leaders to take?</strong></p><p><strong>Mohamed: </strong>The things that QuantumBloom solves are very early career and focused on individuals. At the same time, companies need to be thinking about top-down culture change and industry transformation. Those are longer-term horizon things to fix. </p><p>People join companies and quit bosses. The relationship with your boss is so important. You can be in a relatively terrible organization culturally and have a wonderful boss, and you can have career success. Vice versa, you could be in an awesome corporate culture with a terrible boss and not thrive. If we can improve that primary work relationship, build more empathy for each other’s experiences at a local level, we can improve work outcomes and retention. And then things start to spread. That manager who may be supporting a particular woman in our program, they learn skills and tools to be more inclusive leaders that extends beyond just that woman. </p><p>We’re doing that more at that local level, but man, companies really need to be addressing top-down transformation and culture change. <span>At the end of the day, we need semiconductor leaders to envision becoming a magnet for all talent, and then commit the resources and organizational changes needed to make that vision reality.</span></p><p><span><a href="#top">Return to top</a></span></p>]]></description><pubDate>Mon, 16 Jun 2025 14:56:52 +0000</pubDate><guid>https://spectrum.ieee.org/women-in-semiconductors-workforce</guid><category>Culture</category><category>Semiconductor industry</category><category>Women in engineering</category><category>Workforce development</category><dc:creator>Gwendolyn Rak</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-a-gloved-hand-holding-a-computer-chip-against-a-gradient-background.jpg?id=61009400&amp;width=980"></media:content></item><item><title>Europe’s Plan for Faster Space Travel</title><link>https://spectrum.ieee.org/esa-nuclear-rocket</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/schematic-collage-of-an-alumni-nuclear-thermal-propulsion-system-surrounded-by-a-dark-starry-sky.jpg?id=60934597&width=1200&height=800&coordinates=0%2C250%2C0%2C250"/><br/><br/><p><a data-linked-post="2650251627" href="https://spectrum.ieee.org/rockets-for-the-red-planet" target="_blank">Getting to Mars</a><span> takes a really long time, about nine months using today’s rocket technology. This is because regular rocket engines burn fuel and oxygen together (like a car engine), but they’re not very efficient. The fundamental problem is that spacecraft must carry both fuel and oxidizer since there’s no air in space to support combustion. This creates a vicious circle: The more fuel you carry to go faster, the heavier your spacecraft becomes, requiring even more fuel to accelerate that extra weight. To go faster, you’d need massive amounts of fuel, making the rockets incredibly expensive and heavy. Current chemical propulsion systems are just about at their theoretical limits, with little room for improvement in efficiency.</span></p><p class="ieee-inbody-related"><span><strong>RELATED: <a href="https://spectrum.ieee.org/rockets-for-the-red-planet" target="_blank">Rockets for the Red Planet</a></strong><br/></span></p><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"><a href="https://www.universetoday.com/"></a><a class="shortcode-media-lightbox__toggle shortcode-media-controls__button material-icons" title="Select for lightbox">aspect_ratio</a><a href="https://www.universetoday.com/" target="_blank"><img alt='Universe Today logo; text reads "This post originally appeared on Universe Today."' class="rm-shortcode" data-rm-shortcode-id="087bc0ad57330681cee5d3354f3d2ac7" data-rm-shortcode-name="rebelmouse-image" id="b9dd2" loading="lazy" src="https://spectrum.ieee.org/media-library/universe-today-logo-text-reads-this-post-originally-appeared-on-universe-today.png?id=60568425&width=980"/></a></p><p>Whilst <a href="https://arstechnica.com/science/2025/06/5-things-in-trumps-budget-that-wont-make-nasa-great-again/" target="_blank">NASA funding has been slashed</a> by the Trump administration with no allocation for <a data-linked-post="2652903460" href="https://spectrum.ieee.org/nuclear-powered-rockets-get-a-second-look-for-travel-to-mars" target="_blank">nuclear thermal propulsion</a> and/or nuclear electric propulsion, scientists at the European Space Agency (ESA) have been studying nuclear propulsion. Here’s how it works: Instead of burning fuel with oxygen, a nuclear reactor heats up a propellant like hydrogen. The super-heated propellant then shoots out of the rocket nozzle, pushing the spacecraft forward. This method is much more efficient than chemical rockets.</p><h2>Revisiting Nuclear Rockets for Mars</h2><p>Nuclear rockets offer several key advantages, such as cutting Mars trip times in half—from nine months to about four to five months. The efficiency gains come from the fact that nuclear reactors produce far more energy per unit of fuel than chemical reactions. Surprisingly, astronauts would actually receive less harmful radiation on shorter trips, even though the engine itself produces radiation. This happens because space travelers are constantly bombarded by cosmic radiation during their journey, and cutting travel time in half significantly reduces their total exposure. These engines work best for big spacecraft that need to speed up and slow down dramatically, perfect for moon and Mars missions where rapid velocity changes of at least 25,000 km/h are required.</p><p>The study, called <a href="https://www.esa.int/ESA_Multimedia/Images/2025/06/Alumni_nuclear_thermal_propulsion_system_schematic" target="_blank">“Alumni,”</a> prioritized safety through careful design. The nuclear reactor only turns on when the spacecraft is far from Earth in a safe orbit. Before activation, the uranium fuel has very low radioactivity and isn’t toxic. Multiple radiation shields protect the crew during the short engine burns that last less than two hours. The reactor is designed never to return to Earth’s atmosphere. The research team spent over a year analyzing this technology and concluded it’s feasible for long-term development. However, there’s still significant work ahead, including laboratory testing of the new ceramic-metal reactor design, building safe testing facilities, and solving technical challenges like fuel sourcing and reactor restart systems.</p><p>Nuclear thermal propulsion could revolutionize space travel, making missions to Mars and the moon faster and more practical. While the technology is promising and appears safe, it will take many years of development before we see nuclear-powered spacecraft heading to the Red Planet. It’s great to see Europe demonstrating that it has the expertise to develop this technology, potentially ushering in a new era of space exploration where distant worlds become more accessible than ever before.</p>]]></description><pubDate>Sat, 14 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/esa-nuclear-rocket</guid><category>Rockets</category><category>Mars</category><category>Esa</category><category>Nasa</category><dc:creator>Mark Thompson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/schematic-collage-of-an-alumni-nuclear-thermal-propulsion-system-surrounded-by-a-dark-starry-sky.jpg?id=60934597&amp;width=980"></media:content></item><item><title>Video Friday: AI Model Gives Neo Robot Autonomy</title><link>https://spectrum.ieee.org/video-friday-neo-humanoid-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/robot-and-person-standing-face-to-face-in-a-wooden-hallway-with-tall-bushy-plants.png?id=60988606&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p><span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span></p><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, THE NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN, CHINA</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="qnzl5dvtdkk">Introducing Redwood—1X’s breakthrough <a data-linked-post="2671886754" href="https://spectrum.ieee.org/chain-of-thought-prompting" target="_blank">AI model</a> capable of doing chores around the home. For the first time, <a data-linked-post="2671238743" href="https://spectrum.ieee.org/video-friday-good-over-all-terrains" target="_blank">NEO Gamma</a> moves, understands, and interacts autonomously in complex human environments. Built to learn from real-world experiences, Redwood empowers NEO to perform end-to-end mobile manipulation tasks like retrieving objects for users, opening doors, and navigating around the home gracefully, on top of hardware designed for compliance, safety, and resilience.</blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ebbfc0b339e850cd28b7e5f5fac9c43c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/qnzL5dVTDKk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p class="shortcode-media shortcode-media-youtube"> <span class="rm-shortcode" data-rm-shortcode-id="a2f3f4b5f15fde9ec50d5116b96b764a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Dp6sqx9BGZs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span> <small class="image-media media-caption" placeholder="Add Photo Caption...">- YouTube</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..."><a href="https://www.youtube.com/watch?v=Dp6sqx9BGZs" target="_blank">www.youtube.com</a></small></p><p>[ <a href="https://www.1x.tech/discover/redwood-ai">1X Technology</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="7gap8k9jz2q"><a data-linked-post="2650251927" href="https://spectrum.ieee.org/therapeutic-robots-paro-and-keepon-are-cute-but-still-costly" target="_blank">Marek Michalowski</a>, who co-created <a data-linked-post="2650276785" href="https://spectrum.ieee.org/keepon-helps-kids-learn-to-argue-better" target="_blank">Keepon</a>, has not posted to his YouTube channel in 17 years—until this week. The new post? It’s about a project from 10 years ago!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="f79c76127d0315323179d7d47ac57117" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/7gAp8k9jZ2Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://jonathanproto.com/project-sundial">Project Sundial</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="lkc2y0yb89u"><em>Helix can now handle a wider variety of packaging approaching human-level dexterity and speed, bringing us closer to fully autonomous package sorting. This rapid progress underscores the scalability of Helix’s learning-based approach to robotics, translating quickly into real-world applications.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d3d0673b70a96eaa2d77f39e3bf16d02" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lkc2y0yb89U?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.figure.ai/news/helix">Figure</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="nzuvdu2q0zo">This is certainly an atypical Video Friday selection, but I saw this Broadway musical called “Maybe Happy Ending” a few months ago because the main characters are deprecated humanoid home-service robots. It was utterly charming, and it just won the Tony award for best new musical among others.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e25d5605adc019d20ed170c527ed4700" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/nZUVDu2q0Zo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ “<a href="https://www.maybehappyending.com/">Maybe Happy Ending</a>” ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ptydwp9utis"><a data-linked-post="2664552687" href="https://spectrum.ieee.org/boston-dynamics-dancing-robots" target="_blank">Boston Dynamics</a> brought a bunch of Spots to “America’s Got Talent,” and kudos to them for recovering so gracefully from an on-stage failure.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7ec3c1744aa63eeb5904fa3ee0779adc" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ptYDWP9uTis?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://bostondynamics.com/products/spot/">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="41xnc4mu-hs">I think this is the first time I’ve seen end-effector changers used for either feet or heads.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1a29c3d692365d22f63dade5335a6c2f" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/41XNc4Mu-hs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://unit.aist.go.jp/jrl-22022/en/">CNRS-AIST Joint Robotics Laboratory</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="tbdtcwzfeiu"><em>ChatGPT has gone fully Navrim—complete with existential dread and maximum gloom! Watch as the most pessimistic ChatGPT-powered robot yet moves chess pieces across a physical board, deeply contemplating both chess strategy and the futility of existence. Experience firsthand how seamlessly AI blends with robotics, even if Navrim insists there’s absolutely no point.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="08d839d8c7f07846798dcb0748af4c05" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TbDTCwzFeIU?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Not bad for $219 all in.</p><p>[ <a href="https://vassarrobotics.com/">Vassar Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="9u0hocl0aj4"><em>We present a single-layer multimodal sensory skin made using only a highly sensitive hydrogel membrane. Using electrical impedance tomography techniques, we access up to 863,040 conductive pathways across the membrane, allowing us to identify at least six distinct types of multimodal stimuli, including human touch, damage, multipoint insulated presses, and local heating. To demonstrate our approach’s versatility, we cast the hydrogel into the shape and size of an adult human hand.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="30c9c1e376a765bd37eb45ea36471e1c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/9U0hoCL0aJ4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.dshardman.co.uk/publication/scirohand/">Bio-Inspired Robotics Laboratory</a> ] paper published by [ <a href="https://www.science.org/journal/scirobotics" target="_blank">Science Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="pqdqamtjwrw"><em>This paper introduces a novel robot designed to exhibit two distinct modes of mobility: rotational aerial flight and terrestrial locomotion. This versatile robot comprises a sturdy external frame, two motors, and a single wing embodying its fuselage. The robot is capable of vertical takeoff and landing in mono-wing flight mode, with the unique ability to fly in both clockwise and counterclockwise directions, setting it apart from traditional mono-wings.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fea3e49bf322eec7899d20a2236783a4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PQDqAMTjWrw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://journals.sagepub.com/doi/10.1177/02783649251344968">AIR Lab</a> paper ] published in [ <a href="https://journals.sagepub.com/home/ijra" target="_blank">The International journal of Robotics Research</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="hzq2hhoi6la">When TRON 1 goes to work, all he does is steal snacks from hoomans. Apparently.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="8d3121ce94715e6d531c92002d4575b7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hZQ2hhoi6lA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.limxdynamics.com/en/tron1">LimX Dynamics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="jb2txzph7xs"><em>The 100,000th robot has just rolled off the line at Pudu Robotics’ Super Factory! This key milestone highlights our cutting-edge manufacturing strength and marks a global shipment volume of over 100,000 units delivered worldwide.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="5bc0b47f42e246bf950808b9fc53d28e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Jb2tXzph7Xs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.pudurobotics.com/en">Pudu Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="cn5whdtrlv0">Now that is a big saw.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e4d5683d9d16f4c931c72693ecd4b188" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/CN5WhDTRlV0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.kuka.com/en-se/industries/solutions-database/2025/05/catonator_smartproduction-nordic">Kuka Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="2rih4zintzm"><em>NASA Jet Propulsion Laboratory has developed the Exploration Rover for Navigating Extreme Sloped Terrain or ERNEST. This rover could lead to a new class of low-cost planetary rovers for exploration of previously inaccessible locations on Mars and the moon.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="ccaf709dda32b8b5778c7a0f178c877e" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2RiH4ZInTZM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.hou.usra.edu/meetings/lpsc2025/pdf/1729.pdf">NASA Jet Propulsion Laboratory</a> paper ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="zobe3aoz5fw"><em>Brett Adcock, founder and CEO of Figure AI, speaks with Bloomberg Television’s Ed Ludlow about how the company is training humanoid robots for logistics, manufacturing, and future roles in the home at Bloomberg Tech in San Francisco.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2467bd8c8c1fc278a40446b7ee5655e9" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/zObe3aOz5fw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.figure.ai/">Figure</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="b6pz2r1uhxw"><em>Peggy Johnson, CEO of Agility Robotics, discusses how humanoid robots like Digit are transforming logistics and manufacturing. She speaks with Bloomberg Businessweek’s Brad Stone about the rapid advances in automation and the next era of robots in the workplace at Bloomberg Tech in San Francisco.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="d7a77a584e471d9a057897f5a2a150d7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/B6pz2R1UHXw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="piyyufkvg1e">This ICRA 2025 Plenary is from Allison Okamura, titled “Rewired: The Interplay of Robots and Society.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c721b1931ac1615ffee7f918db1f8861" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/PIYyufKvG1E?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://2025.ieee-icra.org/program/plenary-sessions/">ICRA 2025</a> ]</p><div class="horizontal-rule"></div>]]></description><pubDate>Fri, 13 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-neo-humanoid-robot</guid><category>Video friday</category><category>Humanoid robots</category><category>Autonomous robots</category><category>Dexterity</category><category>Dancing robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/robot-and-person-standing-face-to-face-in-a-wooden-hallway-with-tall-bushy-plants.png?id=60988606&amp;width=980"></media:content></item><item><title>Telecom Expert Honored By IEEE Standards Association</title><link>https://spectrum.ieee.org/telecom-kevin-lu-ieee-standards</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/headshot-of-kevin-lu-with-a-closed-mouth-smile-he-is-wearing-a-suit-and-tie.jpg?id=60768495&width=1200&height=800&coordinates=0%2C201%2C0%2C202"/><br/><br/><p>Growing up in Taipei, Taiwan, in the 1960s with limited access to television and other forms of entertainment, <a href="https://kevinwlu.github.io/" rel="noopener noreferrer" target="_blank">Kevin Lu</a> amused himself by examining how machines worked. He became fascinated by heavy construction equipment and built miniature versions of the machinery out of scrap materials.</p><p>“We didn’t have a lot at the time,” Lu recalls. “TV was just becoming available to the average household, and there weren’t many toys. So I made my own.”</p><h3>Kevin Lu</h3><br/><p><strong>Employer: </strong></p><p><strong></strong>Stevens Institute of Technology, in Hoboken, N.J.</p><p><strong>Title: </strong></p><p><strong></strong>Teaching Professor and Associate Chair for Undergraduate Studies in the Department of Electrical & Computer Engineering</p><p><strong>Member grade: </strong></p><p><strong></strong>Life senior member</p><p><strong>Alma maters:</strong> </p><p>National Chiao Tung University in Hsinchu; Washington University, in St. Louis<br/></p><p>That boy would grow up to publish pioneering work on optical networks, have a long career in <a href="https://spectrum.ieee.org/topic/telecommunications/" target="_self">telecommunications</a> R&D, and teach students about the emerging <a href="https://spectrum.ieee.org/tag/internet-of-things" target="_self">Internet of Things</a>.</p><p>Lu, an IEEE Life senior member, also has played a significant role in IEEE’s global standards development program. He was honored last year with the<a href="https://standards.ieee.org/about/awards/dsvs/" rel="noopener noreferrer" target="_blank"> IEEE Standards Board Distinguished Service Award</a> for “superior IEEE SA governance leadership as the IEEE SA Standards Board audit committee chair and as the IEEE SA Industry Connections committee chair.”</p><p>Now approaching retirement, Lu reflects on his career, which has gracefully threaded together engineering, teaching, and volunteerism, with no signs of slowing down.</p><h2>Switching from an interest in mechanics to electronics </h2><p>Born in Taipei City, Lu was the youngest of four siblings. He says he was influenced by his family and circumstances. His father, a nontechnical administrative staff member at<a href="https://www.cht.com.tw/en/home/cht" rel="noopener noreferrer" target="_blank"> ChungHwa Telecom</a>, the country’s telephone company, kept the home filled with telecom newsletters. Lu says his brother performed bold chemistry experiments that sometimes ended with singed eyebrows or small explosions. Kevin gravitated toward mechanical projects, such as building scale models of cranes, before eventually embracing electronics.</p><p>“My parents encouraged a career in engineering because they thought it would provide a good living,” he says.</p><p>He earned a bachelor’s degree in control engineering from the<a href="https://en.wikipedia.org/wiki/National_Chiao_Tung_University" rel="noopener noreferrer" target="_blank"> </a><a href="https://www.nycu.edu.tw/nycu/en/index" rel="noopener noreferrer" target="_blank">National Chiao Tung University</a> in Hsinchu in 1979. He then attended <a href="https://washu.edu/" rel="noopener noreferrer" target="_blank">Washington University</a> in St. Louis, earning master’s and doctoral degrees in systems science and mathematics in 1981 and 1984.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Kevin Lu holding an IEEE plaque for Distinguished Service. He is posing in a suit and tie between James E. Matthews and Yatin Trivedi." class="rm-shortcode" data-rm-shortcode-id="2d93d4c59ac43c06bd208e4598fa4bb8" data-rm-shortcode-name="rebelmouse-image" id="bb5fc" loading="lazy" src="https://spectrum.ieee.org/media-library/kevin-lu-holding-an-ieee-plaque-for-distinguished-service-he-is-posing-in-a-suit-and-tie-between-james-e-matthews-and-yatin-tr.jpg?id=60773483&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Kevin Lu [center] shows off the plaque commemorating him being honored with the 2024 IEEE Standards Board Distinguished Service Award. He is flanked by James E. Matthews, president of the IEEE Standards Association, and Yatin Trivedi, a member of the IEEE Standards Association Board of Governors.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Kevin Lu</small></p><h2>A long career with Bellcore</h2><p>A chance meeting at the campus placement office led to a job interview with Bell Communications Research, known as <a href="https://journal.businesstoday.org/bt-online/2017/3/18/the-legacy-of-bellcore" target="_blank">Bellcore</a> (formerly part of<a href="https://spectrum.ieee.org/bell-labs-100-birthday" target="_self"> Bell Labs</a>, now <a href="https://www.nokia.com/bell-labs/" rel="noopener noreferrer" target="_blank">Nokia Bell Labs</a>). He was hired and worked at the company’s facility in Piscataway, N.J.</p><p>The timing couldn’t have been better. In 1984 the U.S. telecommunications industry was undergoing a massive structural change, with<a href="https://spectrum.ieee.org/the-end-of-att" target="_self"> AT&T’s divestiture</a> spawning new entities including Bellcore. His job was “member of the technical staff,” which he took great pride in, he says, noting that “Nobel laureates held that same title at Bell Labs.”</p><p>For the next 28 years, he contributed to projects that shaped the modern communications landscape. In 1990 he wrote the seminal paper “<a href="https://ieeexplore.ieee.org/document/57809" rel="noopener noreferrer" target="_blank">System and Cost Analyses of Broad-Band Fiber Loop Architectures</a>,” which was published in the<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=49" rel="noopener noreferrer" target="_blank"> <em><em>IEEE Journal on Selected Areas in Communications</em></em></a>. It advocated for<a href="https://www.cisco.com/c/en/us/products/switches/what-is-passive-optical-networking.html" rel="noopener noreferrer" target="_blank"> passive optical networks</a>—a concept that is now central to global fiber deployment.</p><p>The road from idea to implementation was long, Lu says.</p><p>“It wasn’t until 2009 that <a href="https://www.verizon.com/plans/unlimited/?customer_id=279-516-8739&cmp=KNC-C-Mobility-NON-R-BPUR-NONE-NONE-2K0VZ0-COE-GAW-586&kpid=go_cmp-12581404974_adg-116456529981_ad-744100437038_kwd-13038366_dev-c_ext-_prd-_sig-Cj0KCQjw0LDBBhCnARIsAMpYlAq9WwyKM15CSLCEQRyF4Vw6_61Zv40sVedCSuoNbs5B2EDj-J7ixo0aAhWEEALw_wcB&gad_source=1&gad_campaignid=12581404974&gbraid=0AAAAAD6-lLtQ9KmXfAIaC7HQNGbpj53de&gclid=Cj0KCQjw0LDBBhCnARIsAMpYlAq9WwyKM15CSLCEQRyF4Vw6_61Zv40sVedCSuoNbs5B2EDj-J7ixo0aAhWEEALw_wcB" rel="noopener noreferrer" target="_blank">Verizon</a> installed a unit in my home,” he says, laughing. “Fiber is expensive, so companies deployed wireless first to build up enough revenue.”</p><p>Bellcore eventually became Telcordia, which Ericsson <a href="https://www.ericsson.com/en/about-us/history/company/the-consequences-of-expansion/ericsson-acquired-telcordia" rel="noopener noreferrer" target="_blank">acquired in 2012</a>. Although Lu had risen through the ranks to become Telcordia’s chief scientist, he left during the Ericsson acquisition and joined<a href="https://www.broadcom.com/" rel="noopener noreferrer" target="_blank"> Broadcom</a>. There he worked on cellphone chips and contributed to mobile standards for the<a href="https://www.3gpp.org/about-us" rel="noopener noreferrer" target="_blank"> 3rd Generation Partnership Project</a> (3GPP), a global consortium of telecommunications standards organizations that creates and maintains specifications for mobile systems.</p><p>After Broadcom exited the cellular baseband chip business, Lu left in 2013, for a job in academia.</p><h2>An academic career at Stevens</h2><p>In 2015 Lu joined the <a href="https://www.stevens.edu/" rel="noopener noreferrer" target="_blank">Stevens Institute of Technology</a>, in Hoboken, N.J., as an adjunct professor in the electrical and computer engineering department. He became a full-time professor in 2018.</p><p>Now, he says, he sees academia as a continuation of—not a departure from—his life’s work.</p><p>“The decades I spent in that world give me insights students won’t get from textbooks,” he says.</p><p class="pull-quote"><span>“When students tell me they’ve discovered their path … that’s the most rewarding thing.”</span></p><p>In May 2019 Stevens honored him with its <a href="https://www.stevens.edu/news/stevens-honors-excellence-research-teaching-during-2018-2019-academic-year" target="_blank">Morton Distinguished Teaching Professor Award</a>.</p><p>He encourages his students to embrace lifelong learning and develop soft skills alongside technical knowledge. He doesn’t just teach engineering, he says; he works “to help students discover who they are and where they might thrive.”</p><p>Although he recently announced his intention to retire, the school has persuaded him to remain, with the offer of a new role, to be formally announced before the next semester.</p><p>“I’ll continue on for at least three more years,” he says.</p><h2>Involvement with standards development</h2><p>Throughout his career, IEEE has remained a constant, he says. He joined in 1980 as a student member, drawn by the affordability of dues and publishing opportunities.</p><p>His early IEEE involvement was rooted in power systems—an echo of his dissertation. His career in the telecom industry led him to become involved with the<a href="https://www.comsoc.org/" target="_blank"> IEEE Communications Society</a> and the <a href="https://standards.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Standards Association</a>. He served as the society’s director of standards development in 2012 and 2013. In that role, he chaired its <a href="https://www.comsoc.org/about/boards/standards-development-board" rel="noopener noreferrer" target="_blank">Standards Development Board</a>. He also served on the society’s <a href="https://www.comsoc.org/about/boards/standardization-programs-development-board" rel="noopener noreferrer" target="_blank">Standardization Programs Development Board</a> for several years.</p><p>Lu now chairs the IEEE Standards Board’s I<a href="https://standards.ieee.org/about/bog/iccom/" rel="noopener noreferrer" target="_blank">ndustry Connections committee</a>, which ensures that proposed Industry Connections activities are within the scope and purpose of IEEE. The committee, he says, is “a well-oiled machine.” He has led the group since 2018, and although he has given a lot of thought to turning over the reins to a successor, he has stayed on as chair to ensure its continuity.</p><p>He also has served on the <a href="https://standards.ieee.org/about/sasb/audcom/" rel="noopener noreferrer" target="_blank">audit</a>, <a href="https://standards.ieee.org/about/sasb/patcom/" rel="noopener noreferrer" target="_blank">patent</a>, <a href="https://standards.ieee.org/about/sasb/procom/" rel="noopener noreferrer" target="_blank">procedures</a>, and <a href="https://standards.ieee.org/about/sasb/nescom/" rel="noopener noreferrer" target="_blank">new standards</a> committees.</p><p>Even after decades of professional achievement, he says, he remains focused on learning, mentoring, and building bridges between generations of engineers.</p><p>What excites him most about the direction his career has taken, he says, is “when students tell me they’ve discovered their career path.”</p><p>“That’s the most rewarding thing,” he says. “That’s when I know I’ve done my job. I take pride in seeing them embrace my philosophy of making lifelong learning a daily habit.”</p>]]></description><pubDate>Thu, 12 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/telecom-kevin-lu-ieee-standards</guid><category>Bellcore</category><category>Careers</category><category>Ieee member news</category><category>Ieee standards</category><category>Stevens institute of technology</category><category>Telecommunications</category><category>Type:ti</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/headshot-of-kevin-lu-with-a-closed-mouth-smile-he-is-wearing-a-suit-and-tie.jpg?id=60768495&amp;width=980"></media:content></item><item><title>Anti-Distraction Systems Shut Down Smartphone Use</title><link>https://spectrum.ieee.org/distracted-driving-smartphone</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-man-driving-a-car-touches-a-smartphone-on-his-dashboard-the-image-on-the-phone-reads-safe-mode.jpg?id=60567887&width=1200&height=800&coordinates=187%2C0%2C188%2C0"/><br/><br/><p>As mobile phone use continues to be a leading cause of vehicle accidents, a range of technologies has emerged designed to combat <a href="https://spectrum.ieee.org/texting-while-driving-were-really-bad-at-it-but-we-think-were-good" target="_blank">distracted driving</a>. From mobile apps to hardware-integrated systems, these tools aim to limit phone use behind the wheel. But a closer look reveals significant differences in how effectively they prevent distractions—especially in fleet vehicles.</p><p>While apps like <a href="https://www.att.com/" target="_blank">AT&T</a>’s<a href="https://www.att.com/support/article/wireless/KM1000730/" target="_blank"> DriveMode</a> and <a href="https://www.apple.com/store?afid=p240%7Cgo~cmp-21644513200~adg-172212795549~ad-748928271266_kwd-10778630~dev-c~ext-~prd-~mca-~nt-search&cid=aos-us-kwgo-brand-iphone-tradeinpromo-052225-" target="_blank">Apple</a>’s built-in<a href="https://youtu.be/FXNrstZG5v4?si=eHXtjQxy18S9_T0n" target="_blank"> Do Not Disturb While Driving</a> offer basic protections, they rely heavily on driver cooperation. Many can be bypassed with a swipe or a second phone, limiting their effectiveness when liability and safety are paramount.</p><p>“We think technologies that reduce visual-manual interaction with phones are obviously a good thing,”<a href="https://www.linkedin.com/in/ian-reagan-03b09631" target="_blank"> Ian Reagan</a>, a senior research scientist at the <a href="https://www.iihs.org/" target="_blank">Insurance Institute for Highway Safety</a> told <em><a href="https://spectrum.ieee.org/" target="_blank">IEEE Spectrum</a></em>. “But most are opt-in. We’d like to see them as opt-out by default.”</p><p class="pull-quote">“Mobile use while driving is an addiction. We needed a system that prevents distraction without waiting for the driver to choose safety. That’s what we built.” <strong>Ori Gilboa, SaverOne</strong></p><p>Now, a new generation of anti-distraction technology is shifting from soft nudges to hard enforcement. And for companies managing fleets of drivers, the stakes—and the solutions—are getting more serious.</p><h2>The Need for Enforceable Solutions</h2><p>“There’s a difference between tools that monitor and tools that prevent,” says <a href="https://il.linkedin.com/in/ori-gilboa-6a8a07128" target="_blank">Ori Gilboa</a>, CEO of <a href="https://saver.one/" target="_blank">SaverOne</a>, a Tel Aviv–area startup leading a new wave of hardware-integrated solutions that make driver cooperation a nonissue. “That distinction matters when lives are on the line.”</p><p>SaverOne’s system uses a passive sensor network to scan the vehicle cabin for phones, identify the driver’s device, and place it into “safe mode”—automatically blocking risky apps while allowing essential functions like navigation and preapproved voice calls. Crucially, the system works even if the driver tries to cheat by disabling Bluetooth or by bringing a second phone.</p><h2>Designed to Be Driverproof</h2><p>The system consists of four small hidden sensors and a central receiver—about the size of an <a href="https://www.apple.com/iphone/?afid=p240%7Cgo~cmp-14645102856~adg-126654827229~ad-735242796883_kwd-334361787~dev-c~ext-~prd-~mca-~nt-search&cid=wwa-us-kwgo-iphone-Core-iPhone-Avail-iPhone-Core-Exact-iPhone-Exact-iphone" target="_blank">iPhone</a>—installed inside the vehicle. It can pinpoint mobile devices within centimeters and distinguishes between driver and passenger phones. If the driver’s phone is active and doesn’t connect to SaverOne’s app, a buzzer sounds until the issue is resolved.</p><p>“What sets us apart is our prevention-first approach,” says Gilboa. “Most systems focus on what went wrong after the fact. We stop the distraction before it happens.”</p><p>Gilboa said the system’s design respects driver usability, preserving tools like turn-by-turn navigation and voice calls to approved contacts. “We want drivers to be reachable—but not distracted,” he adds.</p><h2>Global Expansion, Measurable Impact</h2><p>Since launching its second-generation product in 2022, SaverOne has rapidly expanded. After early pilot deployments with Israeli fleet operators such as <a href="https://www.bynet.co.il/en/about/" target="_blank">Bynet Data Communications</a>,<a href="https://iec-global.com/" target="_blank"> Israel Electric Corporation</a>, and ice-cream purveyor <a href="https://www.froneri.com/" target="_blank">Froneri</a>, the company gained traction, securing deals with a broader array of Israeli companies. By mid-2023, <a href="https://www.cemex.com/" target="_blank">Cemex</a> Israel, the global cement giant’s local subsidiary, had agreed to deploy the driver-distraction-prevention system on its 380-vehicle fleet. In January 2024, following a successful trial with 17 trucks, <a href="https://www.strauss-group.com/" target="_blank">Strauss Group</a>, one of Israel’s largest food and beverage companies, decided to install the SaverOne system on its fleet of 80 food-delivery trucks. Though smaller than the Cemex Israel contract, that agreement proved significant because Strauss accumulated data demonstrating a statistically significant reduction in accident rates among the equipped vehicles. That news has helped SaverOne in its bid to go global. CEMEX has since outfitted trucks in fleets across Europe. In the United States, SaverOne is now being adopted by <a href="https://www.fedex.com/global/choose-location.html" target="_blank">FedEx</a> contractors in North Carolina and Philadelphia, says Gilboa.</p><p>Some fleet operators report as much as a 60 percent reduction in accident rates post-installation. While those figures are difficult to verify independently, a more concrete metric is phone interaction. Fleet managers have observed a dramatic drop—from drivers checking their phones 10 times per hour to near zero.</p><p>“The system educates through behavior,” says Gilboa. “It’s not about punishment—it’s about making the right choice automatic.”</p><p>But Reagan cautions that long-term behavioral change remains unproven, comparing it to early<a href="https://en.wikipedia.org/wiki/Intelligent_speed_assistance" target="_blank"> intelligent speed assistance</a> trials in Europe using systems that detected vehicles’ locations, used digital maps to keep track of local speed limits, and reduced engine power to prevent the vehicles from exceeding the legal limit. “When the limiter was on,” Reagan says, “people obeyed the posted speed limits. When it was turned off, they sped again. Whether tech like this [driver-distraction-prevention system] creates lasting change—well, we just don’t know yet.”</p><h2>Could Regulation Be the Tipping Point?</h2><p>Despite promising results, broader adoption—particularly in the consumer market—may hinge on regulation. IIHS’s Reagan notes that although <a href="https://www.nhtsa.gov/risky-driving/distracted-driving" target="_blank">distracted driving</a> officially accounts for about 10 percent of crash fatalities, or roughly 3,500 deaths per year, the real figure is likely far higher. Despite the undercount, the urgency is still hard to ignore. As Reagan put it, “Phones let you mentally escape the car, even when you’re barreling down the highway at 115 kilometers per hour [about 70 miles per hour]. That’s the real danger.”</p><p>He adds that government regulation requiring carmakers to install systems like SaverOne’s could be a game changer. “The tech exists,” Reagan said. “What we need is the political will to mandate it.”</p><p>SaverOne is still focused on fleet customers, but the company is in discussions with insurers exploring offering discounts to young or high-risk drivers who use distraction-prevention systems, Gilboa says.</p><p>“Mobile use while driving is an addiction,” he says. “We needed a system that prevents distraction without waiting for the driver to choose safety. That’s what we built.”</p>]]></description><pubDate>Wed, 11 Jun 2025 14:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/distracted-driving-smartphone</guid><category>Driver distraction</category><category>Smartphone app</category><category>Driving safety</category><category>Distracted driving</category><dc:creator>Willie D. Jones</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-man-driving-a-car-touches-a-smartphone-on-his-dashboard-the-image-on-the-phone-reads-safe-mode.jpg?id=60567887&amp;width=980"></media:content></item><item><title>Three Steps to Stopping Killer Asteroids</title><link>https://spectrum.ieee.org/planetary-defense-killer-asteroids</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/spacecraft-approaches-large-asteroid-against-starry-background.png?id=60694025&width=1200&height=800&coordinates=416%2C0%2C417%2C0"/><br/><br/><p><strong>Impact was imminent. </strong>Occasional gasps arose as the asteroid took shape and a jagged, rocky surface filled the view. Then the images abruptly stopped.</p><p>The mission control room at Johns Hopkins University Applied Physics Lab in Laurel, Md., erupted in cheers. “We have impact!” said the lead engineer, who gave a two-handed high five to a nearby colleague. Others waved their hands in the air in victory and slapped each other on the back.</p><p>This had been a test, and humanity had passed it, taking one crucial step closer to protecting Earth from an asteroid impact. The <a href="https://dart.jhuapl.edu/Gallery/media/videos/From%20Impact%20to%20Innovation_%20A%20Year%20of%20Science%20and%20Triumph%20for%20Historic%20DART%20Mission.mp4" target="_blank">test</a> was the culmination of NASA’s <a href="https://science.nasa.gov/mission/dart/" target="_blank">Double Asteroid Redirection Test</a> (DART) mission, for which I was the coordination lead. On 26 September 2022, the DART spacecraft had successfully crashed into Dimorphos, a roughly 150-meter-diameter asteroid that was 11 million kilometers from Earth. The collision nudged the asteroid and modified its trajectory.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="Diagram of DART impacting Dimorphos, altering its orbit around asteroid Didymos." class="rm-shortcode" data-rm-shortcode-id="d720ffe9969cc7ec026bc60d50cd9d15" data-rm-shortcode-name="rebelmouse-image" id="3a002" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-dart-impacting-dimorphos-altering-its-orbit-around-asteroid-didymos.png?id=60755072&width=980"/><small class="image-media media-caption" placeholder="Add Photo Caption...">In 2022, NASA’s Double Asteroid Redirection Test slammed a golf-cart-size spacecraft, DART, into the near-Earth asteroid Dimorphos (1). DART—which first deployed a small observer craft, LICIACube, to observe the collision (2)—bumped Dimorphos’s trajectory (3) enough to alter its future course (4).</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">GyGinfographics; Source: NASA</small></p><p>The celebrations in the control room were the culmination of years of effort to prove that the momentum from a golf-cart-size spacecraft can alter an asteroid’s future path.  And DART’s collision with asteroid Dimorphos kicked off a new era in space exploration, in which technologies for planetary defense are now taking shape.</p><p>If one day an asteroid like Dimorphos is discovered to be headed toward Earth, an interceptor craft like DART could collide with the asteroid years in advance to avert disaster. Here’s how that might work.</p><h2>Step 1: Find and Track Near-Earth Asteroids  </h2><p>The first step in averting an asteroid impact with Earth is just to know what near-Earth objects (NEOs) are out there.</p><p>The University of Hawaii’s <a href="https://atlas.fallingstar.com/" target="_blank">Asteroid Terrestrial-impact Last Alert System</a> (ATLAS) station, in Chile plays a critical role in these observations of NEOs, which are asteroids orbiting near Earth’s orbit. In late December, it detected a previously unknown NEO during a routine sweep of the skies. The asteroid was given the name 2024 YR4, following the <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">standard astronomical convention</a> for new objects. “2024 Y” <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">represents the 24th-half-month of the year 2024</a>—that is, 16 to 31 December. The “R4” encodes the <a href="https://sci.esa.int/web/home/-/30244-asteroid-numbers-and-names" target="_blank">sequence of discovery</a>—in this case, that it was <a href="https://asteroidday.org/resources/event-resources/how-do-asteroids-get-provisional-designation/" target="_blank">the 117th object</a> found during the year’s final couple of weeks.</p><h3>Hera </h3><br/><img alt="Illustration of a yellow satellite with two blue solar panels deployed." class="rm-shortcode" data-rm-shortcode-id="855d19cc951655fcf339a06fa6dde6b4" data-rm-shortcode-name="rebelmouse-image" id="f536f" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-yellow-satellite-with-two-blue-solar-panels-deployed.png?id=60756477&width=980"/><p>This European Space Agency mission will rendezvous with the Didymos–Dimorphos asteroid system and study the aftereffects of NASA’s DART impact close up.</p><p><strong>Launch:</strong></p><p>2024</p><p><strong>Rendezvous: </strong></p><p>2026</p><h3></h3><br/><p>Until that point in the year, <a href="https://cneos.jpl.nasa.gov/stats/totals.html" rel="noopener noreferrer" target="_blank">more than 3,000 NEOs</a> had already been discovered. Nothing about <a href="https://science.nasa.gov/solar-system/asteroids/2024-yr4/" rel="noopener noreferrer" target="_blank">2024 YR4</a> initially stood out as concerning. It was a seemingly run-of-the-mill asteroid. However, further observations soon suggested it wasn’t ordinary at all.</p><p>Throughout the first weeks of 2025, the probability of a 2024 YR4 collision with Earth kept growing. On 29 January, astronomers calculated its odds of eventual impact to be 1.3 percent. And in crossing the 1 percent threshold, 2024 YR4 triggered an alert from the <a href="https://iawn.net/" rel="noopener noreferrer" target="_blank">International Asteroid Warning Network</a> to the United Nations’ <a href="https://www.unoosa.org/" rel="noopener noreferrer" target="_blank">Office for Outer Space Affairs</a> about the potential impact. Such alerts are posted publicly on the IAWN’s <a href="https://iawn.net/" rel="noopener noreferrer" target="_blank">website</a>. The <a href="https://iawn.net/documents/NOTIFICATIONS/2024-YR4_IAWN_Potential-Impact-Notification_20250129.pdf" rel="noopener noreferrer" target="_blank">29 January notice</a> assessed the regions of the planet at highest risk from 2024 YR4 (also known as its risk corridor), as well as the expected damage if the asteroid did crash into Earth.</p><p>On average, an object of 2024 YR4’s size—estimated at <a href="https://webbtelescope.org/contents/media/images/01JQSF5C4CGVMCS3EV9YX6CQAR" rel="noopener noreferrer" target="_blank">60 meters</a> across—slams into our planet once every thousand years. It’s considered a “city-killer” asteroid—not big enough to trigger a mass extinction, like the estimated 10-km one that likely killed the dinosaurs, but still big enough to be deadly up to roughly 50 km from the impact location. Fortunately, by 24 February, <a href="https://www.minorplanetcenter.net/db_search/show_object?object_id=2024+YR4" rel="noopener noreferrer" target="_blank">further observations</a> by telescopes across the globe had refined the asteroid’s trajectory enough to rule out near-term Earth impact.</p><p>Yet when it comes to asteroids and Earth, there won’t always be such an uncomplicated, happy ending. Another asteroid that size or even larger will eventually be on a collision course with the planet [see chart below].</p><h3></h3><br/><img alt="Near-Earth objects threat; size, frequency, damage, energy, discovery percentage comparison." class="rm-shortcode" data-rm-shortcode-id="1a163985e40a1e1b33f1a47a44dfeabd" data-rm-shortcode-name="rebelmouse-image" id="1d275" loading="lazy" src="https://spectrum.ieee.org/media-library/near-earth-objects-threat-size-frequency-damage-energy-discovery-percentage-comparison.png?id=60762777&width=980"/><h3></h3><br/><p>The world’s space agencies track an estimated <a href="https://science.nasa.gov/science-research/planetary-science/planetary-defense/near-earth-asteroids/" target="_blank">95 percent of NEOs greater than 1 km in diameter.</a> The International Asteroid Warning Network and a related <a href="https://www.cosmos.esa.int/web/smpag/smpag_members" target="_blank">Space Mission Planning Advisory Group</a> (SMPAG) are global coordinating bodies that monitor these efforts. And thankfully, none of the giant NEOs tracked by the above pose an impact risk to Earth for at least the next hundred years. (Meanwhile, comet impacts with Earth are even rarer than those of asteroids.)</p><p>But you can only track the NEOs that are known. And plenty of city-killer asteroids remain lurking and undiscovered, potentially still posing a real risk to life on the planet. In the 50-meter range, a meager 7 percent of NEOs have been found. That’s not for lack of trying. It’s just more difficult to find small asteroids because smaller asteroids appear dimmer than larger ones.</p><h3>NASA’s & FEMA’s 2024 Planetary-Defense Exercise</h3><br/><p>Last year, NASA and the Federal Emergency Management Agency sponsored an <a href="https://www.nasa.gov/wp-content/uploads/2024/06/ttx5-quicklook-report-final.pdf" target="_blank"> Interagency Tabletop Exercise</a> around a hypothetical asteroid impact threat. In the fictional scenario, telescope observations detected an NEO, yielding a 72 percent chance of the object colliding with Earth in 2038. I served as a facilitator for this tabletop exercise, which aimed to further discussion and opportunities to stress-test new approaches to a realistic “killer asteroid” scenario.</p><p>One complicating factor we introduced was that the NEO’s size remained alarmingly difficult to pin down: Was it a 60-meter city killer? Or was it an 800-meter object that could devastate a country? If the latter, it would have risked the lives and livelihoods of more than 10 million people.</p><p>To keep the exercise focused, we centered it around the hypothetical NEO’s detection—and the decisions and next actions that would follow. We tracked the unfolding discussions and decisions in the aftermath of detection. Several U.S. agencies and organizations participated, as did the <a href="https://www.unoosa.org/" target="_blank">U.N. Office of Outer Space Affairs</a> and international partners.</p><p>One of the key gaps identified was the limited readiness to quickly deploy space missions for reconnaissance of the asteroid threat and for preventing Earth impact. The scenario’s large uncertainties underscored the need for capabilities to rapidly obtain better information about the asteroid.</p><p>“I know what I would prefer [to do],” said one anonymous participant quoted in the exercise’s quick-look <a href="https://www.nasa.gov/wp-content/uploads/2024/06/ttx5-quicklook-report-final.pdf" target="_blank">report</a>. “But Congress will tell us to wait.” —<em>N.L.C.</em></p><h3></h3><br/><p>New hardware is clearly needed. Sometime soon, the <a href="https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory" rel="noopener noreferrer" target="_blank">Vera C. Rubin Observatory</a>, in Chile, is expected to see first light. The observatory will survey the entire visible sky every few nights, through a 3,200-megapixel camera on an 8.4-meter telescope. No Earth-based telescope in the history of the NEO hunt can match its capabilities. Adding to our NEO search will be NASA’s <a href="https://en.wikipedia.org/wiki/NEO_Surveyor" rel="noopener noreferrer" target="_blank">NEO Surveyor</a>, an infrared space telescope scheduled to launch as soon as 2027. Together, the two new facilities are expected to discover thousands of new-to-us near-Earth asteroids. For objects 140 meters and larger, the two telescopes will locate an anticipated 90 percent of the entire population.</p><p>Once an NEO has been discovered, astronomers routinely track its orbit and extrapolate its trajectory over the coming century. So any NEO already on the books (for example, in NASA’s <a href="https://cneos.jpl.nasa.gov/" rel="noopener noreferrer" target="_blank">database</a> or ESA’s <a href="https://neo.ssa.esa.int/" rel="noopener noreferrer" target="_blank">database</a>) is quite likely to come with decades of warning. Ideally, that should leave ample time to develop and deploy a spacecraft to learn more about it and redirect the wayward space rock if necessary.</p><h2>Step 2: Send an NEO Reconnaissance Mission</h2><p>Imagine that the probability of 2024 YR4 colliding with Earth rose instead of fell, with the estimated impact to take place sometime in 2032. Here’s why that would have been especially worrying.</p><p>Asteroid 2024 YR4’s elongated <a href="https://neo.ssa.esa.int/documents/d/guest/close-approach-fact-sheet-for-asteroid-2024yr4-version-1-0-" rel="noopener noreferrer" target="_blank">orbit</a> made it unobservable from Earth after mid-May of this year. So we wouldn’t have been able to see it with even the most sensitive telescopes until its next swing through our region of the solar system—around June 2028.</p><p>In that alternate universe, we would’ve had to wait three years to launch a reconnaissance mission to study the object up close. Only then would we have known the next steps to take to redirect the asteroid away from Earth before its fated visit four years later.</p><p>As it happens, SMPAG held <a href="https://www.cosmos.esa.int/web/smpag" target="_blank">preliminary discussions</a> about 2024 YR4 in late January and early February. However, because the asteroid’s risk of collision with Earth soon dwindled to zero, the group didn’t develop specific recommendations.</p><h3>Hayabusa2#</h3><br/><img alt="Illustration of a yellow satellite with blue solar panels in space." class="rm-shortcode" data-rm-shortcode-id="1cd9cf1a8a713bccf46db5c0457d0c9e" data-rm-shortcode-name="rebelmouse-image" id="249e4" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-yellow-satellite-with-blue-solar-panels-in-space.png?id=60762281&width=980"/><p>The Japan Aerospace Exploration Agency has extended a previous mission (Hayabusa2) to encounter two more near-Earth asteroids over the next six years.</p><p><strong>Flyby:</strong></p><p>2026</p><p><strong>Rendezvous</strong></p><p>2031</p><h3></h3><br/><p>DART would have provided a foundation for a 2028 reconnaissance mission, as would NASA’s <a href="https://en.wikipedia.org/wiki/Lucy_(spacecraft)" target="_blank">Lucy mission</a>, which flew past the <a href="https://en.wikipedia.org/wiki/152830_Dinkinesh" target="_blank">asteroid Dinkinesh</a> in 2023. Reconnaissance flybys provide as little as a few precious seconds to capture the needed data about the target asteroid. Of course, inserting a reconnaissance craft into orbit around the asteroid would allow more detailed measurements. However, few NEO trajectories offer the opportunity for any maneuver other than a flyby—especially when time is of the essence.</p><p>Whatever the trajectory, the most important question for a reconnaissance mission would be whether the asteroid was in fact on a collision course with Earth in 2032. If so, where on the planet would it hit? That future impact location could potentially be <a href="https://cneos.jpl.nasa.gov/pd/cs/" target="_blank">narrowed down</a> to <a href="https://cneos.jpl.nasa.gov/pd/cs/ttx24/03_TTX_module2.pdf" target="_blank">within a hundred kilometers</a>.</p><p>The mission might also uncover some complications. For starters, we might discover that the asteroid is actually plural. Some 15 percent of NEOs are believed to have secondary objects orbiting them—they’re asteroids with moons. And some asteroids are essentially a flying jumble of rocks.</p><p>Another wrinkle comes in determining the asteroid’s mass. We need to know the mass to calculate the damage it could cause on impact, as well as the oomph required to divert it.</p><p>Unfortunately, the technology to measure the mass of a city-killer asteroid doesn’t exist. The mass of a larger, kilometer-size asteroid is measured by determining the gravitational pull on the reconnaissance spacecraft, but that trick doesn’t work for smaller asteroids. Right now, the best we can do is estimate the mass by measuring the asteroid’s physical size from closeup imaging during a flyby and then inferring the composition.</p><p>These challenges will need to be mastered in time for the reconnaissance mission, as the spacecraft—traveling at up to 90,000 kilometers per hour—flies past the potentially irregularly shaped object or objects half-shrouded in darkness. So it probably makes sense to tackle those challenges now rather than waiting until an actual threat emerges.</p><h2>Step 3: Change NEO’s Course With Interceptor</h2><p>If the reconnaissance mission does conclude that a killer asteroid is on the way and narrows down the date of impact, then what? Returning to 2024 YR4, that might make 22 December 2032 a very bad day for one city-size region of the planet. Even if it fell in the ocean, we’d need to look at geological and oceanic computer models to forecast the tsunami risk. If that risk is small, then world leaders and NEO advisors might opt to let the asteroid proceed.</p><p>On the other hand, if the asteroid is on course to strike a highly populated area, then launching a spacecraft to deflect the asteroid and prevent impact might be warranted.</p><h3>NEO Surveyor </h3><br/><img alt="Diagram of the EM Spectrum Explorer satellite design with shaded components." class="rm-shortcode" data-rm-shortcode-id="a15d11841b516e46c3979de971530464" data-rm-shortcode-name="rebelmouse-image" id="13e5d" loading="lazy" src="https://spectrum.ieee.org/media-library/diagram-of-the-em-spectrum-explorer-satellite-design-with-shaded-components.png?id=60762509&width=980"/><p>NASA’s infrared space telescope has been designed to detect and track near-Earth object (NEO) asteroids that are potentially hazardous to Earth.</p><p><strong>Launch: </strong></p><p>as early as 2027</p><p>Here, lessons from DART are instructive. For one thing, a spacecraft impact can pack only so much punch. It’s unclear whether a deflection spacecraft the size of the DART would be able to nudge a 2024 YR4–like asteroid with enough force to avoid Earth. It’s also possible the impactor’s nudge could inadvertently cause it to land in an even worse spot, inflicting more damage. And if the asteroid is only weakly held together, a DART-like collision might break it into multiple, smaller rubble piles—one or more of which could still reach Earth. So any kind of deflection mission has to be carefully considered.</p><p>Other asteroid defense technologies are also worth considering. These other options are still untested, but we might as well get started, when nothing’s yet at stake.</p><p>If you have decades of lead time, for instance, a rendezvous spacecraft could be dispatched to orbit the killer asteroid and slowly and continually act on it. Researchers have suggested <a href="https://www.planetary.org/articles/asteroid-deflection-techniques-to-save-the-earth" target="_blank">using such a spacecraft’s gravity</a> to tug the asteroid off its path or <a href="https://arc.aiaa.org/doi/10.2514/6.2025-0088" target="_blank">ion-beam engines</a> to gradually push it. The spacecraft could use one or both techniques over the span of years or decades to cause a large enough change in the asteroid’s trajectory to prevent Earth impact.</p><p>But if time is short, there are far fewer options. If the situation is dire enough, with a monster asteroid likely heading for a populated area, then using a nuclear explosive to break up or divert the asteroid could be on the table. That’s the premise of the 1998 blockbuster <a href="https://www.imdb.com/title/tt0120591/" target="_blank"><em>Armageddon</em></a> (as well as the 2021 Netflix satire <a href="https://www.imdb.com/title/tt11286314" target="_blank"><em>Don’t Look Up</em></a>). Absurd, yes, but worth considering if you’re otherwise out of options.</p><p>Of course, the whole idea of planetary defense is to have options and to do as much advance preparation as possible. A number of countries have planetary-defense missions currently in space or planned in the next few years.</p><p>The ESA’s <a href="https://www.heramission.space/" target="_blank">Hera</a> mission launched last year and is on its way to rendezvous late next year with the asteroid system that DART struck, to investigate the aftermath of DART’s 2022 deflection test. The Japanese Aerospace Exploration Agency’s <a href="https://en.wikipedia.org/wiki/Hayabusa2" target="_blank">Hayabusa2</a> is set to fly by an NEO in 2026 and rendezvous with a different asteroid in 2031. It’s the next chapter to JAXA’s original Hayabusa2 mission, which brought back <a href="https://spectrum.ieee.org/japan-prepares-to-welcome-home-asteroid-explorer-hayabusa2" target="_self">samples of the asteroid Ryugu</a> in 2020. China plans to <a href="https://spacenews.com/china-reschedules-planetary-defense-mission-for-2027-launch/" target="_blank">perform a kinetic impactor demonstration</a> similar to DART, with an observer spacecraft to watch, scheduled to launch in 2027.</p><p>And in 2029, a 340-meter asteroid called <a href="https://science.nasa.gov/solar-system/asteroids/apophis/" target="_blank">Apophis</a>—after the Egyptian god of chaos and darkness—will pass within 32,000 km of Earth, which is closer than some geosynchronous satellites. This will happen on 13 April 2029—Friday the 13th, that is. Apophis won’t hit Earth, but its close pass has prompted the U.N. to designate 2029 the <a href="https://www.un.org/en/observances/asteroid-awareness-year" target="_blank">International Year of Asteroid Awareness and Planetary Defense</a>. The asteroid will be bright enough to be seen by the naked eye across parts of Europe, Asia, and Africa. And NASA has redirected its OSIRIS-REx spacecraft (which <a href="https://spectrum.ieee.org/distributed-acoustic-sensing-2671316332" target="_blank">returned samples of the asteroid Bennu</a> to Earth in 2023) to rendezvous with Apophis. The renamed <a href="https://science.nasa.gov/mission/osiris-apex/" target="_blank">OSIRIS-APEX</a> mission will give astronomers an important opportunity to further refine how we measure and characterize NEO asteroids.</p><p>While NEO researchers will continue to collect new data and develop new insights and perspectives, leading toward, we hope, better and stronger planetary defense, one perennial will hold as true in the future as it does today: In this very high-stakes game, you never get to pick the asteroid. The asteroid always picks you. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Wed, 11 Jun 2025 12:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/planetary-defense-killer-asteroids</guid><category>Space exploration</category><category>Asteroid tracking</category><category>Aerospace engineering</category><category>Space technology</category><category>Near-earth objects</category><category>Planetary defense</category><dc:creator>Nancy L. Chabot</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/spacecraft-approaches-large-asteroid-against-starry-background.png?id=60694025&amp;width=980"></media:content></item><item><title>IBM Says It’s Cracked Quantum Error Correction</title><link>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p>IBM has unveiled a new quantum computing architecture it says will slash the number of qubits required for error correction. The advance will underpin its goal of <a href="https://spectrum.ieee.org/ibm-quantum-computer-2668978269" target="_blank">building a large-scale, fault-tolerant quantum computer</a>, called Starling, that will be available to customers by 2029.</p><p>Because of the inherent unreliability of the qubits (the quantum equivalent of bits) that quantum computers are built from, error correction will be crucial for building reliable, large-scale devices. <a data-linked-post="2650272311" href="https://spectrum.ieee.org/google-tests-first-error-correction-in-quantum-computing" target="_blank">Error-correction approaches</a> spread each unit of information across many physical qubits to create “logical qubits.” This provides redundancy against errors in individual physical qubits.</p><p>One of the most popular approaches is known as a surface code, which requires roughly 1,000 physical qubits to make up one logical qubit. This was the approach IBM focused on initially, but the company eventually realized that creating the hardware to support it was an “engineering pipe dream,” <a href="https://research.ibm.com/people/jay-gambetta" target="_blank">Jay Gambetta</a>, the vice president of IBM Quantum, said in a press briefing.</p><p>Around 2019, the company began to investigate alternatives. In a <a href="https://www.nature.com/articles/s41586-024-07107-7" target="_blank">paper</a> published in <em>Nature</em> last year, IBM researchers outlined a new error-correction scheme called quantum low-density parity check (qLDPC) codes that would require roughly one-tenth of the number of qubits that surface codes need. Now, the company <a href="https://newsroom.ibm.com/2025-06-10-IBM-Sets-the-Course-to-Build-Worlds-First-Large-Scale,-Fault-Tolerant-Quantum-Computer-at-New-IBM-Quantum-Data-Center" target="_blank">has unveiled a new quantum-computing architecture</a> that can realize this new approach.</p><p>“We’ve cracked the code to quantum error correction and it’s our plan to build the first large-scale, fault-tolerant quantum computer,” said Gambetta, who is also an IBM Fellow. “We feel confident it is now a question of engineering to build these machines, rather than science.”</p><h2>IBM Unveils New Quantum Roadmap</h2><p>IBM will take the first step towards realizing this architecture later this year with a processor called Loon. This chip will feature couplers that can connect distant qubits on the same chip, which is key for implementing qLDPC codes. These “non-local” interactions are what make the approach more efficient than the surface code, which relies solely on qubits communicating with their neighbors.</p><p>According to <a href="https://www.ibm.com/downloads/documents/us-en/131cf87ab63319bf" rel="noopener noreferrer" target="_blank">a roadmap</a> released alongside details of the new architecture, the company plans to build a follow-on processor called Kookaburra in 2026 that will feature both a logical processing unit and a quantum memory. This will be the first demonstration of the kind of base module that subsequent systems will be built from. The following year IBM plans to link two of these modules together to create a device called Cockatoo.</p><p>The road map doesn’t detail how many modules will be used to create Starling, IBM’s planned commercial offering, but the computer will feature 200 logical qubits and be capable of running 100 million quantum operations. Exactly how many physical qubits will be required is yet to be finalized, said <a href="https://research.ibm.com/people/matthias-steffen" target="_blank">Matthias Steffen</a>, IBM Fellow, who leads the quantum-processor technology team. But the new architecture is likely to require on the order of several hundred physical qubits to create 10 logical qubits, he added.</p><p>IBM plans to build Starling by 2028, before making it available on the cloud the following year. It will be housed in a new quantum data center in Poughkeepsie, N.Y., and will lay the foundations for the final system on IBM’s current road map, a 2,000 logical qubit machine codenamed Blue Jay.</p><p>IBM’s new architecture is a significant advance over its previous technology, says <a href="https://www.gartner.com/en/experts/mark-horvath" target="_blank">Mark Horvath</a>, a vice president analyst at Gartner, who was briefed in advance of the announcement. The new chip’s increased connectivity makes it substantially more powerful and is backed up by significant breakthroughs in 3D fabrication. And if it helps IBM reach 200 logical qubits, that would bring quantum computers into the realm of solving practical problems, Horvath says.</p><p>However, Horvath adds that the modular approach IBM is banking on to get there could prove challenging. “That’s a very complicated task,” he says. “I think it will eventually work. It’s just, it’s a lot further off than people think it is.”</p><p>One of biggest remaining hurdles is improving gate fidelities across the device. To successfully implement this new architecture, error rates need to come down by an order of magnitude, admitted IBM’s Steffen, though the company is confident this is achievable. One of the main paths forward will be to improve the coherence times of the underlying qubits, which refers to how long they can maintain their quantum state. “We do have evidence that this is really one of the main bottlenecks to improving gate errors,” Steffen says.</p><p>In isolated test devices, IBM has managed to push average coherence times to 2 milliseconds but translating that to larger chips is not straightforward. Steffen said the company recently made progress with its Heron chips, going from around 150 to 250 microseconds.</p><p>Significant engineering challenges remain in supporting infrastructure as well, said Steffen, including connectors that link together different parts of the system and amplifiers. But a big advantage of the new architecture is that it requires far fewer components due to the reduced number of physical qubits. “This is one of the reasons why we’re so excited about these qLDPC codes, because it also reduces all of the nonquantum-processor overhead,” he says.<br/></p><p><em>This story was updated on 10 June 2025 to correct some details of IBM’s current roadmap.</em><br/></p>]]></description><pubDate>Tue, 10 Jun 2025 14:00:10 +0000</pubDate><guid>https://spectrum.ieee.org/ibm-quantum-error-correction-starling</guid><category>Quantum computing</category><category>Ibm</category><category>Error correction</category><category>Quantum computers</category><category>Qubits</category><dc:creator>Edd Gent</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/complex-circuit-pattern-with-blue-squares-and-red-lines-on-a-white-background.jpg?id=60760877&amp;width=980"></media:content></item><item><title>Navigating the Dual-Use Dilemma</title><link>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&width=1200&height=800&coordinates=70%2C0%2C70%2C0"/><br/><br/><p>Open-source technology developed in the civilian sector has the capacity to also be used in military applications or be simply misused. Navigating this <a href="https://link.springer.com/article/10.1007/s11948-009-9159-9" rel="noopener noreferrer" target="_blank">dual-use</a> potential is becoming more important across engineering fields, as innovation goes both ways. While the “openness” of open-source technology is part of what drives innovation and allows everyone access, it also, unfortunately, means it’s just as easily accessible to others, including the military and criminals.</p><p>What happens when a rogue state, a nonstate militia, or a school shooter displays the same creativity and innovation with open-source technology that engineers do? This is the question we are discussing here: How can we uphold our principles of open research and innovation to drive progress while mitigating the inherent risks that come with accessible technology?</p><p>More than just open-ended risk, let’s discuss the specific challenges open-source technology and its dual-use potential have on robotics. Understanding these challenges can help engineers learn what to look for in their own disciplines.</p><h2>The Power and Peril of Openness</h2><p>Open-access publications, software, and educational content are fundamental to advancing robotics. They have democratized access to knowledge, enabled reproducibility, and fostered a vibrant, collaborative international community of scientists. Platforms like arXiv and GitHub and open-source initiatives like the <a href="https://www.ros.org/" rel="noopener noreferrer" target="_blank">Robot Operating System</a> (ROS) and the <a href="https://github.com/open-dynamic-robot-initiative/" rel="noopener noreferrer" target="_blank">Open Dynamic Robot Initiative</a> have been pivotal in accelerating robotics research and innovation, and there is no doubt that they should remain openly accessible. Losing access to these resources would be devastating to the robotics field.</p><p>However, robotics carries inherent dual-use risks since most robotics technology can be repurposed <a href="https://spectrum.ieee.org/autonomous-weapons-challenges" target="_blank">for military use</a> or <a href="https://spectrum.ieee.org/why-you-should-fear-slaughterbots-a-response" target="_blank">harmful purposes</a>. One recent example of custom-made drones in current conflicts is particularly insightful. The resourcefulness displayed by Ukrainian soldiers in repurposing and sometimes <a href="https://www.cnas.org/publications/reports/evolution-not-revolution" rel="noopener noreferrer" target="_blank">augmenting civilian drone technology</a> received worldwide, often admiring, news coverage. Their creativity has been made possible through the affordability of commercial drones, spare parts, 3D printers, and the availability of open-source software and hardware. This allows people with little technological background and money to easily create, control, and repurpose robots for military applications. One can certainly argue that this has had an empowering effect on Ukrainians defending their country. However, these same conditions also present opportunities for a wide range of potential bad actors.</p><p>Openly available knowledge, designs, and software can be misused to enhance existing weapons systems with capabilities like vision-based <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/1758-5899.12663" rel="noopener noreferrer" target="_blank">navigation, autonomous targeting, or swarming</a>. Additionally, unless proper security measures are taken, the public nature of open-source code makes it vulnerable to cyberattacks, potentially allowing malicious actors to gain control of robotic systems and cause them to malfunction or be used for <a href="https://www.sciencedirect.com/science/article/pii/S2667305323000625" rel="noopener noreferrer" target="_blank">malevolent purposes</a>. Many ROS users already recognize that they do not invest enough in <a href="https://aliasrobotics.com/files/robot_cybersecurity_review.pdf" rel="noopener noreferrer" target="_blank">cybersecurity</a> for their applications.</p><h2>Guidance Is Necessary</h2><p>Dual-use risks stemming from openness in research and innovation are a concern for many engineering fields. Did you know that engineering was originally a military-only activity? The word “engineer” was coined in the Middle Ages to describe “a designer and constructor of fortifications and weapons.” Some engineering specializations, especially those that include the development of weapons of mass destruction (chemical, biological, radiological, and nuclear), have developed clear guidance, and in some cases, regulations for how research and innovation can be conducted and disseminated. They also have community-driven processes intended to mitigate dual-use risks associated with spreading knowledge. For instance, BioRxiv and MedRxiv—the preprint servers for biology and health sciences—screen submissions for material that poses a biosecurity or health risk before publishing them.</p><p>The field of robotics, in comparison, offers no specific regulation and little guidance as to how roboticists should think of and address the risks associated with openness. Dual-use risk is not taught in most universities, despite it being something that students will likely face in their careers, such as when assessing whether their work is subject to <a href="https://www.sipri.org/publications/2020/policy-reports/responsible-artificial-intelligence-research-and-innovation-international-peace-and-security" rel="noopener noreferrer" target="_blank">export-control regulations on dual-use items</a>.</p><p>As a result, roboticists may not feel they have an incentive or are equipped to evaluate and mitigate the dual-use risks associated with their work. This represents a major problem, as the likelihood of harm associated with the misuse of open robotic research and innovation is likely higher than that of nuclear and biological research, both of which require significantly more resources. Producing “do-it-yourself” robotic weapon systems using open-source design and software and off-the-shelf commercial components is relatively easy and accessible. With this in mind, we think that it’s high time for the robotics community to work toward its own set of sector-specific guidance for how researchers and companies can best navigate the dual-use risks associated with the open diffusion of their work.</p><h2>A Road Map for Responsible Robotics</h2><p>Striking a balance between security and openness is a complex challenge, but one that the robotics community must embrace. We cannot afford to stifle innovation, nor can we ignore the potential for harm. A proactive, multipronged approach is needed to navigate this dual-use dilemma. Drawing lessons from other fields of engineering, we propose a road map focusing on four key areas: education, incentives, moderation, and red lines.</p><h3>Education</h3><p>Integrating responsible research and innovation into robotics education at all levels is paramount. This includes not only dedicated courses but also the <a href="https://journals.uclpress.co.uk/lre/article/id/129/" rel="noopener noreferrer" target="_blank">systematic inclusion</a> of dual-use and cybersecurity considerations within core <a href="https://link.springer.com/article/10.1007/s11948-019-00164-6" rel="noopener noreferrer" target="_blank">robotics curricula</a>. We must foster a culture of responsible innovation so that we can empower roboticists to make informed decisions and proactively address potential risks.</p><p>Educational initiatives could include:</p><ul><li>Developing and disseminating open-source educational materials on responsible robotics for robotics teachers, researchers, and professionals from resources such as the <a href="https://disarmament.unoda.org/responsible-innovation-ai/resources/" rel="noopener noreferrer" target="_blank">United Nations Office for Disarmament Affairs</a> (UNODA) and the <a href="https://airesponsibly.net/education/" rel="noopener noreferrer" target="_blank">Center for Responsible AI</a> at New York University. </li><li>Organizing workshops and seminars on dual-use and ethical considerations at robotics conferences and universities.</li><li>Encouraging universities to offer courses or modules dedicated to <a href="https://journals.sagepub.com/doi/10.1177/20539517231219958" rel="noopener noreferrer" target="_blank">responsible research and innovation in robotics</a>.</li></ul><h3>Incentives</h3><p>Everyone should be encouraged to assess the potential negative consequences of making their work fully or partially open. Funding agencies can mandate risk assessments as a condition for project funding, signaling their importance. Professional organizations, like the <a href="https://www.ieee-ras.org/" rel="noopener noreferrer" target="_blank">IEEE Robotics and Automation Society</a> (RAS), can adopt and promote <a href="https://www.ieee-ras.org/industry-government/standards" rel="noopener noreferrer" target="_blank">best practices</a>, providing tools and frameworks for researchers to identify, assess, and mitigate risks. Such tools could include self-assessment checklists for individual researchers and guidance for how faculties and labs can set up ethical review boards. Academic journals and conferences can make peer-review risk assessments an integral part of the publication process, especially for high-risk applications.</p><p>Additionally, incentives like awards and recognition programs can highlight exemplary contributions to risk assessment and mitigation, fostering a culture of responsibility within the community. Risk assessment can also be encouraged and rewarded in more informal ways. People in leadership positions, such as Ph.D. supervisors and heads of labs, could build ad hoc opportunities for students and researchers to discuss possible risks. They can hold seminars on the topic and provide introductions to external experts and stakeholders like social scientists and experts from NGOs.</p><h3>Moderation</h3><p>The robotics community can implement <a href="https://dl.acm.org/doi/10.1145/3593013.3593981" rel="noopener noreferrer" target="_blank">self-regulation mechanisms</a> to moderate the diffusion of high-risk material. This could involve:</p><ul><li>Screening work prior to publication to prevent the dissemination of content posing serious risks.</li><li>Implementing graduated access controls (“gating”) to certain source code or data on open-source repositories, potentially requiring users to identify themselves and specify their intended use.</li><li>Establishing clear guidelines and community oversight to ensure transparency and prevent misuse of these moderation mechanisms. For example, organizations like RAS could design categories of risk levels for robotics research and applications and create a monitoring committee to track and document real cases of the misuse of robotics research to understand and visualize the scale of the risks and create better mitigation strategies.</li></ul><h3>Red Lines</h3><p>The robotics community should also seek to define and enforce red lines for the development and deployment of robotics technologies. Efforts to define red lines have already been made in that direction, notably in the context of the <a href="https://standards.ieee.org/industry-connections/ec/autonomous-systems/" rel="noopener noreferrer" target="_blank">IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</a>. Companies, including <a href="https://bostondynamics.com/" rel="noopener noreferrer" target="_blank">Boston Dynamics</a>, <a href="https://www.unitree.com/" rel="noopener noreferrer" target="_blank">Unitree</a>, <a href="https://www.agilityrobotics.com/" rel="noopener noreferrer" target="_blank">Agility Robotics</a>, <a href="https://clearpathrobotics.com/" rel="noopener noreferrer" target="_blank">Clearpath Robotics</a>, <a href="https://www.anybotics.com/" rel="noopener noreferrer" target="_blank">ANYbotics</a>, and <a href="https://www.openrobotics.org/" rel="noopener noreferrer" target="_blank">Open Robotics</a> wrote an open letter calling for regulations on the <a href="https://bostondynamics.com/news/general-purpose-robots-should-not-be-weaponized/" rel="noopener noreferrer" target="_blank">weaponization of general-purpose robots</a>. Unfortunately, their efforts were very narrow in scope, and there is a lot of value in further mapping end uses of robotics that should be deemed off-limits or demand extra caution.</p><p>It will absolutely be difficult for the community to agree on standard red lines, because what is considered ethically acceptable or problematic is highly subjective. To support the process, individuals and companies can reflect on what they consider to be unacceptable use of their work. This could result in policies and terms of use that beneficiaries of open research and open-source design software would have to formally agree to (such as specific-use open-source licenses). This would provide a basis for revoking access, denying software updates, and potentially suing or blacklisting people who misuse the technology. Some companies, including Boston Dynamics, have already implemented these measures to some extent. Any person or company conducting open research could replicate this example.</p><p>Openness is the key to innovation and the democratization of many engineering disciplines, including robotics, but it also amplifies the potential for misuse. The engineering community has a responsibility to proactively address the dual-use dilemma. By embracing responsible practices, from education and risk assessment to moderation and red lines, we can foster an ecosystem where openness and security coexist. The challenges are significant, but the stakes are too high to ignore. It is crucial to ensure that research and innovation benefit society globally and do not become a driver of instability in the world. This goal, we believe, aligns with the mission of the IEEE, which is to “advance technology for the benefit of humanity.” The engineering community, especially roboticists, needs to be proactive on these issues to prevent any backlash from society and to preempt potentially counterproductive measures or international regulations that could harm open science.</p>]]></description><pubDate>Tue, 10 Jun 2025 13:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/navigating-the-dual-use-dilemma</guid><category>Robotics</category><category>Guest articles</category><category>Dual-use</category><dc:creator>Vincent Boulanin</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/robotic-arm-holding-a-scalpel-merging-into-a-digital-blueprint-on-a-black-and-white-background.png?id=60656210&amp;width=980"></media:content></item><item><title>IEEE’s 5 New E-Books Provide On-ramp to Engineering</title><link>https://spectrum.ieee.org/tryengineering-5-new-ebooks</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&width=1200&height=800&coordinates=140%2C0%2C140%2C0"/><br/><br/><p>As the home for IEEE’s preuniversity resources, activities, and hands-on experiences, <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering</a> serves as a hub for educators, parents, and IEEE volunteers to teach school-age children about engineering.</p><p>With support from IEEE partners, TryEngineering has launched a series of <a href="https://tryengineering.org/news/discover-tryengineerings-ebooks/" rel="noopener noreferrer" target="_blank">e-books</a>. Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way. Visually appealing, the books use colorful charts and graphs to grab children’s attention.</p><p>Each of the five English-language publications provides an overview of a technology or topic. The books include stories about <a href="https://spectrum.ieee.org/topic/careers/" target="_self">engineers</a>, technologists, and early pioneers.</p><h2>Engineering disciplines, solutions, and ethics</h2><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2023/" rel="noopener noreferrer" target="_blank"><em><em>Engineers Make the World a Better Place</em></em></a> was created with funding from the <a href="https://www.ieee.org/about/corporate/initiatives/initiatives-committee.html" rel="noopener noreferrer" target="_blank">IEEE New Initiatives Committee</a>. The book introduces students to engineering disciplines and explains how engineers improve society by solving challenging problems, such as<a href="https://tryengineering.org/news/high-school-students-modify-ride-on-cars-for-disabled-children/" rel="noopener noreferrer" target="_blank"> improving access for children with limited physical mobility</a>.</p><p>With support from <a href="https://www.onsemi.com/" rel="noopener noreferrer" target="_blank">Onsemi</a>’s <a href="https://spectrum.ieee.org/ieee-tryengineering-onsemi-grant" target="_self">Giving Now</a> program, IEEE <a href="https://spectrum.ieee.org/topic/semiconductors/" target="_self">semiconductor</a> experts wrote <a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_2024/" rel="noopener noreferrer" target="_blank"><em><em>Microchip Adventures: A Journey Into the World of Semiconductors</em></em></a>. It includes an introduction to the field, a list of commonly used terms, an explanation of how chips are made, and an overview of the <a href="https://spectrum.ieee.org/topic/tech-history/" target="_self">technology’s history</a>.</p><p class="pull-quote">Bolstered by input from IEEE members who are experts in their field, the e-books use open-source, free materials written to teach complex engineering topics in an age-appropriate way.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_signalprocessing_2025/" rel="noopener noreferrer" target="_blank"><em>Wave Wonders: A Signal Processing Journey</em></a> was written with experts from the <a href="https://signalprocessingsociety.org/" rel="noopener noreferrer" target="_blank">IEEE Signal Processing Society</a>. It teaches students how to tell the difference between digital and analog signals. The e-book introduces readers to the inventor of the <a href="https://spectrum.ieee.org/the-first-transatlantic-telegraph-cable-was-a-bold-beautiful-failure" target="_self">telegraph</a>, <a href="https://www.britannica.com/biography/Samuel-F-B-Morse" rel="noopener noreferrer" target="_blank">Samuel Morse</a>. Also included is the <a href="https://tryengineering.org/resource/lesson-plan/electric-messages-then-and-now/" rel="noopener noreferrer" target="_blank">Electric Messages lesson plan</a>, which explains how early telegraphs worked.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_oceanengineering/" target="_blank"><em><em>Ocean Engineering Heroes: Making the Oceans and the World a Better Place</em></em></a> was created in partnership with the <a href="https://ieeeoes.org/" rel="noopener noreferrer" target="_blank">IEEE Oceanic Engineering Society</a>. It includes video interviews with several society leaders about oceans and ways to help keep them clean. It also discusses the impact of pollution including sound pollution from ships and other sources. Included are links to resources from other organizations such as the <a href="https://www.noaa.gov/" rel="noopener noreferrer" target="_blank">National Oceanic and Atmospheric Administration</a>.</p><p><a href="https://www.nxtbook.com/nxtbooks/ieee/tryengineering_aiadventures/" rel="noopener noreferrer" target="_blank"><em><em>AI Adventures: Exploring the World of Artificial Intelligence</em></em></a> was written with assistance from the <a href="https://www.computer.org/" rel="noopener noreferrer" target="_blank">IEEE Computer Society</a>. The publication describes how <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">AI</a> models work and explains commonly used terms including <em><em>machine learning</em></em> and <em><em>neural networks</em></em>. The book covers the importance of <a href="https://spectrum.ieee.org/ai-ethics-advice" target="_self">ethics when using AI</a>.</p><p>Visit the <a href="https://tryengineering.org/" rel="noopener noreferrer" target="_blank">TryEngineering website</a> for the e-books and many other resources for educators, parents, and volunteers. To help expand the site’s pool of offerings, consider donating to the <a href="https://secure.ieeefoundation.org/site/Donation2?df_id=1720&mfc_pref=T&1720.donation=form1" rel="noopener noreferrer" target="_blank">IEEE TryEngineering Fund</a>.</p>]]></description><pubDate>Mon, 09 Jun 2025 18:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/tryengineering-5-new-ebooks</guid><category>Artificial intelligence</category><category>Education</category><category>Ieee products and services</category><category>Ocean engineering</category><category>Signal processing</category><category>Students</category><category>Type:ti</category><dc:creator>Debra Gulick</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/three-laptop-computers-against-a-colorful-geometric-background-each-devices-screen-displays-a-different-educational-e-book-fro.jpg?id=60680838&amp;width=980"></media:content></item><item><title>Doctors Could Hack the Nervous System With Ultrasound</title><link>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&width=1200&height=800&coordinates=0%2C800%2C0%2C800"/><br/><br/><p><strong>Inflammation: It’s the body’s</strong> natural response to injury and infection, but medical science now recognizes it as a double-edged sword. When inflammation becomes chronic, it can contribute to a host of serious health problems, including arthritis, heart disease, and certain cancers. As this understanding has grown, so too has the search for effective ways to manage harmful inflammation.</p><p>Doctors and researchers are exploring various approaches to tackle this pervasive health issue, from new medications to dietary interventions. But what if one of the most promising treatments relies on a familiar technology that’s been in hospitals for decades?</p><p>Enter <a href="https://www.sciencedirect.com/science/article/abs/pii/S0165027020301448" target="_blank">focused ultrasound stimulation</a> (FUS), a technique that uses sound waves to reduce inflammation in targeted areas of the body. It’s a surprising new application for ultrasound technology, which most people associate with prenatal checkups or diagnostic imaging. And FUS may help with many other disorders too, including diabetes and obesity. By modifying existing ultrasound technology, we might be able to offer a novel approach to some of today’s most pressing health challenges.</p><p>Our team of biomedical researchers at the <a href="https://feinstein.northwell.edu/institutes-researchers/bioelectronic-medicine" target="_blank">Institute of Bioelectronic Medicine</a> (part of the Feinstein Institutes for Medical Research), in Manhasset, N.Y., has made great strides in learning the electric language of the nervous system. Rather than treating disease with drugs that can have broad side effects throughout the body, we’re learning how to stimulate nerve cells, called neurons, to intervene in a more targeted way. Our goal is to activate or inhibit specific functions within organs.</p><p>The relatively new application of FUS for <a href="https://www.neuromodulation.com/about-neuromodulation" target="_blank">neuromodulation</a>, in which we hypothesize that sound waves activate neurons, may offer a precise and safe way to provide healing treatments for a wide range of both acute and chronic maladies. The treatment doesn’t require surgery and potentially could be used at home with a wearable device. People are accustomed to being prescribing pills for these ailments, but we imagine that one day, the prescriptions could be more like this: “Strap on your ultrasound belt once per day to receive your dose of stimulation.”</p><h2>How Ultrasound Stimulation Works</h2><p><a href="https://spectrum.ieee.org/mems-ultrasound-history" target="_self">Ultrasound</a> is a time-honored medical technology. Researchers began experimenting with ultrasound imaging in the 1940s, bouncing low-energy ultrasonic waves off internal organs to construct medical images, typically using intensities of a few hundred milliwatts per square centimeter of tissue. By the late 1950s, some doctors were using the technique to show expectant parents the <a href="https://en.wikipedia.org/wiki/Obstetric_ultrasonography" target="_blank">developing fetus inside the mother’s uterus</a>. And high-intensity ultrasound waves, which can be millions of milliwatts per square centimeter, have a variety of therapeutic uses, including <a href="https://my.clevelandclinic.org/health/treatments/16541-hifu-high-intensity-focused-ultrasound" target="_blank">destroying tumors</a>.</p><p>The use of low-intensity ultrasound (with intensities similar to that of imaging applications) to alter the activity of the nervous system, however, is relatively unexplored territory. To understand how it works, it’s helpful to compare FUS to the most common form of neuromodulation today, which uses electric current to alter the activity of neurons to treat conditions like Parkinson’s disease. In that technique, electric current increases the voltage inside a neuron, causing it to “fire” and release a neurotransmitter that’s received by connected neurons, which triggers those neurons to fire in turn. For example, the deep brain stimulation used to treat Parkinson’s activates certain neurons to restore healthy patterns of brain activity.</p><h3>How It Works</h3><br/><img alt="Neuron impulse transmission showing ion flow through cell membrane." class="rm-shortcode" data-rm-shortcode-id="7e69502283206bf183e56ebf15b850d4" data-rm-shortcode-name="rebelmouse-image" id="d2e57" loading="lazy" src="https://spectrum.ieee.org/media-library/neuron-impulse-transmission-showing-ion-flow-through-cell-membrane.png?id=60681884&width=980"/><h3></h3><br/><p><strong></strong>In FUS, by contrast, the sound waves’ vibrations interact with the membrane of the neuron, <a href="https://www.nature.com/articles/s41467-022-28040-1" target="_blank">opening channels</a> that allow ions to flow into the cell, thus indirectly changing the cell’s voltage and causing it to fire. One promising use is <a href="https://doi.org/10.1016/j.clinph.2021.12.010" target="_blank">transcranial ultrasound stimulation</a>, which is being tested extensively as a noninvasive way to stimulate the brain and treat neurological and psychiatric diseases.</p><p>We’re interested in FUS’s effect on the peripheral nerves—that is, the nerves outside the brain and spinal cord. We think that activating specific nerves in the abdomen that regulate inflammation or metabolism may help address the root causes of related diseases, rather than just treating the symptoms.</p><h2>FUS for Inflammation</h2><p>Inflammation is something that we know a lot about. Back in 2002, <a href="https://feinstein.northwell.edu/institutes-researchers/our-researchers/kevin-j-tracey-md" target="_blank">Kevin Tracey</a>, currently the president and CEO of the Feinstein Institutes, upset the conventional wisdom that the nervous system and the immune system operate independently and serve distinct roles. He discovered the body’s <a href="https://www.nature.com/articles/nature01321" rel="noopener noreferrer" target="_blank">inflammatory reflex</a>: a two-way neural circuit that sends signals between the brain and body via the vagus nerve and the nerves of the spleen. These nerves control the release of <a href="https://en.wikipedia.org/wiki/Cytokine" rel="noopener noreferrer" target="_blank">cytokines</a>, which are proteins released by immune cells to trigger inflammation. Tracey and colleagues found that stimulating nerves in this neural circuit suppressed the inflammatory response. The discoveries led to the first clinical trials of electrical neuromodulation devices to treat chronic inflammation and launched the field of bioelectronic medicine.</p><h3>Hacking the Immune System</h3><br/><img alt="Ultrasound transducer scans kidney, showing bacteria spreading and evading immune response." class="rm-shortcode" data-rm-shortcode-id="f79f5c1b9ad1a15af4a3d7e98b7b7946" data-rm-shortcode-name="rebelmouse-image" id="7e37e" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-transducer-scans-kidney-showing-bacteria-spreading-and-evading-immune-response.png?id=60559935&width=980"/><h3></h3><br/><p>Tracey has been a pioneer in treating inflammation with <a href="https://spectrum.ieee.org/the-vagus-nerve-a-back-door-for-brain-hacking" target="_self">vagus nerve stimulation</a> (VNS), in which electrical stimulation of the vagus nerve activates neurons in the spleen. In animals and humans, VNS has been shown to reduce harmful inflammation in both chronic diseases such as arthritis and acute conditions such as sepsis. But direct VNS requires surgery to place an implant in the body, which makes it risky for the patient and expensive. That’s why we’ve pursued noninvasive ultrasound stimulation of the spleen.</p><p>Working with Tracey, collaborators at <a href="https://www.gehealthcare.com/" target="_blank">GE Research</a>, and others, we first experimented with rodents to show that ultrasound stimulation of the spleen <a href="https://www.nature.com/articles/s41467-019-08750-9" rel="noopener noreferrer" target="_blank">affects an anti-inflammatory pathway</a>, just as VNS does, and reduces cytokine production as much as a VNS implant does. We then conducted the first-in-human trial of <a href="https://feinstein.northwell.edu/news/the-latest/non-invasive-ultrasound-stimulation-spleen-reduces-inflammation-in-humans-new-study" target="_blank">FUS for controlling inflammation</a>.</p><p>We initially enrolled 60 healthy people, none of whom had signs of chronic inflammation. To test the effect of a 3-minute ultrasound treatment, we were measuring the amount of a molecule called <a href="https://en.wikipedia.org/wiki/Tumor_necrosis_factor" rel="noopener noreferrer" target="_blank">tumor necrosis factor</a> (TNF), which is a biomarker of inflammation that’s released when white blood cells go into action against a perceived pathogen. At the beginning of the study, 40 people received focused ultrasound stimulation, while 20 others, serving as the control group, simply had their spleens imaged by ultrasound. Yet, when we looked at the early data, <em>everyone</em> had lower levels of TNF, even the control group. It seemed that even imaging with ultrasound for a few minutes had a moderate anti-inflammatory effect! To get a proper control group, we had to recruit 10 more people for the study and devise a different sham experiment, this time unplugging the ultrasound machine.</p><h3></h3><br/><img alt="Abstract collage with neuron, brain textures, and dynamic wave patterns in pastel colors." class="rm-shortcode" data-rm-shortcode-id="2ae16b235445ec02f72e63b3c909cc7a" data-rm-shortcode-name="rebelmouse-image" id="f166b" loading="lazy" src="https://spectrum.ieee.org/media-library/abstract-collage-with-neuron-brain-textures-and-dynamic-wave-patterns-in-pastel-colors.jpg?id=60560952&width=980"/><h3></h3><br/><p>After the subjects received either the real or sham stimulation, we took blood samples from all of them. We next simulated an infection by adding a bacterial toxin to the blood in the test tubes, then measured the amount of TNF released by the white blood cells to fight the toxin. The results, which we published in the journal <em>Brain Stimulation</em> in 2023, showed that people who had received FUS treatments <a href="https://www.sciencedirect.com/science/article/pii/S1935861X23017436" target="_blank">had lower levels of TNF</a> than the true control group. We saw no problematic side effects of the ultrasound: The treatment didn’t adversely affect heart rate, blood pressure, or the many other biomarkers that we checked.</p><p>The results also showed that when we repeated the blood draw and experiment 24 hours later, the treatment groups’ TNF levels had returned to baseline. This finding suggests that if FUS becomes a treatment option for inflammatory diseases, people might require regular, perhaps even daily, treatments.</p><p>One surprising result was that it didn’t seem to matter which location within the spleen we targeted—all the locations we tried produced similar results. Our hypothesis is that hitting any target within the spleen activates enough nerves to produce the beneficial effect. What’s more, it didn’t matter which energy intensity we used. We tried intensities ranging from about 10 to 200 mW per cm<sup>2</sup>, well within the range of intensities used in ultrasound imaging; remarkably, even the lowest intensity level caused subjects’ TNF levels to drop.</p><p>Our big takeaway from that first-in-human study was that targeting the spleen with FUS is not just a feasible treatment but could be a gamechanger for inflammatory diseases. Our next steps are to investigate the mechanisms by which FUS affects the inflammatory response, and to conduct more animal and human studies to see whether prolonged administration of FUS to the spleen can treat chronic inflammatory diseases.</p><h2>FUS for Obesity and Diabetes</h2><p>For much of our research on FUS, we’ve partnered with GE Research, whose parent company is one of the world’s leading makers of ultrasound equipment. One of our first projects together explored the potential of FUS as a treatment for the widespread inflammation that often accompanies obesity, a condition that now affects about <a href="https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight" target="_blank">890 million people</a> around the world. In this study, we fed lab mice a high-calorie and high-fat “Western diet” for eight weeks. During the following eight weeks, half of them received ultrasound stimulation while the other half received daily sham stimulation. We found that the <a href="https://feinstein.northwell.edu/news/the-latest/feinstein-institutes-and-ge-research-demonstrate-ultrasound-stimulation-reduces-obesity" target="_blank">mice that received FUS had lower levels of cytokines</a>—and to our surprise, those mice also ate less and lost weight.</p><p>In related work with our GE colleagues, we examined the potential of FUS as a <a href="https://www.northwell.edu/news/the-latest/research-team-treats-diabetes-using-ultrasound" rel="noopener noreferrer" target="_blank">treatment for diabetes</a>, which now affects <a href="https://www.who.int/news-room/fact-sheets/detail/diabetes" rel="noopener noreferrer" target="_blank">830 million people</a> around the world. In a healthy human body, the liver stores glucose as a reserve and releases it only when it registers that glucose levels in the bloodstream have dropped. But in people with diabetes, this sensing system is dysfunctional, and the liver releases glucose even when blood levels are already high, causing a host of health problems.</p><h3>Hacking the Metabolic System</h3><br/><img alt="Ultrasound scan diagram showing brain-liver connection through neural pathways." class="rm-shortcode" data-rm-shortcode-id="1c7dde5728bf4fc1d4f0d8460e4cde13" data-rm-shortcode-name="rebelmouse-image" id="257de" loading="lazy" src="https://spectrum.ieee.org/media-library/ultrasound-scan-diagram-showing-brain-liver-connection-through-neural-pathways.png?id=60681922&width=980"/><p><span>For diabetes, our ultrasound target was the network of nerves that transmit signals between the liver and the brain: specifically, glucose-sensing neurons in the </span><a href="https://en.wikipedia.org/wiki/Porta_hepatis" target="_blank">porta hepatis</a><span>, which is essentially the gateway to the liver. We gave diabetic rats 3-minute daily ultrasound stimulation over a period of 40 days. Within just a few days, the treatment <a href="https://www.nature.com/articles/s41551-022-00870-w" target="_blank">brought down the rats’ glucose levels</a> from dangerously high to normal range. We got similar results in mice and pigs, and </span>published these exciting results<span> in 2022 in </span><em>Nature Biomedical Engineering</em><span>.</span></p><p>Those diabetes experiments shed some light on why ultrasound had this effect. We decided to zero in on a brain region called the <a href="https://en.wikipedia.org/wiki/Hypothalamus" target="_blank">hypothalamus</a>, which controls many crucial automatic body functions, including metabolism, circadian rhythms, and body temperature. Our colleagues at GE Research started investigating by blocking the nerve signals that travel from the liver to the hypothalamus in two different ways—both cutting the nerves physically and using a local anesthetic. When we then applied FUS, we didn’t see the beneficial decrease in glucose levels. This result suggests that the ultrasound treatment works by changing glucose-sensing signals that travel from the liver to the brain—which in turn changes the commands the hypothalamus issues to the metabolic systems of the body, essentially telling them to lower glucose levels.</p><p>The next steps in this research involve both technical development and clinical testing. Currently, administering FUS requires technical expertise, with a sonographer looking at ultrasound images, locating the target, and triggering the stimulation. But if FUS is to become a practical treatment for a chronic disease, we’ll need to make it usable by anyone and available as an at-home system. That could be a wearable device that uses ultrasound imaging to automatically locate the anatomical target and then delivers the FUS dose: All the patient would have to do is put on the device and turn it on. But before we get to that point, FUS treatment will have to be tested clinically in randomized controlled trials for people with obesity and diabetes. GE HealthCare recently <a href="https://www.gehealthcare.com/about/newsroom/press-releases/ge-healthcare-and-novo-nordisk-to-collaborate-to-advance-novel-non-invasive-treatment-for-type-2-diabetes-and-obesity-with-ultrasound" target="_blank">partnered</a> with Novo Nordisk to work on the clinical and product development of FUS in these areas.</p><h2>FUS for Cardiopulmonary Diseases</h2><p>FUS may also help with chronic cardiovascular diseases, many of which are associated with immune dysfunction and inflammation. We began with a disorder called <a href="https://en.wikipedia.org/wiki/Pulmonary_hypertension" target="_blank">pulmonary arterial hypertension</a>, a rare but incurable disease in which blood pressure increases in the arteries within the lungs. At the start of our research, it wasn’t clear whether inflammation around the pulmonary arteries was a cause or a by-product of the disease, and whether targeting inflammation was a viable treatment. Our group was the first to try FUS of the spleen in order to reduce the inflammation associated <a href="https://feinstein.northwell.edu/feinstein-institutes-ultrasound-neuromodulation-for-pulmonary-hypertension" target="_blank">with pulmonary hypertension</a> in rats.</p><p>The results, published last year, were very encouraging. We found that 12-minute FUS sessions <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCRESAHA.123.323679" target="_blank">reduced pulmonary pressure</a>, improved heart function, and reduced lung inflammation in the animals in the experimental group (as compared to animals that received sham stimulation). What’s more, in the animals that received FUS, the progression of the disease slowed significantly even after the experiment ended, suggesting that this treatment could provide a lasting effect.</p><p class="pull-quote">One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation.</p><p>This study was, to our knowledge, the first to successfully demonstrate an ultrasound-based therapy for any cardiopulmonary disease. And we’re eager to build on it. We’re next interested in studying whether FUS can help with congestive heart failure, a condition in which the heart can’t pump enough blood to meet the body’s needs. In the United States alone, more than <a href="https://www.thecardiologyadvisor.com/ddi/heart-failure-in-the-united-states/" target="_blank">6 million people</a> are living with heart failure, and that number could surpass 8 million by 2030. We know that inflammation plays a significant role in heart failure by damaging the heart’s muscle cells and reducing their elasticity. We plan to test FUS of the spleen in mice with the condition. If those tests are successful, we could move toward clinical testing in humans.</p><h2>The Future of Ultrasound Stimulation</h2><p>We have one huge advantage as we think about how to bring these results from the lab to the clinic: The basic hardware for ultrasound already exists, it’s already FDA approved, and it has a stellar safety record through decades of use. Our collaborators at GE have already experimented with modifying the typical ultrasound devices used for imaging so that they can be used for FUS treatments.</p><p>Once we get to the point of optimizing FUS for clinical use, we’ll have to determine the best neuromodulation parameters. For instance, what are the right acoustic wavelengths and frequencies? Ultrasound imaging typically uses higher frequencies than FUS does, but human tissue absorbs more acoustic energy at higher frequencies than it does at lower frequencies. So to deliver a good dose of FUS, researchers are exploring a wide range of frequencies. We’ll also have to think about how long to transmit that ultrasound energy to make up a single pulse, what rate of pulses to use, and how long the treatment should be.</p><p>In addition, we need to determine how long the beneficial effect of the treatment lasts. For some of the ailments that researchers are exploring, like <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8261174/" target="_blank">FUS of the brain to treat chronic pain</a>, a patient might be able to go to the doctor’s office once every three months for a dose. But for diseases associated with inflammation, a regular, several-times-per-week regimen might prove most effective, which would require at-home treatments.</p><p>For home use to be possible, the wearable device would have to locate the targets automatically via ultrasound imaging. As vast databases already exist of human ultrasound images from the liver, spleen, and other organs, it seems feasible to train a machine-learning algorithm to detect targets automatically and in real time. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. A few startups are working on building such wearable devices, which could take the form of a belt or a vest. For example, the company <a href="https://www.secondwaveus.com/" target="_blank">SecondWave Systems</a>, which has partnered with the University of Minnesota, in Minneapolis, has already conducted a small <a href="https://www.businesswire.com/news/home/20240501993182/en/SecondWave-Systems-Demonstrates-Reduction-in-Disease-Activity-in-Clinical-Study-Evaluating-Novel-Ultrasound-Based-Anti-Inflammatory-Therapy-in-Rheumatoid-Arthritis" target="_blank">pilot study</a> of its wearable device, trying it out on 13 people with rheumatoid arthritis and seeing positive outcomes.</p><p>While it will be many years before FUS treatments are approved for clinical use, and likely still more years for wearable devices to be proven safe enough for home use, the path forward looks very promising. We believe that FUS and other forms of bioelectronic medicine offer a new paradigm for human health, one in which we reduce our reliance on pharmaceuticals and begin to speak directly to the body electric.</p>]]></description><pubDate>Mon, 09 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes</guid><category>Ultrasound</category><category>Neuromodulation</category><category>Neural stimulation</category><category>Diabetes</category><category>Arthritis</category><dc:creator>Sangeeta S. Chavan</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/colorful-abstract-of-human-silhouette-with-anatomical-overlay-and-dynamic-wave-patterns.jpg?id=60557881&amp;width=980"></media:content></item><item><title>Intel Upgrades Chip Packaging for Bigger AI</title><link>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&width=1200&height=800&coordinates=116%2C0%2C116%2C0"/><br/><br/><p><span>This week at the <a href="https://ectc.net/" target="_blank">IEEE Electronic Components and Packaging Technology Conference</a>, <a href="https://www.intel.com/" target="_blank">Intel</a> unveiled that it is developing new chip-packaging technology that will allow for bigger processors for AI.</span></p><p>With Moore’s Law slowing down, makers of advanced GPUs and other data-center chips are having to add more silicon area to their products to keep up with the relentless rise of AI’s computing needs. But the maximum size of a single silicon chip is fixed at around 800 square millimeters (with <a href="https://spectrum.ieee.org/tag/cerebras" target="_blank">one exception</a>), so manufacturers have had to turn to <a href="https://spectrum.ieee.org/tag/advanced-packaging" target="_blank">advanced packaging technologies</a> that integrate multiple pieces of silicon in a way that lets them act like a single chip.</p><p>Three of the innovations Intel unveiled at ECTC were aimed at tackling limitations in just how much silicon you can squeeze into a single package and how big that package can be. They include improvements to the technology Intel uses to link adjacent silicon dies together, a more-accurate method for bonding silicon to the package substrate, and a system to expand the size of a critical part of the package that removes heat. Together, the technologies enable the integration of more than 10,000 square millimeters of silicon within a package that can be bigger than 21,000 mm<sup>2</sup>—a massive area about the size of four and a half credit cards.</p><h2>EMIB gets a 3D upgrade</h2><p>One of the limitations on how much silicon can fit in a single package has to do with connecting a large number of silicon dies at their edges. Using an organic polymer package substrate to interconnect the silicon dies is the most affordable option, but a silicon substrate allows you to make more dense connections at these edges.</p><p>Intel’s solution, introduced more than five years ago, is to embed a small sliver of silicon in the organic package beneath the adjoining edges of the silicon dies. That sliver of silicon, called EMIB, is etched with fine interconnects that increase the density of connections beyond what the organic substrate can handle.</p><p>At ECTC, Intel unveiled the latest twist on the EMIB technology, called EMIB-T. In addition to the usual fine horizontal interconnects, EMIB-T provides relatively thick vertical copper connections called through-silicon vias, or TSVs. The TSVs allow power from the circuit board below to directly connect to the chips above instead of having to route around the EMIB, reducing power lost by a longer journey. Additionally, EMIB-T contains a copper grid that acts as a ground plane to reduce noise in the power delivered due to process cores and other circuits suddenly ramping up their workloads.</p><p>“It sounds simple, but this is a technology that brings a lot of capability to us,” says Rahul Manepalli, vice president of substrate packaging technology at Intel. With it and the other technologies Intel described, a customer could connect silicon equivalent to more than 12 full-size silicon dies—10,000 mm<sup>2</sup> of silicon—in a single package using 38 or more EMIB-T bridges.</p><h2>Thermal control</h2><p>Another technology Intel reported at ECTC that helps increase the size of packages is low-thermal-gradient thermal compression bonding. It’s a variant of the technology used today to attach silicon dies to organic substrates. Micrometer-scale bumps of solder are positioned on the substrate where they will connect to a silicon die. The die is then heated and pressed onto the microbumps, melting them and connecting the package’s interconnects to the silicon’s.</p><p>Because the silicon and the substrate expand at different rates when heated, engineers have to limit the inter-bump distance, or pitch. Additionally, the expansion difference makes it difficult to reliably make very large substrates full of lots of silicon dies, which is the direction AI processors need to go.</p><p>The new Intel tech makes the thermal expansion mismatch more predictable and manageable, says Manepalli. The result is that very large substrates can be populated with dies. Alternatively, the same technology can be used to increase the density of connections to EMIB down to about one every 25 micrometers.</p><h2>A flatter heat spreader</h2><p>These bigger silicon assemblages will generate even more heat than today’s systems. So it’s critical that the heat’s pathway out of the silicon isn’t obstructed. An integrated piece of metal called a heat spreader is key to that, but making one big enough for these large packages is difficult. The package substrate can warp, and the metal heat spreader itself might not stay perfectly flat, so it might not touch the tops of the hot dies it’s supposed to be sucking the heat from. Intel’s solution was to assemble the integrated heat spreader in parts rather than as one piece. This allowed the company to add extra stiffening components, among other things, to keep everything flat and in place.</p><p> “Keeping it flat at higher temperatures is a big benefit for reliability and yield,” says Manepalli.</p><p>Intel says the technologies are still in the R&D stage and would not comment on when these technologies would debut commercially. However, they will likely have to arrive in the next few years for the Intel foundry to compete with <a href="https://spectrum.ieee.org/tsmc-advanced-packaging" target="_blank">TSMC’s planned packaging expansion</a>.</p>]]></description><pubDate>Sun, 08 Jun 2025 13:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/intel-advanced-packaging-for-ai</guid><category>Chip packaging</category><category>Intel</category><category>Thermal control</category><category>Noise reduction</category><category>Interconnects</category><dc:creator>Samuel K. Moore</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-artist-s-rendering-of-the-structure-of-a-chip-package-a-processor-and-memory-are-connected-to-a-a-substrate-with-an-array-of.jpg?id=60463377&amp;width=980"></media:content></item><item><title>Video Friday: Hopping on One Robotic Leg</title><link>https://spectrum.ieee.org/video-friday-one-legged-robot</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&width=1200&height=800&coordinates=181%2C0%2C182%2C0"/><br/><br/><p>
<span>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at </span><em>IEEE Spectrum</em><span> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please </span><a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a><span> for inclusion.</span>
</p><h5><a href="https://www.edrcoalition.com/2025-energy-drone-robotics-summit">2025 Energy Drone & Robotics Summit</a>: 16–18 June 2025, HOUSTON</h5><h5><a href="https://roboticsconference.org/">RSS 2025</a>: 21–25 June 2025, LOS ANGELES</h5><h5><a href="https://robotx.ethz.ch/education/summer-school.html">ETH Robotics Summer School</a>: 21–27 June 2025, GENEVA</h5><h5><a href="https://ias-19.org/">IAS 2025</a>: 30 June–4 July 2025, GENOA, ITALY</h5><h5><a href="https://clawar.org/icres2025/">ICRES 2025</a>: 3–4 July 2025, PORTO, PORTUGAL</h5><h5><a href="https://2025.worldhaptics.org/">IEEE World Haptics</a>: 8–11 July 2025, SUWON, SOUTH KOREA</h5><h5><a href="https://ifac2025-msrob.com/">IFAC Symposium on Robotics</a>: 15–18 July 2025, PARIS</h5><h5><a href="https://2025.robocup.org/">RoboCup 2025</a>: 15–21 July 2025, BAHIA, BRAZIL</h5><h5><a href="https://www.ro-man2025.org/">RO-MAN 2025</a>: 25–29 August 2025, EINDHOVEN, NETHERLANDS</h5><h5><a href="https://clawar.org/clawar2025/">CLAWAR 2025</a>: 5–7 September 2025, SHENZHEN</h5><h5><a href="https://www.corl.org/">CoRL 2025</a>: 27–30 September 2025, SEOUL</h5><h5><a href="https://2025humanoids.org/">IEEE Humanoids</a>: 30 September–2 October 2025, SEOUL</h5><h5><a href="https://worldrobotsummit.org/en/">World Robot Summit</a>: 10–12 October 2025, OSAKA, JAPAN</h5><h5><a href="https://www.iros25.org/">IROS 2025</a>: 19–25 October 2025, HANGZHOU, CHINA</h5><p>
	Enjoy today’s videos!
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="fnzdkxl-jj0">
	This single-leg robot is designed to “form a foundation for future bipedal robot development,” but personally, I think it’s perfect as is.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="e263fb0233d0bb0d075d93a40d651be2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/FNzdKXl-jj0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://dynamicrobot.kaist.ac.kr/">KAIST Dynamic Robot Control and Design Lab</a> ]
</p><div class="horizontal-rule">
</div><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="8a6d56b1cad95583679b96d5194dd022" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wzYtsJwYfTM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Selling 17,000 
	<a data-linked-post="2655919083" href="https://spectrum.ieee.org/social-robots-children" target="_blank">social robots</a> still amazes me. <a data-linked-post="2650251656" href="https://spectrum.ieee.org/aldebaran-robotics-seeking-betatesters-for-its-nao-humanoid-robot" target="_blank">Aldebaran</a> will be missed.
</p><p>
	[ 
	<a href="https://aldebaran.com/en/">Aldebaran</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="udti_d_vif0">
	Nice to see some actual challenging shoves as part of biped testing.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="397e23922e40f8dda09c9558813c3604" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/UdtI_D_vIF0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.ucr.bot/">Under Control Robotics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="j5cfeee5pyi">
<em>Ground Control made multilegged waves at IEEE’s International Conference on Robotics and Automation 2025 in Atlanta! We competed in the Startup Pitch Competition and demoed our robot at our booth, on NIST standard terrain, and around the convention. We were proud to be a finalist for Best Expo Demo and participate in the Robot Parade.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="f805b79697328de135747f04c5a7dac1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/J5cfeEe5pyI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://groundcontrolrobotics.com/">Ground Control Robotics</a> ]
</p><p>
	Thanks, Dan!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="agrtswo4snw">
<em>Humanoid is a U.K.-based robotics innovation company dedicated to building commercially scalable, reliable and safe robotic solutions for real-world applications.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="c6f2dda46adfe06e68b0b4b335ec3291" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/AgrTSWO4Snw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	It’s a nifty bootup screen, I’ll give them that.
</p><p>
	[ 
	<a href="https://thehumanoid.ai/product/">Humanoid</a> ]
</p><p>
	Thanks, Kristina!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="plm9gaq1jxo">
<em>Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they remain limited in performing object interactions that require sustained contact. In this work, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting mechanisms to maintain stability.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="5209d97768c506bd070b00ce7aa8e8b2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/pLm9gaQ1JXo?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://linchangyi1.github.io/LocoTouch/">LocoTouch paper</a> ]
</p><p>
	Thanks, Changyi!
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="2lg-4mdx210">
<em>In this video, Digit is performing tasks autonomously using a whole-body controller for mobile manipulation. This new controller was trained in simulation, enabling Digit to execute tasks while navigating new environments and manipulating objects it has never encountered before.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="4d2772b70353c22ada366d8040940a1a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2lG-4mdx210?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Not bad, although it’s worth pointing out that those shelves are not representative of any market I’ve ever been to.
</p><p>
	[ 
	<a href="https://www.agilityrobotics.com/">Agility Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="xwmwmhrt-fs">
	It’s always cool to see robots presented as an incidental solution to a problem as opposed to, you know, robots.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="686a7d77fbda850290710efc6140a527" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/xWmWmhRt-fs?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	The question that you really want answered, though, is “Why is there water on the floor?”
</p><p>
	[ 
	<a href="https://bostondynamics.com/products/orbit/">Boston Dynamics</a> ]
</p><div class="horizontal-rule">
</div><blockquote class="rm-anchors" id="gqidyj-akaa">
<em>Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. We propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character.</em>
</blockquote><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="daa86fab0c3d3f61cc1ab142a8056ca3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/gQidYj-AKaA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://la.disneyresearch.com/publication/amor-adaptive-character-control-through-multi-objective-reinforcement-learning/">Disney Research</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="igyjdvu2tc0">
	It’s been a week since ICRA 2025, and TRON 1 already misses all the new friends it made!
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="891a8f3fed5dda5103e1a2056cef57e4" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/iGyJdVu2tc0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://www.limxdynamics.com/en">LimX Dynamics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hjpps5vcftg">
	ROB 450 in Winter 2025 challenged students to synthesize the knowledge acquired through their Robotics undergraduate courses at the University of Michigan to use a systematic and iterative design and analysis process and apply it to solving a real open-ended Robotics problem.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="cb4df45971f7989fef2eafcf4708c497" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hjPPS5vcFtg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://robotics.umich.edu/">University of Michigan Robotics</a> ]
</p><div class="horizontal-rule">
</div><p class="rm-anchors" id="hh7fh5ys82q">
	What’s the Trick? A talk on human vs. current robot learning, given by Chris Atkeson at the Robotics and AI Institute.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="ad0b49c258ded8012bd36ea093692f33" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/hh7Fh5YS82Q?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	[ 
	<a href="https://rai-inst.com/">Robotics and AI Institute (RAI)</a> ]
</p><div class="horizontal-rule">
</div>]]></description><pubDate>Fri, 06 Jun 2025 16:30:03 +0000</pubDate><guid>https://spectrum.ieee.org/video-friday-one-legged-robot</guid><category>Video friday</category><category>Robotics</category><category>Humanoid robots</category><category>Aldebaran robotics</category><category>Reinforcement learning</category><category>Quadruped robots</category><dc:creator>Evan Ackerman</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/black-stick-figures-in-a-skating-pose-scattered-across-a-vast-white-icy-landscape.png?id=60524616&amp;width=980"></media:content></item><item><title>Getting Past Procastination</title><link>https://spectrum.ieee.org/getting-past-procastination</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p><em>This article is crossposted from </em><a href="https://spectrum.ieee.org/zaporizhzhia-nuclear-power-plant" target="_self">IEEE Spectrum</a><em>’s careers newsletter. <a href="https://engage.ieee.org/Career-Alert-Sign-Up.html" rel="noopener noreferrer" target="_blank"><em>Sign up now</em></a><em> to get insider tips, expert advice, and practical strategies, <em><em>written i<em>n partnership with tech career development company <a href="https://jointaro.com/" rel="noopener noreferrer" target="_blank">Taro</a> and </em></em></em>delivered to your inbox for free!</em></em></p><p>Across a decade working at hypergrowth tech companies like Meta and Pinterest, I constantly struggled with procrastination. I’d be assigned an important project, but I simply couldn’t get myself to get started. The source of my distraction varied—I would constantly check my email, read random documentation, or even scroll through my social feeds. But the result was the same: I felt a deep sense of dread that I was not making progress on the things that mattered.</p><p>At the end of the day, time is the only resource that matters. With every minute, you are making a decision about how to spend your life. Most of the ways people spend their time are ineffective. Especially in the tech world, our tasks and tools are constantly changing, so we must be able to adapt. What separates the best engineers from the rest of the pack is that they create systems that allow them to be consistently productive.</p><p>Here’s the core idea that changed my perspective on productivity: <strong>Action leads to motivation</strong>, not the other way around. You should not check your email or scroll Instagram while you wait for motivation to “hit you.” Instead, just start doing something, anything, that makes progress toward your goal, and you’ll find that motivation will follow.</p><p>For example, if I have a high-priority, complex bug-fixing challenge at work, my approach is to decompose the problem into something much simpler. <em>Could I simply</em> add a log statement that prints the value of a relevant variable? My goal at this point is not to solve the bug, it’s simply to take a tiny step forward.</p><p>This creates a powerful flywheel: you’re productive → you feel good → you’re more productive.</p><p>Unfortunately, many engineers are stuck in the opposite flywheel, a downward spiral of procrastination: you’re unproductive → you feel bad → you’re unproductive.</p><p>The idea that motivation follows naturally from progress lets us lower the activation energy needed to enter the upward spiral. Author and motivational speaker Tony Robbins talks about a related concept that “motion creates emotion.” The actions we take, and even the way we move our body, affect how we feel. Once you realize that you can control your motivation, you can achieve stress-free productivity.</p><p>—Rahul</p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank">Overcoming Tech Workforce Shortages With IEEE Microcredentials</a></h2><p><span>A shortage of technical workers is coming. Currently, most of these roles require university degrees, but specialized training through focused, skills-based microcredential courses could provide an alternative and expand the workforce. IEEE’s microcredentials program offers credentials that focus on the skills needed to become a technician, electrician, or programmer, regardless of educational background.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="How Software Engineers Actually Use AI">How Software Engineers Actually Use AI</a></h2><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXRxzqsX3H7CzoAP8tlH0fjowYKuaOpFAGfQg611FSGQCPicCY_Zu77pP38Bq7HyWSAs=" target="_blank" title="Read more here."></a><span>Amidst conflicting accounts of how programmers use AI on the job, Wired surveyed 730 coders to get more clarity—then used ChatGPT to comb through the data, with plenty of help from human editors and fact-checkers. The survey asked coders how much they use AI, their outlook on the technology, and how it has changed their jobs, among other questions.</span></p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXexwH67TVCzP_FrLeWBMKMZkkLIpLauUNIFTyCH7znsUKaiGLKVc-0hYeTdBLeD5zds=" target="_blank" title="Read more here.">Read more here.</a></p><h2><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Profile: A Knee Injury Launched This VR Pioneer’s Career">Profile: A Knee Injury Launched This VR Pioneer’s Career</a></h2><p>Unlike many engineers, Carolina Cruz-Neira had little interest in technology as a child. Instead, she dreamed of becoming a professional ballerina. But when an injury forced her to pivot, Cruz-Neira found success in computer science, eventually blending her interests in art and science as a pioneer in virtual reality. </p><p><a href="https://connect.ieee.org/NzU2LUdQSC04OTkAAAGaz-6IXn-WPi12NZwyS58w9WNKrtyQ406RJlXcxcYHA9l4y1kUGMCWFQCXUYSqJI-5igxkdCY=" rel="noopener noreferrer" target="_blank" title="Read more here.">Read more here.</a></p>]]></description><pubDate>Thu, 05 Jun 2025 19:32:12 +0000</pubDate><guid>https://spectrum.ieee.org/getting-past-procastination</guid><category>Career development</category><category>Careers</category><category>Practical strategies</category><category>Tech careers</category><category>Careers newsletter</category><dc:creator>Rahul Pandey</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-illustration-of-stylized-people-wearing-business-casual-clothing.jpg?id=59104110&amp;width=980"></media:content></item><item><title>IEEE Honors Engineering Visionaries at Annual Summit</title><link>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&width=1200&height=800&coordinates=172%2C0%2C172%2C0"/><br/><br/><p>I attended this year’s <a href="https://corporate-awards.ieee.org/event/vic-summit-honors-ceremony-gala/" rel="noopener noreferrer" target="_blank">IEEE Vision, Innovation, and Challenges Summit and Honors Ceremony</a> on 23 and 24 April at the <a href="https://www.hilton.com/en/hotels/tyotohi-hilton-tokyo-odaiba/" rel="noopener noreferrer" target="_blank">Hilton Tokyo Odaiba</a> hotel. The event celebrates <a href="https://spectrum.ieee.org/ieee-celebrates-engineering-brilliance" target="_self">pioneers</a> in engineering who have developed technology that changes the way people connect and learn about the world. This year’s celebrants included the engineers behind the <a href="https://spectrum.ieee.org/henry-samueli-1999" target="_self">first digital cable set-top box modem chipset</a> and the <a href="https://science.nasa.gov/mission/webb/" rel="noopener noreferrer" target="_blank">James Webb Space Telescope</a>.</p><p>The event included the inaugural <a href="https://corporate-awards.ieee.org/yp_laureate_forum/" rel="noopener noreferrer" target="_blank">IEEE Distinguished Young Professionals and Laureate Forum</a>. Fifty young professionals attended the networking event with IEEE leaders, <a href="https://corporate-awards.ieee.org/recipients/ieee-medal-of-honor-recipients/" rel="noopener noreferrer" target="_blank">IEEE Medal of Honor</a> laureates, and award recipients.</p><p>Here are highlights of the <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-vic-summit-full-stream" rel="noopener noreferrer" target="_blank">sessions</a>, which are available to watch in full on <a href="https://ieeetv.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE.tv</a>.</p><h2>Networking opportunities for young professionals</h2><p>Before the VIC summit got underway on 23 April, the networking forum took place that morning. </p><p>In her speech welcoming the attendees, <a href="https://spectrum.ieee.org/ieee-executive-director-sophia-muirhead" target="_self">Sophia Muirhead</a>, IEEE executive director and chief operating officer, encouraged the young professionals to engage in IEEE’s mission of developing technology for the benefit of humanity.</p><p>The participants heard from 2020 IEEE President <a href="https://ethw.org/Toshio_Fukuda" rel="noopener noreferrer" target="_blank">Toshio Fukuda</a> and award recipient <a href="https://sg.linkedin.com/in/aishbandla" rel="noopener noreferrer" target="_blank">Aishwarya Bandla</a> about their careers and volunteer work. Bandla received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-theodore-w-hissey-outstanding-young-professional-award/" rel="noopener noreferrer" target="_blank">IEEE Theodore W. Hissey Young Professionals Award</a> for her “leadership in patient-centric health technology innovation and inspiring IEEE young professionals to drive meaningful change.” The award is sponsored by the <a href="https://ieeephotonics.org/" rel="noopener noreferrer" target="_blank">IEEE Photonics</a> and <a href="https://ieee-pes.org/" rel="noopener noreferrer" target="_blank">IEEE Power & Energy</a> societies, as well as <a href="https://yp.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Young Professionals</a>.</p><p>She is an IEEE senior member and the clinical innovation manager at <a href="https://paxmanscalpcooling.com/" rel="noopener noreferrer" target="_blank">Paxman</a>, a medical equipment manufacturer headquartered in Huddersfield, England. She is developing a wearable device that cools a person’s limbs. The Paxman limb “cryocompression” system helps prevent nerve damage associated with certain types of intravenous chemotherapy.</p><p>As someone who follows the Japanese concept of <em><em>ikigai</em></em>—a sense of purpose—Bandla said her “passion and profession intersected not at technology in the lab but at bringing technology to the people.”</p><p>She shows similar passion in her role as chair of <a href="https://yp.ieeer10.org/" rel="noopener noreferrer" target="_blank">IEEE Region 10’s Young Professionals group</a>. Encouraging attendees to become active in the organization, she said IEEE has given her a purpose and the opportunity to give back to the community.</p><p class="shortcode-media shortcode-media-rebelmouse-image"> <img alt="A large group of business professionals posing together for a photo. A screen in the background reads, \u201cIEEE Vision and Innovation Challenges Summit\u201d." class="rm-shortcode" data-rm-shortcode-id="393c0426f1fd5e1752af1c9775d7caf9" data-rm-shortcode-name="rebelmouse-image" id="13537" loading="lazy" src="https://spectrum.ieee.org/media-library/a-large-group-of-business-professionals-posing-together-for-a-photo-a-screen-in-the-background-reads-u201cieee-vision-and-inn.jpg?id=60451480&width=980"/> <small class="image-media media-caption" placeholder="Add Photo Caption...">Fifty yong professionals attended the inaugural IEEE Distinguished Young Professionals and Laureate Forum.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Junko Kimura; Tomohiro Ohsumi</small></p><p>Attendees were surprised by a guest speaker whose name is well known outside of the technology and engineering fields: <a href="https://woz.org/" target="_blank">Steve Wozniak</a>. </p><p>The summit’s emcee <a href="https://www.miho-japanesevo.com/" rel="noopener noreferrer" target="_blank">Miho Noguchi</a> interviewed the IEEE Fellow and <a href="https://www.apple.com/" rel="noopener noreferrer" target="_blank">Apple</a> cofounder about what inspired him to pursue a career in engineering and launch a startup. Noguchi, a former Japanese radio broadcaster, is the voice for <a href="https://maps.google.com/" rel="noopener noreferrer" target="_blank">Google Maps</a> navigation in Japanese.</p><p>Wozniak said he was inspired by his father, who was an engineer at <a href="https://www.lockheedmartin.com/en-us/index.html" rel="noopener noreferrer" target="_blank">Lockheed Martin</a> in Sunnyvale, Calif.</p><p>“I visited the company several times and watched him work and started asking him what engineers did,” he said. His father told him engineers build things that make life easier for people.</p><p>When asked what career advice he would give to young professionals, Wozniak said, “Always be a good person. Even if someone is bad to you, always be good to them.”</p><p>Reflecting on the benefits of IEEE membership, he said being an IEEE Fellow is the most important honor he has received.</p><p>The forum concluded with a networking opportunity. Each table was given a set of questions to break the ice. Attendees paired up and were given 10 minutes to ask each other about their schooling, work experiences, and career aspirations. When the time was up, they switched partners.</p><p>Several young professionals I interviewed about their experience said they enjoyed the event. One said she really liked learning about where everyone came from, their work, and their passions. All said they were in awe that they got the opportunity to see and hear Wozniak. </p><h2>Governing AI and a telescope’s foray into the past</h2><p>The summit featured a “fireside chat” with <a href="https://spectrum.ieee.org/henry-samueli-moh" target="_self">Henry Samueli</a>, this year’s <a href="https://spectrum.ieee.org/ieee-medal-of-honor-2025" target="_self">IEEE Medal of Honor</a> recipient for his “pioneering research and commercialization of <a href="https://spectrum.ieee.org/tag/broadband" target="_self">broadband</a> communication and networking technologies” and his promotion of STEM education. He is the first recipient of the award since its <a href="https://spectrum.ieee.org/ieee-medal-of-honor" target="_self">monetary prize was increased to US $2 million from $50,000</a>.</p><p><a href="https://spectrum.ieee.org/u/glenn-zorpette" target="_self">Glenn Zorpette</a>, <a href="https://spectrum.ieee.org/navigating-the-dual-use-dilemma" target="_self"><em><em>IEEE Spectrum</em></em></a>’s editorial director for content development, interviewed Samueli, who reminisced about working in his parents’ liquor/grocery store in Los Angeles as a teenager, where he stocked shelves, operated the cash register, and helped out with the bookkeeping. He told Zorpette that his parents inspired him to become an entrepreneur and that a hands-on project in a seventh-grade shop class prompted him to become an engineer.</p><p>Samueli helped to found <a href="https://spectrum.ieee.org/tag/broadcom" target="_self">Broadcom</a> in 1991 in San Jose, Calif. The company developed the first digital cable set-top box modem chipset, which served as the cable signal receiver. Today he is chairman of the company’s board.</p><p>When asked about the future of broadband, he said the application of existing technology is more important than its advancement. He added that he’s excited to see what the future will bring.</p><p>An audience member asked him what <a href="https://spectrum.ieee.org/henry-samueli-advice" target="_self">advice he would give</a> to engineers in developing countries.</p><p>“Take it one step at a time and let [your career] unfold how it is meant to,” he said.</p><p>The conversation was followed by keynote speeches and panel discussions with award recipients on topics including <a href="https://spectrum.ieee.org/topic/artificial-intelligence/" target="_self">artificial intelligence</a> and <a href="https://spectrum.ieee.org/topic/aerospace/" target="_self">space exploration</a>. </p><p class="pull-quote">“[The IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies] represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”<strong> </strong><span><strong>— Frederick A. Kish Jr</strong></span></p><p><span></span>During a presentation on artificial intelligence, <a href="https://unu.edu/about/staff/tshilidzi-marwala" target="_blank">Tshilidzi Marwala</a>, rector of the <a href="https://unu.edu/" target="_blank">United Nations University</a> in Tokyo, led a deep dive into how lawmakers can create policies to govern AI use.</p><p>While AI is already being used by <a href="https://google.com/" target="_blank">Google</a>, <a href="https://www.microsoft.com/" target="_blank">Microsoft</a>, and <a href="https://twitter.com/" target="_blank">X</a>, as well as students and professionals in different fields, Marwala noted there are still many concerns surrounding the technology, especially when it comes to safety and information accuracy.</p><p>He stressed the importance of international collaboration, and he called for lawmakers to involve technologists when creating policy.</p><p>AI is complicated, he pointed out and “needs consistency when it comes to writing rules for its use.”</p><p>AI might be the future, but innovations such as the <a href="https://spectrum.ieee.org/rogue-planet" target="_self">James Webb Space Telescope</a> (JWST) are helping people understand the past.</p><p>The telescope, which <a href="https://spectrum.ieee.org/at-last-first-light-for-the-james-webb-space-telescope" target="_self">gathers images of stars and galaxies created soon after the big bang</a>, took 20 years to develop and build. Its development was led by <a href="https://corporate-awards.ieee.org/speaker/bill-ochs/" rel="noopener noreferrer" target="_blank">Bill Ochs</a>, <a href="https://science.nasa.gov/people/webb-people-mike-menzel/" rel="noopener noreferrer" target="_blank">Mike Menzel</a>, and <a href="https://corporate-awards.ieee.org/speaker/scott-willoughby/" rel="noopener noreferrer" target="_blank">Scott Willoughby</a> at NASA’s <a href="https://www.nasa.gov/goddard/" rel="noopener noreferrer" target="_blank">Goddard Space Flight Center</a> in Greenbelt, Md. For their work, they received this year’s <a href="https://corporate-awards.ieee.org/award/ieee-simon-ramo-medal/" rel="noopener noreferrer" target="_blank">IEEE Simon Ramo Medal</a>, sponsored by <a href="https://www.northropgrumman.com/" rel="noopener noreferrer" target="_blank">Northrop Grumman</a>.</p><p>Ochs, who was a project manager at the flight center during its development, is now the principal engineer at <a href="https://www.fts-intl.com/" rel="noopener noreferrer" target="_blank">FTS International</a>, in Chantilly, Va. Menzel is the telescope’s mission systems engineer, and Willoughby is vice president and project manager at the flight center.</p><p>In a panel session moderated by Noguchi, the three talked about the challenges they faced during the JWST spacecraft’s development and launch. </p><p>One hurdle was the inability to test the telescope’s flight capabilities before launch, Ochs said. The telescope was built to orbit the sun, and it isn’t possible to simulate that environment on Earth. Therefore, Ochs said, the team completed tests and analyses on the telescope’s components and systems to mitigate potential risks.</p><p>The Webb telescope, which launched in 2022, is still collecting data.</p><p>The three engineers also shared advice about what it takes to be a project manager. Take one big problem and break it down into several small problems you can solve, Willoughby said. He added that managers need to communicate with the entire team and “get empirical, fast.”</p><h2>A royal visitor and honoring innovators</h2><p>This year’s <a href="https://ieeetv.ieee.org/channels/ieee_awards/2025-ieee-honors-ceremony-full-stream" rel="noopener noreferrer" target="_blank">IEEE Honors Ceremony</a>, held on the evening of 24 April, recognized people who spearheaded innovations in areas including solid-state circuits, wireless communication, and broadband technology.</p><p>The opening speaker was <a href="https://en.wikipedia.org/wiki/Hisako,_Princess_Takamado" rel="noopener noreferrer" target="_blank">Hisako, Princess Takamodo</a> of Japan. “It is a great honor that Japan has been allowed to host this premier event,” she said.</p><p>This was the first time the ceremony was held in Asia.</p><p>“I stand here in total awe of how far the human brain has come in the past century,” the princess said.</p><p>LEDs play an important role in sustainability, as they reduce energy consumption and can be recycled, unlike incandescent lighting. The technology wouldn’t have been possible without photonic integrated circuits developed by <a href="https://ethw.org/Frederick_A._Kish,_Jr." rel="noopener noreferrer" target="_blank">Frederick A. Kish Jr</a>.</p><p>Kish, an IEEE Fellow, also advanced telecommunications technology by creating and integrating a full optical system for transmissions onto a single chip, reducing manufacturing costs and enabling significantly higher bandwidths and faster data transfer speeds. For his innovations, he received the <a href="https://corporate-awards.ieee.org/award/ieee-nick-holonyak-medal/" rel="noopener noreferrer" target="_blank">IEEE Nick Holonyak Medal for Semiconductor Optoelectronic Technologies</a>. The award is sponsored by Friends of Nick Holonyak Jr. The <a href="https://spectrum.ieee.org/red-hot" target="_self">2003 Medal of Honor recipient</a> invented the first practical visible-spectrum LED.</p><p>Kish thanked his former colleagues at <a href="https://www.google.com/aclk?sa=l&ai=DChcSEwick7_e1rWNAxVWSEcBHWS4F0EYABABGgJxdQ&co=1&ase=2&gclid=CjwKCAjw87XBBhBIEiwAxP3_AzMlWVi0er_7aNExyr7nFlrhOlrskfS4AAq6S-PdnD-37OFIvyXQThoCbmcQAvD_BwE&category=acrcp_v1_5&sig=AOD64_2_79wIKcMbzCyXgqxASx3pyYEDMg&q&nis=4&adurl&ved=2ahUKEwj8p7je1rWNAxXHF1kFHQIJDMAQ0Qx6BAgmEAE" rel="noopener noreferrer" target="_blank">Agilent Technologies</a>,<a href="https://www.hpe.com/us/en/home.html" rel="noopener noreferrer" target="_blank"> Hewlett Packard</a>, the <a href="https://illinois.edu/" rel="noopener noreferrer" target="_blank">University of Illinois Urbana-Champaign</a>, and other organizations.</p><p>“We’ve worked together to leave the world brighter, greener, and more connected,” he said. “This medal represents the power of collaboration, the strength of shared innovation, and the enduring spirit of those who dare to dream.”</p><p>Honoring work that helps connect the world continued with the presentation of the <a href="https://spectrum.ieee.org/mildred-dresselhaus-the-queen-of-carbon-science-has-ieee-medal-named-in-her-honor" target="_self">IEEE Mildred Dresselhaus Medal</a> to IEEE Fellow <a href="https://spectrum.ieee.org/princeton-dean-andrea-goldsmith" target="_self">Andrea Goldsmith</a>, who received the award for “contributions to and leadership in wireless communications theory and practice.” The award is sponsored by <a href="https://about.google/" rel="noopener noreferrer" target="_blank">Google</a>.</p><p>While at <a href="https://www.stanford.edu/" rel="noopener noreferrer" target="_blank">Stanford</a>, Goldsmith developed foundational mathematical approaches for increasing the capacity, speed, and range of wireless systems. She helped found two communications startups: <a href="https://www.linkedin.com/company/quantenna-communications" rel="noopener noreferrer" target="_blank">Quantenna Communications</a> of San Jose, Calif., and <a href="https://www.plume.com/" rel="noopener noreferrer" target="_blank">Plume Design</a> of Palo Alto, Calif. This year, the current dean of engineering and applied sciences at <a href="https://www.princeton.edu/" rel="noopener noreferrer" target="_blank">Princeton</a> was appointed president of <a href="https://www.stonybrook.edu/" rel="noopener noreferrer" target="_blank">Stony Brook University</a>, in New York. She is set to start her new position on 1 August.</p><p>“Mildred Dresselhaus was a pioneer in the days when there were very few women in science and technology,” Goldsmith said. “She was a role model and an early champion of diversity, ensuring the best and the brightest could enter the field and thrive within it. Her contributions to science and engineering are unparalleled, and receiving an award named for her is deeply meaningful to me.”</p><p>The ceremony concluded with the presentation of the IEEE Medal of Honor to Samueli, who received a standing ovation.</p><p>At the end of his speech, he announced that he was giving his $2 million prize to <a href="https://hkn.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE–Eta Kappa Nu</a>, the organization’s honors society.</p><p>“I was made an eminent member of IEEE-HKN in 2019, and [the <a href="https://www.samueli.org/" rel="noopener noreferrer" target="_blank">Samueli Foundation</a>] has supported [the society] for years,” he said. “It is truly an honor for me to endow such a wonderful student organization.”</p>]]></description><pubDate>Wed, 04 Jun 2025 18:00:03 +0000</pubDate><guid>https://spectrum.ieee.org/ieee-vic-summit-awards-2025</guid><category>Broadband</category><category>Ieee awards</category><category>Ieee medal of honor</category><category>Ieee news</category><category>Semiconductors</category><category>Type:ti</category><dc:creator>Joanna Goodrich</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/henry-samueli-dressed-in-a-tuxedo-and-smiling-behind-a-podium-on-stage-the-space-behind-him-is-decorated-with-illuminated-japan.jpg?id=60450951&amp;width=980"></media:content></item><item><title>Look for These 7 New Technologies at the Airport</title><link>https://spectrum.ieee.org/7-new-airport-tech</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&width=1200&height=800&coordinates=7%2C0%2C8%2C0"/><br/><br/><p><strong>Take a look around</strong> the airport during your travels this summer and you might spot a string of new technologies at every touchpoint: from pre-arrival, bag drop, and security to the moment you board the plane.</p><p>In this new world, your face is your boarding pass, your electronic luggage tag transforms itself for each new flight, and gate scanners catch line cutters trying to sneak onto the plane early.</p><p>It isn’t the future—it’s now. Each of the technologies to follow is in use at airports around the world today, transforming your journey-before-the-journey.</p><h2>Virtual queuing speeds up airport security</h2><p>As you pack the night before your trip, you ponder the age-old travel question: What time should I get to the airport? The right answer requires predicting the length of the security line. But at some airports, you no longer have to guess; in fact, you don’t have to wait in line at all.</p><p>Instead, you can book ahead and choose a specific time for your security screening—so you can arrive right before your reserved slot, confident that you’ll be whisked to the front of the line, thanks to <a href="https://copenhagenoptimization.com/" rel="noopener noreferrer" target="_blank">Copenhagen Optimization</a>’s Virtual Queuing system.</p><p>Copenhagen Optimization’s machine learning models use linear regression, heuristic models, and other techniques to forecast the volume of passenger arrivals based on historical data. The system is integrated with airport programs to access flight schedules and passenger-flow data from boarding-pass scans, and it also takes in data from lidar sensors and cameras at security checkpoints, X-ray luggage scanners, and other areas.</p><p>If a given day’s passenger volume ends up differing from historical projections, the platform can use real-time data from these inputs to adjust the Virtual Queuing time slots—and recommend that the airport make changes to security staffing and the number of open lanes. The Virtual Queuing system is constantly adjusting to flatten the passenger arrival curve, tactically redistributing demand across time slots to optimize resources and reduce congestion.</p><p>While this system is doing the most, you as a passenger can do the least. Just book a time slot on your airport’s website or app, and get some extra sleep knowing you’ll waltz right up to the security check tomorrow morning.</p><h2>Electronic bag tags</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Line drawing of a woman lifting suitcase at airport baggage check-in with barcode in focus." class="rm-shortcode" data-rm-shortcode-id="64ff97b084fbc93dd936889921e516d7" data-rm-shortcode-name="rebelmouse-image" id="f8bea" loading="lazy" src="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-lifting-suitcase-at-airport-baggage-check-in-with-barcode-in-focus.png?id=60389664&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Checking a bag? Here’s another step you can take care of before you arrive: Skip the old-school paper tags and generate your own electronic <a href="https://bagtag.com/" rel="noopener noreferrer" target="_blank">Bagtag</a>. This e-ink device (costing about US $80, or €70) looks like a traditional luggage-tag holder, but it can generate a new, paperless tag for each one of your flights.</p><p>You provide your booking details through your airline’s app or the Bagtag app, and the Bagtag system then uses application programming interfaces and secure data protocols to retrieve the necessary information from the airline’s system: your name, flight details, the baggage you’re allowed, and the unique barcode that identifies your bag. The app uses this data to generate a digital tag. Hold your phone near your Bagtag, and it will transmit the encrypted tag data via Bluetooth or NFC. Simultaneously, your phone’s NFC antenna powers the battery-free Bagtag device.</p><p>On the Bagtag itself, a low-power microcontroller decrypts the tag data and displays the digital tag on the e-ink screen. Once you’re at the airport, the tag can be scanned at the airline’s self-service bag drop or desk, just like a traditional paper tag. The device also contains an RFID chip that’s compatible with the luggage-tracking systems that some airlines are using, allowing your bag to be identified and tracked—even if it takes a different journey than you do. When you arrive at the airport, just drop that checked bag and make your way to the security area.</p><h2>Biometric boarding passes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman using kiosk for facial recognition ID verification." class="rm-shortcode" data-rm-shortcode-id="af8a923d85c7eac7ca6873db756cc3fb" data-rm-shortcode-name="rebelmouse-image" id="3dfdf" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-using-kiosk-for-facial-recognition-id-verification.png?id=60389955&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Over at security, you’ll need your boarding pass and ID. Compared with the old days of printing a physical slip from a kiosk, digital QR code boarding passes are quite handy—but what if you didn’t need anything besides your face? That’s the premise of <a href="https://www.idemia.com/" rel="noopener noreferrer" target="_blank">Idemia Public Security</a>’s biometric boarding-pass technology.</p><p>Instead of waiting in a queue for a security agent, you’ll approach a self-service kiosk or check-in point and insert your government-issued identification document, such as a driver’s license or passport. The system uses visible light, infrared, and ultraviolet imaging to analyze the document’s embedded security features and verify its authenticity. Then, computer-vision algorithms locate and extract the image of your face on the ID for identity verification.</p><p>Next, it’s time for your close-up. High-resolution cameras within the system capture a live image of your face using 3D and infrared imaging. The system’s antispoofing technology prevents people from trying to trick the system with items like photos, videos, or masks. The technology compares your live image to the one extracted from your ID using facial-recognition algorithms. Each image is then converted into a compact biometric template—a mathematical representation of your facial features—and a similarity score is generated to confirm a match.</p><p>Finally, the system checks your travel information against secure flight databases to make sure the ticket is valid and that you’re authorized to fly that day. Assuming all checks out, you’re cleared to head to the body scanners—with no biometric data retained by Idemia Public Security’s system.</p><h2>X-rays that can tell ecstasy from eczema meds </h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of an X-ray machine scanning luggage with schematic view of interior components above." class="rm-shortcode" data-rm-shortcode-id="0ff27fb1769e9930b81f30bde1d86244" data-rm-shortcode-name="rebelmouse-image" id="9c471" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-an-x-ray-machine-scanning-luggage-with-schematic-view-of-interior-components-above.png?id=60389973&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>While you pass through your security screening, that luggage you checked is undergoing its own screening—with a major new upgrade that can tell exactly what’s inside.</p><p>Traditional scanners use one or a few X-ray sources and work by transmission, measuring the attenuation of the beam as it passes through the bag. These systems create a 2D “shadow” image based on differences in the amount and type of the materials inside. More recently, these systems have begun using <a href="https://spectrum.ieee.org/invention-of-ct-scanner" target="_blank">computed tomography</a> to scan the bag from all directions and to reconstruct 3D images of the objects inside. But even with CT, harmless objects may look similar to dangerous materials—which can lead to false positives and also require security staff to visually inspect the X-ray images or even bust open your luggage.</p><p>By contrast, <a href="https://www.smithsdetection.com/" rel="noopener noreferrer" target="_blank">Smiths Detection</a>’s new <a href="https://spectrum.ieee.org/future-baggage-scanners-will-tell-us-what-things-are-made-of" target="_blank">X-ray diffraction</a> machines measure the molecular structure of the items inside your bag to identify the exact materials—no human review required.</p><p>The machine uses a multifocus X-ray tube to quickly scan a bag from various angles, measuring the way the radiation diffracts while switching the position of the focal spots every few microseconds. Then, it analyzes the diffraction patterns to determine the crystal structure and molecular composition of the objects inside the bag—building a “fingerprint” of each material that can much more finely differentiate threats, like explosives and drugs, from benign items.</p><p>The system’s algorithms process this diffraction data and build a 3D spatial image, which allows real-time automated screening without the need for manual visual inspection by a human. After your bag passes through the X-ray diffraction machine without incident, it’s loaded into the cargo hold. Meanwhile, you’ve passed through your own scan at security and are ready to head toward your gate.</p><h2>Airport shops with no cashiers or checkout lanes</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt='Illustration of a woman entering a store with a "Just Walk Out" shopping system.' class="rm-shortcode" data-rm-shortcode-id="995f20a20ef07fc6697d2cac5737b9c1" data-rm-shortcode-name="rebelmouse-image" id="8a4c0" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-entering-a-store-with-a-just-walk-out-shopping-system.png?id=60390007&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>While meandering over to your gate from security, you decide you could use a little pick-me-up. Just down the corridor is a convenience store with snacks, drinks, and other treats—but no cashiers. It’s a contactless shop that uses <a href="https://www.justwalkout.com/" rel="noopener noreferrer" target="_blank">Just Walk Out</a> technology by Amazon.</p><p>As you enter the store with the tap of a credit card or mobile wallet, a scanner reads the card and assigns you a unique session identifier that will let the Just Walk Out system link your actions in the store to your payment. Overhead cameras track you by the top of your head, not your face, as you move through the store.</p><p>The Just Walk Out system uses a deep-learning model to follow your movements and detect when you interact with items. In most cases, computer vision can identify a product you pick up simply based on the video feed, but sometimes weight sensors embedded in the shelves provide additional data to determine what you removed. The video and weight data are encoded as tokens, and a neural network processes those tokens in a way similar to how large language models encode text—determining the result of your actions to create a “virtual cart.”</p><p>While you shop, the system continuously updates this cart: adding a can of soda when you pick it up, swapping one brand of gum for another if you change your mind, or removing that bag of chips if you put it back on the shelf. Once your shopping is complete, you can indeed just walk out with your soda and gum. The items you take will make up your finalized virtual cart, and the credit card you entered the store with will be charged as usual. (You can look up a receipt, if you want.) With provisions procured, it’s onward to the gate.</p><h2>Airport-cleaning robots</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman watching an automated floor cleaning robot cleaning up a spilled drink in the airport." class="rm-shortcode" data-rm-shortcode-id="910995e84aefefbcacb0afaefa6e6a37" data-rm-shortcode-name="rebelmouse-image" id="a8ced" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-watching-an-automated-floor-cleaning-robot-cleaning-up-a-spilled-drink-in-the-airport.png?id=60390051&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>As you amble toward the gate with your luggage and snacks, you promptly spill that soda you just bought. Cleanup in Terminal C! Along comes <a href="https://avidbots.com/" rel="noopener noreferrer" target="_blank">Avidbots’ Neo</a>, a fully autonomous floor-scrubbing robot designed to clean commercial spaces like airports with minimal human intervention.</p><p>When a Neo is first delivered to the airport, the robot performs a comprehensive scan of the various areas it will be cleaning using lidar and 3D depth cameras. Avidbots software processes the data to create a detailed map of the environment, including walls and other obstacles, and this serves as the foundation for Neo’s cleaning plans and navigation.</p><p>Neo’s human overlords can use a touchscreen on the robot to direct it to the area that needs cleaning—either as part of scheduled upkeep, or when someone (ahem) spills their soda. The robot springs into action, and as it moves, it continuously locates itself within its map and plans its movements using data from wheel encoders, inertial measurement units, and a gyroscope. Neo also updates its map and adjusts its path in real time by using the lidar and depth cameras to detect any changes from its initial mapping, such as a translocated trash can or perambulating passengers.</p><p>Then comes the scrubbing. Neo’s software plans the optimal path for cleaning a given area at this moment in time, adjusting the robot’s speed and steering as it moves along. A water-delivery system pumps and controls the flow of cleaning solution to the motorized brushes, whose speed and pressure can also be adjusted based on the surface the robot is cleaning. A powerful vacuum system collects the dirty water, and a flexible squeegee prevents slippery floors from being left behind.</p><p>While the robot’s various sensors and planning algorithms continuously detect and avoid obstacles, any physical contact with the robot’s bumpers triggers an emergency stop. And if Neo finds itself in a situation it’s just not sure how to handle, the robot will stop and call for assistance from a human operator, who can review sensor data and camera feeds remotely to help it along.</p><h2>“Wrong group” plane-boarding alarm</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-float-left rm-resized-container rm-resized-container-25" data-rm-resized-container="25%" style="float: left;"> <img alt="Illustration of a woman waiting in line at boarding gate E6, with notification bell icon above." class="rm-shortcode" data-rm-shortcode-id="2b01cd5c1c6b0ed8dd2962347946988a" data-rm-shortcode-name="rebelmouse-image" id="bdce3" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-woman-waiting-in-line-at-boarding-gate-e6-with-notification-bell-icon-above.png?id=60390066&width=980"/> <small class="image-media media-photo-credit" placeholder="Add Photo Credit...">MCKIBILLO</small></p><p>Your airport journey is coming to an end, and your real journey is about to begin. As you wait at the gate, you notice a fair number of your fellow passengers hovering to board even before the agent has made any announcements. And when boarding does begin, a surprising number of people hop in line. <em><em>Could all these people really be in boarding groups 1 and 2?</em></em> you wonder.</p><p>If they’re not…they’ll get called out. American Airlines’ new boarding technology stops those pesky passengers who try to join the wrong boarding group and sneak onto the plane early.</p><p>If one such passenger approaches the gate before their assigned group has been called, scanning their boarding pass will trigger an audible alert—notifying the airline crew, and everyone else for that matter. The passengers will be politely asked to wait to board. As they slink back into line, try not to look too smug. After all, it’s been a remarkably easy, tech-assisted journey through the airport today. <span class="ieee-end-mark"></span></p>]]></description><pubDate>Wed, 04 Jun 2025 16:00:04 +0000</pubDate><guid>https://spectrum.ieee.org/7-new-airport-tech</guid><category>Airlines</category><category>Facial recognition</category><category>Robot cleaner</category><category>Airports</category><category>X-ray diffraction</category><dc:creator>Julianne Pepitone</dc:creator><media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/line-drawing-of-a-woman-walking-into-an-airport-and-rolling-carryon-luggage-as-she-checks-her-travel-itinerary-on-a-cell-phone.png?id=60389585&amp;width=980"></media:content></item><item><title>Nvidia’s Blackwell Conquers Largest LLM Training Benchmark</title><link>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&width=1200&height=800&coordinates=0%2C137%2C0%2C137"/><br/><br/><p>
<span>For those who enjoy rooting for the underdog, the latest MLPerf benchmark results will disappoint: Nvidia’s GPUs have dominated the competition </span><a href="https://spectrum.ieee.org/mlperf-nvidia-conquers" target="_self">yet</a><span> </span><a href="https://spectrum.ieee.org/ai-training-2669810566" target="_self">again</a><span>. This includes chart-topping performance on the latest and most demanding benchmark, pretraining the Llama 3.1 403B large language model. That said, the computers built around the newest AMD GPU, MI325X, matched the performance of Nvidia’s H200, </span><a href="https://spectrum.ieee.org/nvidia-blackwell" target="_self">Blackwell’s</a><span> predecessor, on the most popular LLM fine-tuning benchmark. This suggests that AMD is one generation behind Nvidia.</span>
</p><p>
<a href="https://mlcommons.org/benchmarks/" target="_blank">MLPerf</a> training is one of the machine learning competitions run by the <a href="https://mlcommons.org/" rel="noopener noreferrer" target="_blank">MLCommons</a> consortium. “AI performance sometimes can be sort of the Wild West. MLPerf seeks to bring order to that chaos,” says <a href="https://www.linkedin.com/in/davesalvator/" rel="noopener noreferrer" target="_blank">Dave Salvator</a>, director of accelerated computing products at Nvidia. “This is not an easy task.”
</p><p>
	The competition consists of six benchmarks, each probing a different industry-relevant machine learning task. The benchmarks are content recommendation, large language model pretraining, large language model fine-tuning, object detection for machine vision applications, image generation, and graph node classification for applications such as fraud detection and drug discovery.
</p><p>
	The large language model pretraining task is the most resource intensive, and this round it was updated to be even more so. The term “pretraining” is somewhat misleading—it might give the impression that it’s followed by a phase called “training.” It’s not. Pretraining is where most of the number crunching happens, and what follows is usually fine-tuning, which refines the model for specific tasks.
</p><p>
	In previous iterations, the pretraining was done on the GPT3 model. This iteration, it was replaced by Meta’s Llama 3.1 403B, which is more than twice the size of GPT3 and uses a four times larger context window. The context window is how much input text the model can process at once. This larger benchmark represents the industry trend for ever larger models, as well as including some architectural updates.
</p><h2>Blackwell Tops the Charts, AMD on Its Tail </h2><p>
	For all six benchmarks, the fastest training time was on Nvidia’s Blackwell GPUs. Nvidia itself submitted to every benchmark (other companies also submitted using various computers built around Nvidia GPUs). Nvidia’s Salvator emphasized that this is the first deployment of Blackwell GPUs at scale and that this performance is only likely to improve. “We’re still fairly early in the Blackwell development life cycle,” he says.
</p><p>
	This is the first time AMD has submitted to the training benchmark, although in previous years other companies have submitted using computers that included AMD GPUs. In the most popular benchmark, LLM fine-tuning, AMD demonstrated that its latest Instinct MI325X GPU performed on par with Nvidia’s H200s. Additionally, the Instinct MI325X showed a 30 percent improvement over its predecessor, the <a href="https://spectrum.ieee.org/amd-mi300" target="_blank">Instinct MI300X</a>. (The main difference between the two is that MI325X comes with 30 percent more high-bandwidth memory than MI300X.)</p><p>For it’s part, Google submitted to a single benchmark, the image-generation task, with its <a href="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus" target="_blank">Trillium TPU</a>.
</p><div class="flourish-embed flourish-scatter" data-src="visualisation/23471261?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="scatter visualization" src="https://public.flourish.studio/visualisation/23471261/thumbnail" width="100%"/></noscript></div><h2>The Importance of Networking</h2><p>
	Of all submissions to the LLM fine-tuning benchmarks, the system with the largest number of GPUs was submitted by Nvidia, a computer connecting 512 B200s. At this scale, networking between GPUs starts to play a significant role. Ideally, adding more than one GPU would divide the time to train by the number of GPUs. In reality, it is always less efficient than that, as some of the time is lost to communication. Minimizing that loss is key to efficiently training the largest models.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23550378?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23550378/thumbnail" width="100%"/></noscript></div><p>
	This becomes even more significant on the pretraining benchmark, where the smallest submission used 512 GPUs, and the largest used 8,192. For this new benchmark, the performance scaling with more GPUs was notably close to linear, achieving 90 percent of the ideal performance.
</p><p>
	Nvidia’s Salvator attributes this to the NVL72, an efficient package that connects 36 Grace CPUs and 72 Blackwell GPUs with <a href="https://www.nvidia.com/en-us/data-center/nvlink/" rel="noopener noreferrer" target="_blank">NVLink</a>, to form a system that “acts as a single, massive GPU,” the <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank">datasheet</a> claims. Multiple NVL72s were then connected with <a href="https://spectrum.ieee.org/co-packaged-optics" target="_self">InfiniBand</a> network technology.
</p><div class="flourish-embed flourish-chart" data-src="visualisation/23508278?1509099"><script src="https://public.flourish.studio/resources/embed.js"></script><noscript><img alt="chart visualization" src="https://public.flourish.studio/visualisation/23508278/thumbnail" width="100%"/></noscript></div><p>
	Notably, the largest submission for this round of MLPerf—at 8192 GPUs—is not the largest ever, despite the increased demands of the pretraining benchmark. Previous rounds saw submissions with over 10,000 GPUs. <a href="https://www.linkedin.com/in/kennethleach/" target="_blank">Kenneth Leach</a>, principal AI and machine learning engineer at Hewlett Packard Enterprise, attributes the reduction to improvements in GPUs, as well as networking between them. “Previously, we needed 16 server nodes [to pretrain LLMs], but today we’re able to do it with 4. I think that’s one reason we’re not seeing so many huge systems, because we’re getting a lot of efficient scaling.”
</p><p>
	One way to avoid the losses associated with networking is to put many AI accelerators on the same huge wafer, as done by <a href="https://spectrum.ieee.org/cerebrass-giant-chip-will-smash-deep-learnings-speed-barrier" target="_self">Cerebras</a>, which recently claimed to <a href="https://www.businesswire.com/news/home/20250528123694/en/Cerebras-Beats-NVIDIA-Blackwell-in-Llama-4-Maverick-Inference" rel="noopener noreferrer" target="_blank">beat</a> Nvidia’s Blackwell GPUs by more than a factor of two on inference tasks. However, that result was measured by <a href="https://artificialanalysis.ai/" rel="noopener noreferrer" target="_blank">Artificial Analysis</a>, which queries different providers without controlling how the workload is executed. So its not an apples-to-apples comparison in the way the MLPerf benchmark ensures.
</p><h2>A Paucity of Power</h2><p>
	The MLPerf benchmark also includes a power test, measuring how much power is consumed to achieve each training task. This round, only a single submitter—Lenovo—included a power measurement in its submission, making it impossible to make comparisons across performers. The energy it took to fine-tune an LLM on two Blackwell GPUs was 6.11 gigajoules, or 1,698 kilowatt-hours, or roughly the energy it would take to heat a small home for a winter. With growing <a href="https://spectrum.ieee.org/ai-energy-consumption" target="_self">concerns</a> about AI’s energy use, the power efficiency of training is crucial, and this author is perhaps not alone in hoping more companies submit these results in future rounds.
</p>]]></description><pubDate>Wed, 04 Jun 2025 15:59:50 +0000</pubDate><guid>https://spectrum.ieee.org/nvidia-blackwell-mlperf-training-5</guid><category>Mlperf</category><category>Nvidia</category><category>Amd</category><category>Networking</category><dc:creator>Dina Genkina</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/two-black-towers-with-nvidia-labels-on-the-side.jpg?id=60398926&amp;width=980"></media:content></item><item><title>Andrew Ng: Unbiggen AI</title><link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1200&height=800&coordinates=0%2C0%2C0%2C210"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description><pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate><guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid><category>Deep learning</category><category>Artificial intelligence</category><category>Andrew ng</category><category>Type:cover</category><dc:creator>Eliza Strickland</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980"></media:content></item><item><title>How AI Will Change Chip Design</title><link>https://spectrum.ieee.org/ai-chip-design-matlab</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it’s not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It’s a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It’s very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description><pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate><guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid><category>Chip fabrication</category><category>Matlab</category><category>Moore’s law</category><category>Chip design</category><category>Ai</category><category>Digital twins</category><dc:creator>Rina Diane Caballar</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980"></media:content></item><item><title>Atomically Thin Materials Significantly Shrink Qubits</title><link>https://spectrum.ieee.org/2d-hbn-qubit</link><description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description><pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate><guid>https://spectrum.ieee.org/2d-hbn-qubit</guid><category>Quantum computing</category><category>2d materials</category><category>Ibm</category><category>Qubits</category><category>Hexagonal boron nitride</category><category>Superconducting qubits</category><category>Mit</category><dc:creator>Dexter Johnson</dc:creator><media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980"></media:content></item></channel></rss>