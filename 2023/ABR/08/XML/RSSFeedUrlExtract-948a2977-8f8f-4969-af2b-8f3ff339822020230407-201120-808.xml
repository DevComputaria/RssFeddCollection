<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>IEEE Spectrum</title>
    <link>https://spectrum.ieee.org/</link>
    <description>IEEE Spectrum</description>
    <atom:link href="https://spectrum.ieee.org/feeds/feed.rss" rel="self">
    </atom:link>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Apr 2023 20:43:04 -0000</lastBuildDate>
    <image>
      <url>https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy8yNjg4NDUyMC9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTY5OTk5OTQzOX0.301VTCew4TbjvOr--sy6SCzN7qjjXsjyR9O35w43oZo/image.png?width=210</url>
      <link>https://spectrum.ieee.org/</link>
      <title>IEEE Spectrum</title>
    </image>
    <item>
      <title>‘AI Pause’ Open Letter Stokes Fear and Controversy</title>
      <link>https://spectrum.ieee.org/ai-pause-letter-stokes-fear</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/the-letters-ai-with-a-pause-symbol-in-the-dot-for-the-i-4-hands-are-reaching-in-with-writing-implements-as-if-signing-it.jpg?id=33433553&width=1200&height=800&coordinates=0%2C125%2C0%2C125"/><br/><br/><p>The recent call for a six-month “AI pause”—in the form of an online letter demanding a temporary artificial intelligence moratorium—has elicited concern among IEEE members and the larger technology world. <em>The Institute </em>contacted some of the members who signed the open letter, which was published online on 29 March. The signatories expressed a range of fears and apprehensions including about rampant growth of AI large-language models (LLMs) as well as of unchecked AI media hype. </p><p>The <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/" rel="noopener noreferrer" target="_blank">open letter, titled “Pause Giant AI Experiments,” </a>was organized by the nonprofit <a href="https://spectrum.ieee.org/tag/future-of-life-institute" target="_self">Future of Life Institute</a> and signed by more than 10,000 people (as of 5 April). It calls for cessation of research on “all AI systems more powerful than <a href="https://spectrum.ieee.org/tag/gpt-4" target="_self">GPT-4</a>.”</p><p>It’s the latest of <a href="https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-slowing-down-ai" rel="noopener noreferrer" target="_blank">a host</a> of <a href="https://www.jonstokes.com/p/heres-what-it-would-take-to-slow" rel="noopener noreferrer" target="_blank">recent</a> “<a href="https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology" rel="noopener noreferrer" target="_blank">AI pause</a>” <a href="https://garymarcus.substack.com/p/is-it-time-to-hit-the-pause-button" rel="noopener noreferrer" target="_blank">proposals</a> including a <a href="https://twitter.com/fchollet/status/1640896398311968773" rel="noopener noreferrer" target="_blank">suggestion</a> by Google’s François Chollet of a six-month “moratorium on people overreacting to LLMs” in either direction. </p><p>In the news media, the open letter has inspired <a href="https://www.theverge.com/2023/3/29/23661374/elon-musk-ai-researchers-pause-research-open-letter" rel="noopener noreferrer" target="_blank">straight </a><a href="https://www.wired.com/story/chatgpt-pause-ai-experiments-open-letter/" rel="noopener noreferrer" target="_blank">reportage</a>, critical accounts for not going far enough (“<a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" rel="noopener noreferrer" target="_blank">shut it all down</a>,” <a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky" rel="noopener noreferrer" target="_blank">Eliezer Yudkowsky</a> wrote in <em>Time </em>magazine), as well as critical accounts for being both <a href="https://www.vice.com/en/article/qjvppm/the-open-letter-to-stop-dangerous-ai-race-is-a-huge-mess" rel="noopener noreferrer" target="_blank">a mess</a> and an <a href="https://fortune.com/2023/03/31/objections-to-elon-musk-ai-pause-letter/" rel="noopener noreferrer" target="_blank">alarmist distraction</a> that overlooks the real AI challenges ahead. </p><p>IEEE members have expressed a similar diversity of opinions. </p><p>“AI can be manipulated by a programmer to achieve objectives contrary to moral, ethical, and political standards of a healthy society,” says IEEE Fellow <a href="https://lsa.umich.edu/physics/people/faculty/dst.html" rel="noopener noreferrer" target="_blank">Duncan Steel</a>, a professor of electrical engineering, computer science, and physics at the <a href="https://umich.edu/" rel="noopener noreferrer" target="_blank">University of Michigan</a>, in Ann Arbor. “I would like to see an unbiased group without personal or commercial agendas to create a set of standards that has to be followed by all users and providers of AI.” </p><p>IEEE Senior Life Member <a href="https://www.linkedin.com/in/stephen-deiss-33476b3" rel="noopener noreferrer" target="_blank">Stephen Deiss</a>—a retired neuromorphic engineer from the <a href="https://ucsd.edu/" rel="noopener noreferrer" target="_blank">University of California, San Diego</a>—says he signed the letter because the AI industry is “unfettered and unregulated.”</p><p>“This technology is as important as the coming of electricity or the Net,” Deiss says. “There are too many ways these systems could be abused. They are being freely distributed, and there is no review or regulation in place to prevent harm.”</p><p><a href="https://www.nellwatson.com/" rel="noopener noreferrer" target="_blank">Eleanor “Nell” Watson</a>, an AI ethicist who has taught <a href="https://ieeexplore.ieee.org/courses/details/EDP595" rel="noopener noreferrer" target="_blank">IEEE courses</a> <a href="https://ieeexplore.ieee.org/courses/details/EDP596" rel="noopener noreferrer" target="_blank">on the subject</a>, says the open letter raises awareness over such near-term concerns as AI systems cloning voices and performing automated conversations—which she says presents a “serious threat to social trust and well-being.”</p><p>Although Watson says she’s glad the open letter has sparked debate, she says she confesses “to having some doubts about the actionability of a moratorium, as less scrupulous actors are especially unlikely to heed it.”</p><p class="pull-quote">“There are too many ways these systems could be abused. They are being freely distributed, and there is no review or regulation in place to prevent harm.”</p><p><span></span>IEEE Fellow <a href="https://www.cs.utexas.edu/~pstone/" target="_blank">Peter Stone</a>, a computer science professor at the <a href="https://www.utexas.edu/" target="_blank">University of Texas at Austin</a>, says some of the biggest threats posed by LLMs and similar big-AI systems remain unknown.</p><p>“We are still seeing new, creative, unforeseen uses—and possible misuses—of existing models,” Stone says.</p><p>“My biggest concern is that the letter will be perceived as calling for more than it is,” he adds. “I decided to sign it and hope for an opportunity to explain a more nuanced view than is expressed in the letter.</p><p>“I would have written it differently,” he says of the letter. “But on balance I think it would be a net positive to let the dust settle a bit on the current LLM versions before developing their successors.”</p><p><em>IEEE Spectrum</em> has <a href="https://spectrum.ieee.org/industry-urges-united-nations-to-ban-lethal-autonomous-weapons-in-new-open-letter" target="_self">extensively</a> <a href="https://spectrum.ieee.org/lethal-microdrones-dystopian-futures-and-the-autonomous-weapons-debate" target="_self">covered</a> one of the Future of Life Institute’s <a href="https://spectrum.ieee.org/debating-slaughterbots" target="_self">previous campaigns</a>, <a href="https://futureoflife.org/project/lethal-autonomous-weapons-systems/" rel="noopener noreferrer" target="_blank">urging a ban</a> on “<a href="https://spectrum.ieee.org/united-nations-killer-robots-ban" target="_self">killer robots</a>.” The outlines of the debate, which began with a <a href="https://futureoflife.org/open-letter/open-letter-autonomous-weapons-ai-robotics/" rel="noopener noreferrer" target="_blank">2016 open letter</a>, parallel the criticism being leveled at the current “AI pause” campaign: that there are real problems and challenges in the field that, in both cases, are at best poorly served by sensationalism. </p><p>One outspoken AI critic, <a href="https://spectrum.ieee.org/timnit-gebru-dair-ai-ethics" target="_self">Timnit Gebru</a> of the <a href="https://www.dair-institute.org/" rel="noopener noreferrer" target="_blank">Distributed AI Research Institute</a>, is similarly critical of the open letter. She describes the fear being promoted in the “AI pause” campaign as stemming from what she calls “long-termism”—discerning AI’s threats only in some futuristic, dystopian sci-fi scenario, rather than in the present day, where AI’s <a href="https://spectrum.ieee.org/engineering-bias-out-of-ai" target="_self">bias amplification</a> and <a href="https://www.nytimes.com/2019/09/26/technology/ai-computer-expense.html" rel="noopener noreferrer" target="_blank">power concentration</a> problems are well known. </p><p>IEEE Member <a href="https://es.linkedin.com/in/jhiguera" rel="noopener noreferrer" target="_blank">Jorge E. Higuera</a>, a senior systems engineer at <a href="https://circontrol.com/" rel="noopener noreferrer" target="_blank">Circontrol</a> in Barcelona, says he signed the open letter because “it can be difficult to regulate superintelligent AI, particularly if it is developed by authoritarian states, shadowy private companies, or unscrupulous individuals.”</p><p>IEEE Fellow <a href="https://research.ibm.com/people/grady-booch" rel="noopener noreferrer" target="_blank">Grady Booch</a>, chief scientist for software engineering at <a href="http://ibm.com" rel="noopener noreferrer" target="_blank">IBM</a>, signed although he also, in his discussion with <em>The Institute, </em>cited Gebru’s work and reservations about AI’s pitfalls.</p><p>“Generative models are unreliable narrators,” Booch says. “The problems with large-language models are many: There are legitimate concerns regarding their use of information without consent; they have demonstrable racial and sexual biases; they generate misinformation at scale; they do not understand but only offer the illusion of understanding, particularly for domains on which they are well-trained with a corpus that includes statements of understanding.</p><p>“These models are being unleashed into the wild by corporations who offer no transparency as to their corpus, their architecture, their guardrails, or the policies for handling data from users. My experience and my professional ethics tell me I must take a stand, and signing the letter is one of those stands.” </p><p><em>Please share your thoughts in the comments section below.</em></p>]]></description>
      <pubDate>Fri, 07 Apr 2023 18:00:02 +0000</pubDate>
      <guid>https://spectrum.ieee.org/ai-pause-letter-stokes-fear</guid>
      <category>Ai</category>
      <category>Ai ethical design</category>
      <category>Ai pause</category>
      <category>Artificial intelligence</category>
      <category>Ethics</category>
      <category>Ieee member news</category>
      <category>Large-language models</category>
      <category>Type:ti</category>
      <dc:creator>Margo Anderson</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/the-letters-ai-with-a-pause-symbol-in-the-dot-for-the-i-4-hands-are-reaching-in-with-writing-implements-as-if-signing-it.jpg?id=33433553&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Video Friday: Peep Handling</title>
      <link>https://spectrum.ieee.org/video-friday-peep-handling</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-robot-hand-with-soft-purple-fingers-gently-grasps-a-pink-marshmallow-peep.png?id=33429254&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://www.roboticssummit.com/">Robotics Summit & Expo</a>: 10–11 May 2023, BOSTON</h5><h5><a href="https://www.icra2023.org/">ICRA 2023</a>: 29 May–2 June 2023, LONDON</h5><h5><a href="https://2023.robocup.org/en/home/">RoboCup 2023</a>: 4–10 July 2023, BORDEAUX, FRANCE</h5><h5><a href="https://roboticsconference.org/">RSS 2023</a>: 10–14 July 2023, DAEGU, SOUTH KOREA</h5><h5><a href="http://ro-man2023.org/main">IEEE RO-MAN 2023</a>: 28–31 August 2023, BUSAN, SOUTH KOREA</h5><h5><a href="https://clawar.org/clawar23/">CLAWAR 2023</a>: 2–4 October 2023, FLORIANOPOLIS, BRAZIL</h5><h5><a href="https://2023.ieee-humanoids.org/">Humanoids 2023</a>: 12–14 December 2023, AUSTIN, TEXAS</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><p>Happy Peep Consumption Day!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1ac6c6a22b9f930733963b975be37416" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/0-2B1Vn75Qg?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.softroboticsinc.com/resource/just-born-mgrip-solution-keeps-up-with-changing-product-mix/">Soft Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Appropriate facial expressions for Ameca in this video are selected by GPT-3 . We also tried GPT-4; the processing time with 4 was longer and made Ameca appear less responsive.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="663f1d7140cbc3d43f0cb2a4fdb908a6" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/yUszJyS3d7A?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.engineeredarts.co.uk/robot/ameca/">Engineered Arts</a> ]</p><div class="horizontal-rule"></div><p>Kittens: The enemy of lazy robots everywhere.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="210c9bda0278c0ddacedd6601bf73c06" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lFNGHqFmFtE?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>I’m very curious about what “<3 Fluffy Pants” is all about.</p><p>[ <a href="https://agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>This is a summary video of KIMLAB Robot Demos at UIUC Engineering Open House 2023.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c83e205e87bf3bc2c8589ca469e09dc7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/uq5AF7WDP1Y?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://publish.illinois.edu/kimlab2020/">KIMLAB</a> ]</p><div class="horizontal-rule"></div><blockquote><em>SRI International’s latest robotic application focuses on the integration of active surfaces with a compliant robot grasper to provide a high dexterity and robustness for robot in-hand manipulation. The grasper can pick up a wide range of sizes and shape in power grasps and move them around in different axes without lifting the fingers. The ability to reorient objects in a power grasp provides unique value to both industrial and domestic applications.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1bf157034e1c6c5cb6be938e834e2abb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/6rN8__cFjrY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Sigh, I wish my fingers could do that.</p><p>[ <a href="https://missinglight.github.io/">Yilin Cai</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Our new walk is based on our 2022 walk. To ensure stability, we use a regulation to modify the allowed rotation speed of the support foot’s joints. Thus, the different leg parts will still execute the intended motion, but based on the center of mass and the measured rotation errors of the support foot, some leg parts are slowed down if needed. Additionally, to handle more extreme cases at higher walking speeds, a neural network is used to predict future joint-position measurements to calculate future position errors. The robots are now able to handle more difficult situations. Also, as an unintended effect, the robots lift up on the tip of the supporting foot, just like humans do.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6d65185c2e71350b8602739cd8ce40ef" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/ifbU9LnZpLA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.b-human.de/">B-Human</a> ]</p><div class="horizontal-rule"></div><p>A motion platform is just an upside-down quadruped robot.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="aa872e92f0b00f7a0e3942440750e825" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/thXPA2MYcQw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://github.com/nayan-pradhan/solo-6dof-motion-platform">GitHub</a> ]</p><div class="horizontal-rule"></div><p>This is getting out of hand—now there are two of them!</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="7243376938691e0bbd1e29e73078a178" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/kZRGhbRrNtI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.sanctuary.ai/">Sanctuary AI</a> ]</p><div class="horizontal-rule"></div><p>The little doghouse in this video is super cute.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="77f48dac75402edcbfc19af877021637" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/m5__1M6gYwM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.unitree.com/en/">Unitree</a> ]</p><div class="horizontal-rule"></div><blockquote><em>ORNL researchers study the effects of energy use on waterways and develop solutions to limit water pollution. This segment demonstrates how scientists use drones to assess biodiversity and ecosystem health.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="3b0fe2a662c8f898e5d668484aa08cd1" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/sr33E43mwzk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.ornl.gov/content/aquatic-ecology-laboratory">ORNL</a> ]</p><div class="horizontal-rule"></div><blockquote><em>More and more new cars come equipped with autonomous driving features. By 2030, an estimated 4 million fully autonomous vehicles will travel U.S. roadways. A significant challenge is ensuring these vehicles are safe enough to operate on public roads.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="55f785fa0eac111b318dea61bd514143" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/3t-vdQU_kPk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.nsf.gov/news/mmg/index.jsp?series_name=NSF%20Science%20Now">NSF</a> ]</p><div class="horizontal-rule"></div><p>This GRASP Seminar is from Larry Matthies at the Jet Propulsion Laboratory, on “Autonomous mobility in Mars exploration: recent achievements and future prospects.”</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="c7662372ed27a6cba32bed6b6be69bdd" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/y2v1NTy8oGM?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><blockquote><em>This talk will summarize key recent advances in autonomous surface and aerial mobility for Mars exploration, then discuss potential future missions and technology needs for Mars and other planetary bodies. Rover mission concepts recently suggested for the moon would drive about 1,500 to 2,000 kilometers in under 4 years, which requires significant advances in autonomy. Successors to the Ingenuity helicopter are now under development for use in a mission planned for later this decade to return Mars samples to Earth that Perseverance is collecting. Much larger helicopter concepts are being studied to enable carrying larger science instrument payloads for potential future Mars missions. Robotic surface and aerial vehicles, as well as drilling systems for subsurface access, potentially could play a role in NASA’s goals for a human mission to Mars roughly two decades from now.</em></blockquote><p>[ <a href="https://www.grasp.upenn.edu/events/spring-2023-grasp-seminar-larry-matthies/">GRASP</a> ]</p><div class="horizontal-rule"></div>]]></description>
      <pubDate>Fri, 07 Apr 2023 16:00:02 +0000</pubDate>
      <guid>https://spectrum.ieee.org/video-friday-peep-handling</guid>
      <category>Video friday</category>
      <category>Robotics</category>
      <dc:creator>Evan Ackerman</dc:creator>
      <media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-robot-hand-with-soft-purple-fingers-gently-grasps-a-pink-marshmallow-peep.png?id=33429254&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Fast Food, Fast Charge</title>
      <link>https://spectrum.ieee.org/dc-fast-charger</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-car-charges-at-a-station-labelled-7charge.jpg?id=33422190&width=1200&height=800&coordinates=38%2C0%2C39%2C0"/><br/><br/><p>It sounds so obvious: Public EV chargers should be located <em>in public</em>, where people tend to gather. The reality has often been otherwise, with too many chargers tucked into desolate parking lots or seemingly random locations. And since even the speediest DC chargers take roughly 30 minutes to juice up an EV, drivers and passengers have time to kill, including enough time for a bite.</p><p>To keep cars and passengers full, 7-Eleven and Subway are among the companies planning to serve up EV charging along with their Big Gulps and sandwiches.</p><p>7-Eleven envisions its <a href="https://www.7-eleven.com/7charge" target="_blank">7Charge network</a> as one of the biggest fast-charging networks among convenience stores in the United States. The company hasn’t cited exact numbers of how many DC chargers it hopes to open, but 7-Eleven operates about 9,400 stores in the United States, and nearly 600 more in Canada. The company currently has more than 30 fast chargers in California, Colorado, Texas, and Florida.</p><p>The new chargers aim to welcome owners of any EV, by offering both the CCS (Combined Charging System) connector and the CHAdeMO plug. Tesla owners can already hook into CCS stations by means of an adapter. A 7Charge app will offer a station locator and an easy-payment method through a linked account.</p><p>7-Eleven isn’t alone. Subway will partner with Miami-based GenZ EV Solutions to create <a href="https://electrek.co/2023/02/21/subway-builds-an-ev-charging-oasis-so-you-can-recharge-while-you-eat-fresh/" target="_blank">Subway EV Charging Oasis</a> parks with fast-charging stalls, picnic tables, Wi-Fi, playgrounds, and restrooms. California is determined to require all new vehicles sold to be either EVs or plug-in hybrids by 2035. And a Taco Bell franchisee with more than 300 locations has launched the first of 100 planned California charging stations in San Francisco.</p><p>Restaurant and convenience-store operators cite ongoing gaps in public charging, including in rural, urban, and other underserved areas.</p><p>“Equitable access to charging is essential to the adoption of EVs, and Subway’s scale will play an important role in democratizing charging infrastructure for millions of Americans,” said Jose Valls, CEO of GenZ EV Solutions.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Illustration showing cars parked at a Subway charging station." class="rm-shortcode" data-rm-shortcode-id="e7d708bf0926af95466c903a28824243" data-rm-shortcode-name="rebelmouse-image" id="2e692" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-showing-cars-parked-at-a-subway-charging-station.jpg?id=33422192&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Subway will be partnering with GenZ EV Solutions to roll out their EV Charging Oasis parks with charging, Wi-Fi, playgrounds, and picnic tables.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Subway</small></p><p>These new networks look to backstop President Biden’s plan to build 500,000 new EV chargers by 2030, up from around 130,000 today. The moves also underscore Tesla’s foresight in building its own proprietary Supercharger network—the first erected before it sold its first Model S in 2012—specifically to reduce range anxiety and assure potential buyers they could take longer trips in a Tesla. In February, Tesla agreed to open 3,500 of its Supercharger stalls to non-Tesla customers by late 2024, as part of that US $7.5 billion federal bid to expand plug-in infrastructure. Elon Musk’s company also plans to open access to 4,000 of its slower, Level 2 chargers, most in heavily trafficked locations such as hotels and restaurants.</p><p>Tesla aside, most major automakers have avoided getting deeply involved in the charging business, akin to car companies also operating gas stations. But with EVs exploding to 5.8 percent of all new-car sales in 2022, several are scrambling to fill a vacuum that’s been left to private charging outfits such as Chargepoint, EVGo, and Electrify America, the latter funded by Volkswagen’s multibillion-dollar “Dieselgate” emissions cheating settlement. And those non-Tesla providers have often been assailed with criticism over balky or broken chargers.</p><p class="pull-quote">EVs are finally fulfilling the dream of truly practical fill-ups on Interstate runs and vacations, without having to kill hours at every charging stop.</p><p>A J.D. Power study of EV owners’ charging experiences found plenty of concerns, including that 20 percent of respondents who visited a public charging station found it broken or out of service.</p><p>Aside from balky chargers or stations that dispense juice at lower-than-expected rates, other notorious issues include cords that are too short to easily reach EV charging ports. Scrawny parking spaces and poorly aligned chargers can make it difficult or impossible to access a plug, especially for brawnier electric SUVs and pickups.</p><p>Taking matters into its own hands, <a href="https://fordauthority.com/2021/12/ford-mustang-mach-e-charge-angels-program-details-revealed/" target="_blank">Ford created “ChargeAngels”</a> to personally visit chargers, report any problems to operators, and get them fixed to ensure that Ford EV owners—the vast majority driving their first-ever EV—don’t get soured on the experience.</p><p>General Motors, for its part, is partnering with <a href="https://www.evgo.com/" target="_blank">EVGo</a> on a national network of 2,000 DC fast-charging stalls, located at 500 Pilot and Flying J travel centers. GM is also lining up more than 4,400 dealers to build up to 10 Level 2 charging stations each, at both dealers and key locations, including underserved urban and rural communities. Ninety percent of the U.S. population lives within 10 miles of a GM dealer, according to the company. </p><p><a href="https://www.electrifyamerica.com/" target="_blank">VW’s Electrify America</a> is teaming with TravelCenters of America to install about 1,000 DC fast chargers at 200 TA/Petro locations along major highways. Those chargers will include EA’s megapowered 350-kilowatt stalls, currently among the world’s most powerful public units.</p><p>Pair those industrial-strength units with EVs that can accept that level of power—meaning cars with architectures of 800 volts or higher, such as the Lucid Air, Porsche Taycan, or the latest Hyundai, Kia, and Genesis models—and the dawdling Level 2 chargers of old can seem nearly obsolete in the public realm. In my recent tests of models including the Hyundai Ioniq and Kia EV6 GT, these stellar examples of the modern EV art charged their batteries to 80 percent full in 20 minutes or less. In other words, EVs are finally fulfilling the dream of truly practical fill-ups on Interstate runs and vacations, without having to kill hours at every charging stop. Such 20-minute stops, of course, are ideally suited to a fast bite on the road, or a needed rest stop on a family vacation. </p><p>Brent Gruber, J.D. Power executive director of global automotive, welcomes the rapid expansion of chargers. But Gruber says simply expanding the number of stations isn’t enough.</p><p>“Stations need to be added to areas where there are gaps in heavily traveled routes and in high-density areas for people who don’t have access to residential charging, but most importantly, designed with things for users to do while charging—regardless of the use case,” Gruber said. “Then, we need to make sure those stations are reliable.”</p>]]></description>
      <pubDate>Thu, 06 Apr 2023 20:39:07 +0000</pubDate>
      <guid>https://spectrum.ieee.org/dc-fast-charger</guid>
      <category>7charge network</category>
      <category>Ev charging oasis</category>
      <category>Evs</category>
      <category>Ev charger</category>
      <category>Dc fast-charging</category>
      <dc:creator>Lawrence Ulrich</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-car-charges-at-a-station-labelled-7charge.jpg?id=33422190&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Once More, With Feeling: Exploring Relatable Robotics at Disney</title>
      <link>https://spectrum.ieee.org/disney-robot-indestructibles</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-animated-gif-of-a-grey-humanoid-robot-in-a-helmet-knee-pads-and-pink-skates-falling-doing-a-roll-and-getting-back-up.gif?id=33424714&width=1200&height=800&coordinates=0%2C74%2C0%2C75"/><br/><br/><p><em>This is a guest post. The views expressed here are solely those of the author and do not represent positions of </em><a href="https://spectrum.ieee.org/" target="_self">IEEE Spectrum</a><em> or the IEEE.</em></p><p>
	Most robotics projects focus on the output: What does this robot do? Is it reliable, is it precise, and can it achieve its goals? But at Disney, our focus is on the story: How does this robot make you feel? Is it emotive, is it relatable, and does it authentically reflect a character people know in its mannerisms, gait, or expressions?
</p><p>
	This context changes everything. Take walking, for example—in robotics, it’s generally a priority to maximize the stability of a walking gait, since falling down doesn’t help you move crates or explore terrain. <a href="https://sites.disney.com/waltdisneyimagineering/" target="_blank">At Disney</a>, however, a stable walking gait is less important than a gait that brings a character to life. Falling down can be wildly entertaining, as long as the falling happens in character!
</p><p>
	About a year ago, <a href="https://sites.disney.com/waltdisneyimagineering/our-culture/" target="_blank">our team</a> came to a realization: We needed robots that didn’t mind taking the occasional tumble. If we’re going to be free to explore fun and evocative performances with our robots, failure had to be an option. And not only that—failure had to be expected, and built into the design. We called our new project “Indestructibles” and set out toward the goal implied by that name.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A cartoonish grey humanoid robot with its arms held out." class="rm-shortcode" data-rm-shortcode-id="34bba446462caface3ceedc7f5c0ca0c" data-rm-shortcode-name="rebelmouse-image" id="31d14" loading="lazy" src="https://spectrum.ieee.org/media-library/a-cartoonish-grey-humanoid-robot-with-its-arms-held-out.jpg?id=33401265&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Disney</small></p><hr/><p>
	On 10 March 2023, we were able to show off our latest Indestructibles prototype <a href="https://www.instagram.com/reel/CpoAZNqrhy1/?igshid=MGU3ZTQzNzY=" target="_blank">at SXSW</a> in Austin, Texas. We were nervous. We knew this little character had charmed us, but we couldn’t be sure her personality would come through on such a big stage with a brand-new audience. But from the moment she peeked her head out of her crate, the energy in the crowd let us know they were not only seeing her but cheering for her. We were thrilled!</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="316a15a11fe3631f32498bf17828c1cf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/lPqzLE4KjhI?rel=0&start=260" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>Getting to this point took a lot of exploration. At first, we were a little intimidated by the idea of making a robot that would bounce back from a fall. But after a few months of dropping ideas (and robots) on the floor, we found it was a fairly tractable problem. What’s more, we discovered it’s possible to make components out of ordinary materials that can survive large drops and big hits, especially at smaller scales where strength-to-weight ratios are in our favor. Protecting delicate electrical equipment was a bigger challenge, but reducing our points of failure and providing shock absorption in the right places kept us moving forward.</p><p>
	As excited as we were by the durability of our robot, we realized durability alone was not enough.
</p><p>
	In conventional robotics, physics is the final judge of what works best. Sizing a strut and positioning the center of mass can be done carefully in a computer, and the result in hardware is likely to match the intention. When the goal is creating a character, human hearts and minds are the final judge instead. People are much harder to simulate, and the full effect of a performance can only really be felt by being in the same room as the robot. So, the speed of our development was limited by how fast we could translate a new idea from concept to embodied performance.
</p><p>
	That meant a new approach to both hardware and software. On the hardware side, we needed to be able to have a robot that could change and adapt in a matter of days rather than weeks, all while maintaining reliability. And on the software side, we needed elegant interfaces that could allow us to rapidly try out new motions—in search of emotions.
</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="b2786552cadf07ca43d068fc1c5fd745" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/XEp3OV9k_O4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>
	Mechanically, we adopted a modular design strategy built around a single size of actuator, keeping the scale small while using carbon fiber to minimize weight. We also made the decision to tolerate a certain amount of flexibility in the joints, sacrificing rigidity for motor protection and ease of construction. This made it easy to change the proportions of the robot to match different characters. Just as important, it made it easy to add and subtract degrees of freedom. “What if the robot was on roller skates?” became a question that could be answered quickly with a new pair of feet.
</p><p>
	We also developed a simple, interactive software interface for the robot. We can move the robot by hand into keyframe poses, then blend those together smoothly to create motions already grounded in the physics of the robot. We kept the onboard code light, avoiding autonomy and restricting sensing to just motor positions so we could rapidly adapt the software as we iterated on the hardware.
</p><p>
	We also developed ways to pull key poses out of motion-capture data so we could directly integrate the human aspects of a physical performance. One early learning from this process was how holistic real human movement is. Every part of the body moves sympathetically with every other part, even if it’s subtle. Programming a robot directly, it can be easy to only move the joints producing the main action, and some life is lost as a result.
</p><p>
	Moving from motion capture to the robot isn’t seamless—the robot has a different mass distribution than a person, and a much more limited set of joints. But the effort is worth it when you see our latest prototype wobbling around in a way that makes you feel like you’re looking at a character rather than a robot.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A man wearing a nametag that says Morgan holds a child-size robot on his shoulders." class="rm-shortcode" data-rm-shortcode-id="fcb59c1a7c9eccea286c70cd4b591a85" data-rm-shortcode-name="rebelmouse-image" id="30fb6" loading="lazy" src="https://spectrum.ieee.org/media-library/a-man-wearing-a-nametag-that-says-morgan-holds-a-child-size-robot-on-his-shoulders.jpg?id=33401342&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Author Morgan Pope with the Indestructibles robot on his shoulders.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Disney</small></p><p>
	So what’s our biggest takeaway from the past year? There is a huge unexplored space of robotic locomotion that evokes human emotion. By shifting our emphasis toward <em>the way </em>we reach our goal—rather than just the end result—we’ve opened up what feels like a world of possibilities for dynamic and expressive robots.
</p><p>
	Which is why we’re so excited about what lies ahead!
</p>]]></description>
      <pubDate>Thu, 06 Apr 2023 18:47:49 +0000</pubDate>
      <guid>https://spectrum.ieee.org/disney-robot-indestructibles</guid>
      <category>Entertainment robots</category>
      <category>Disney</category>
      <category>Robotics</category>
      <category>Guest articles</category>
      <dc:creator>Morgan Pope</dc:creator>
      <media:content medium="image" type="image/gif" url="https://spectrum.ieee.org/media-library/an-animated-gif-of-a-grey-humanoid-robot-in-a-helmet-knee-pads-and-pink-skates-falling-doing-a-roll-and-getting-back-up.gif?id=33424714&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Survey Shows Overwhelming Interest in a Four-Day Workweek</title>
      <link>https://spectrum.ieee.org/four-day-workweek-survey-results</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/illustration-of-a-person-standing-next-to-a-giant-calendar-and-holding-a-giant-pencil.jpg?id=33422210&width=1200&height=800&coordinates=0%2C105%2C0%2C105"/><br/><br/><p><em>The Institute</em> conducted an<a href="https://spectrum.ieee.org/four-day-workweek" target="_self"> online poll</a> in December, asking readers for their thoughts on a four-day workweek. About 95 percent of the respondents said they want to work a shorter week—which to me means employees think they can complete their tasks in four days rather than five. However, 89 percent of the respondents’ companies don’t offer that type of work schedule.</p><p>The poll results inspired me to explore the matter further. I interviewed several engineers and tech company executives about four-day workweeks.</p><h2>Providing more flexibility</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="portrait of a man smiling in a green plaid shirt against a tan colored background" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="7478d3d119d3483e09bca7158639e4b5" data-rm-shortcode-name="rebelmouse-image" id="8cdba" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-man-smiling-in-a-green-plaid-shirt-against-a-tan-colored-background.jpg?id=33422241&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Before he retired, IEEE Senior Member John McWilliams was a senior innovation engineer at the Dairyland Power Cooperative, in La Crosse, Wis.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">John McWilliams</small></p><p>IEEE Senior Member <a href="https://www.linkedin.com/in/john-mcwilliams-senior-innovation-engineer/" rel="noopener noreferrer" target="_blank">John McWilliams</a> reflected on his time as a field service engineer from 1978 to 1988 with <a href="https://westinghouse.com/" rel="noopener noreferrer" target="_blank">Westinghouse</a> and how difficult it was to be available and prepared to go anywhere at any time. He retired in October from his job as senior innovation engineer at the <a href="https://dairylandpower.com/" rel="noopener noreferrer" target="_blank">Dairyland Power Cooperative</a>, in La Crosse, Wis.</p><p>“That was tough,” McWilliams recalls. “I did not get the time to physically and mentally recover from previous assignments. There was no time to go on a vacation, attend a concert or to just relax at home.” </p><p>Working that type of schedule at an early age negatively impacted his marriage and health, he says.</p><p>Westinghouse then assigned him to work on construction projects in Saudi Arabia in 50 °C weather, without time to rest after the job was completed. </p><p>“Making young professionals work a long work schedule is not right and has to stop,” McWilliams says. “They should be given a flexible work schedule to encourage them to join a company and stay there for a longer period of time.”</p><p>It wasn’t until 1988—when McWilliams began working at <a href="https://megger.com/" rel="noopener noreferrer" target="_blank">Megger</a>, a manufacturing company in Dover, England—that he experienced a shortened workweek: Employees worked half a day on Friday. He says it was wonderful because he could spend more time with his family and return to work Monday much happier and ready to get down to business.</p><p>McWilliams joined Dairyland Power in 1999 and went back to working eight hours per day, five days a week. The COVID-19 pandemic changed that, though, and allowed him to work from home. Dairyland then implemented a four-day workweek, which he says provided him with the best working conditions in his 44-year career.</p><h2>Taking care of tech workers’ mental health</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="woman wearing a white headscarf in front of a bush" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="dce8b4be01a36cfa053a068ca7669a94" data-rm-shortcode-name="rebelmouse-image" id="a9a49" loading="lazy" src="https://spectrum.ieee.org/media-library/woman-wearing-a-white-headscarf-in-front-of-a-bush.jpg?id=33422290&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">IEEE Member Amel Chenouf is an electronics engineer and a postdoctoral researcher at the Center for Development of Advanced Technologies, in Algiers.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Amel Chenouf</small></p><p>IEEE Member <a href="https://www.linkedin.com/in/amelchenouf/?originalSubdomain=dz" rel="noopener noreferrer" target="_blank">Amel Chenouf</a>, an electronics engineer and a postdoctoral researcher at the <a href="https://www.cdta.dz/en/" rel="noopener noreferrer" target="_blank">Center for Development of Advanced Technologies</a>, an R&D hub in Algiers, has never had a job that allowed her to work fewer than five days per week. </p><p>“The only time I worked a nonstandard five-day workweek was during the pandemic, when my company had employees work virtually,” Chenouf says. “It was the only opportunity I had to be with my family.”</p><p>A shorter workweek, she says, would encourage more girls and women to join the engineering industry. She says many female engineers she went to school with did not pursue a senior-level position at their company because they did not want to give up time with their family.</p><p>“A four-day workweek would allow them to spend quality time with their families and address any care issues,” she says.</p><p>She also says giving employees more time for themselves and more time with their family and friends would be good for their mental health. When she would return to work after a weekend spent with friends, she says, her colleagues noticed she was full of energy.</p><h2>A work/life balance</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="portrait of a woman with long hair and glasses wearing a denim jacket" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="60867a74445367ce07f001ee9f9d4169" data-rm-shortcode-name="rebelmouse-image" id="0f545" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-long-hair-and-glasses-wearing-a-denim-jacket.jpg?id=33422303&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Amanda Barbosa is a robotics engineering student at Federal University of ABC, in Brazil.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Amanda Barbosa</small></p><p>Many people, including <a href="https://www.linkedin.com/in/amanda-barbosa-ab2053245/" rel="noopener noreferrer" target="_blank">Amanda Barbosa</a>, are excited for the potential of a four-day workweek. Barbosa is a robotics engineering student at <a href="https://www.ufabc.edu.br/en/" rel="noopener noreferrer" target="_blank">Federal University of ABC</a>, in Brazil, who is working full time in Santos for <a href="https://www.leroymerlin.fr/" rel="noopener noreferrer" target="_blank">Leroy Merlin</a>, a household goods company.</p><p>“With a four-day workweek, I can focus more on my schoolwork and have more time to be with my friends and family,” Barbosa says.</p><p>“Advancements in technologies, like AI and automation, will help in accelerating the transition to a shorter week,” she adds. “It will minimize the time needed to be in-person at work.”</p><h2>Increasing productivity with a shorter workweek</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="portrait of a man with dark hair and wearing a red sweater against a bright blue background" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="2be900fda6b2de12e1a3c44f2784ec57" data-rm-shortcode-name="rebelmouse-image" id="cdbbf" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-man-with-dark-hair-and-wearing-a-red-sweater-against-a-bright-blue-background.jpg?id=33422320&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Dante Medina is an electrical and electronics engineering specialist at Mercedes-Benz Argentina.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Dante Medina</small></p><p><a href="https://www.linkedin.com/in/dante-medina/" rel="noopener noreferrer" target="_blank">Dante Medina</a>, an electrical and electronics engineering specialist at <a href="https://www.mercedes-benz.com.ar/?group=all&subgroup=all.saloon&view=BODYTYPE" rel="noopener noreferrer" target="_blank">Mercedes-Benz Argentina</a>, says he believes a four-day workweek would be beneficial for employees.</p><p>Mercedes-Benz doesn’t provide its employees with the option of working a shorter week. But Medina says that if it did, employees could have a better work/life balance and therefore be more productive at work.</p><p>But, he adds, companies need to coordinate employee schedules to ensure the business’s services or products are delivered on time. Switching from providing uninterrupted technical support to an assembly plant from five days to four days, for example, would require planning, he notes.</p><h2>Creating a more flexible work environment</h2><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="man with glasses wearing a red tie, blue shirt and black blazer in front of a gray background" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="421debb586147cc59841c1f6e78dc341" data-rm-shortcode-name="rebelmouse-image" id="7ceb8" loading="lazy" src="https://spectrum.ieee.org/media-library/man-with-glasses-wearing-a-red-tie-blue-shirt-and-black-blazer-in-front-of-a-gray-background.jpg?id=33422345&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;"> Life Senior Member Walter D. Downing is chief operating officer at Southwest Research Institute, in San Antonio.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Walter D. Downing</small></p><p>The pandemic prompted the <a href="https://www.swri.org/" rel="noopener noreferrer" target="_blank">Southwest Research Institute</a>, in San Antonio, to be creative when it came to scheduling. The nonprofit provides engineering and science contract research and development services to government and industrial clients.</p><p> SwRI’s chief operating officer, IEEE Life Senior Member <a href="https://www.swri.org/walt-downing" rel="noopener noreferrer" target="_blank">Walter D. Downing</a>, says that because of how diverse the organization’s client base is, providing employees with a flexible work schedule has been the best approach.</p><p>The majority of employees opt to work remotely, Downing says, unless they need to attend a meeting or to complete a task in the laboratory. The flexible schedule lets them adapt to their clients’ schedules, he says, adding that it also has helped SwRI attract more women and young professionals.</p><p>“The flexible schedule has allowed employees to adjust their working hours to meet their family’s needs,” he says. “Staff members who take care of their parents or have children can choose the days and hours they work as long as they are available during core hours.”</p><p>Employees have to be available three days a week, whether in person or virtually, to do some core activities, he says.</p><p>SwRI’s flexible scheduling is just one example of how an employer can accommodate employees.</p><p>More than <a href="https://www.marketwatch.com/story/these-61-companies-moved-to-a-4-day-work-week-heres-what-happened-to-revenue-and-employees-relationship-to-their-job-2fe42224" rel="noopener noreferrer" target="_blank">90 percent of the 61 businesses</a> in the United Kingdom that participated in a study last year of a compressed workweek reported they will continue to offer a four-day week.</p><p>A <a href="https://www.businessinsider.com/congressman-takano-reintroduces-bill-to-make-4-day-workweek-law-2023-3" rel="noopener noreferrer" target="_blank">bill</a> was reintroduced in the U.S. House of Representatives in March to establish a standard 32-hour workweek.</p><p>It is likely to take time for a four-day workweek to become standard across the globe, but I believe it will happen.</p>]]></description>
      <pubDate>Thu, 06 Apr 2023 18:00:03 +0000</pubDate>
      <guid>https://spectrum.ieee.org/four-day-workweek-survey-results</guid>
      <category>Careers</category>
      <category>Ieee member news</category>
      <category>Type:ti</category>
      <category>Worklife balance</category>
      <dc:creator>Qusi Alqarqaz</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/illustration-of-a-person-standing-next-to-a-giant-calendar-and-holding-a-giant-pencil.jpg?id=33422210&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Remembering the Legacy of Trailblazing Technologist Gordon Moore</title>
      <link>https://spectrum.ieee.org/the-legacy-of-gordon-moore</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-black-and-white-photo-of-a-man-in-a-tie-and-glasses.jpg?id=33401131&width=1200&height=800&coordinates=0%2C76%2C0%2C41"/><br/><br/><p><a href="https://www.intel.com/content/www/us/en/homepage.html?cid=sem&source=sa360&campid=2023_q1_cbu_us_gmocoma_gmobao_awa_text-link_brand_exact_cd_Intel-Brand-SEM_3500088532_google_b2c_is_non-pbm_intel&ad_group=brand_freeform_b2b1-awa&intel_term=intel&sa360id=43700071555342085&&&gclid=Cj0KCQjw2v-gBhC1ARIsAOQdKY2JUvX0J4WjSlQsOZm3CRnRjxsliDOH-yRwIvRfOtkc1lUQCLfwQioaAieGEALw_wcB&gclsrc=aw.ds" rel="noopener noreferrer" target="_blank">Intel</a> cofounder <a href="https://spectrum.ieee.org/gordon-moores-next-act" target="_self">Gordon E. Moore</a>, the man behind Moore’s Law, died on 24 March at the age of 94.</p><p>The IEEE Fellow was awarded the 2008 <a href="https://corporate-awards.ieee.org/recipients/ieee-medal-of-honor-recipients/" rel="noopener noreferrer" target="_blank">IEEE Medal of Honor</a> for “pioneering technical roles in integrated-circuit processing, and leadership in the development of MOS memory, the microprocessor computer, and the semiconductor industry.” </p><p>Moore founded Intel in 1968 with computing pioneer <a href="https://spectrum.ieee.org/robert-noyce-and-the-tunnel-diode" target="_self">Robert Noyce</a>. Moore, Noyce, and other Intel engineers are credited with bringing laptop computers and numerous other electronics to millions of people thanks to their semiconductor development. Intel microprocessors now power personal computers made by major manufacturers including <a href="https://www.dell.com/en-us/dt/corporate/about-us/who-we-are.htm" rel="noopener noreferrer" target="_blank">Dell</a>, <a href="http://www.hp.com/" rel="noopener noreferrer" target="_blank">HP</a>, and <a href="https://www.ibm.com/" rel="noopener noreferrer" target="_blank">IBM</a>.</p><p>Moore is best known for his 1965 prediction, which would become known as <a href="https://spectrum.ieee.org/gordon-moore-the-man-whose-name-means-progress" target="_self">Moore’s Law</a>: the observation that the number of transistors on an integrated circuit would grow exponentially while the retail cost of computers would decrease.</p><p>His original hypothesis, published in a 1965 <a href="https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf" rel="noopener noreferrer" target="_blank"><em>Electronics</em> magazine article</a>, was that the number of transistors would double each year. His projection came true over the decade that followed. In 1975 he revised the theory and forecast that transistors would double every 18 months—a statement that held true for several decades. Moore’s Law set the bar for semiconductor manufacturers and is still driving computing innovations today.</p><p>“Gordon Moore, with his prediction that turned to <em>law</em>, captured the very gestalt of the semiconductor industry as an exponential ambition,” says IEEE Fellow <a href="https://spectrum.ieee.org/aart-de-geus-profile" target="_self">Aart de Geus</a>, CEO of <a href="http://www.synopsys.com/" rel="noopener noreferrer" target="_blank">Synopsys</a>. “He became not only a visionary but also our coach, pushing us to build the impossible. Now, 58 years later, classic Moore’s Law has morphed into <a href="https://www.synopsys.com/glossary/what-is-sysmoore.html" rel="noopener noreferrer" target="_blank">SysMoore</a>—systemic complexity with a Moore’s Law ambition. His legacy fuels our aspirations and inspirations to further decades of exponential impact.</p><p>“Gordon, thank you for being <em>the</em> motivating coach in our field and on my own professional path!”</p><h2>From researcher to entrepreneur</h2><p>Moore received a bachelor’s degree in chemistry in 1950 from the <a href="https://www.berkeley.edu/" rel="noopener noreferrer" target="_blank">University of California, Berkeley</a>. After earning his Ph.D., also in chemistry, in 1954 from <a href="https://www.caltech.edu/" rel="noopener noreferrer" target="_blank">Caltech</a>, he began his career as a researcher in the <a href="https://www.jhuapl.edu/" rel="noopener noreferrer" target="_blank">Applied Physics Laboratory</a> at <a href="https://www.jhu.edu/" rel="noopener noreferrer" target="_blank">Johns Hopkins University</a>, in Baltimore.</p><p>After two years, he moved back to California and joined <a href="https://en.wikipedia.org/wiki/Shockley_Semiconductor_Laboratory" rel="noopener noreferrer" target="_blank">Shockley Semiconductor</a>, a West Coast division of <a href="https://www.bell-labs.com/" rel="noopener noreferrer" target="_blank">Bell Labs</a> that set out to develop an inexpensive silicon transistor. Unhappy with William Shockley’s leadership, Moore, Noyce, and six other Shockley associates left the company on the same day in 1957. They became known as the “<a href="https://www.pbs.org/transistor/album1/eight/index.html" rel="noopener noreferrer" target="_blank">traitorous eight</a>” when they left to form <a href="https://en.wikipedia.org/wiki/Fairchild_Semiconductor" rel="noopener noreferrer" target="_blank">Fairchild Semiconductor</a>, a division of <a href="https://en.wikipedia.org/wiki/Fairchild_Camera_and_Instrument" rel="noopener noreferrer" target="_blank">Fairchild Camera and Instrument</a> in Sunnyvale, Calif. The company became a pioneer in the manufacturing of transistors and ICs.</p><h2>The cornerstone of Silicon Valley</h2><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A photo of 3 men behind the image of a microchip. " class="rm-shortcode" data-rm-shortcode-id="a43e863d4788989e1b10099b1b622969" data-rm-shortcode-name="rebelmouse-image" id="1cd32" loading="lazy" src="https://spectrum.ieee.org/media-library/a-photo-of-3-men-behind-the-image-of-a-microchip.jpg?id=33401171&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Moore [right], along with Andy Grove [left] and Robert Noyce founded Intel in 1968.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Intel</small></p><p>Moore and Noyce decided in 1968 to leave Fairchild and start their own company dedicated to semiconductor memory. The two engineers, along with <a href="https://spectrum.ieee.org/profile-andy-grove" target="_self">Andrew Grove</a>, an IC engineer and former assistant director of development at Fairchild, founded Integrated Electronics (later shortened to Intel). Moore served as the company’s executive vice president.</p><p>The founders experimented with silicon-gate metal-oxide semiconductors. To create MOS, they deposited aluminum wires connecting multiple transistors on the surface of a thumbnail-size piece of silicon. The chemically treated substance was key to the development of smaller and smaller electronic circuitry that would work at increasingly higher speeds.</p><p>Intel’s first product, the <a href="https://www.intel.com/content/www/us/en/history/virtual-vault/articles/intels-first-product-3101.html" rel="noopener noreferrer" target="_blank">3101</a> 64-bit SRAM, was released in 1969. It was nearly twice as fast as existing memory products by competitors including Fairchild and the <a href="https://tsukuba.fandom.com/wiki/ETL" rel="noopener noreferrer" target="_blank">Electrotechnical Laboratory</a> of Tsukuba, Japan. Intel’s <a href="https://en.wikipedia.org/wiki/Intel_1103" rel="noopener noreferrer" target="_blank">1103</a> was released in 1970, and it became the world’s best-selling semiconductor memory chip by 1972.</p><p>The company created the first commercially available microprocessor, the <a href="https://en.wikipedia.org/wiki/Intel_4004" rel="noopener noreferrer" target="_blank">4004</a>, in 1971. It miniaturized the central processing unit, enabling small electronics to perform calculations that only large machines had been capable of doing.</p><p>Moore served as Intel’s president from 1975 to 1979, and then became CEO and chairman of the board. In the early 1980s, inspired by the success of the 4004, Moore decided to shift the company’s focus from semiconductors to microprocessors.</p><p>Intel supplied microprocessors to several companies, including <a href="https://www.ibm.com/us-en" rel="noopener noreferrer" target="_blank">IBM</a>, helping it capitalize on the rapidly growing PC market and ushering in a 10-year period of unprecedented growth.</p><p>Moore stepped down as CEO in 1987 but remained chairman until he retired in 1997. He served as chairman emeritus until 2006.</p><p>Under his leadership, Intel didn’t just fuel the growth of personal computing; it also provided the foundation of what became known as Silicon Valley, as detailed in his <a href="https://www.washingtonpost.com/obituaries/2023/03/24/gordon-moore-intel-founder-dead/" rel="noopener noreferrer" target="_blank"><em>Washington Post</em></a> obituary. Intel helped cement the region as a global center for technological innovation, the article says.</p><h2>Moore’s Law: a self-fulfilling prophecy</h2><p>In Moore’s now-famous article for <em>Electronics</em>, he predicted the trajectory of how powerful microchips would become over time, while costs to the consumer would continue to drop.</p><p>“At the time I wrote the article, I thought I was just showing a local trend,” he told <a href="https://spectrum.ieee.org/gordon-moore-the-man-whose-name-means-progress" target="_self"><em>IEEE Spectrum</em></a> in 2015. “The integrated circuit was changing the economy of the whole [electronics] industry, and this was not yet generally recognized. So I wrote the article to try to get the point across: This is the way the industry is going to get things really cheap.”</p><p>His theory came from an observation of the planar transistor—designed in 1957 by Fairchild physicist <a href="https://en.wikipedia.org/wiki/Jean_Hoerni" rel="noopener noreferrer" target="_blank">Jean Hoerni</a>, in which the oxide layer is left in place on a silicon wafer to protect the sensitive semiconductor materials underneath.</p><p>“I noticed that the [number of components] had about doubled every year. And I just did a wild extrapolation, saying it’s going to continue to double every year for the next 10 years,” he told <em>IEEE Spectrum</em>.</p><p>Nearly 60 years later, his prediction is still driving the industry forward. As of December 2022, the largest transistor count on a commercial processor—<a href="https://www.apple.com/newsroom/2022/03/apple-unveils-m1-ultra-the-worlds-most-powerful-chip-for-a-personal-computer/" rel="noopener noreferrer" target="_blank">Apple’s M1 Ultra chip</a>—was 114 billion.</p><p>Although Moore’s Law will inevitably slow and come to an end, <a href="https://www.intel.com/content/www/us/en/newsroom/news/moores-law-paves-way-trillion-transistors-2030.html" rel="noopener noreferrer" target="_blank">Intel predicts</a> that chip density will continue to increase to 3 trillion transistors by 2030. </p><h2>A lasting legacy</h2><p>Moore received several IEEE recognitions for his pioneering innovations. In addition to the 2008 Medal of Honor, he received the <a href="https://www.computer.org/" rel="noopener noreferrer" target="_blank">IEEE Computer Society’s</a> 1978 <a href="https://www.computer.org/volunteering/awards/goode" rel="noopener noreferrer" target="_blank">Goode Memorial Award</a> and its 1978 <a href="https://www.computer.org/volunteering/awards/mcdowell" rel="noopener noreferrer" target="_blank">McDowell Award</a>, and he and Noyce received its 1986 <a href="https://www.computer.org/volunteering/awards/entrepreneur" rel="noopener noreferrer" target="_blank">Computer Entrepreneur Award</a>.</p><p>In 2002 Moore received a U.S. <a href="https://en.wikipedia.org/wiki/Presidential_Medal_of_Freedom" rel="noopener noreferrer" target="_blank">Presidential Medal of Freedom</a>—the country’s highest civilian honor. He also was awarded a <a href="https://nationalmedals.org/laureate/gordon-moore/" rel="noopener noreferrer" target="_blank">National Medal of Technology and Innovation</a> in 1990. </p><p>Moore was a dedicated philanthropist who donated to charities devoted to environmental conservation, science, and improved health care. Along with his wife of 72 years, in 2000 Moore established the <a href="https://www.moore.org/" rel="noopener noreferrer" target="_blank">Gordon and Betty Moore Foundation</a>, which has donated more than US $5.1 billion to charitable causes.</p><p>“Gordon Moore’s contributions to society went far beyond semiconductors and Moore’s Law,” says IEEE Member <a href="https://spectrum.ieee.org/stopping-the-spread-of-disinformation" target="_self">Siavash Alamouti</a>, cofounder of computing company <a href="https://mimik.com/mimikcontact/" rel="noopener noreferrer" target="_blank">Mimik</a> and the 2022 Marconi Prize recipient. “He was a champion for digital inclusion and supported our initiatives for affordable and open mobile Internet and many other impactful technologies with a direct impact on our lives. He will be sorely missed.”</p>]]></description>
      <pubDate>Wed, 05 Apr 2023 12:00:01 +0000</pubDate>
      <guid>https://spectrum.ieee.org/the-legacy-of-gordon-moore</guid>
      <category>Ieee member news</category>
      <category>Type:ti</category>
      <category>In memoriam</category>
      <category>Moore's law</category>
      <category>Gordon moore</category>
      <category>Intel</category>
      <category>Obituary</category>
      <category>Obit</category>
      <category>Semiconductors</category>
      <category>Ic</category>
      <dc:creator>Amanda Davis</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-black-and-white-photo-of-a-man-in-a-tie-and-glasses.jpg?id=33401131&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Learn From the Best Minds in Commercial Robotics Development</title>
      <link>https://spectrum.ieee.org/robotics-summit-expo</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/conference-advertisement-banner-features-on-the-left-a-robotic-arm-being-observed-by-people-and-on-the-right-the-words-robotics.jpg?id=33406380&width=1200&height=800&coordinates=12%2C0%2C246%2C0"/><br/><br/><p><em>This sponsored article is brought to you by <a href="https://www.roboticssummit.com/" rel="noopener noreferrer" target="_blank">Robotics Summit & Expo</a>.</em></p><p>The <a href="https://www.roboticssummit.com/" rel="noopener noreferrer" target="_blank">Robotics Summit & Expo</a>, taking place May 10-11 at the Boston Convention Center, will bring together the brightest minds in robotics to share their commercial robotics development experiences. </p><p>Learn from industry-leading speakers, build new relationships by networking, see demos from 150+ exhibitors showcasing enabling technologies to help build commercial robots.</p><hr/><h3></h3><br/><img alt="Event logo has white and gray geometric figure on the left and on the right the words Robotics Summit & Expo in black and red." class="rm-shortcode" data-rm-shortcode-id="39bb8739e4109a45ed21539a734face9" data-rm-shortcode-name="rebelmouse-image" id="297f5" loading="lazy" src="https://spectrum.ieee.org/media-library/event-logo-has-white-and-gray-geometric-figure-on-the-left-and-on-the-right-the-words-robotics-summit-expo-in-black-and-red.png?id=29742035&width=980"/><a href="https://www.roboticssummit.com/" rel="noopener noreferrer" target="_blank">Use code IEEE25</a> at checkout to save 25% off your full conference pass!<h3></h3><br/><p>The conference programming will provide professionals the information they need to successfully develop the next generation of commercial robots. This year’s program has an exceptional lineup of <a href="https://www.roboticssummit.com/speakers/" target="_blank">speakers</a>.</p><p>The <a href="https://www.roboticssummit.com/" target="_blank">Robotics Summit</a> keynote speakers include the following: </p><ul><li>Howie Choset, Professor of Robotics, Carnegie Mellon University: “Idea to Reality: Commercializing Robotics Technologies”</li><li>Laura Major, CTO, Motional: “Scalable AI Solutions for Driverless Vehicles”</li><li>Marc Raibert, Executive Director, AI Institute: “The Next Decade in Robotics”</li><li>Martin Buehler, Global Head of Robotics R&D, Johnson & Johnson MedTech: “The Future of Surgical Robotics”</li><li>Nicolaus Radford, CEO, Nauticus Robotics: “Developing Robots for Final Frontiers” </li></ul><p>The expo hall at the Robotics Summit will have more than 150 exhibitors showcasing their latest enabling technologies, products and services that can help robotics engineers throughout their development journey. </p><p>The <a href="https://www.roboticssummit.com/" target="_blank">Robotics Summit</a> also offers networking opportunities, a Career Fair, a robotics development challenge and much more.</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="bb27b2d92404ce2ade88cae0a780a70c" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/8TDm7ynAoPY?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><h3></h3><br/><p>Gain full access to the world’s leading event dedicated to commercial robotics development with our discounted rate.<br/></p><p><a href="https://www.roboticssummit.com/" target="_blank">Use code IEEE25</a> at checkout to save 25% off your full conference pass!</p><p>Discounts are also available for academia, associations, and corporate groups. Please e-mail <a href="mailto:events@wtwhmedia.com" rel="noopener noreferrer">events@wtwhmedia.com</a> for more details about our discount programs.</p><p>Expo only tickets are just $75. Attendees can purchase tickets for the event here.</p>]]></description>
      <pubDate>Tue, 04 Apr 2023 15:57:30 +0000</pubDate>
      <guid>https://spectrum.ieee.org/robotics-summit-expo</guid>
      <category>Robotics summit</category>
      <category>Robotics</category>
      <category>Conferences</category>
      <dc:creator>Robotics Summit &amp; Expo</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/conference-advertisement-banner-features-on-the-left-a-robotic-arm-being-observed-by-people-and-on-the-right-the-words-robotics.jpg?id=33406380&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Ending an Ugly Chapter in Chip Design</title>
      <link>https://spectrum.ieee.org/chip-design-controversy</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/six-squares-contain-variously-sized-rectangles-of-four-colors-with-differently-colored-blobs-filling-in-gaps-between-the-rectang.jpg?id=33402790&width=1200&height=800&coordinates=0%2C52%2C0%2C52"/><br/><br/><p>
	Discussions at chip design conferences rarely get heated. But a year ago at the <a href="https://ispd.cc/ispd2023/index.php" target="_blank">International Symposium on Physical Design</a> (ISPD), things got out of hand. It was described by observers as a “trainwreck” and an “ambush.” The crux of the clash was whether Google’s AI solution to one of chip design’s thornier problems was really better than those of humans or state-of-the-art algorithms. It pitted established male electronic design automation (EDA) experts against two young female Google computer scientists, and the underlying argument had already led to the firing of one Google researcher.<br/>
</p><p>
	This year at that same conference, a leader in the field, <a href="https://cse.ucsd.edu/people/faculty-profiles/andrew-b-kahng" target="_blank">IEEE Fellow Andrew Kahng</a>, hoped to put an end to the acrimony once and for all. He and colleagues at the University of California, San Diego, delivered what he called <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#faqs" rel="noopener noreferrer" target="_blank">“an open and transparent assessment” of Google’s reinforcement learning</a> approach. Using Google’s open-source version of its process, called Circuit Training, and reverse-engineering some parts that were not clear enough for Kahng’s team, they set reinforcement learning against a human designer, commercial software, and state-of-the-art academic algorithms. Kahng declined to speak with <em>IEEE Spectrum</em> for this article, but he spoke to engineers last week at ISPD, which was held virtually.
</p><p>
	In most cases, Circuit Training was not the winner, but it was competitive. That’s especially notable given that the experiments did not allow Circuit Training to use its signature ability—to improve its performance by learning from other chip designs.
</p><p>
	 “Our goal has been clarity of understanding that will allow the community to move on,” he told engineers. Only time will tell whether it worked.
</p><h2>The Hows and the Whens</h2><p>
	The problem in question is called placement. Basically, it is the process of determining where chunks of logic or memory should be placed on a chip in order to maximize the chip’s operating frequency while minimizing its power consumption and the area it takes up. Finding an optimal solution to this puzzle is among the most difficult problems around, with more possible permutations than the game Go.
</p><p>
	But Go was ultimately defeated by a type of AI called deep reinforcement learning, and that’s just what former Google Brain researchers Azalia Mirhoseini and Anna Goldie applied to the placement problem. The scheme, then called Morpheus, treats placing large pieces of circuitry, called macros, as a game, learning to find an optimal solution. (The locations of macros have an outsize impact on the chip’s characteristics. In Circuit Training and Morpheus, a separate algorithm fills in the gaps with the smaller parts, called standard cells. Other methods use the same process for both macros and standard cells.)
</p><p>
	Briefly, this is<strong> how it works</strong>: The chip’s design file starts as what’s called a netlist—which macros and cells are connected to which others according to what constraints. The standard cells are then collected into clusters to help speed up the training process. Circuit Training then starts placing the macros on the chip “canvas” one at a time. When the last one is down, a separate algorithm fills in the gaps with the standard cells, and the system spits out a quick evaluation of the attempt, encompassing the length of the wiring (longer is worse), how densely packed it is (more dense is worse), and how congested the wiring is (you guessed it, worse). Called proxy cost, this acts like the score would in a reinforcement-learning system that was figuring out how to play a video game. The score is used as feedback to adjust the neural network, and it tries again. Wash, rinse, repeat. When the system has finally learned its task, commercial software does a full evaluation of the complete placement, generating the kind of metrics that chip designers care about, such as area, power consumption, and constraints on frequency.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A chart showing how Google's reinforcement leaning system works." class="rm-shortcode" data-rm-shortcode-id="b85ad387ddef89aee43898599e8aaa23" data-rm-shortcode-name="rebelmouse-image" id="d6a41" loading="lazy" src="https://spectrum.ieee.org/media-library/a-chart-showing-how-google-s-reinforcement-leaning-system-works.png?id=33406238&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Google’s reinforcement learning system treats placing large circuit blocks called macros as a game. The agent places one block at a time on the chip canvas. Then a separate algorithm fills in smaller parts called standard cells. The placement is scored according to several metrics, and that score is used as feedback to improve the agent.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IEEE Spectrum</small>
</p><p>
	Mirhoseini and Goldie published the results and method of Morpheus in <a href="https://www.nature.com/articles/s41586-021-03544-w" target="_blank"><em>Nature</em> in June 2021</a>, following a seven-month review process. (<a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https:/static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03544-w/MediaObjects/41586_2021_3544_MOESM1_ESM.pdf" rel="noopener noreferrer" target="_blank">Kahng was reviewer No. 3</a>.) And the technique was used to design more than one generation of <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">Google’s TPU AI accelerator chips</a>. (So yes, data you used today may have been processed by an AI running on a chip partly designed by an AI. But that’s increasingly the case as EDA vendors such as Cadence and Synopsys go <a href="https://news.synopsys.com/2023-03-29-Synopsys-ai-Unveiled-as-Industrys-First-Full-Stack,-AI-Driven-EDA-Suite-for-Chipmakers" rel="noopener noreferrer" target="_blank">all in on AI-assisted chip design</a>.) In January 2022, they released an open-source version, <a href="https://github.com/google-research/circuit_training" rel="noopener noreferrer" target="_blank">Circuit Training, on GitHub</a>. But Kahng and others claim that even this version was not complete enough to reproduce the research.
</p><p>
	In response to the <em>Nature</em> publication, a separate group of engineers, mostly within Google, began research aimed at what they believed to be a better way of comparing reinforcement learning to established algorithms. But this was no friendly rivalry. According to <a href="https://www.wired.com/story/google-brain-ai-researcher-fired-tension/" rel="noopener noreferrer" target="_blank">press reports</a>, its leader Satrajit Chatterjee, repeatedly undermined Mirhoseini and Goldie personally and was fired for it in 2022.
</p><p>
	While Chatterjee was still at Google, his team produced a paper titled “<a href="https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/05/MLcontra.pdf" rel="noopener noreferrer" target="_blank">Stronger Baselines</a>,” critical of the research published in <em>Nature</em>. He sought to have it presented at a conference, but after review by an independent resolution committee, Google refused. After his termination, an early version of the paper was leaked via an <a href="https://twitter.com/CADEXPERT4" rel="noopener noreferrer" target="_blank">anonymous Twitter account</a> just ahead of ISPD in 2022, leading to the public confrontation.
</p><h2>Benchmarks, Baselines, and Reproducibility</h2><p>
	When <em>IEEE Spectrum</em> spoke with EDA experts following ISPD 2022, detractors had three interrelated concerns—benchmarks, baselines, and reproducibility.
</p><p>
<strong>Benchmarks</strong> are openly available blocks of circuitry that researchers test their new algorithms on. The benchmarks when Google began its work were already about two decades old, and their relevance to modern chips is debated. University of Calgary professor Laleh Behjat compares it to planning a modern city versus planning a 17th-century one. The infrastructure needed for each is different, she says. However, others point out that there is no way for the research community to progress without everyone testing on the same set of benchmarks.
</p><p>
	Instead of the benchmarks available at the time, the <em>Nature</em> paper focused on doing the placement for Google’s TPU, a complex and cutting-edge chip whose design is not available to researchers outside of Google. The leaked “Stronger Baselines” work placed TPU blocks but also used the old benchmarks. While Kahng’s new work also did placements for the old benchmarks, the main focus centered on three more-modern designs, two of which are newly available, including a multicore RISC-V processor.
</p><p>
<strong>Baselines</strong> are the state-of-the art algorithms your new system competes against. <em>Nature</em> compared a human expert using a commercial tool to reinforcement learning and to the leading academic algorithm of the time, RePlAce. Stronger Baselines contended that the <em>Nature</em> work didn’t properly execute RePlAce and that another algorithm, simulated annealing, needed to be compared as well. (To be fair, simulated annealing results appeared in the addendum to the <em>Nature</em> paper.)
</p><p>
	But it’s the <strong>reproducibility</strong> bit that Kahng was really focused on. He claims that Circuit Training, as it was posted to GitHub, fell short of allowing an independent group to fully reproduce the procedure. So they took it upon themselves to reverse engineer what they saw as missing elements and parameters.
</p><p>
	Importantly, Kahng’s group publicly <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#readme" rel="noopener noreferrer" target="_blank">documented the progress, code, data sets, and procedure</a> as an example of how such work can enhance reproducibility. In a first, they even managed to persuade EDA software companies Cadence and Synopsys to allow the publication of the high-level scripts used in the experiments. “This was an absolute watershed moment for our field,” said Kahng.
</p><p>
	The UCSD effort, which is referred to simply as <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#readme" rel="noopener noreferrer" target="_blank">MacroPlacement</a>, was not meant to be a one-to-one redo of either the <em>Nature</em> paper or the leaked Stronger Baselines work. Besides using modern public benchmarks unavailable in 2020 and 2021, MacroPlacement compares Circuit Training (though not the most recent version) to a commercial tool, <a href="https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/innovus-mixed-placer" rel="noopener noreferrer" target="_blank">Cadence’s Innovus concurrent macro placer (CMP)</a>, and to a method developed at Nvidia called <a href="https://developer.nvidia.com/blog/autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus/" rel="noopener noreferrer" target="_blank">AutoDMP</a> that is so new it was only publicly introduced at ISPD 2023 minutes before Kahng spoke.
</p><h2>Reinforcement Learning vs. Everybody</h2><p>
	Kahng’s paper reports results on the three modern benchmark designs implemented using two technologies—NanGate45, which is open source, and <a href="https://gf.com/gf-press-release/globalfoundries-introduces-new-12nm-finfet-technology-high-performance-applications/" rel="noopener noreferrer" target="_blank">GF12, which is a commercial GlobalFoundries FinFET process</a>. (The TPU results reported in <em>Nature</em> used even more advanced process technologies.) Kahng’s team measured the same six metrics Mirhoseini and Goldie did in their <em>Nature</em> paper: area, routed wire length, power, two timing metrics, and the previously mentioned proxy cost. (Proxy cost is not an actual metric used in production, but it was included to mirror the <em>Nature</em> paper.) The results were mixed.
</p><p>
	As it did in the original <em>Nature</em> paper, reinforcement learning beat RePlAce on most metrics for which there was a head-to-head comparison. (RePlAce did not produce an answer for the largest of the three designs.) Against a human expert, Circuit Training frequently lost. Versus simulated annealing, the contest was a bit more even. </p><p>
	For these experiments, the big winners were the newest entrants CMP and AutoDMP, which delivered the best metrics in more cases than any other method.
</p><p>
	In the tests meant to match Stronger Baselines, using older benchmarks, both RePlAce and simulated annealing almost always beat reinforcement learning. But these results report only one production metric, wire length, so they don’t present a complete picture, argue Mirhoseini and Goldie.
</p><h2>A Lack of Learning</h2><p>
	Understandably, Mirhoseini and Goldie have their own criticisms of the MacroPlacement work, but perhaps the most important is that it did not use neural networks that had been pretrained on other chip designs, robbing their method of its main advantage. Circuit Training “unlike any of the other methods presented, can learn from experience, producing better placements more quickly with every problem it sees,” they wrote in an email.
</p><p>
	But in the MacroPlacement experiments each Circuit Training result came from a neural network that had never seen a design before. “This is analogous to resetting AlphaGo before each match…and then forcing it to learn how to play Go from scratch every time it faced a new opponent!”
</p><p>
	The results from the <em>Nature</em> paper bear this out, showing that the more blocks of TPU circuitry the system learned from, the better it placed macros for a block of circuitry it had not yet seen. It also showed that a reinforcement-learning system that had been pretrained could produce a placement in 6 hours of the same quality as an untrained one after 40 hours.
</p><h3>Reinforcement Learning vs. Everybody</h3><br/><div class="flourish-embed" data-src="story/1878375?607871"><script src="https://public.flourish.studio/resources/embed.js"></script></div><p class="caption">Circuit training (Google’s open-source reinforcement-learning placement tool) was pitted against placement algorithms (RePlAce, Simulated annealing, and AutoDMP) as well as a commercial software tool (CMP) and a human expert. The tests were on three designs implemented in two process technologies (NG45 and GF12). All values are normalized to Circuit Training’s result. Smaller numbers are better. Note that RePlAce did not deliver a result for the largest design, MemPool.</p><h3></h3><br/><h2>New Controversy?</h2><p>Kahng’s ISPD presentation emphasized a particular discrepancy between the methods described in <em>Nature</em> and those of the open-source version, Circuit Training. Recall that, as a preprocessing step, the reinforcement-learning method gathers up the standard cells into clusters. In Circuit Training, that step is enabled by commercial EDA software that outputs the netlist—what cells and macros are connected to each other—and an initial placement of the components.</p><p>According to Kahng, the existence of an initial placement in the <em>Nature</em> work was unknown to him even as a reviewer of the paper. According to Goldie, generating the initial placement, called physical synthesis, is <a href="https://www.synopsys.com/glossary/what-is-physical-synthesis.html" rel="noopener noreferrer" target="_blank">standard industry practice</a> because it guides the creation of the netlist, the input for macro placers. All placement methods in both <em>Nature</em> and MacroPlacement were given the same input netlists.</p><p>Does the initial placement somehow give reinforcement learning an advantage? Yes, according to Kahng. His group did <a href="https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Docs/OurProgress#Question1" rel="noopener noreferrer" target="_blank">experiments</a> that fed three different impossible initial placements into Circuit Training and compared them to a real placement. Routed wire lengths for the impossible versions were between 7 and 10 percent worse.</p><p>Mirhoseini and Goldie counter that the initial placement information is used only for clustering standard cells, which reinforcement learning does not place. The macro-placing reinforcement learning portion has no knowledge of the initial placement, they say. What’s more, providing impossible initial placements may be like taking a sledgehammer to the standard cell-clustering step and therefore giving the reinforcement-learning system a false reward signal. “Kahng has introduced a disadvantage, not removed an advantage,” they write.</p><p>Kahng suggests that more carefully designed experiments are forthcoming.</p><h2>Moving On</h2><p>This dispute has certainly had consequences, most of them negative. Chatterjee is locked in a wrongful-termination lawsuit with Google. Kahng and his team have spent a great deal of time and effort reconstructing work done—perhaps several times—years ago. After spending years fending off criticism from unpublished and unrefereed research, Goldie and Mirhoseini, whose aim was to help improve chip design, have left a field of engineering that has historically struggled to attract female talent. Since August 2022 they’ve been at <a href="https://www.anthropic.com/" rel="noopener noreferrer" target="_blank">Anthropic</a> working on <a href="https://arxiv.org/abs/2212.08073" rel="noopener noreferrer" target="_blank">reinforcement learning for large language models</a>.</p><p>If there’s a bright side, it’s that Kahng’s effort offers a model for open and reproducible research and added to the store of openly available tools to push this part of chip design forward. That said, Mirhoseini and Goldie’s group at Google had already made an <a href="https://github.com/google-research/circuit_training" rel="noopener noreferrer" target="_blank">open-source version of their research</a>, which is not common for industry research and required some nontrivial engineering work.</p><p>Despite all the drama, the use of machine learning generally, and reinforcement learning specifically, in chip design, has only spread. More than one group was able to <a href="https://dl.acm.org/doi/pdf/10.1145/3489517.3530617" rel="noopener noreferrer" target="_blank">build on Morpheus</a> even before it was made open source. And machine learning is assisting in ever-growing aspects of commercial EDA tools, such as those from <a href="https://www.synopsys.com/ai.html" rel="noopener noreferrer" target="_blank">Synopsys</a> and <a href="https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/soc-implementation-and-floorplanning/cerebrus-intelligent-chip-explorer.html?utm_campaign=Cerebrus_GoogleSearch_Adtxt_03_22&utm_source=Google&utm_medium=Adtxt&s_kwcid=AL!14272!3!587746534731!p!!g!!cadence%20machine%20learning&gclid=CjwKCAjw5pShBhB_EiwAvmnNVwIETAtqyH7Ys_TFDfTGeY0R3xjb2bkJMhulfFCX8hqTxo6pPMy7sBoC578QAvD_BwE" rel="noopener noreferrer" target="_blank">Cadence</a>.</p><p>But all that good could have happened without the unpleasantness.</p><p><em>This post was corrected on 4 April. CMP was originally incorrectly characterized as being a new tool. On 5 April context and correction was added about how CT faired against a human and against simulated annealing. A statement regarding the clarity of experiments surrounding the initial placement issue was removed.</em></p><h2>To Probe Further:</h2><p>The <a href="https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Docs/OurProgress#readme" rel="noopener noreferrer" target="_blank">MacroPlacement</a> project is extensively documented on GitHub.</p><p>Google’s Circuit Training entry on GitHub is <a href="https://github.com/google-research/circuit_training" target="_blank">here</a>.</p><p>Andrew Kahng documents his involvement with the <em>Nature</em> paper <a href="https://docs.google.com/document/d/1vkPRgJEiLIyT22AkQNAxO8JtIKiL95diVdJ_O4AFtJ8/edit" rel="noopener noreferrer" target="_blank">here</a>. <em>Nature</em> published the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03544-w/MediaObjects/41586_2021_3544_MOESM1_ESM.pdf" rel="noopener noreferrer" target="_blank">peer-review file</a> in 2022.</p><p>Mirhoseini and Goldie’s response to MacroPlacement can be found <a href="https://www.annagoldie.com/home/statement" target="_blank">here</a>.</p>]]></description>
      <pubDate>Tue, 04 Apr 2023 15:00:05 +0000</pubDate>
      <guid>https://spectrum.ieee.org/chip-design-controversy</guid>
      <category>Electronic design automation</category>
      <category>Eda</category>
      <category>Reinforcement learning</category>
      <category>Machine learning</category>
      <category>Google</category>
      <category>Artificial intelligence</category>
      <category>Chip design</category>
      <dc:creator>Samuel K. Moore</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/six-squares-contain-variously-sized-rectangles-of-four-colors-with-differently-colored-blobs-filling-in-gaps-between-the-rectang.jpg?id=33402790&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Neuralink’s FDA Troubles Are Just the Beginning</title>
      <link>https://spectrum.ieee.org/neuralink-seeks-fda-approval</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/an-abstract-portrait-of-a-man-with-white-and-green-lines-and-dots-a-smartphone-in-the-front-with-screen-that-reads-breakthroug.jpg?id=33388909&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>Neuralink, the <a href="https://spectrum.ieee.org/tag/neuralink" target="_self">neurotechnology company founded by Elon Musk</a>, is at best having a rough initial go-round with the Food and Drug Administration’s human-trials application process. The company also faces additional investigations by two other U.S. government agencies. These setbacks and the broad, cure-all expectations that its founder has placed on the company’s neural implant could now incite increased scrutiny from regulators. By contrast, however, other neurotech companies have, to date, managed largely to avoid such intense regulatory scrutiny.<br/></p><p>Neuralink is developing The Link, a brain-computer interface that records and stimulates electrical activity within a user’s brain. Brain-computer interfaces, or BCIs, are <a href="https://spectrum.ieee.org/what-is-neural-implant-neuromodulation-brain-implants-electroceuticals-neuralink-definition-examples" target="_self">neural implants</a> that connect a user’s brain to external electronics. Getting the device into people’s heads for the first time is a major goal for the company. Musk himself stated at a <a href="https://www.theverge.com/2022/11/30/23487307/neuralink-elon-musk-show-and-tell-2022" target="_blank">press event</a> the company held last November that they planned to start human trials by the end of May of this year and are,<a href="https://www.reuters.com/business/healthcare-pharmaceuticals/musks-brain-implant-company-search-human-trials-partner-2023-03-27/" rel="noopener noreferrer" target="_blank"> reportedly, looking for a partner</a> with which to coordinate initial experiments.</p><p>Despite Musk’s <a href="https://analyticsindiamag.com/the-many-failed-predictions-promises-by-elon-musk/" target="_blank">characteristically overeager</a> timetable, Neuralink’s challenges in gaining FDA approval are only mounting. <a href="https://spectrum.ieee.org/elon-musk-brain-neuralink" target="_blank">The Link</a>, which is inserted across a user’s skull and into their brain, is categorized by the FDA as a <a href="https://www.fda.gov/medical-devices/consumers-medical-devices/learn-if-medical-device-has-been-cleared-fda-marketing" target="_blank">Class III device</a>, a designation of medical devices that “sustain or support life, are implanted, or present potential unreasonable risk of illness or injury.” FDA approval for a human test to potentially qualify for class III device status—known as an investigational device exemption (IDE)—can be granted only after an involved application process. In it, Neuralink must document how The Link is sufficiently safe and capable for its labeled indications. Just to initiate medical trials, in other words, substantial hurdles must be cleared first. </p><p class="pull-quote">“If the experiments are tainted in some way, then the FDA may require new data. If the animal studies are questionable, they will have to be redone.”<br/>—Victor Krauthamer, George Washington University</p><p>Neuralink has <a href="https://www.reuters.com/investigates/special-report/neuralink-musk-fda/" rel="noopener noreferrer" target="_blank">already failed this application once</a> on the grounds of “dozens of deficiencies” cited by the FDA. These concerns included the stability of the device’s battery and charging system, the potential for its implanted electrodes to migrate throughout and damage brain tissue, and the damage that could be caused to the brain should the device be removed or upgraded.</p><p>The company’s path toward human trials will be further complicated by attention from other government regulators. The U.S. Department of Agriculture opened an <a href="https://www.reuters.com/technology/musks-neuralink-faces-federal-probe-employee-backlash-over-animal-tests-2022-12-05/" rel="noopener noreferrer" target="_blank">investigation into the company’s alleged animal abuse</a> in late 2022. The Department of Transportation opened <a href="https://www.reuters.com/technology/elon-musks-neuralink-may-have-illegally-transported-pathogens-animal-advocates-2023-02-09/" rel="noopener noreferrer" target="_blank">a separate investigation</a> into the company’s <a href="https://pcrm.widen.net/s/fmbplnppxw/request-for-dot-investigation-of-neuralink-with-enclosures-02.09.23" rel="noopener noreferrer" target="_blank">alleged mismanagement and interstate portage of biohazardous materials</a> including neural implants collected from diseased animal subjects.</p><p>Ongoing investigations will not likely factor directly into how the FDA evaluates the data in Neuralink’s next IDE application, but it will likely complicate the process. Data from diseased animals and contaminated equipment may need to be rejected, says Victor Krauthamer, <a href="https://scholar.google.com/citations?user=4Kz3_zMAAAAJ&hl=en" target="_blank">visiting professor of biomedical engineering</a> at George Washington University and the former acting director of the FDA’s <a href="https://www.fda.gov/about-fda/cdrh-offices/oht5-office-neurological-and-physical-medicine-devices-office-product-evaluation-and-quality" target="_blank">Division of Neurological and Physical Medicine Devices</a>. “If the experiments are tainted in some way, then the FDA may require new data. If the animal studies are questionable, they will have to be redone,” he says.</p><p>Further complicating Neuralink’s application are the many claims Musk and other company representatives have made regarding The Link’s purportedly extensive list of capabilities. Though the company has asserted that its device could <a href="https://www.cnet.com/science/neuralink-upgraded-brain-chip-hopes-to-help-the-blind-see-and-the-paralyzed-walk/" target="_blank">restore sight to the blind</a>, <a href="https://www.bluebadgeinsurance.com.au/blog/neuralink-chip-what-is-it-can-it-cure-paralysis/" target="_blank">enable the paralyzed to walk</a>, and perhaps even <a href="https://fortune.com/2022/07/07/elon-musk-neuralink-brain-implant-claims/" rel="noopener noreferrer" target="_blank">connect a user’s mind to unspecified “superintelligent AI,”</a> the company’s previous IDE application concerned the system’s ability to let users type out letters without using their hands or a keyboard.</p><p>IDE applications are evaluated based on device safety and efficacy. The latter is assessed with respect to a specific use, condition,  or indication. While Neuralink’s current application will be limited to a single indication, like thought-to-type, Krauthamer believes that the larger claims made around the device may draw increased scrutiny from FDA regulators: “It may raise more questions for them because of these very broad, unsubstantiated claims. Officially, the FDA just reviews the evidence in front of them, but there’s a context to that, and I think that context may bring out additional caution.”</p><p>The FDA approval process is not without support, however. The National Institutes of Health maintains several programs to promote and assist neurotechnology research in both academia and industry. Neuralink chose to go without the assistance those programs offer, according to Kip Ludwig, a former NIH program director overseeing several neuroengineering initiatives: “Musk does not want anybody with any real medical-device-industry experience. He didn’t want to work with the government because of the bureaucracy. NIH is just trying to help people.”</p><p>Synchron, a separate neurotechnology company designing an electrode array embedded into a venous stent, was awarded an IDE in July 2021 and recently <a href="https://www.businesswire.com/news/home/20230320005156/en/Synchron-to-Begin-Patient-Enrollment-for-the-COMMAND-Trial-of-Brain-Computer-Interface-at-Gates-Vascular-Institute" rel="noopener noreferrer" target="_blank">announced it will begin human trials</a> with the Gates Vascular Institute in Buffalo, N.Y. Synchron began the FDA application process in 2016. According to Cristin Welle, former lead scientist of the NIH’s Neural Implant laboratory, Synchron’s success may be attributable to its device’s similarity to previously approved stents, which are more familiar to the FDA than Neuralink’s Link device is: “If it’s possible to draw specific criteria from past approvals, they will take that into account. They try to be consistent in their approach to a given technology.”</p>]]></description>
      <pubDate>Sun, 02 Apr 2023 14:00:05 +0000</pubDate>
      <guid>https://spectrum.ieee.org/neuralink-seeks-fda-approval</guid>
      <category>Bci</category>
      <category>Brain-computer interface</category>
      <category>Neurotechnology</category>
      <category>Neuralink</category>
      <category>Neuralink</category>
      <dc:creator>Michael Nolan</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/an-abstract-portrait-of-a-man-with-white-and-green-lines-and-dots-a-smartphone-in-the-front-with-screen-that-reads-breakthroug.jpg?id=33388909&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Tiny Exploding Houses Promoted 18th-Century Lightning Rods</title>
      <link>https://spectrum.ieee.org/lightning-rod</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-dollhouse-size-model-house-and-tower-made-of-painted-metal-the-tower-has-a-long-rod-protruding-from-its-top.jpg?id=33280903&width=1200&height=800&coordinates=0%2C150%2C0%2C150"/><br/><br/><p>
<strong>Imagine if engineers </strong>were required to build a working model to demonstrate every new technological concept to the general public. Done right, tech literacy might soar! A compelling visual example can really help people understand the applications and implications of new technologies. That was the idea behind the <a href="https://www.nms.ac.uk/explore-our-collections/collection-search-results/thunder-house/602877" rel="noopener noreferrer" target="_blank">thunder house</a>, which in the second half of the 1700s became a popular means of getting out the word about Benjamin Franklin’s experiments on the lightning rod.
</p><p>
	The thunder house was a simple tabletop device to show the benefits of a grounded versus ungrounded conductor. The demonstrator would place a small amount of gunpowder inside the model and then zap the house with an electric charge from an early battery called a Leyden jar. If the house had a lightning rod, simulated by a grounded conductor, the charge would pass through without incident. But if the house was set up with an open conductor, the charge would ignite the gunpowder and the sides of the house would collapse with a loud bang. This modern demonstration shows how it was done:
</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="9bbef8f8b86938ad9d562520d823107a" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/faeTcX-g-WA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Thunder house</small>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">
<a href="https://www.youtube.com/watch?v=faeTcX-g-WA" target="_blank">www.youtube.com</a>
</small>
</p><p>
	The demos were certainly dramatic, and yet it took years for lightning rods to catch on.
</p><h2>Ben Franklin’s experiments confirmed that lightning carries electricity</h2><p>
	Franklin’s experiments with electricity began in 1745 when <a href="https://en.wikipedia.org/wiki/Peter_Collinson_(botanist)" rel="noopener noreferrer" target="_blank">Peter Collinson</a>, a Fellow of the Royal Society and patron of the American Philosophical Society, sent him a glass tube and a set of directions for experiments. Franklin enthusiastically <a href="https://spectrum.ieee.org/ben-franklins-iotheri-great-electrical-discovery-turkey-tenderization" target="_self">undertook the research</a>. By 1750 he had written back to Collinson proposing his own experiment to determine if the lightning he observed in the sky had the same electrical properties as that contained in a Leyden jar.
</p><p>
	Thanks to a popular <a href="https://commons.wikimedia.org/wiki/File:Benjamin_Franklin_Lightning_Experiment_1752.jpg" rel="noopener noreferrer" target="_blank">Currier and Ives print</a> and some embellished mythmaking, most Americans assume Franklin made this breakthrough discovery with his famous kite experiment in 1752. But the engraving was made in 1876, more than 100 years after the fact, and it clearly took some liberties with the truth. For example, Franklin’s son William, who is pictured as an eager young boy assisting his father, was <a href="https://founders.archives.gov/documents/Franklin/01-04-02-0135" rel="noopener noreferrer" target="_blank">21 years old</a> at the time of the experiment. Also, the image shows the pair in an open field, but Franklin thought the experiment should take place on top of a tall tower or church spire; indeed, he delayed doing the experiment because there wasn’t an appropriately high building in Philadelphia at the time.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="An engraving of a man in 18th-century clothing holding the string of a kite in a thunderstorm, while a young boy stands next to him.  " class="rm-shortcode" data-rm-shortcode-id="d5019942c1e2c11cb804fcc3760f2c1d" data-rm-shortcode-name="rebelmouse-image" id="550bd" loading="lazy" src="https://spectrum.ieee.org/media-library/an-engraving-of-a-man-in-18th-century-clothing-holding-the-string-of-a-kite-in-a-thunderstorm-while-a-young-boy-stands-next-to.jpg?id=33281287&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Ben Franklin most likely conducted his famous lightning experiment in 1752, but not in an open field.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Getty Images</small></p><p>
	Of course, most myths have some basis in truth, and it is likely that Franklin did conduct the kite experiment with his son in June 1752. But the experiment Franklin proposed for determining if the clouds were electrified did not involve a kite at all.
</p><p>
	On 29 July 1750, Franklin wrote to Collinson and suggested that a sentry box be placed on a tower or steeple. The box, big enough to hold a man and some electrical apparatus, was covered and kept dry, and a 6- to 9-meter iron rod passed upward through the box, ending in a sharp point. Franklin believed the rod could be used to indicate whether overhead thunderclouds were electrified. He published the sentry box experiment in his book, <a href="https://library.si.edu/digital-library/book/experimentsobser00fran" rel="noopener noreferrer" target="_blank"><em>Experiments and Observations on Electricity</em></a><em>, </em>which was issued in London in 1751 and translated and published in France the following year.
</p><p>
	Franklin’s suggested experiments met a tepid reception in England, but King Louis XV of France wanted to see them performed. Naturalist Thomas-François Dalibard set up a sentry box with a 12.2-meter pointed iron bar in the garden at his home at Marly-la-Ville, 25 kilometers north of Paris. On 10 May 1752, storm clouds gathered overhead. Dalibard was out of town, but he left instructions with his assistant, Coiffier, and the village priest, Father Raulet. Coiffier was thus the first to conduct Franklin’s experiment. When he brought a brass wire within a few centimeters of the iron bar, a spark jumped and emitted a sulfur smell, very similar to that from electrical experiments with Leyden jars. Raulet then successfully repeated the experiment six times over the course of 4 minutes, until the thunder and lightning dissipated and it started to hail.
</p><p>
	Raulet sent an account of the experiment to Dalibard, who <a href="https://founders.archives.gov/documents/Franklin/01-04-02-0105" rel="noopener noreferrer" target="_blank">read it </a>to the Académie Royale des Sciences in Paris on 13 May. Five days later, a colleague successfully replicated the experiment. Soon the experiment was being conducted across Europe. But the news was slow to reach North America, and Franklin conducted his kite experiment without knowing that the French had already proved his theory correct with the sentry box experiment.
</p><h2>The lightning rod’s adoption was not lightning fast</h2><p>
	If science in real life proceeded as it does in the movies, Franklin’s verification that storm clouds are electrified would have swiftly led to the adoption of his suggested protection: the lightning rod. But reality rarely follows a tidy script, and historians have to pick through the evidence to figure out the actual order of events. I. Bernard Cohen published an essay on Franklin’s lightning rods in 1952 as part of the American Philosophical Society’s celebration of the bicentennial of the experiments. He later expanded this into his book <a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674066595" rel="noopener noreferrer" target="_blank"><em>Benjamin Franklin’s Science</em></a><em>.</em>
</p><p>
	Franklin had <a href="https://founders.archives.gov/documents/Franklin/01-04-02-0006" rel="noopener noreferrer" target="_blank">proposed</a> the usefulness of a lightning rod as early as 1750: “Would not these pointed Rods probably draw the Electrical Fire silently out of a Cloud before it came nigh enough to strike, and thereby secure us from that most sudden and terrible Mischief!” By 1751, Franklin’s friend and collaborator <a href="https://archives.upenn.edu/exhibits/penn-people/biography/ebenezer-kinnersley/" rel="noopener noreferrer" target="_blank">Ebenezer Kinnersley</a> was using a thunder house to demonstrate the concept, giving lectures on the “Newly Discovered Electrical Fire” in New York, Boston, and Newport, R.I.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Two engravings show a well-dressed 18th-century woman wearing a wide-brimmed hat from which a long metal strand reaches to the ground and a well-dressed man carrying an umbrella with a similar metal strand." class="rm-shortcode" data-rm-shortcode-id="32d5f3985ce36e2378efac99d8995f73" data-rm-shortcode-name="rebelmouse-image" id="9b831" loading="lazy" src="https://spectrum.ieee.org/media-library/two-engravings-show-a-well-dressed-18th-century-woman-wearing-a-wide-brimmed-hat-from-which-a-long-metal-strand-reaches-to-the-g.jpg?id=33281298&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Parisian designers came up with a lightning-conducting hat [left] and umbrella [right]. Should the wearer be struck, the metal strand was meant to discharge the electricity to the ground.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Getty Images</small></p><p>
	But more than a decade later, Franklin wrote to Kinnersley from England that he was disheartened by the lack of adoption: “Here [the lightning rod] is very little regarded; so little, that though it is now seven or eight years since it was made public, I have not heard of a single house as yet attempted to be secured by it.” Franklin was frustrated by people’s inertia when presented with what he perceived to be a clear solution to a problem. But he was fighting religious beliefs, personal grudges, and simple fear.
</p><p>
	Long-standing Catholic and Protestant tradition (if not doctrine) called for the ringing of bells during thunderstorms to dissipate the diabolical nature of the tempest. As early as the 17th century, Cardinal Bellarmine (who crossed paths with Galileo) had reportedly denounced this practice, and Pope Benedict XIV advocated for the use of lightning rods soon after they were proven effective.
</p><p>
	Unfortunately, some traditions die hard—and so did many bell ringers. Church steeples are of course favored spots for lightning strikes. Between 1753 and 1786, 103 church tower bell ringers died by lightning strike in France alone. I know if it were my job, I’d be begging for a lightning rod.
</p><p>
	Petty disputes also got in the way of the lightning rod. For instance, Dalibard and Georges-Louis Leclerc had neglected to mention the work of Jean-Antoine Nollet, a French clergyman and leading electrical experimentalist, in their short introduction to Dalibard’s translation of Franklin’s work. In retaliation, Abbé Nollet attempted to suppress the news of the success of the sentry box experiment. When that failed, he engaged in fearmongering by capitalizing on the electrocution of <a href="https://www.lindahall.org/about/news/scientist-of-the-day/georg-wilhelm-richmann" rel="noopener noreferrer" target="_blank">Georg Wilhelm Richmann</a> in 1753.
</p><p>
	At the time of his death, Richmann had been attempting to replicate the sentry box experiment, but with the twist of hooking up an electrometer to measure the force of the atmospheric electricity. Unfortunately, he made the fundamental mistake of using an ungrounded rod, and on 6 August 1753 his became the first recorded death from electrical experimentation.
</p><p>
	Franklin <a href="https://founders.archives.gov/documents/Franklin/01-05-02-0064" rel="noopener noreferrer" target="_blank">reprinted an account</a> of the accident in the <em>Pennsylvania Gazette </em>and ended it with his own plug for the lightning rod: “And had his Apparatus been intended for the Security of his House, and the Wire (as in that Case it ought to be) continued without Interruption from the Roof to the Earth, it seems more than probable that the Lightning would have follow’d the Wire, and that neither the House nor any of the Family would have been hurt by that unfortunate Stroke.”
</p><p>
	 Eventually, lightning rods did become widely accepted, even if the science behind them remained a little murky for most people. Case in point: In 1778, French fashion designers proposed <a href="https://archive.org/details/drawlightningdow00mich/page/190/mode/2up?view=theater" rel="noopener noreferrer" target="_blank">ladies’ hats</a> and umbrellas with built-in lightning conductors. A metal chain trailed behind to dissipate the energy into the ground. Although such accessories were unlikely to attract a lightning strike, they also were unlikely to provide any protection. Perhaps if the fashion houses had seen a demonstration of a thunder house, they would have been less eager to design potentially fatal millinery.
</p><p>
<em>Part of a </em><a href="https://spectrum.ieee.org/collections/past-forward/" target="_self"><em>continuing series</em></a> <em>looking at historical artifacts that embrace the boundless potential of technology.</em>
</p><p>
<em>An abridged version of this article appears in the April 2023 print issue as “</em><em>Feel the Thunder</em><em>.”</em>
</p><p><em></em></p>]]></description>
      <pubDate>Sat, 01 Apr 2023 15:00:03 +0000</pubDate>
      <guid>https://spectrum.ieee.org/lightning-rod</guid>
      <category>Past forward</category>
      <category>Ben franklin</category>
      <category>Lightning rod</category>
      <category>Thunder house</category>
      <category>Type:departments</category>
      <dc:creator>Allison Marsh</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-dollhouse-size-model-house-and-tower-made-of-painted-metal-the-tower-has-a-long-rod-protruding-from-its-top.jpg?id=33280903&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Video Friday: Grain Weevil</title>
      <link>https://spectrum.ieee.org/video-friday-grain-weevil</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-small-blue-robot-with-linear-corkscrew-wheels-sits-on-a-pile-of-grain-in-a-huge-dark-grain-bin.jpg?id=33387367&width=1319&height=856&coordinates=0%2C284%2C729%2C225"/><br/><br/><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br/></p><h5><a href="https://www.roboticssummit.com/">Robotics Summit & Expo</a>: 10–11 May 2023, BOSTON</h5><h5><a href="https://www.icra2023.org/">ICRA 2023</a>: 29 May–2 June 2023, LONDON</h5><h5><a href="https://2023.robocup.org/en/home/">RoboCup 2023</a>: 4–10 July 2023, BORDEAUX, FRANCE</h5><h5><a href="https://roboticsconference.org/">RSS 2023</a>: 10–14 July 2023, DAEGU, KOREA</h5><h5><a href="http://ro-man2023.org/main">IEEE RO-MAN 2023</a>: 28–31 August 2023, BUSAN, KOREA</h5><h5><a href="https://clawar.org/clawar23/">CLAWAR 2023</a>: 2–4 October 2023, FLORIANOPOLIS, BRAZIL</h5><h5><a href="https://2023.ieee-humanoids.org/">Humanoids 2023</a>: 12–14 December 2023, AUSTIN, TEXAS, USA</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div style="page-break-after: always"><span style="display:none"> </span></div><p>This is the Grain Weevil, and it’s designed to keep humans out of grain bins. I love this because it’s an excellent example of how to solve a real, valuable problem uniquely with a relatively simple, focused robot.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="fc09d627a23e1c08398d5075a7d2e935" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/BXZQ-GSFsd0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.grainweevil.com/">Grain Weevil</a> ]</p><div class="horizontal-rule"></div><blockquote><em>As the city of Paris sleeps, Spot is hard at work inspecting some of RATP Group’s 35,000 civil works components. The RATP (Autonomous Parisian Transportation Administration) Group is a French state-owned public transport operator and maintainer for the Greater Paris area. With thousands of civil works to inspect each year, the company has turned to mobile robotics to inspect hard-to-reach and hazardous areas in order to keep employees out of harm’s way.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="821dedd61db9ad984b84c2e4188dd2a0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/bKDhmENcKto?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/resources/case-study/ratp">Boston Dynamics</a> ]</p><p>Thanks, Renee!</p><div class="horizontal-rule"></div><p>Looks like Agility Robotics and the new Digit had a productive (and popular!) time at ProMat.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="22756f0552f38d28b679862c08bb178d" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/RVQ68Iagnb0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://agilityrobotics.com/">Agility Robotics</a> ]</p><div class="horizontal-rule"></div><p>I still cannot believe that this makes sense. But it does?</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="23bc173de34ee46034cf2531a01449ac" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/Tp1KtHV9lTA?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.tevel-tech.com/">Tevel</a> ]</p><div class="horizontal-rule"></div><p>Unitree sells a lidar now, and it’s US $330.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="2203976e4bdde5b5256dd6327469fbd3" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/2wJatVbJfns?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://m.unitree.com/LiDAR/">Unitree</a> ]</p><div class="horizontal-rule"></div><blockquote><em>1`We recently had the privilege to host Madeline Gannon (robot whisperer and head of Atonaton) at our HQ in Portland. It’s no surprise that a week in our shop resulted in a game of industrial basketball with our ABB IRB 8700-turned-basketball hoop.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="1eb1352f90a6c450023525e752ddb3d2" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/sFS6fAlWEvk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://atonaton.com/robotic-basketball-hoop">Loupe</a> ]</p><p>Thanks, Madeline!</p><div class="horizontal-rule"></div><blockquote><em>We demonstrated Stretch, our autonomous case-handling robot, automating trailer unloading at ProMat 2023. From efficiency to ease of use, hear from our team to learn how Stretch works, what’s new, and what’s coming next for warehouse automation!</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e46436206900033fc651987ef122afeb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/TVL4S05O-C8?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.bostondynamics.com/products/stretch">Boston Dynamics</a> ]</p><div class="horizontal-rule"></div><blockquote><em>Korea Advanced Institute of Science & Technology (KAIST) has developed a quadrupedal robot locomotion technology that moves it up and down stairs without the aid of visual or tactile sensors in a disaster situation where it is impossible to see due to smoke, and it moves without falling over bumpy environments such as tree roots.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="6f1747b13c0a1051954f4e8f95637ecf" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/JC1_bnTxPiQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://urobot.kaist.ac.kr/">KAIST</a> ]</p><div class="horizontal-rule"></div><p>Here’s how Pickle’s box-unloading robot has been doing.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e393efc6e558c195604e3ca319414fd7" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/YIhGGzJFGvI?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://picklerobot.com/">Pickle Robot</a> ]</p><div class="horizontal-rule"></div><p>Quite possibly the most destructive combat robot ever designed has been revamped and is heading to RoboGames 2023.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="e208c4d44c95fd9043247ca2179b09a0" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/DNfy53PEHJQ?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>Use the code “HardCore” for a discount on your RoboGames tickets!</p><p>[ <a href="http://robogames.net/buy-tix-RG23.php">RoboGames</a> ]</p><div class="horizontal-rule"></div><p>Is AI smarter than babies? Depends what you mean by “smarter,” of course.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="eadc88982adcdcebadf1488b82474edb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/dW-REzvNk1E?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.nsf.gov/news/mmg/mmg_disp.jsp?med_id=188523&from=">NSF</a> ]</p><div class="horizontal-rule"></div><p>Someone can do this through a telepresence robot, and I can’t even do it in real life (sigh).</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="11523a767dc1b81ddf57a967648bdb72" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/_v6PsG6C3B4?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://www.sanctuary.ai/">Sanctuary AI</a> ]</p><div class="horizontal-rule"></div><p>Chen Li, from the Terradynamics Lab at Johns Hopkins University, gives a talk on the need for and feasibility of alternative robots to traverse sandy and rocky extraterrestrial terrain.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" data-rm-shortcode-id="63d329e11cfef5b6abcefa0a120764db" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/bfcA0SE9aGw?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span></p><p>[ <a href="https://li.me.jhu.edu/">JHU</a> ]</p><div class="horizontal-rule"></div>]]></description>
      <pubDate>Fri, 31 Mar 2023 16:52:32 +0000</pubDate>
      <guid>https://spectrum.ieee.org/video-friday-grain-weevil</guid>
      <category>Video friday</category>
      <category>Robotics</category>
      <dc:creator>Evan Ackerman</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-small-blue-robot-with-linear-corkscrew-wheels-sits-on-a-pile-of-grain-in-a-huge-dark-grain-bin.jpg?id=33387367&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>The Staggering Scale of the EV Transition</title>
      <link>https://spectrum.ieee.org/the-ev-transition-explained-2659623150</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-bearded-man-in-glasses-blue-shirt-and-jacket.png?id=33321321&width=1200&height=800&coordinates=0%2C111%2C0%2C170"/><br/><br/><p>
<strong>Over the past 20</strong> or so years, contributing editor <a href="https://spectrum.ieee.org/u/robert-n-charette" target="_self">Robert N. “Bob” Charette</a> has written about some of the thorniest issues facing the planet at large and engineers in particular. For <em>IEEE Spectrum</em>, he’s dug into <a href="https://spectrum.ieee.org/why-software-fails" target="_self">software reliability</a> and <a href="https://spectrum.ieee.org/inside-hidden-world-legacy-it-systems" target="_self">maintenance</a>, the so-called <a href="https://spectrum.ieee.org/the-stem-crisis-is-a-myth" target="_self">STEM crisis</a>, and <a href="https://spectrum.ieee.org/automated-to-death" target="_self">the automation paradox</a>, examining those complex topics through the eyes of a seasoned risk analyst who has consulted for governments and corporations for five decades.
</p><p>
	I’ve been fortunate to be Bob’s editor for many of his ambitious projects. We often converse on Friday afternoons about what he’s hearing from industry insiders and academics on whatever subject he’s currently investigating. Our conversations are jovial, sometimes alarming, and always edifying, at least for me.
</p><p>
	So when he called me on a Friday afternoon in the summer of 2021 to propose an article delving into the complexities of the global transition to electric vehicles, I knew that he’d do the research at a deeper level than any tech journalist, and that he’d explore angles that wouldn’t even occur to them.
</p><h3></h3><br/><p ev-ebook.pdf"="" https:=" " spectrum.ieee.org="" style="color: #fa502a; font-size: 20px;>
	<a href=" target="_self"><strong><a href="https://spectrum.ieee.org/ev-ebook.pdf" target="_blank">Download</a> “The EV Transition Explained” e-book for free.
	</strong>
</p><h3></h3><br/><p>Take power-grid transformers. These essential voltage-converting components are designed to cool down at night, when power consumption is typically low. But with more people charging their EVs at home at night, the 30-year design life of a transformer will drop—to perhaps no more than three years once mass adoption of EVs takes hold. Transformers can cost more than <a href="https://www.utilitydive.com/news/distribution-transformer-shortage-appa-casten/639059/" target="_blank">US $20,000 each</a>, and they’re already in short supply in many countries. Bob examined factors like that and dozens of others during the last year and a half.</p><p>Throughout his research and reporting, Bob focused on the EV transition “at scale”: What needs to happen in order for electric vehicles to displace internal-combustion-engine vehicles and have a measurable impact on climate change by midcentury? Quite a lot, it turns out. Humans must change two foundational sectors of modern civilization—energy and transportation—to achieve the targeted reductions in greenhouse gas emissions. These simultaneous global overhauls will involve trillions of dollars in investments, tens of millions of workers, millions of new EVs, tens of thousands of kilometers of new transmission lines to carry electricity from countless new wind and solar farms, and dozens of new battery plants and new mines to feed them. Then there are the lifestyle compromises that most people living in developed countries will have to make.</p><h3></h3><br/><p>“As always, Bob approached this tangle of issues from a systems engineering perspective.”</p><h3></h3><br/><p>As always, Bob approached this tangle of issues from a systems engineering perspective. For a typical assignment, his first draft runs to thousands of words over the assigned length, and we manage to chisel the manuscript down to a single feature article, often leaving a lot of interesting material on the cutting-room floor. This time, we decided to let Bob go long, so he could paint as detailed a picture as possible of a fast-moving and multifaceted target.</p><p>The process was kind of like building an EV while driving it. As Bob’s editor, my job was to help him synthesize his findings into a snapshot of the EV industry at a pivotal moment in history. Every Friday, we’d discuss some new announcement or jaw-dropping data point that needed to find its way into what I came to call The Opus. As Bob talked to more people and read more policy documents, research reports, and public-meeting minutes, the assignment went from two or three articles to twelve, covering the technological hurdles, policy battles, and consumer attitudes surrounding the EV transition.</p><p>The Opus is now an e-book, <em>The EV Transition Explained</em>, the introduction to which you can read on page 40. The e-book itself is available for download exclusively for IEEE members via our website or the QR code at the end of the article. Bob’s hope, and ours, is that policymakers, auto-industry executives, engineers, and consumers will use his analysis to inform their discussions and decisions about the best ways to transition to EVs—at scale.</p><h3>The EV Transition Explained</h3><br/><img alt='Photo of a man and a car with "The Rules Have Changed" on a sign behind.' class="rm-shortcode" data-rm-shortcode-id="6f539210b91733744f0d73aee9e7f714" data-rm-shortcode-name="rebelmouse-image" id="60639" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-a-man-and-a-car-with-the-rules-have-changed-on-a-sign-behind.jpg?id=32998055&width=980"/><h5></h5><p class="caption"><strong>« Previous</strong></p><h5><a href="https://spectrum.ieee.org/the-ev-transition-explained-2659368857" target="_self">The Aftershocks of the EV Transition Could Be Ugly</a></h5><h3></h3><br/><img alt="" class="rm-shortcode" data-rm-shortcode-id="d12bffc908fbeeb7fe8d6a0a1e5a3559" data-rm-shortcode-name="rebelmouse-image" id="587d6" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=33390328&width=980"/><p class="caption"><strong>Next »</strong></p><h5><a href="https://spectrum.ieee.org/the-ev-transition-explained-2659602311" target="_self">The EV Transition Is Harder Than Anyone Thinks</a></h5>]]></description>
      <pubDate>Fri, 31 Mar 2023 15:00:07 +0000</pubDate>
      <guid>https://spectrum.ieee.org/the-ev-transition-explained-2659623150</guid>
      <category>Transportation</category>
      <category>Evs</category>
      <category>The ev transition</category>
      <category>Policy</category>
      <category>Renewables</category>
      <category>Climate change</category>
      <dc:creator>Harry Goldstein</dc:creator>
      <media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/photo-of-a-bearded-man-in-glasses-blue-shirt-and-jacket.png?id=33321321&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Functional Programming: The Biggest Change Since We Killed the Goto?</title>
      <link>https://spectrum.ieee.org/functional-programming-biggest-change</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.webp?id=33385326&width=980"/><br/><br/><iframe frameborder="no" height="180" scrolling="no" seamless="" src="https://share.transistor.fm/e/da246c70" width="100%"></iframe><h3>Transcript</h3><p><strong>Stephen Cass:</strong> Welcome to <em>Fixing the Future</em>, an <em>IEEE Spectrum</em> podcast. I’m senior editor Stephen Cass, and this episode is brought to you by IEEE Explorer, your gateway to trusted engineering and technology research with nearly 6 million documents with research and abstracts. Today we are talking with <a href="https://www.linkedin.com/in/cscalfani/" target="_blank">Charles Scalfani</a>, CTO of <a href="https://panosoft.com/" target="_blank">Panoramic Software</a>, about how adopting functional programming could lead to cleaner and more maintainable code. Charles, welcome to <em>Fixing the Future</em>.</p><p><strong>Charles Scalfani</strong>: Thank you.</p><p><strong>Cass: </strong>So you recently wrote an <a href="https://spectrum.ieee.org/functional-programming" target="_self">expert feature for us that turned out to be incredibly popular</a> with readers. That argued that we should be adopting this thing called functional programming. Can you briefly explain what that is?</p><p><strong>Scalfani:</strong> Okay. Functional programming is an older version of programming, actually, than what we do today. It is basically, as it says, it’s basically based around functions. So where object oriented programming is has an object model, where it’s everything— you see everything through the lens of an object, and the whole world is an object, and everything in that world is an object. In functional programming, it’s the similar, it’s you see everything as a function, and the whole world looks like— everything in the world looks like a function. You solve all your problems with functions. The reason it’s older and wasn’t adopted is because the ideas were there, the mathematics, the ideas, and everything were there, the hardware just couldn’t keep up with it. So it became relegated to academia and the hardware just wasn’t available to do all of the things. That has been, since probably the 90s, it’s been not a problem anymore.</p><p><strong>Cass:</strong> So I just wanted to like, as somebody who is, I would call itself a kind of a very journeyman programmer. So one of the first things I learned when I’m using a new language is usually the section says, how to define a function, and there’s a little— you know, everybody’s got it, Python’s got it, you know, even some versions of Basic used to have it, C has it. So I think function here means something different to those functions I’m used to in something like C or Python.</p><p><strong>Scalfani:</strong> Yeah. I have a joke that I always say is that when I learned C, the first program I wrote was “hello world.” And when I learned Haskell, a functional programming language, the <em>last</em> thing I learned was “hello world.” And so you really, with C, you did, your first “hello world” was a print function, something that printed to the console, and you could say, “yay, I got my first C program working. Here it is.” But the complexity of doing side effects and IO and all of that is such that it gets pushed aside for just pure functional programming. What does that look like? How do you put functions together? How do you compose them? How do you take these smaller pieces and put them all together? And the idea of side effects is something that’s more advanced. And so when you get into a standard language, you just, kind of, jump in and start writing— everybody writes the “hello world,” thanks to Kernighan and Ritchie, <a href="https://en.wikipedia.org/wiki/The_C_Programming_Language" target="_blank">what they did in their book</a>, but you really don’t get to do that for a very long time. In fact, in the <a href="https://leanpub.com/fp-made-easier" rel="noopener noreferrer" target="_blank">book that I wrote</a>, it isn’t for hundreds of pages before you actually get to putting something on the screen. It’s relegated to the fourth section of the book. So it is a difference in that. Side effects where you can affect the world is very standard in imperative languages. The languages that everybody uses C, and Java, and JavaScript, and Python and you name it, the standard languages.</p><p>And that’s why it’s very easy when you first learn a language is just hop in and feel like you’re able to do lots of stuff, and get lots of things done very quickly. And that gets kind of deferred in a functional language. You tend to learn that later. So the kinds of functions that we deal with in functional languages were called pure functions. They’re very different than how we think of functions in programming today, but more how you think of functions in math. Right? So you have inputs, you have processing that happens in the function, computations that are going to occur in that function, and then you have those outputs. And that’s all. You don’t get to manipulate the world in any way, shape, or form.</p><p>Cass: So I want to get back into a little bit of that tutorial on how you get started up on stuff. But it sounds to me a little bit like, I’m searching for a model, my previous model of experience. It sounds to me a little bit like kind of the Unix philosophy of piping very discrete little utility programs together, and then getting results at the end. And that kind of philosophy.</p><p><strong>Scalfani:</strong> Yes. Yeah. That’s a great example. That’s like composing functions using pipes— I’m sorry, composing programs using pipes, and we compose functions in the very same way. And the power of being able to do that, the power they figured out back in Unix, to be able to just say, well, I’ll write this very simple little program that just does one little thing, and then I’ll just take its output and feed it into the next. And it does one little thing. And it’s exactly the same thing, just at a smaller level. Because you’re dealing with functions and not full programs.</p><p><strong>Cass:</strong> Got it. But this does seem like a fairly big cultural shift where you’re telling people, you don’t even get to print until you’re halfway through the book and so on. But I think this is something you raised in the article. We have asked programmers before to do, make fairly big shifts, and the benefits have been immense. And the one you talk about, is getting rid of goto, whereby, you know, in the beginning, we all, you know, ten, goto, whatever. And it was this goto palazza. And then we kind of realized that <a href="https://homepages.cwi.nl/~storm/teaching/reader/Dijkstra68.pdf" rel="noopener noreferrer" target="_blank">goto had some problems</a>. But even though it was this very simple tool that every program are used, we’ve kind of mostly weaned ourselves off goto. Can you talk a little bit about sort of the parallels between saying bye bye to goto and maybe saying bye bye to some of this imperative stuff? And these things like side effects and then maybe talk a little bit about what you mean about like global state, and then— because I think that will perhaps illuminate a little bit more about what you mean about side effects.</p><p><strong>Scalfani: </strong>When I started in programming it was way back in you know 78, 79, around that time and everything was a go— you had Basic, a machine with 8K of RAM. That was it. K. You didn’t have you didn’t have room to do all the fancy stuff we can do today. And so you had to try to make things as efficient as possible. And it really comes from <a href="https://www.tutorialspoint.com/assembly_programming/assembly_conditions.htm" rel="noopener noreferrer" target="_blank">branching down in the assembly language</a>, right? Everybody was used to doing that, goto the, just jump over here and do this thing and then jump back maybe or return from a subroutine and you had very little machine power to do things. So goto came out of assembly language. And as it got in the higher and higher level languages, and as things got more complicated, then you wound up with what’s called spaghetti code, because you can’t follow the code. It’s like trying to follow a strand of spaghetti in a bowl of spaghetti. And so you’re like, well this is jumping to this and that’s jumping to this and you don’t even remember where you were anymore. And I remember looking at code like that and mostly written in assembly language.</p><p>And so as structured languages came about, people realized that if we could have this kind of branching but do it in a do it in a way in which we could abstract it. We could think about it in a more abstract level than down in the details. And so if you look at that, I use it as an example because I look to the past to try to figure out what are we doing today? If we take imperative languages and if we move to functional, we are giving up a lot of things. You can’t do this and you can’t do that. You don’t do side effects. You don’t have global state. There’s all these things that you— there’s no such thing as a <a href="https://en.wikipedia.org/wiki/Null_pointer" rel="noopener noreferrer" target="_blank">null pointer</a> or a null value. Those things don’t exist here in this way of thinking. And it’s like you have to ask yourself, wait, wait, I’m giving up these things that I’m very familiar with and well, how do you do things then in this new way? And is it beneficial or is it just a burden? So at first, it feels like a burden, an absolute burden. It’s going to because you’re so used to falling back on these old ways of doing things in old ways of thinking. And especially when I— I was like 36 years or 30 some odd years into programming and imperative languages, and then all of a sudden I’m thinking functionally. And now I have to change my whole mode of thinking. And you really have to say, well, is it beneficial?</p><p>So I kind of look to the past. Getting rid of the go to was highly beneficial. And I would never advocate for it back. And people did comment on the article saying, “well, yeah, <em>these</em> languages have goto,” but not the goto I’m talking about. They still have these kind of controlled gotos in C, not where you could just jump to the middle of anywhere. And that’s the way things were back in the day. So, yeah, things were pretty wild back then. And we wrote much simpler bits of software. You didn’t use libraries. You didn’t run in operating systems always. I did a lot of embedded coding in the early days. And so you wrote everything. It was all your own code. And now, you might have written, I don’t know, maybe you wrote a thousand lines of code. And now we’re working in millions of lines of code. So it’s a very different world, but when we came out of that early stage, we started shedding these bad habits. And we haven’t done that over time. And I think you have to shed some bad habits to move to functional.</p><p><strong>Cass:</strong> So I do want to talk really getting into the benefits of functional programming are, especially with, I think, the idea of like thinking about maintenance instead of sort of the white hot moment of creation that everybody loves to write that first draft, really thinking about how software is used. But I did just want to unpack a sentence there. And it’s something that also comes from C, and it’s not necessarily something that is baked into assembly in the same way, but it does come in to C, which is this idea of the null pointer. You mentioned the null. And can you talk just a little bit about the null and why it causes so much problems, not just for C, but for all of the sort of, as you call them, curly bracket languages that inherit from it.</p><p><strong>Scalfani:</strong> Right. So in most of those languages, they all support this idea of a null. That is you don’t have anything. So you either have a value or you don’t have a value. And it’s not— it’s sort of like just this idea of that every reference to something could be potentially not— have no reference, right? You have no reference. So think of a plan of an empty bucket, right?</p><p><strong>Cass: </strong>Just for maybe readers who are not familiar. So a pointer is something that points to a bit of memory where something of information is stored. And usually at that point, there’s a valuable number. But sometimes there’s just junk. And so a null pointer kind of helps you tell, ideally, what are the pointers pointing to something useful or it’s pointing to to junk? Would that be kind of a fair summary or am I butchering it a little?</p><p><strong>Scalfani: </strong>Yeah, I think at the lowest level, like if you think about C or assembly, you always have a value somewhere, right? And so what you would do is you would say, okay, so they always point to something. But if I have an address of zero at the very lowest level here, if I have an address— so if my register has a value of zero in it, and I usually use that register to dereference memory to point to someplace in memory, then just that’s going to be treated specially as, oh, that’s not pointing anywhere in particular. There is no value that I’m referencing. So it’s a non, I have no reference. I have nothing, basically, in my hands.</p><p><strong>Cass: </strong>So it’s not something there, it’s just the language is trained that if I see a zero, that’s a flag, there’s nothing there.</p><p><strong>Scalfani:</strong> Right. Right. Exactly, exactly.</p><p><strong>Cass: </strong>And then so then how does this then— so that sounds like a great idea. Wonderful. So how does this then—</p><p><strong>Scalfani:</strong> It is.</p><p><strong>Cass: </strong>Well, how does this cause problems later on? I’ve got this magic number that tells me that it’s bad stuff there. Why does this thing cause problems? And then how can functional programming really help with that?</p><p><strong>Scalfani: </strong>Okay. So the problem isn’t in this idea. It’s sort of a hack. It’s like, oh, well, we’ll just put a zero in there. And then we’ll have to— so that was, okay, that solved that problem. But now you’re just kicking the can. So everywhere down the road where you’re dealing with this thing, now everybody has to check all the time. Right? And it’s not a matter of having to check, because the situation of where you have something or you don’t have something is something that’s valid situation, right? So that’s a perfectly valid thing. But it’s when you forget to check that you get burned. And it’s not built into most of the languages to where it does the checking for you and you have to say, oh, well, this thing is a null or if it’s not a null, then do this you. There’s all these if checks. And you just pollute your code with all the checks everywhere. Now, functional programming doesn’t eliminate that. It’s not magic. It doesn’t eliminate it. But many of the functional languages, at least the ones that I’ve worked in, they have this concept of a maybe, right? So a maybe is, it can either be nothing, or it can be just something. And it’s other languages call it an option. But it’s the same idea. And so you either have nothing, or you just have this value. And because of that, it forces— because of the way that that’s implemented, and I won’t go into gory details, but because of it, they force you to the compiler won’t compile if you didn’t handle both cases.</p><p>And so you’re forced to always handle it, as opposed to the null, you can choose to handle it or not, and you could choose to forget it, or you could go— you could not even know that it could be a null, and you could just assume you have a good value all the time. And then you don’t know until you’re running your program that, oh, you made a mistake. The last place you want to find out is in production when you hit a piece of code that is run rarely, but then you didn’t do your null check, and then it crashes in production and you’ve got problems. With the maybe, you don’t have a choice. You can’t compile it. You can’t even build your program. It really is a great tool. And many times, I still don’t like the maybe. Because it’s like, ugh, I have to handle maybe. Because it forces your hand. You don’t have a choice. Ideally, yes, that’s the right thing, but I still grumble.</p><p><strong>Cass:</strong> I mean, I think the tendency is always to take the shortcut because you think to yourself, oh, this will never— This will never be wrong. It’s <em>fine</em>. I mean, I just all the time. I know when I write even the limited— I know I should be checking a return value. I should be writing it so that it returns. If something goes wrong, it should return an error value, and I should be checking for that error value. But do I do that? No, I just carry on my merry way.</p><p><strong>Scalfani:</strong> Because we know better, right? We know better.</p><p><strong>Cass:</strong> Right. So I do want to talk a little bit about the benefits, then, that functional programming can build. And you make the case for some of these concrete benefits. And especially when it comes to maintenance. And as I say, I think, one of the charges that’s fairly laid against maybe sort of the software enterprise as a whole is that it’s great at creating stuff and inventing stuff, but not so good at maintaining stuff, even though there are examples we have of code, very important code that runs very important systems, that sits around for decades. So maintainability is kind of actually super important. So can you talk a little bit about those benefits, especially with regard to maintainability?</p><p><strong>Scalfani: </strong>Yeah. So I think, so before you even get into maintainability, there’s always the architectural phase, right? You want to model the problem well. So you want to have a language that can do really— can really aid you in the proper modeling of your types. And so that you can model the domain. So that’s the first step, because you can write bad in any code, right? In any technology, you can destroy it. No matter how great the technology is, you can wreak havoc with it. So no technology is magical in that it’s going to keep you from doing bad things. The trick about technology is that you want it to help you do good things. And encourage you and make it easy to do those good things. So that’s the first step, is to have a language that’s really good about modeling. And then the next thing is you want to— we haven’t talked about global state, but you need to control the global state in your program. And in the early days, going back to assembly, every variable, every memory location is global, right? There is no local. The only local data you might have is if you allocated memory on a stack, or if you have registers and you pushed your old registers as you went into a subroutine, things like that. But basically everything was global.</p><p>And so we’ve been we’ve been, as languages have been progressing, we’ve been making things more local, what’s in scope. Who has access to this variable? Who doesn’t have access to the variable? And the more, if you just follow that line as you get to functional programming, you control your global state, right? And so there is no global state. You actually are passing state around all the time. So in a lot of modern, say, JavaScript, frameworks do a lot of that. They’ve taken a lot architecturally from functional programming, like React is one that it’s a matter of how do you control your state? And that’s been a problem in the browser since day one. So controlling the state is another important thing. And why am I mentioning these other things about maintainability? Because if you do these things right, if you get these things right, it aids in your maintainability, right? There’s nothing that’s going to fix logic problems. There’s always logic, right? And if you get— if you make a logic problem mistake, there’s nothing there. Like you just made the wrong call. No language is going to save you because it’s got to be powerful enough so you can make those mistakes. Otherwise, you can’t make all the things.</p><p>So but what it can do is it can restrict you to, you can’t make <em>this</em> mistake, and you can’t make <em>that</em> mistake, and you won’t make this mistake. It restricts you in the mistakes, right? And it makes it easy to do the other things. And that’s where the maintainability really, I think, comes in is the ability to create a system where, if you got the proper modeling of the problem, you’ve properly managed— because really, what are you maintaining software for? You’re fixing problems, right? Or you’re adding features. So that’s all there really is. So if you’re spending all your time fixing problems, then you don’t have time to add any features. And I found that we’ve spent— in the old days we spent more time fixing problems than adding new features. Why? Because why are you adding features when you have bugs, right? So you have to fix the bugs first. So when we move to functional programming, I found that we were spending yeah, we still have logic problems here and there. I mean, we’re still human, but most of our time was spent thinking about new features. Like we would put something into production, you got to have good QA, no matter how great the language is. But if you have good QA and you do your job right, and you have a good solid language that helps you architect it originally correct, then you don’t think about like, oh, I have all these bugs all the time, or these crashes in production. You just don’t have crashes in production. Most of that stuff’s caught before that. The language doesn’t let you paint yourself into a corner.</p><p>So there’s a lot of those kinds of things. So you’re like, oh, well, what can I add? Oh, let’s add this new feature. And that’s really value add, at the business level, because that’s really at the end of the day, it doesn’t matter how cool some technology is. But if it doesn’t really have a bottom line return on investment, there’s no sense in doing it. Unless it’s a hobby, but for most of us, it’s a job, and it matters the bottom line of the business. And the bottom line of the business is you want to make improvements to your product so you can get either greater market share, keep your customers happy and keep them from moving to people who can add features to their products. Competitors and so forth. So I think the maintainability part comes with, originally with really good implementation, initial implementation.</p><p><strong>Cass:</strong> So I want to get that idea of implementations. So oftentimes, when I think about— maybe I’m in the past, I’ve thought about functional languages. And I have thought about them in this kind of academic way, or else things that live in deep black boxes way down in the system. But you have been working on <a href="https://www.purescript.org/" rel="noopener noreferrer" target="_blank">PureScript</a>, which is something that is directly applicable to web browsers, which is, when I think about advanced clever mathematical code models, browsers are not necessarily what I would associate. That’s kind of a very fast and loose environment, historically. So can you talk a little bit about PureScript and how people can kind of get a little bit of experience in that?</p><p><strong>Scalfani:</strong> PureScript is a statically typed, purely functional language that has its lineage from <a href="https://www.haskell.org/" rel="noopener noreferrer" target="_blank">Haskell</a>, which would start as an academic language. And it compiles into <a href="https://en.wikipedia.org/wiki/JavaScript" rel="noopener noreferrer" target="_blank">JavaScript</a> so that it can run in the browser, but it also can run on the back end, running in Node. Or you can write it and have your program run in Electron, which is like a desktop application. So pretty much everywhere JavaScript works, you can pretty much get PureScript to work. I’ve done it in backends, and I’ve done it in browsers. I haven’t done it in Electron yet, but it’s pretty academic. So that’s totally doable. I know other people have done it. So it doesn’t get more run of the mill, kind of, programming than the browser, right? And JavaScript is a pretty terrible language, honestly. It’s terrible on so many ways because you can shoot your foot off in so many different ways in JavaScript. And every time I have to write a little bit of JavaScript, just the tiniest bit of JavaScript, I’m always getting burned constantly.</p><p>And so anyway, so what is a pure functional language? A pure functional language is that all your functions are pure, and a pure function is what I talked about earlier. It only has access to the inputs to a function, it does its computations, and it has its outputs. So that’s kind of like what we did in math, right? You have a function, <em>f</em> of <em>x</em>, <em>x</em> gets some value, and maybe your function is <em>x</em>+2, and so it takes the <em>x</em>, it adds two to it, and the result is whatever that value is, right? Whatever the computation is. So that’s what it purely functional language is. It’s completely pure. And there are languages that are hybrids, right? PureScript, Haskell, Elm. These are all languages that are pure. And they don’t compromise. So compromised languages are really great in the beginning, but you can easily lose out on all the benefits, right? So if you can— it’s the same thing with the goto, right? If we had, if we relegated goto to, like, okay, we’re going to stick it in this corner and you sort of don’t want to use it. It doesn’t stop you from pulling that off the shelf and using it all day, right? So it’s best to just eliminate something and not compromise. Not have a compromise language. To me, Scala as a compromise language. It’s not fully functional. And there are lots, like Clojure, I believe, has— even JavaScript. JavaScript is actually, for me, was my introduction to functional programming. There’s functional concepts in JavaScript.</p><p>And I thought JavaScript was the best thing since sliced bread when I had those things. I didn’t know they were functional at the time, but I’m like, this is something that I’ve been looking for for years, and I finally have it in this language called JavaScript, and I can pass a function as a parameter. I mean, I wanted that for decades. And all of a sudden, I could do it. And so I’m a big proponent of a purely functional languages because of that. Because of hybrids don’t work well. And all you need is a single library that you’re using that didn’t— the author didn’t use all the benefits, and all of a sudden, now your whole thing is messed up. Whatever you’ve built is tainted by this library that isn’t that isn’t pure, let’s say. So I think that the benefits of Haskell and PureScript being fully pure are really great. Complications are, you have to think very differently because of that, because we’re not used to thinking that way. There’s all these extra things that have to be built that are all part of the libraries that make that much, much easier. But then you have to understand the concepts. So I hope that explains PureScript a little bit.</p><p><strong>Cass:</strong> Well, I literally could go back and forth with you all day because this really is truly fascinating, but I’m afraid we’re out of time. So I do very much want to thank you for talking with us today.</p><p><strong>Scalfani: </strong>Great. Thank you. It was fun.</p><p><strong>Cass: </strong>Yeah. I really was. So today in <em>Fixing the Future</em>, we were talking with Charles Scalfani about functional programming and creating better code. I’m Stephen Cass of <em>IEEE Spectrum,</em> and I hope you’ll join us next time. </p>]]></description>
      <pubDate>Thu, 30 Mar 2023 20:31:25 +0000</pubDate>
      <guid>https://spectrum.ieee.org/functional-programming-biggest-change</guid>
      <category>Type:podcast</category>
      <category>Fixing the future</category>
      <category>Purescript</category>
      <category>Haskall</category>
      <category>Javascript</category>
      <category>Functional programming</category>
      <dc:creator>Stephen Cass</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/33385326/origin.webp">
      </media:content>
    </item>
    <item>
      <title>Why Governments’ Involvement in Standards Development is Crucial</title>
      <link>https://spectrum.ieee.org/government-in-standards-is-crucial</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/group-of-people-in-a-meeting-sitting-around-a-table-in-front-of-a-world-map.jpg?id=33380991&width=1200&height=800&coordinates=0%2C209%2C0%2C0"/><br/><br/><p>With the rapid rate of technological advancements and evolution, coupled with urgent global challenges, the world’s need for technical standards has never been greater. Technology standards <a href="https://standards.ieee.org/beyond-standards/what-are-standards-why-are-they-important/" rel="noopener noreferrer" target="_blank">establish specifications and procedures</a> designed to maximize the reliability of materials, products, methods, and services. In short, standards help increase the reliability and effectiveness of many of the goods and services people interact with daily.</p><p>Technical standards also play an important role as governmental bodies around the globe explore and establish policies and regulations to address challenges such as data governance, privacy, security, climate change, and the impact of rapid technological advances on citizens.</p><p>Global standards are critical instruments in areas such as innovation, trade, and public policy. But it can be challenging for government representatives to gain access to technical experts, best practices, and other resources needed to help them understand the standardization landscape and navigate issues that arise.</p><h2>A complex system of standards bodies</h2><p>One level of complexity governments face is that standards are developed by different types of organizations.</p><p>Standards bodies with a geographic designation generally are categorized as international, regional, or national, and they typically function under a nation-centric approach whereby governments, or groups closely coupled to governments, mandate standards.</p><p class="pull-quote">Technical standards play an important role as governmental bodies explore and establish policies and regulations to address challenges such as data governance, privacy, security, and climate change.</p><p>There are also standards-developing bodies with a global technical or industry designation. They include state-independent standards-developing bodies, such as <a href="http://standards.ieee.org" rel="noopener noreferrer" target="_blank">IEEE</a>, with a globally open participation mode. </p><h2>A program specific to governments</h2><p>The <a href="https://standards.ieee.org/about/intl/government-engagement-program/" rel="noopener noreferrer" target="_blank">IEEE Government Engagement Program on Standards</a> helps agencies, commissions, ministries, and regulatory bodies better understand the standards development process. The program facilitates the global interoperability of devices and systems, provides a way to strengthen standards portfolios, and supports efforts to minimize redundant or conflicting standards.</p><p>Participation in IEEE GEPS is free, and there are no membership requirements. Participants have observer status on the <a href="https://standards.ieee.org/about/sasb/" rel="noopener noreferrer" target="_blank">IEEE SA Standards Board</a>—which gives them insight into the mechanics of standards development, access to experts, and participation in national, regional, and international standards-setting bodies.</p><p>There also are quarterly webinars given by experts on topics that are of interest to GEPS participants, such as artificial intelligence, cybersecurity, and digital governance.</p><p>Other benefits include:</p><ul><li>Helping governments better understand their standards needs.</li><li>Access to information about policymaking and regulations.</li><li>Keeping government officials up to date on IEEE’s latest initiatives, including providing an inside look at standards in development.</li><li>Increasing governments’ understanding of the IEEE SA process, and how to best take advantage of it.</li><li>Receiving input and guidance on aspects of the broader standardization ecosystem, such as<a href="https://standards.ieee.org/products-programs/icap/" rel="noopener noreferrer" target="_blank"> conformity assessment and certification</a>, <a href="https://standards.ieee.org/products-programs/ams/" rel="noopener noreferrer" target="_blank">technical alliance management</a>, and<a href="https://standards.ieee.org/industry-connections" rel="noopener noreferrer" target="_blank"> industry connections</a>.</li></ul><p>Since the program launched in 2017, more than 57 governmental bodies from 35 countries have joined. They include Argentina’s<a href="https://www.enacom.gob.ar/" rel="noopener noreferrer" target="_blank"> ENACOM</a>, India’s <a href="https://cea.nic.in/?lang=en" rel="noopener noreferrer" target="_blank">Central Electricity Authority (CEA)</a>, Israel’s <a href="https://www.gov.il/en/departments/ministry_of_communications" rel="noopener noreferrer" target="_blank">Ministry of Communications</a>, Nigeria’s <a href="https://nitda.gov.ng/" rel="noopener noreferrer" target="_blank">National Information Technology Development Agency</a>, and the United Kingdom’s <a href="https://www.gov.uk/government/organisations/department-for-science-innovation-and-technology" rel="noopener noreferrer" target="_blank">Department for Science, Technology, and Innovation</a>.</p><p><em>This article is an edited excerpt of the “</em><a href="https://standards.ieee.org/beyond-standards/how-ieee-sa-connects-with-governments/#:~:text=The%20IEEE%20Government%20Engagement%20Program%20on%20Standards%20(GEPS)%20helps%20government,the%20local%20and%20global%20levels." rel="noopener noreferrer" target="_blank"><em>How IEEE SA Connects With Governments</em></a><em>” blog entry published in January.</em><br/></p>]]></description>
      <pubDate>Thu, 30 Mar 2023 18:00:04 +0000</pubDate>
      <guid>https://spectrum.ieee.org/government-in-standards-is-crucial</guid>
      <category>Ieee standards association</category>
      <category>Ieee government engagement program on standards</category>
      <category>Ieee products services</category>
      <category>Standards</category>
      <category>Technical standards</category>
      <category>Type:ti</category>
      <dc:creator>Karen McCabe</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/group-of-people-in-a-meeting-sitting-around-a-table-in-front-of-a-world-map.jpg?id=33380991&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Lightning Eyes 10-Minute Charging for Its Motorbikes</title>
      <link>https://spectrum.ieee.org/lightning-motorcycle</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-helmeted-person-rides-a-blue-motorcycle.jpg?id=33380473&width=1200&height=800&coordinates=0%2C125%2C0%2C125"/><br/><br/><p><a href="https://lightningmotorcycle.com/" target="_blank">Lightning Motorcycles</a> is already known for record-setting speeds. In 2011, the company’s <a href="https://lightningmotorcycle.com/home/product/specifications/" target="_blank">LS-218 SuperBike</a> set a land speed record for production electric motorcycles at the <a href="https://www.utah.com/destinations/natural-areas/bonneville-salt-flats/" target="_blank">Bonneville Salt Flats</a> in Utah, with a 347.55 kilometer-per-hour (215.91 mile-per-hour) average run and a 351 km/h (218 mph) peak. That SuperBike topped every internal combustion engine motorcycle en route to a <a href="https://ppihc.org/" target="_blank">Pikes Peak International Hill Climb</a> win in 2013, on the Colorado gauntlet that’s among the world’s highest-profile tech challenges for cars and motorcycles alike.</p><p>The Southern California company is looking to speed things up again—this time with record-setting charging stops. Founder <a href="https://www.linkedin.com/in/richard-hatfield-951aa511/" target="_blank">Richard Hatfield</a> claims that the company’s <a href="https://lightningmotorcycle.com/strike-carbon-edition-specifications/" target="_blank">Lightning Strike</a> motorcycle can fill its battery from 20 percent to 80 percent in a little over 10 minutes on a <a href="https://www.forbes.com/wheels/advice/ev-charging-levels/" target="_blank">Level 3 DC fast charger</a>. That time isn’t much longer than a gasoline fill-up, especially for motorcyclists who don’t mind some stretching and recovery after hours in the saddle.</p><p>Faster pit stops could also help unlock sales for electric two-wheelers, which have been slow to catch on due to dawdling charge times and limited riding range. Whereas electric carmakers have vastly more space for batteries, motorcycle purveyors can only stuff so many cells into a slender frame before a bike becomes impractically heavy, cumbersome to ride, or ungainly in appearance.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A white electric motorcycle charging in a parking lot." class="rm-shortcode" data-rm-shortcode-id="50fefc25beabdb441a1256ea0cf02efc" data-rm-shortcode-name="rebelmouse-image" id="abdcf" loading="lazy" src="https://spectrum.ieee.org/media-library/a-white-electric-motorcycle-charging-in-a-parking-lot.jpg?id=33380500&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Lightning Motorcycles claims that the Lightning Strike motorcycle can fill its battery from 20 percent to 80 percent in a little over 10 minutes on a Level 3 DC fast charger. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Lightning Motorcycles</small></p><p>“Costs are becoming competitive, so the last big issues are range and charge time,” Hatfield says. “For the bikes that don’t have Level 3 charging, it’s a big penalty if you’re trying to ride longer than the battery can take you.”</p><p>For the US $13,000 Lightning Strike, zippy charging stops begin with a pouch-format battery from <a href="https://www.enevate.com/" target="_blank">Enevate</a>, with a high concentration of silicon in its anode. That California company claims its silicon-dominant anode material has an impressive specific capacity of about 3,000 milliampere-hours per gram. Lightning Motorcycles integrated the Strike’s 24-kilowatt-hour battery in the same space as 20-kWh pack in its LS-218, for a 20 percent gain in capacity. The result is a curb weight of just under 227 kilograms (500 pounds), about 45 kilos (100 pounds) more than that of top sport bikes.</p><p>The battery pack holds more energy than the 22-kWh pack in BMW’s <a href="https://en.wikipedia.org/wiki/BMW_i3" target="_blank">original i3 electric car</a> that debuted in 2012. Lightning is also developing a 28-kWh pack for its race-oriented LS-218.</p><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="6b2efc3b1f576baca19dc8e2b2f532da" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/X3w-TAZzj80?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
<small class="image-media media-caption" placeholder="Add Photo Caption...">This video shows an acceleration comparison between Lightning and Harley-Davidson. </small>
</p><p>Hatfield says the Strike’s efficient battery delivers a useful real-world range of 150 miles at a 70-mph clip, and closer to 165 or 170 miles in mellower riding. When it’s time to stop, the Strike can absorb Level 3 juice at up to 120 kilowatts. The company posted a <a href="https://www.youtube.com/watch?v=md9rPJ9GD2A&ab_channel=LightningMotorcycle" target="_blank">YouTube video</a> pitting its bike’s charging time against a <a href="https://www.livewire.com/" target="_blank">Harley-Davidson Live Wire</a> and <a href="https://zeromotorcycles.com/model/zero-srs" target="_blank">Zero Motorcycle’s SR/S</a>.</p><p>In the video, the Strike adds 12 kWh of power in just under 11 minutes on a typical DC fast charger. That’s closer to a 50 percent recharge than the company’s 60 percent (from 20-to-80 percent) claim. Yet the bike still slurps juice nearly four times as fast as the Harley’s 3.3-kilowatt rate, and more than 10 times as fast as  the Zero, which, like the vast majority of e-motos, is limited to Level 2 AC charging. The Strike’s silicon-anode cells are just one element of the fast pace.</p><p>“Every component in between—the cabling, interconnects, even the contactors, had to be reengineered,” Hatfield says.</p><p>As with any motorcycle, generating cooling air is no problem when the Lightning is on the move. But managing the intense temperatures generated by DC charging requires a half-dozen fans to move air through the bike’s fairing and migrate heat from components. Software keeps a close eye on thermal metrics during charging, “so we can push harder on a 50-degree morning than a 90-degree afternoon,” Hatfield says. “The cells could charge even faster, but we’re trying to get a balance of system to manage thermal issues.”</p><p>The Strike’s electrical architecture operates in the 300-to-400-volt range, depending on the application. But the company is developing an 800-volt architecture for the LS-218, on par with today’s fastest-charging electric cars, such as the <a data-linked-post="2652903587" href="https://spectrum.ieee.org/2021s-top-ten-tech-cars-lucid-air" target="_blank">Lucid Air</a>, <a data-linked-post="2650279004" href="https://spectrum.ieee.org/2020-porsche-taycan-electric-car" target="_blank">Porsche Taycan</a>, or models from the South Korean trio of Genesis, Hyundai, and Kia.</p><p>“If we’re doing 300 amps at 400 volts, that’s a (peak) 120-kilowatt charging rate,” Hatfield said. “But 300 amps at 800 volts, and now you’re at 240 kilowatts. That’s really the direction we need to go with this.”</p><p>That robust architecture could also generate more sheer force. The standard Strike has up to 132 kilowatts (180 horsepower) and an electric motor that peaks around 7,500 rpm. Boosting voltage, Hatfield said, could push the electric motor closer to 12,500 rpm, generating more than 149 kilowatts (200 horsepower). That figure would<a href="https://www.cycleworld.com/story/bikes/5-most-powerful-motorcycles-tested-on-dyno/#:~:text=The%20Aprilia%20RSV4%20Factory%20produced,on%20the%20Cycle%20World%20dyno." rel="noopener noreferrer" target="_blank"> top every production internal-combustion-engine motorcycle</a>, including the Aprilia RSV4 Factory’s 140 kilowatts (190 horsepower), <a href="https://www.cycleworld.com/aprilia/rsv4/" target="_blank">as tested by <em>Cycle World</em></a>.</p><p>If there’s any downside, it’s that Lightning, like many other two-wheeled dreamers, hasn’t delivered many motorcycles. Hatfield would not reveal how many bikes the company has made, but said the company can satisfy an order in 90 days.</p><p>The company is also aiming to break its own electric speed record, beginning this spring with runs at El Mirage in California, and later at Bonneville—conditions willing on the otherworldly yet <a href="https://www.sltrib.com/news/environment/2022/11/05/utahs-salt-flats-are-shrinking/#:~:text=Bonneville%20Salt%20Flats%20have%20thinned,2022%2C%20near%20Wendover%2C%20Utah." rel="noopener noreferrer" target="_blank">shrinking and deteriorating</a> salt surface. The latest target is 402 km/h (250 mph), a mind-bending pace on two wheels that requires a special, steely nerved rider. That rider would be <a href="https://www.youtube.com/watch?v=yptghf8lX1M" rel="noopener noreferrer" target="_blank">Joe Amo</a>, who has already blown past those speeds on a specially modified, gasoline-powered Kawasaki. (The Lightning holds the land speed record for street-legal production motorcycles). But even the world’s fastest internal-combustion-engine bikes can’t match the Lightning’s tricks, including using a solar-panel array on a mobile van to provide electricity for those record-smashing runs.</p>]]></description>
      <pubDate>Thu, 30 Mar 2023 15:11:23 +0000</pubDate>
      <guid>https://spectrum.ieee.org/lightning-motorcycle</guid>
      <category>Motorcycle</category>
      <category>Electric vehicles</category>
      <category>Charging stations</category>
      <dc:creator>Lawrence Ulrich</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-helmeted-person-rides-a-blue-motorcycle.jpg?id=33380473&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Robots Using Legs as Arms to Climb and Push Buttons</title>
      <link>https://spectrum.ieee.org/climbing-legged-robot</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-small-quadrupedal-robot-balances-on-two-legs-against-a-wall-and-uses-one-of-its-front-legs-to-push-a-button-to-open-a-door.png?id=33367334&width=1200&height=800&coordinates=150%2C0%2C150%2C0"/><br/><br/><p>We’ve gotten used to thinking of quadrupedal robots as robotic versions of dogs. And, to be fair, it’s right there in the word “quadrupedal.” But if we can just get past the Latin, there’s absolutely no reason why quadrupedal robots have to restrict themselves to using all four of their limbs as legs all of the time. And in fact, most other quadrupeds are versatile like this: four-legged animals frequently use their front limbs to interact with the world around them for nonlocomotion purposes.</p><p>Roboticists at Carnegie Mellon University and the University of California, Berkeley, are <a href="https://robot-skills.github.io/" rel="noopener noreferrer" target="_blank">training robot dogs to use their legs for manipulation</a>, not just locomotion, demonstrating skills that include climbing walls, pressing buttons, and even kicking a soccer ball.</p><hr/><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" data-rm-shortcode-id="d46491ccdcd2ad916122e1a4ff95d506" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/d3YCmkEC7V0?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span>
</p><p>Training a robot to do both locomotion and manipulation at the same time with the same limbs can be tricky using reinforcement-learning techniques, because you can get stuck in <a href="https://arxiv.org/abs/1909.05035" target="_blank">local minima</a> while trying to optimize for skills that are very different and (I would guess) sometimes in opposition to each other. So, the researchers split the training into separate manipulation and locomotion policies, and trained each in simulation, although that meant an extra step smooshing those separate skills together in the real world to perform useful tasks.</p><p>Successfully performing a combined locomotion and manipulation task requires one high-quality expert demonstration. The robot remembers what commands the human gave during the demonstration, and then creates a behavior tree that it can follow that breaks up the tasks into a bunch of connected locomotion and manipulation subtasks that it can perform in order. This also adds robustness to the system, because if the robot fails any subtask, it can “rewind” its way back through the behavior tree until it gets back to a point of success, and then start over from there.</p><p>This particular robot (a Unitree Go1 with an Intel RealSense for perception) manages to balance itself against a wall to press a wheelchair access button that’s nearly a meter high, and then walk out the open door, which is pretty impressive. More broadly, this is a useful step toward helping nonhumanoid robots to operate in human-optimized environments, which might be more important than it seems. It’s certainly possible to modify our environments to be friendlier to robots, and we see this in places like hospitals (and some hotels) where robots are able to directly control elevators. This makes it much easier for the robots to get around, but it’s annoying enough to have to do that in some cases. It’s more practical (if not necessarily simpler) to <a href="https://www.youtube.com/watch?v=jUKCI0ynm6s" rel="noopener noreferrer" target="_blank">just build a button-pushing robot instead</a>. There’s perhaps an argument to be made that the best middle ground here is just to build broadly accessible infrastructure in the first place, by making sure that neither robots nor humans should have to rely on a specific manipulation technique to operate anything. But until we make that happen, skills like these will be critical for helpful legged robots.</p><p><a href="https://robot-skills.github.io/" rel="noopener noreferrer" target="_blank"><em>Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion</em></a>, by Xuxin Cheng, Ashish Kumar, and Deepak Pathak from Carnegie Mellon University and the University of California, Berkeley, will be presented next month at <a href="https://www.icra2023.org/" target="_blank">ICRA 2023</a> in London.</p>]]></description>
      <pubDate>Wed, 29 Mar 2023 13:00:07 +0000</pubDate>
      <guid>https://spectrum.ieee.org/climbing-legged-robot</guid>
      <category>Quadrupedal robots</category>
      <category>Reinforcement learning</category>
      <category>Unitree go1</category>
      <category>Legged robots</category>
      <category>Robotics</category>
      <dc:creator>Evan Ackerman</dc:creator>
      <media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/a-small-quadrupedal-robot-balances-on-two-legs-against-a-wall-and-uses-one-of-its-front-legs-to-push-a-button-to-open-a-door.png?id=33367334&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>New Award Recognizes IEEE Society’s Work in DEI</title>
      <link>https://spectrum.ieee.org/new-ieee-award-dei</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/group-of-business-people-following-a-business-woman-holding-a-flag.jpg?id=33364543&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>
	For the past 20 years, the 
	<a href="https://ieee-ims.org/" rel="noopener noreferrer" target="_blank">IEEE Instrumentation and Measurement Society</a> has been working to become more welcoming and inclusive for women, members from outside the United States and Canada, students, and young professionals. Its hard work in diversity, equity, and inclusion (DEI) has paid off.
</p><p>
	IEEE IMS has increased its number of women leaders, conference speakers, and editors of its publications. The society also has expanded its chapters to other parts of the world. To encourage more students to join, it holds competitions and offers scholarships, and a mentoring program was created to attract students and young professionals.
</p><p>
	The society’s efforts have been recognized with the new 
	<a href="https://www.ieee.org/about/volunteers/tab.html" rel="noopener noreferrer" target="_blank">IEEE Technical Activities Board</a> Award for Society/Council Impact in DEI. The award was established last year to honor an IEEE society or council that has encouraged DEI by developing activities, programs, and services that promote efforts in the area. IEEE IMS is the first society to be given the award.
</p><p>
	“The society received the news [about the award] with great joy,” says IEEE Senior Member 
	<a href="https://ieee-ims.org/contact/juan-manuel-ramirez-cortes" rel="noopener noreferrer" target="_blank">Juan Manuel Ramirez Cortés</a>, the IMS president. “Being the inaugural recipient of the award is a true honor and serves as significant motivation for our ongoing DEI efforts.”
</p><h2>Increasing the number of women leaders</h2><p>
	The society’s nominations and appointments committee is dedicated to recommending experienced women to serve in leadership positions.
</p><p>
	In 1992 there were no women on the society’s administrative committee, AdCom, and only one member was from outside the United States and Canada. The committee is composed of elected officers and nonelected leaders.
</p><h3>The IEEE Instrumentation and Measurements Society by the Numbers </h3><br/><p><strong>
	Total membership</strong></p><p>
	3,820
</p><p><strong>
	Number of chapters</strong></p><p>
	72
</p><p><strong>
	Number of student chapters</strong></p><p>
	15
</p><p><strong>Number of technical committees</strong></p><p>23</p><p>
	The oversight changed when IEEE Life Fellow 
	<a href="https://ieee-ims.org/contact/stephen-dyer" rel="noopener noreferrer" target="_blank">Stephen A. Dyer</a> joined the committee as editor in chief of the <a href="https://ieee-ims.org/publication/ieee-tim" rel="noopener noreferrer" target="_blank"><em>IEEE Transactions on Instrumentation and Measurement</em></a>. He identified women and individuals from other geographic regions who were qualified to be AdCom candidates.
</p><p>
	“A diverse AdCom promotes more effective and more engaging operational programs,” Ramirez Cortés says.
</p><p>
	In 1999 Dyer’s wife, 
	<a href="https://ece.k-state.edu/people/faculty/rdyer/index.html" rel="noopener noreferrer" target="_blank">Ruth</a>, became the first woman elected to the committee.
</p><p>
	It then took a decade until a second woman was selected to serve.
</p><p>
	Between 2010 and 2021 the society had between six and nine female voting members in leadership positions, including Ruth Dyer. The IEEE Life Fellow was elected the 2016–2017 society president and 2021 Division II director.
</p><p>
	Since 2007, IEEE IMS has appointed at least one female undergraduate, graduate, or 
	<a href="https://yp.ieee.org/" rel="noopener noreferrer" target="_blank">IEEE Young Professional</a> representative almost every year to its AdCom.
</p><p>
<a href="https://ieee-ims.org/contact/ferdinanda-ponci" rel="noopener noreferrer" target="_blank">Ferdinanda Ponci</a> was elected in 2010 as the <a href="https://www.google.com/aclk?sa=l&ai=DChcSEwj70O3chPD9AhXqH60GHcJxDhkYABAAGgJwdg&sig=AOD64_36a6sTd5wS79dAzW8oBwfopGmNLg&q&adurl&ved=2ahUKEwi5rOXchPD9AhVeJUQIHSAYDo0Q0Qx6BAgEEAE" rel="noopener noreferrer" target="_blank">IEEE Women in Engineering</a> liaison. The IEEE senior member was designated as the committee’s point of contact to the TAB’s diversity and inclusion committee.
</p><p>
	The IEEE IMS AdCom now has seven vice president positions, Ramirez Cortés says, and every position has been held by at least one woman in the past 20 years. Seventy percent of the committee is from outside the United States and Canada.
</p><p>
	Women members and individuals from different IEEE geographic areas who are experts in the society’s fields of interest have been encouraged to publish their research in the society’s publications and to serve as reviewers and associate editors.
</p><p>
<a href="https://ieee-ims.org/contact/wendy-van-moer" rel="noopener noreferrer" target="_blank">Wendy Van Moer</a> in 2015 became the first female editor in chief of the <em>IEEE Transactions on Instrumentation and Measurement</em>. In the past five years, the percentage of women who have served as associate editors of the publication has risen to 19 percent from 5 percent, Ramirez Cortés says.
</p><p class="pull-quote">
	“Diversity with regard to race, religion, gender, disability, age, national origin, sexual orientation, gender identity, and gender expression brings the society scientific, technological, and cultural richness.”</p><p>Members of the society’s editorial board are from the 25 countries that have contributed the most articles to the society’s publications.
</p><p>
	Women have served in roles such as chair of the society’s flagship gathering, the IEEE 
	<a href="https://i2mtc2023.ieee-ims.org/" rel="noopener noreferrer" target="_blank">International Instrumentation and Measurement Technology Conference</a> (I2MTC) and its <a href="https://ieee-ims.org/post/call-applications-ims-distinguished-lecturers-0" rel="noopener noreferrer" target="_blank">distinguished lecturers program</a>.
</p><p>
	In June the society joined 21 other IEEE organizational units that took the 
	<a href="https://spectrum.ieee.org/ieee-women-in-engineering-leads-a-pledge-to-make-speaker-panels-more-gender-balanced" target="_self">IEEE Women in Engineering pledge</a> to work toward “gender-diversified panels at all IEEE meetings, conferences, and events.”
</p><p>
	IEEE IMS conferences have increased the number of female speakers and technical program committee chairs. Meanwhile, a diversity chair is appointed for each of the society’s conferences to ensure variety in the speakers, programs, and panels. Keynote speakers are from academia as well as industry—many of them women.
</p><p>
	“The society wants to ensure that qualified women and other underrepresented groups have an opportunity to participate,” Ramirez Cortés says.
</p><p>
	The society has about 3,820 members. Almost 9 percent are women.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="three people posing for a portrait while holding a plaque" class="rm-shortcode" data-rm-shortcode-id="91766b97c731baa3f5989f2aa024d5df" data-rm-shortcode-name="rebelmouse-image" id="84abf" loading="lazy" src="https://spectrum.ieee.org/media-library/three-people-posing-for-a-portrait-while-holding-a-plaque.jpg?id=33364553&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">John Verboncoeur, president of the IEEE Technical Activities Board, presented the award to Dalma Novak [left] and Ruth Dyer at the February board meeting series. Novak is the chair of the IEEE TAB Committee on Diversity and Inclusion. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Tom Compton</small></p><h2>Diversifying geographic representation</h2><p>
	Geographic diversity is an important factor in the society’s DEI efforts, Ramirez Cortés says. Conference locations are selected to increase accessibility and participation, he says. Within the past decade, I2MTC has been held three times each in Regions 1–6 (United States), Region 7 (Canada), Region 8 (Africa, Europe, Middle East), and Region 10 (Asia and Pacific), and once in Region 9 (South America).
</p><p>
	“This action has promoted geographic diversity in both authorship of conference papers and society membership,” Ramirez Cortés says, “and fostered an inclusive environment for members of different cultures.”
</p><p>
	Another way the society is increasing diversity is through its outreach program for chapters, which helps them thrive and establishes new ones. Liaisons for each IEEE region visit inactive chapters to help reestablish them, and they participate in active chapters’ events such as workshops and regions to encourage growth. The liaisons also seek out members who are experts in their field to participate in the society’s lecturer program so they can share their knowledge.
</p><p>
	“The outreach program allows the society to address each region by following and respecting their geographical and cultural norms,” Ramirez Cortés says.
</p><p>
	 IEEE IMS in 2017 launched a three-year 
	<a href="https://ieee-ims.org/africa-initiative" rel="noopener noreferrer" target="_blank">Africa initiative</a> because it decided it needed more representation on the continent. The initiative is designed to improve services offered in Africa, increase membership, and add chapters. Since the initiative was launched, the society established its first African chapter, in <a href="https://ieee-ims.org/chapter/south-africa-section-joint-societies-chapter" rel="noopener noreferrer" target="_blank">South Africa</a>, followed by one in <a href="https://ieee-ims.org/chapter/angola-section-chapter" rel="noopener noreferrer" target="_blank">Angola</a>. A chapter in Ghana is in the process of forming. The society also has increased membership in Kenya, Nigeria, Tunisia, Uganda, and Zambia.
</p><h2>Supporting students and young professionals</h2><p>
	Panels that cater to students and young professionals are part of IEEE IMS conferences. They cover how to become an IEEE leader, tips on finding a job, and related topics.
</p><p>
	The society holds events for students at its conferences, such as a 
	<a href="https://ims-ieee.org/2023SDC" rel="noopener noreferrer" target="_blank">design competition, and it provides </a>partial funding for travel. The winner of the design competition receives a cash prize and is honored at a luncheon.
</p><p>
	About 12 percent of the society’s members are students.
</p><p>
	IEEE IMS recently launched a mentoring program for students and young professionals that pairs young members with seasoned professionals in their field.
</p><p>
	“The mentors work in academia, industry, and government,” Ramirez Cortés says. “Many of them are leaders in the society. They help guide participants throughout their careers, technical work, and getting involved in IEEE.”
</p><p>
	Diversity with regard to race, religion, gender, disability, age, national origin, sexual orientation, gender identity, and gender expression, Ramirez Cortés says, brings the society “scientific, technological, and cultural richness.”
</p><p>
<em>There are many DEI initiatives happening in societies like IEEE IMS across IEEE. Learn more on IEEE’s <a href="https://www.ieee.org/about/diversity-index.html" rel="noopener noreferrer" target="_blank">DEI website</a>.</em>
</p>]]></description>
      <pubDate>Tue, 28 Mar 2023 18:00:06 +0000</pubDate>
      <guid>https://spectrum.ieee.org/new-ieee-award-dei</guid>
      <category>Ieee women in engineering</category>
      <category>Ieee young professionals</category>
      <category>Diversity</category>
      <category>Equity</category>
      <category>Ieee awards</category>
      <category>Ieee instrumentation and measurement society</category>
      <category>Ieee news</category>
      <category>Ieee society</category>
      <category>Inclusion</category>
      <category>Type:ti</category>
      <dc:creator>Joanna Goodrich</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/group-of-business-people-following-a-business-woman-holding-a-flag.jpg?id=33364543&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>How and When the Chip Shortage Will End, in 4 Charts</title>
      <link>https://spectrum.ieee.org/chip-shortage</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-worker-fuels-a-volkswagen-golf-8-automobile-on-the-assembly-line.jpg?id=27044601&width=1200&height=800&coordinates=0%2C103%2C0%2C104"/><br/><br/><p>
<strong>One looming artifact</strong> of the pandemic that remains in 2023—the 
		<a href="https://spectrum.ieee.org/tag/chip-shortage" target="_self">global chip shortage</a>—has gratefully <a href="https://www.techrepublic.com/article/is-chip-shortage-over/" target="_blank">begun to recede</a>. Unlike the state of things in mid-2021—when crimps in the semiconductor supply chain cropped up in big ways—supply and demand have become <a href="https://www.marketplace.org/2023/01/03/auto-industry-chip-shortage-buyers/" target="_blank">much less of a mismatch</a>.
	</p><p>
As <em>IEEE </em><em>Spectrum</em>
		reported in the months since this story originally posted, the broken supply chains caused by the chip shortage have practically 
		<a href="https://spectrum.ieee.org/chip-shortage-rewiring-tech" target="_self">rewired whole segments of the tech industry</a>. For the automotive industry, as we summarized in <a href="https://spectrum.ieee.org/global-chip-shortage-charts" target="_self">five charts that helped demystify the chip shortage</a>, time eventually brought carmakers up from the end of a 52-week waiting list to get the chips they needed for their entertainment and driving-assistance systems. With chips finally reaching factory floors, their own manufacturing capacities were restored to prepandemic levels by the end of 2022.
	</p><p>
Meanwhile, the mid-2022 passage of the 
		<a href="https://spectrum.ieee.org/chips-act-of-2022">CHIPS Act</a> in the United States yielded a multibillion-dollar investment pool, some of which was dedicated to ramping up American manufacturing of the mature-generation chips upon which many industries—auto and otherwise—are so dependent. In March of 2023, the U.S. began <a href="https://spectrum.ieee.org/us-chips-act-funding" target="_self">disbursing CHIPS Act funding</a>, while the E.U. considered <a href="https://spectrum.ieee.org/eu-chips-act-imec" target="_self">getting into the chip-stimulus game</a> as well. </p><p>The aim of Washington’s $50 billion expenditure is to prevent U.S. industrial concerns from falling victim to similar semiconductor supply chain snafus in the future. The EU’s Chips Act <a href="https://techhq.com/2022/11/the-e43-billion-eu-chips-act-gets-green-light-from-european-nations-whats-next/" rel="noopener noreferrer" target="_blank">legislation</a> has similar aims; also chief among them is bolstering its constituent nations’ resilience in the face of such supply-chain interruptions.
	</p><p>
<em>Story from 29 June 2021 follows:</em>
</p><p>
<strong>Historians will probably</strong> spend decades picking apart the consequences of the COVID-19 epidemic. But the shortage of chips that it’s caused will be long over by then. A variety of analysts agree that the most problematic shortages will begin to ease in the third or fourth quarter of 2021, though it could take much of 2022 for the resulting chips to work their way through the supply chain to products. The supply relief will not be coming from the big, national investments in the works right now by South Korea, the United States, and Europe but from older chip fabs and foundries running processes far from the cutting edge and on comparatively small silicon wafers.
</p><hr/><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The auto industry is a relatively small chip end-user, but it\u2019s growing fast." class="rm-shortcode" data-rm-shortcode-id="df03817bd97c6c5f5505d9c577e52e77" data-rm-shortcode-name="rebelmouse-image" id="8ef0c" loading="lazy" src="https://spectrum.ieee.org/media-library/the-auto-industry-is-a-relatively-small-chip-end-user-but-it-u2019s-growing-fast.jpg?id=27044602&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">The auto industry is a relatively small chip end-user, but it’s growing fast.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">IDC</small>
</p>
</div><p>
	Before we get into how the shortage will end, it’s worth summing up how it began. With panic, lockdowns, and general uncertainty rolling across the globe, automakers cancelled orders. However, those conditions meant a big fraction of the workforce recreated the office at home, purchasing computers, monitors, and other equipment. At the same time entire school systems switched to virtual learning via laptops and tablets. And more time at home also meant more spending on home entertainment, such as TVs and game consoles. These, the 5G rollout, and continued growth in cloud computing quickly hoovered up the capacity automakers had unceremoniously freed. By the time car makers realized people still wanted to buy their goods they found themselves at the back of the line for the chips they needed.
</p><p>
	At $39.5 billion, the auto industry makes up less than 9 percent of chip demand by revenue, according to market research firm IDC. That figure is set to increase by about 10 percent per year to 2025. However, the auto industry—
	<a href="https://www.ilo.org/wcmsp5/groups/public/---ed_dialogue/---sector/documents/meetingdocument/wcms_741659.pdf">which employs more than 10 million people globally</a>— is something both consumers and politicians are acutely sensitive to, especially in the United States and Europe.
</p><p>
	Chips for the automotive sector are made using processes intended to meet 
	<a href="https://en.wikipedia.org/wiki/ISO_26262">safety criteria</a> that are different from those meant for other industries. But they are still fabricated on the same production lines as the analog ICs, power management chips, display drivers, microcontrollers, and sensors that go in everything else. “The common denominator is the process technology is 40 nanometers and older,” says <a href="https://www.idc.com/getdoc.jsp?containerId=PRF000268">Mario Morales</a>, vice president, enabling technologies and semiconductors at IDC.
</p><div class="ieee-sidebar-medium">
<figure class="rt med" role="img">
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="It relies on chips made using mature\u201440-nanometer and older\u2014manufacturing processes. Those processes make up most of the installed capacity." class="rm-shortcode" data-rm-shortcode-id="3d5b515c99787df99fa1ad5c9ea64fcc" data-rm-shortcode-name="rebelmouse-image" id="0335d" loading="lazy" src="https://spectrum.ieee.org/media-library/it-relies-on-chips-made-using-mature-u201440-nanometer-and-older-u2014manufacturing-processes-those-processes-make-up-most-of-t.jpg?id=27044603&width=980"/>
</p>
<div>
<figcaption>
		Cars rely on chips made using mature—40-nanometer and older—manufacturing processes. Those processes make up most of the installed capacity. 
		</figcaption>
</div>
</figure>
</div><p>
	This chip manufacturing technology was last cutting edge 15 years ago or earlier, lines producing chips at these old nodes represent a full 54 percent of installed capacity, according to IDC. Today these old nodes are typically used on 200-mm wafers of silicon. To reduce cost, the industry began moving to 300-mm wafers in 2000, but much of the old 200-mm infrastructure continued and even expanded.
	<a class="flourish-credit" href="https://public.flourish.studio/visualisation/6565153/?utm_source=embed&utm_campaign=visualisation/6565153" target="_top"> </a>
</p><p>
	Despite the auto industry’s desperation, there’s no great rush to build new 200-mm fabs. “The return on investment just isn’t there,” says Morales. What’s more there are already many legacy-node plants in China that are not operating efficiently right now, but “at some point, they will,” he says, further reducing the incentive to build new fabs. According to the chip manufacturing equipment industry association SEMI, the number of 200-mm fabs will go from 212 in 2020 to 222 in 2022, about half the expected increase of the more profitable 300-mm fabs.
</p><div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="chart of number of 200 mm fabs" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="59d66e817527676195453a54bb662248" data-rm-shortcode-name="rebelmouse-image" id="10318" loading="lazy" src="https://spectrum.ieee.org/media-library/chart-of-number-of-200-mm-fabs.jpg?id=27044604&width=980" style="float: left"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">There are few new 200-mm fabs...</small>
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="capital spending on 200 mm fabs" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="3d8238d115ebc21d0745f10f5394136c" data-rm-shortcode-name="rebelmouse-image" id="6cfb3" loading="lazy" src="https://spectrum.ieee.org/media-library/capital-spending-on-200-mm-fabs.jpg?id=27044605&width=980" style="float: right"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">...but companies are investing in equipping the old ones.</small>
</p>
</div><p>
	More than 40 companies will increase capacity by more than 750,000 wafers-per-month from the beginning of 2020 to the end of 2022. The
	<a href="https://www.prnewswire.com/news-releases/global-200mm-fab-capacity-on-pace-to-record-growth-to-meet-surging-demand-and-address-chip-shortage-semi-reports-301298260.html"> long-term trend to the end of 2024 is for a 17 percent</a> increase in capacity for 200-mm facilities. Spending on equipment for these fabs is set to rise to $4.6 billion in 2021 after crossing the $3-billion mark in 2020 for the first time in years, SEMI says. But then spending will drop back to $4 billion in 2022. In comparison, spending to equip 300-mm fabs is expected to hit $78-billion in 2021.
</p><p>
	The chip shortage is happening simultaneously with national and regional efforts to boost advanced logic chip manufacturing. 
	<a href="/tech-talk/semiconductors/devices/south-koreas-450billion-investment-latest-in-chip-making-push">South Korea announced a push worth $450-</a>billion over ten years, the <a href="/tech-talk/semiconductors/processors/us-takes-strategic-step-to-onshore-electronics-manufacturing">United States</a> is pushing legislation worth $52 billion, and the EU could plow up to $160-billion into its semiconductor sector. Chipmakers were already on a spending spree. Globally, capital equipment for semiconductor production grew 56 percent year-on-year through April 2021, according to SEMI. SEMI’s 3 June 2021 World Fab Forecast indicates that 10 new 300-mm fabs will start operation in 2021 with 14 more coming up in 2022.
</p><p>
	“The push for building IC capacity around the world will certainly drive fab investment of the current decade to a new high,” says C
	<a href="https://www.linkedin.com/in/christian-gregor-dieseldorff-25403022/">hristian Gregor Dieseldorff</a>, senior principal for semiconductors at SEMI. “We expect to see record spending and more new fab announcements in the next few years.”
</p><p>
	One potential hiccup on the road to ending the shortage is that some of the skyrocketing demand appears to be from customers that are double-ordering to bulk up on inventory, says 
	<a href="https://semico.com/content/jim-feldhan">Jim Feldhan, president of Semico Research</a>. “I don’t know of any product that needs twice the amount of analog” as the year before, he says.  But manufacturers “don’t want a 12-cent part to hold up a 4K television,” so they’re stocking up.
</p><p>
	The auto industry needs to do more than just stock up, according to 
	<a href="https://www.kearney.com/bharat-kapoor">Bharat Kapoor</a>, lead partner, Americas, in the high-tech practice of global strategy and management consulting firm, Kearney. To keep future shortages at bay, the chip industry and auto executives need a more direct connection going forward so signals about supply and demand are clearer, he says.
</p><p>
<em>This post was corrected on 30 June to clarify historic 200-mm fab equipment spending.</em>
</p><p>
<em>This article appears in the August 2021 print issue as “How and When the Chip Shortage Will End.”</em>
</p>]]></description>
      <pubDate>Tue, 28 Mar 2023 15:10:00 +0000</pubDate>
      <guid>https://spectrum.ieee.org/chip-shortage</guid>
      <category>Devices</category>
      <category>Processors</category>
      <category>Covid-19</category>
      <category>Analog</category>
      <category>Fabs</category>
      <category>Shortages</category>
      <category>Power management</category>
      <category>Automobiles</category>
      <category>Wafers</category>
      <category>Microcontrollers</category>
      <category>Chip shortage</category>
      <category>Microchip shortage</category>
      <category>Chips act</category>
      <dc:creator>Samuel K. Moore</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-worker-fuels-a-volkswagen-golf-8-automobile-on-the-assembly-line.jpg?id=27044601&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>The EV Transition Is Harder Than Anyone Thinks</title>
      <link>https://spectrum.ieee.org/the-ev-transition-explained-2659602311</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image-of-a-tangled-line-between-a-car-and-a-charger.png?id=33327369&width=1200&height=800&coordinates=0%2C554%2C0%2C555"/><br/><br/><p>
<strong>Volvo Cars CEO Jim Rowan </strong><a href="https://www.autonews.com/automakers-suppliers/volvo-ceos-bold-prediction-ev-ice-price-parity-2025" rel="noopener noreferrer" target="_blank">boldly proclaims</a> that electric vehicles will reach price parity with internal-combustion-engine (ICE) vehicles by 2025. Not likely, <a href="https://www.roadandtrack.com/news/a39627829/when-will-evs-be-cheaper/" rel="noopener noreferrer" target="_blank">counter</a> Mercedes-Benz’s chief technology officer Markus Schäfer and Renault Group CEO Luca de Meo.
</p><p>
	The 
	<a href="https://www.iea.org" rel="noopener noreferrer" target="_blank">International Energy Agency</a> <a href="https://www.iea.org/reports/by-2030-evs-represent-more-than-60-of-vehicles-sold-globally-and-require-an-adequate-surge-in-chargers-installed-in-buildings" rel="noopener noreferrer" target="_blank">predicts</a> that EVs will make up more than 60 percent of vehicles sold globally by 2030. But given the sheer tonnage of lithium, cobalt, and other raw materials needed for EV batteries, that figure is overly optimistic, <a href="https://www.benchmarkminerals.com/membership/more-than-300-new-mines-required-to-meet-battery-demand-by-2035/" rel="noopener noreferrer" target="_blank">suggests</a> the mineral market analysis company <a href="https://www.benchmarkminerals.com/about/" rel="noopener noreferrer" target="_blank">Benchmark</a> Mineral Intelligence, unless nearly 300 new mines and supporting refineries open by then.
</p><h3></h3><br/><p ev-ebook.pdf"="" https:=" " spectrum.ieee.org="" style="color: #fa502a; font-size: 20px;>
	<a href=" target="_self"><strong><a href="https://spectrum.ieee.org/ev-ebook.pdf" target="_blank">Download</a> “The EV Transition Explained” e-book for free.
	</strong>
</p><h3></h3><br/><p>EV owners should be urged to charge at night to save not only money and the power grid but “ <a href="https://www.bloomberg.com/news/articles/2018-02-25/why-charging-your-electric-car-at-night-could-save-the-world" rel="noopener noreferrer" target="_blank">the world</a>,” a news headline cries out. Not so fast, exclaim researchers at Stanford University, who <a href="https://news.stanford.edu/press-releases/2022/09/22/charging-cars-honight-not-way-go/" rel="noopener noreferrer" target="_blank">state</a> that charging EVs during the day is actually cheaper, better for the grid, and healthier for the environment.</p><p>And so goes the litany of contradictory statements about the transition to EVs:</p><ul><li>EVs will/will not collapse the electric grid.</li><li>EVs will/will not cause massive unemployment among autoworkers.</li><li>EVs will/will not create more pollution than they eliminate.</li></ul><p>Confused? Join the crowd.</p><p>Sorting through this contradictory rhetoric can make anyone’s head spin. My response to each proclamation is often a shrug followed by “It depends.”</p><p>Two years ago, I began investigating the veracity of claims surrounding the transition to EVs at scale. The result is a 12-part series and e-book, <a href="https://spectrum.ieee.org/collections/the-ev-transition-explained/" target="_self"><em>The EV Transition Explained</em></a>, that explores the tightly woven technological, policy, and social issues involved. The articles are based on scores of interviews I conducted with managers and engineers in the auto and energy industries, as well as policy experts, academic researchers, market analysts, historians, and EV owners. I also reviewed hundreds of reports, case studies, and books surrounding EVs and electrical grids.</p><h3>GLOBAL ELECTRIC-VEHICLE SALES BY SCENARIO, 2020–2030</h3><br/><img alt="" class="rm-shortcode" data-rm-shortcode-id="2ab7f66575adf02429c444721dc5817d" data-rm-shortcode-name="rebelmouse-image" id="33bcc" loading="lazy" src="https://spectrum.ieee.org/media-library/image.png?id=33328308&width=980"/><p>The International Energy Agency’s Global EV Outlook 2021 shows 2020 electric-vehicle sales [left column], projected EV sales under current climate-mitigation policies [middle], and projected sales under accelerated climate-mitigation policies [right]. </p><h3></h3><br/><p>What I found is an intricately tangled web of technological innovation, complexity, and uncertainty, combined with equal amounts of policy optimism and dysfunction. These last two rest on rosy expectations that the public will quietly acquiesce to the considerable disruptions that will inevitably occur in the coming years and decades. The transition to EVs is going to be messier, more expensive, and take far longer than the policymakers who are pushing it believe.</p><h2>Scaling is hard</h2><p>Let me be very clear: Transitioning to electric vehicles and renewable energy to combat climate change are valid goals in themselves. Drastically reducing our fossil-fuel use is key to realizing those goals. However, attempting to make such transitions <em>at scale </em>in such a short period is fraught with problems, risks, and unanticipated consequences that need honest and open recognition so they can be actively and realistically addressed. Going to scale means not only manufacturing millions of EVs per year but supporting them from recharging to repair.</p><p>A massive effort will be needed to make this happen. For example, in January 2023 the sales of EVs in the United States <a href="https://www.anl.gov/esia/light-duty-electric-drive-vehicles-monthly-sales-updates" target="_blank">reached</a> 7.83 percent of new light-duty vehicle sales, with 66,416 battery-electric vehicles (BEVs) and 14,143 plug-in hybrid vehicles (PHEVs) sold. But consider that also in January, some 950,000 new ICE light-duty vehicles were sold, as well as <a href="https://www.autonews.com/used-cars/used-car-volume-hits-lowest-mark-nearly-decade#:~:text=The%20number%20of%20used%20cars,about%2035.8%20million%20were%20sold." rel="noopener noreferrer" target="_blank">approximately</a> another 3 million used ICE vehicles.</p><h3></h3><br/><img alt="Illustration of a person in glasses and a red hat." class="rm-shortcode" data-rm-shortcode-id="ecc039d268b6366cc96dd5c14260f11b" data-rm-shortcode-name="rebelmouse-image" id="427e0" loading="lazy" src="https://spectrum.ieee.org/media-library/illustration-of-a-person-in-glasses-and-a-red-hat.png?id=33327546&width=980"/><h3></h3><br/><p class="pull-quote">“[Government policymakers assume] that they can incentivize the supply and demand of EVs while paying relatively less attention to the capacity of global supply chains to produce them, along with the energy conversion complex needed to power them. Shifting the auto industry, an apex industry supporting a host of others to meet a new knowledge economy around EVs will be no easy task.”</p><h3></h3><br/><p>Transforming the energy and transportation sectors simultaneously will involve a huge number of known and unknown variables, which will subtly interact in complex, unpredictable ways. As EVs and renewable energy scale up, the problems and the solutions will cover ever-expanding populations and geographies. Each proposed solution will probably createnew difficulties. In addition, going to scale threatens people’s long-held beliefs, ways of life, and livelihoods, many of which will be altered, if not made obsolete. Technological change is hard, social change even harder.</p><p>And yet, the rush to transition to EVs is logical. Parts of the world are already experiencing climate-change-related catastrophes, and governments around the world have pledged to act under the <a href="https://unfccc.int/process-and-meetings/the-paris-agreement/the-paris-agreement" target="_blank">Paris Agreement</a> to limit global temperature rise to 1.5 °C above preindustrial levels. This agreement requires the reduction of greenhouse gases across all industrial sectors. Transportation is one of the <a href="https://www.iea.org/data-and-statistics/charts/global-energy-related-co2-emissions-by-sector" rel="noopener noreferrer" target="_blank">largest contributors</a> of GHG emissions worldwide, and many experts <a href="https://theicct.org/why-are-electric-vehicles-the-only-way-to-quickly-and-substantially-decarbonize-transport/" rel="noopener noreferrer" target="_blank">view</a> replacing ICE vehicles with EVs as being the quickest and easiest way to reach the target of net-zero carbon emissions by 2050.</p><p>However, shifting a 125-year-old auto industry that’s optimized for ICE-vehicle production to EVs using nascent technology is a monumental challenge in itself. Requiring that automakers do so in 15 years or less is even more daunting, although part of it is their own doing by not recognizing earlier thatEVs might be a threat to their business models. EVs <a href="https://spectrum.ieee.org/electric-cars" target="_self">require</a> automakers and their suppliers to<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658463682" target="_self"> reinvent</a> their supply chains, hire employees with <a href="https://spectrum.ieee.org/software-eating-car" target="_self">new software, battery, and mechatronic skill sets</a>, and retrain or else<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797703" target="_self"> lay off workers</a> whose outdated skills are no longer needed.</p><h3></h3><br/><img alt="An illustration of a man with sandy hair in a red shirt." class="rm-shortcode" data-rm-shortcode-id="53b1f41f68b446ccdc3bc56e323e4b5d" data-rm-shortcode-name="rebelmouse-image" id="c7e93" loading="lazy" src="https://spectrum.ieee.org/media-library/an-illustration-of-a-man-with-sandy-hair-in-a-red-shirt.png?id=33328652&width=980"/><h3></h3><br/><p class="pull-quote">“While I have often spoken of my concern about the electrical capacity demands required for a significant adoption of EVs, the series highlights the many different factors that must change for EVs to be successful. The urgent need to upgrade and increase the number of electric transformers the series discusses, for example, is rarely spoken about.”</p><h3></h3><br/><p>The articles in the series address different aspects of this transition, including <a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797703" target="_self">EV-related unemployment</a>,<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658463682" target="_self"> battery issues</a>, the<a href="https://spectrum.ieee.org/the-ev-transition-explained-2658463735" target="_self"> EV charging infrastructure</a>, and <a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797662" target="_self">affordability</a>. One not entirely surprising finding is that the traditional automakers are electrifying their offerings while also squeezing the last bit of profit from their gas guzzlers. That is, they are introducing less-expensive EV models, but their main thrust is still on <a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797662" target="_self">producing</a> profitable luxury EV models that are well beyond the means of the average household while also pushing <a href="https://www.reuters.com/breakingviews/america-isnt-quitting-gas-guzzlers-yet-2023-02-03/" target="_blank">sales</a> of profitable fossil-fuel-powered SUVs.</p><h2>EVs are not just a technology change</h2><p>Electric vehicles are more than just a new technology for combating climate change. In the United States, for instance, policymakers view EVs as the tip of the spear for a vast program of government-directed <a href="https://www.nytimes.com/2021/02/11/magazine/biden-economy.html" target="_blank">economic nationalism</a>—the economic, environmental, and societal change aimed at completely reshaping the nation’s US $26 trillion economy away from fossil fuels. They see normal market forces as inadequate to meet the imposed climate deadlines. Hence, with the Biden administration’s encouragement, ICE-vehicle sales will be banned in 2035 in California and several other states. In the series, I scrutinize several such <a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797652" target="_self">EV policies</a> and take a look at the <a href="https://spectrum.ieee.org/the-ev-transition-explained-2658797681" target="_self">roadblocks</a> that could derail them, such as inadequately sized pole transformers and the failure to issue permits for new electricity transmission lines.</p><h3>BATTERY-CELL PRODUCTION CAPACITY</h3><br/><img alt="An image of a chart that shows BATTERY-CELL PRODUCTION CAPACITY" class="rm-shortcode" data-rm-shortcode-id="368caefe07c654fb8b9c9f04a791fdf5" data-rm-shortcode-name="rebelmouse-image" id="2665d" loading="lazy" src="https://spectrum.ieee.org/media-library/an-image-of-a-chart-that-shows-battery-cell-production-capacity.png?id=33328488&width=980"/><p>This McKinsey & Co. battery tracker from June 2021 shows where batteries are being produced currently and at what volume and where their production is projected to be by the end of the decade. Figures from 2025 and 2030 are estimates based on announcements by battery-cell manufacturers.</p><h3></h3><br/><p>The United States is not alone in seeing EVs as an economic driver, of course. Worldwide, nearly 60 countries are now <a href="https://www.eetimes.com/many-countries-eye-2035-for-serious-ev-conversion/" target="_blank">imposing</a> similar ICE-vehicle sales bans. This has forced EVs into yet another role: as a cudgel to be wielded in the fierce geopolitical competition for economic advantage. For China, Japan, the United Kingdom, the European Union, and the United States, EVs are the vehicle needed to “<a href="https://www.whitehouse.gov/briefing-room/statements-releases/2021/05/18/fact-sheet-the-american-jobs-plan-supercharges-the-future-of-transportation-and-manufacturing/" target="_blank">win the future of transportation and manufacturing</a>.” Consider the reactions to the recent change in <a href="https://afdc.energy.gov/laws/electric-vehicles-for-tax-credit" target="_blank">U.S. EV subsidy policy</a>, which aims to boost domestic EV manufacturing and energy security. The decision deeply angered other countries and is <a href="https://www.reuters.com/business/sustainable-business/eu-lay-out-green-industry-plan-counter-us-china-subsidies-2023-02-01/" target="_blank">sparking moves</a> to counter it.</p><p>EVs alone aren’t sufficient to meet carbon-reduction targets, which means enormous <a href="https://spectrum.ieee.org/the-ev-transition-explained-2659316104" target="_self">lifestyle changes</a> for many of us, as we try to do our part to combat climate change. People will need to drive and fly less, walk and bike more, and take public transportation. We’ll need to switch to a more plant-based diet and convert household appliances powered by fossil fuels to electricity, to name only a few looming adjustments. People’s willingness to accept these changes and their ability to implement them will be crucial to our success at adapting to climate change and mitigating its impacts.</p><p>The introduction of any new system spawns perturbations that create surprises, both wanted and unwanted. We can safely assume that quickly moving to EVs at scale will unleash its fair share of <a href="https://www.vox.com/recode/2023/1/12/23550948/acceleration-cold-weather-tesla-ford-150-electric-vehicle-transition" target="_blank">unpleasant surprises</a>, as well as prove the adage of “haste makes waste.”</p><h2>Take a systems-engineering approach</h2><p>What struck me most in writing the series was that the EV transition is incredibly fluid. Major changes in transportation and energy policy, battery technology, and automakers’ strategies are announced nearly daily, highlighting the many uncertainties. Given the geopolitical nature of the transition, these uncertainties will only increase.</p><h3></h3><br/><img alt="An illustration of a man with a beard and glasses." class="rm-shortcode" data-rm-shortcode-id="65ae475143a90036e213ea27bf294935" data-rm-shortcode-name="rebelmouse-image" id="38320" loading="lazy" src="https://spectrum.ieee.org/media-library/an-illustration-of-a-man-with-a-beard-and-glasses.png?id=33328417&width=980"/><h3></h3><br/><p class="pull-quote"> “Currently, the cadence and sequencing of EV policies and engineering activities are out of whack. This creates a concatenation of costly challenges that turns everything you touch into a Pandora’s box.”</p><h3></h3><br/><p>
	These rapid changes also show the fragility of the transition. The desperate pleas from automakers for more government subsidies is not reassuring. Tesla’s recent <a href="https://www.reuters.com/business/autos-transportation/tesla-cuts-prices-electric-vehicles-us-market-2023-01-13/" target="_blank">price cuts</a>, for instance, have <a href="https://insideevs.com/news/632699/tesla-massive-price-cuts-shocks-auto-industry/" target="_blank">thrown</a> the auto industry into turmoil. Neither is a sign of a market that is sure of itself or its future.
</p><p>
	This fragility is also obvious when you examine the overly optimistic assumptions and the many caveats buried in EV and energy-policy recommendations. Many things need to go exactly right, and very little can go wrong for the EV transition to transpire as planned. At times like these, I’m reminded of Nobel Prize–winning physicist Richard Feynman’s admonishment: “For a successful technology, reality must take precedence over public relations, for Nature cannot be fooled.”
</p><p>
	There is a cacophony of foolishness being spouted by those advocating for the EV transition and by those denouncing it. It is time for the nonsense to stop, and some realistic political and systems thinking to begin. <span class="ieee-end-mark"></span>
</p><p>
<em>This article appears in the April 2023 print issue.</em>
</p><h3>The EV Transition Explained</h3><br/><img alt="Crop of photo of a bearded man in glasses, blue shirt and jacket." class="rm-shortcode" data-rm-shortcode-id="0c6225765bcbb9a031a72be8449c139b" data-rm-shortcode-name="rebelmouse-image" id="1e1cf" loading="lazy" src="https://spectrum.ieee.org/media-library/crop-of-photo-of-a-bearded-man-in-glasses-blue-shirt-and-jacket.jpg?id=33390216&width=980"/><p class="caption"><strong>« Previous</strong></p><h5><a href="https://spectrum.ieee.org/the-ev-transition-explained-2659623150" target="_blank">The Staggering Scale of the EV Transition</a></h5><h3></h3><br/><img alt="Rows of electric cars" class="rm-shortcode" data-rm-shortcode-id="d120cb569b4cbdf2ac24caefa45afa38" data-rm-shortcode-name="rebelmouse-image" id="db196" loading="lazy" src="https://spectrum.ieee.org/media-library/rows-of-electric-cars.jpg?id=32999346&width=980"/><h5></h5><p class="caption"><strong>Next »</strong></p><h5><a href="https://spectrum.ieee.org/the-ev-transition-explained" target="_self">The EV Transition Explained</a></h5>]]></description>
      <pubDate>Tue, 28 Mar 2023 15:00:07 +0000</pubDate>
      <guid>https://spectrum.ieee.org/the-ev-transition-explained-2659602311</guid>
      <category>Electric vehicles</category>
      <category>Policy</category>
      <category>Evs</category>
      <category>Batteries</category>
      <category>Bevs</category>
      <category>Chargers</category>
      <category>The grid</category>
      <dc:creator>Robert N. Charette</dc:creator>
      <media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/image-of-a-tangled-line-between-a-car-and-a-charger.png?id=33327369&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Elephant Robotics Unveils New Desktop Educational Robots for 2023</title>
      <link>https://spectrum.ieee.org/elephant-robotics-educational-robots-2023</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/digital-rendering-showing-three-robotic-arms-the-size-of-desk-lamps-next-to-each-other-with-a-blue-background-and-elephant-robot.jpg?id=33343312&width=1200&height=800&coordinates=125%2C0%2C125%2C0"/><br/><br/><div class="ieee-editors-note">
<p>
<em><del></del>This is a sponsored article brought to you by <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a>.</em>
</p>
</div>
<p>
	In recent years, interest in using robots in education has seen massive growth. Projects that involve robotics, artificial intelligence, speech recognition, and related technologies can help develop students’ analytical, creative, and practical skills. However, a major challenge has been the robots themselves: They are typically big, heavy, and costly. For robots to become widely used in education, they need to be smaller, easier to setup and use, and, more important, they need to be affordable to educators and students.
</p>
<p>
	That’s the goal 
	<a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> aims to achieve with its line of lightweight, smart, and capable robots. The company has launched several desktop collaborative robots over the past few years, including the <a href="https://www.elephantrobotics.com/en/mycobot-280-pi-2023-en/" rel="noopener noreferrer" target="_blank">myCobot</a>, <a href="https://www.elephantrobotics.com/en/mecharm-cn/" rel="noopener noreferrer" target="_blank">mechArm</a>, and <a href="https://www.elephantrobotics.com/en/mypalletizer-260-pi-en/" rel="noopener noreferrer" target="_blank">myPalletizer</a>. To help users achieve more applications in education, <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> has also launched <a href="https://www.elephantrobotics.com/en/artificial-intelligence-kit-2023-en/" rel="noopener noreferrer" target="_blank">AI Robot Kit</a>, a robotic kit that integrates multiple functions like vision, positioning grabbing, and automatic sorting modules. This year, the company is unveiling completely improved and upgraded products to make robotics even more accessible in education.
</p>
<h3>Upgraded Robotic Arms and AI Kits</h3><br/><p>Schools in different countries and regions have been using <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics’</a> robotic arms and AI Kits as educational tools in recent years. The products’ portability, ease of use, and cost-effectiveness have helped schools integrate robotics as part of their programs and courses. The performance of the products and the wide range of built-in software and features help students learn better about robotics and programming. Using the robotic arms and <a href="https://shop.elephantrobotics.com/collections/mycobot/products/artificial-intelligence-kit-2023" rel="noopener noreferrer" target="_blank">AI Kit</a>, students can learn about artificial intelligence and applications such as robot vision, object recognition, manipulation, and more.</p><h3></h3><br/><p>To help more students experience robots and start learning about robotics and programming at a young age, <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> has upgraded its AI Kit to make it more powerful and even easier to use.</p><h3></h3><br/><span class="rm-shortcode" data-rm-shortcode-id="2ba56b5182208e3929a561fcdf6263cb" style="display:block;position:relative;padding-top:56.25%;"><iframe frameborder="0" height="auto" lazy-loadable="true" scrolling="no" src="https://www.youtube.com/embed/wNV0C4JG2Sk?rel=0" style="position:absolute;top:0;left:0;width:100%;height:100%;" width="100%"></iframe></span><p class="caption">Elephant Robotics has upgraded the AI Kit comprehensively, improving the quality of the hardware while optimizing the built-in algorithms and software to make the product more flexible and scalable. Video: Elephant Robotics</p><h3></h3><br/><p>In addition to the upgraded <a href="https://www.elephantrobotics.com/en/artificial-intelligence-kit-2023-en/" rel="noopener noreferrer" target="_blank">AI Kit</a>, <a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> this year released new robotics and AI education material, starting with a book called “Machine Vision and Robotics.” The book explains the topics of robotic arms and vision sensors, and includes five algorithm courses, plus tutorials on programming languages. Users learn about machine vision in a practical way by experimenting with the <a href="https://shop.elephantrobotics.com/collections/mycobot/products/artificial-intelligence-kit-2023" rel="noopener noreferrer" target="_blank">AI Kit</a> through a series of examples and applications. </p><p><a href="https://www.elephantrobotics.com/en/" rel="noopener noreferrer" target="_blank">Elephant Robotics </a>also provides users visualization software and customization options to select the built-in algorithms. The software is very friendly for new robotics users who have yet to gain programming experience.<a href="https://www.elephantrobotics.com/en/artificial-intelligence-kit-2023-en/" rel="noopener noreferrer" target="_blank"> AI Kit 2023</a> uses a camera with higher accuracy and light adjustment for the hardware to make the robotic arm more efficient in object recognition. The suction pump installed at the end of the robotic arm has also been improved, which has higher adaptability and stability when working with different robotic arms.</p><p><a href="https://shop.elephantrobotics.com/" target="_blank">Elephant Robotics</a> has also upgraded the <a href="https://shop.elephantrobotics.com/collections/mycobot" target="_blank">myCobot</a> product into a new version: <a href="https://www.elephantrobotics.com/en/mycobot-280-pi-2023-en/" target="_blank">myCobot 2023</a>, which features more user-friendly software. <a href="https://www.elephantrobotics.com/en/mycobot-280-pi-2023-en/" target="_blank">myCobot 2023</a> allows users to use their mobile phone and gamepad to control the robotic arm and accessories, helping to learn and develop remote wireless operations. To improve safety, <a href="https://shop.elephantrobotics.com/" target="_blank">Elephant Robotics</a> has added algorithms to the robotic arm to prevent it from colliding with other objects while in operation.</p><h3>ultraArm P340: A New Robot for Education</h3><br/><img alt="A small silver metal robotic arm the size of a desk lamp stands on a wooden desk next to small red and green wooden blocks, a computer keyboard, and a monitor." class="rm-shortcode" data-rm-shortcode-id="0a97c3184512518a7961a22aee681445" data-rm-shortcode-name="rebelmouse-image" id="8d4e5" loading="lazy" src="https://spectrum.ieee.org/media-library/a-small-silver-metal-robotic-arm-the-size-of-a-desk-lamp-stands-on-a-wooden-desk-next-to-small-red-and-green-wooden-blocks-a-co.jpg?id=33343290&width=980"/><h3></h3><br/><p>In 2023, <a href="https://shop.elephantrobotics.com/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> is also introducing the <a href="https://www.elephantrobotics.com/en/ultraarm-p340-en/" rel="noopener noreferrer" target="_blank">ultraArm P340,</a> a high-performance robotic arm designed to meet educational needs. <a href="https://shop.elephantrobotics.com/collections/ultraarm" rel="noopener noreferrer" target="_blank">ultraArm </a>uses metal construction to increase the stiffness and its payload capabilities. To provide further stability, the <a href="https://www.elephantrobotics.com/en/ultraarm-p340-en/" rel="noopener noreferrer" target="_blank">ultraArm P340</a> also features stepper motors, which improves its speed and provides repeatable positioning accuracy to ±0.1mm.</p><h3>More AI and Robot Kits for Makers</h3><br/><p><a href="https://shop.elephantrobotics.com/" rel="noopener noreferrer" target="_blank">Elephant Robotics</a> has still more products to announce this year: The company has launched five kits with <a href="https://shop.elephantrobotics.com/collections/ultraarm" rel="noopener noreferrer" target="_blank">ultraArm</a> that provide additional applications for the education field. The Vision educational kits include the Vision & Picking Kit, Vision & Conveyor Belt Kit, and Vision & Slide Rail Kit. These kits help users learn about machine vision and experiment with industrial-like applications such as dynamic and static intelligent recognition and grasping.</p><p>There are two kits in the DIY series: the Drawing Kit and Laser Engraving Kit. This series helps makers achieve high quality reproduction of drawings and laser engraving applications, developing users’ creativity and imagination. To help users quickly achieve DIY production, <a href="https://shop.elephantrobotics.com/" rel="noopener noreferrer" target="_blank">Elephant Robotics </a>created software called Elephant Luban. It is a platform that generates the G-Code track and provides primary cases for users. Users can select multiple functions, such as drawing and laser engraving, with just a few clicks.</p><p>Combined with different robotics kits, the <a href="https://shop.elephantrobotics.com/collections/ultraarm" rel="noopener noreferrer" target="_blank">ultraArm</a> is an excellent and affordable option for many educational applications. These kits offer students the possibility of learning and experimenting with advanced, complex robotics and AI tools that are fun and easy to use.</p><p>And this has long been one of <a href="https://www.elephantrobotics.com/en/" target="_blank">Elephant Robotics</a>‘s main goals: creating innovative products that are specifically designed to provide students with hands-on experience and promote STEM education. The company is committed to keep pursuing that goal by continuing to research and contribute to the development of even better educational robots in the future.</p>]]></description>
      <pubDate>Tue, 28 Mar 2023 13:13:52 +0000</pubDate>
      <guid>https://spectrum.ieee.org/elephant-robotics-educational-robots-2023</guid>
      <category>Stem education</category>
      <category>Artificial intelligence</category>
      <category>Cobots</category>
      <category>Collaborative robots</category>
      <category>Educational robots</category>
      <category>Elephant robotics</category>
      <category>Robot kits</category>
      <dc:creator>Elephant Robotics</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/digital-rendering-showing-three-robotic-arms-the-size-of-desk-lamps-next-to-each-other-with-a-blue-background-and-elephant-robot.jpg?id=33343312&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Neurotech’s Battles Impact Our Brains’ Future</title>
      <link>https://spectrum.ieee.org/neurotechnology-battle-for-your-brain</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/colorful-outline-of-brain-on-black-background.jpg?id=33358750&width=1200&height=800&coordinates=0%2C104%2C0%2C105"/><br/><br/><p>Neurotechnologies today—devices that can measure and influence our brains and nervous systems—are growing in power and popularity. The neurotech marketplace, <a href="https://www.globenewswire.com/news-release/2022/12/22/2578707/0/en/neurotechnology-market-size-to-worth-around-usd-38-17-bn-by-2032.html" target="_blank">according to Precedence Research</a>, is worth US $14.3 billion this year and will exceed $20 billion within four years. Noninvasive <a href="https://spectrum.ieee.org/tag/bci" target="_self">brain-computer interfaces</a>, <a href="https://spectrum.ieee.org/tag/brain-stimulation" target="_self">brain-stimulation devices</a>, and <a href="https://spectrum.ieee.org/brain-implant" target="_self">brain-monitoring hardware</a> (measuring alertness and attention at work, for example) are no longer just laboratory experiments and technological curios. The societal and legal implications of widespread neurotech adoption may be substantial.<br/></p><p>Nita Farahany, professor of law and philosophy at Duke University, has written a new book, <a href="https://us.macmillan.com/books/9781250272966/thebattleforyourbrain" target="_blank"><em>The Battle for Your Brain: Defending the Right to Think Freely in the Age of Neurotechnology</em></a> (Macmillan), which explores how our lives may be impacted by the use of brain-computer interfaces and neural monitoring devices.</p><p>Farahany argues that the development and use of neurotech presents a challenge to our current understanding of human rights. Devices designed to measure, record, and influence our mental processes—used by us or on us—may infringe on our rights to mental privacy, freedom of thought, and mental self-determination. She calls this collection of freedoms the right to cognitive liberty. <em>IEEE </em><em>Spectrum</em> spoke with Farahany recently about the future and present of neurotech and how to weigh its promises—enhanced capabilities, for instance, including <a href="https://spectrum.ieee.org/tag/bionics" target="_self">bionics and prosthetics</a> and even a <a href="https://spectrum.ieee.org/human-augmentation" target="_self">third arm</a>—against its potential to interfere with people’s mental sovereignty.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25" data-rm-resized-container="25%">
<img alt="portrait of a smiling woman on a white background" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="0e21e3bdc7c1a3ac60455db63a15b5a0" data-rm-shortcode-name="rebelmouse-image" id="0cf52" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-smiling-woman-on-a-white-background.jpg?id=33358768&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">Author, Nita Farahany</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Merritt Chesson</small></p><p> <strong><em>IEEE Spectrum: </em>Your book <em>The Battle for Your Brain </em>defines cognitive liberty as the rights to mental privacy, freedom of thought, and self-determination. Please tell us more about that.</strong> </p><p><strong>Nita Farahany: </strong>The umbrella right, the right to cognitive liberty, is the right to self-determination over our brains and mental experiences. The ways I see that right intersecting with our existing human rights are those three that you listed. The right to mental privacy, which covers all of our mental and effective functions; the right to freedom of thought, which I think relates to complex thoughts and visual imagery, like the things we think of as “thinking;” and self-determination which is really the positive side of cognitive liberty. Mental privacy and freedom of thought are the rights from interference with our brains and mental experiences, while self-determination is the right to access information about our own brains, the right to make changes and to be able to define for ourselves what we want our brains and mental experiences to be like.</p><p><strong>Much of your book is forward-looking, considering what current brain-computer interface technologies are capable of today and how people, businesses, and governments are using them. What current BCI capabilities, in your opinion, run counter to the rights of cognitive liberty?</strong></p><p><strong>Farahany</strong>: I think there are two ways to think about it: There’s what a technology can actually do, and there’s the technology’s chilling effect no matter what it can actually do. If you are some authoritarian regime and you are requiring people to wear brain sensors, even if the technology did nothing, using that at scale on people has a deeply and profoundly chilling effect.</p><p>But it does do something, and the something that it does is enough to also cause real harm and danger by digging into the mental privacy of individuals, particularly when it’s used to probe information, and not just brain states. I think it’s dangerous enough when you’re trying to track attention, engagement, or boredom, or disgust, or simple emotional reactions. It’s even more dangerous when what you’re trying to do is use evoked potentials to understand biases and preferences.</p><p><strong>What are some ways that people are currently using evoked brain potentials? (a.k.a. <a href="https://en.wikipedia.org/wiki/Event-related_potential" rel="noopener noreferrer" target="_blank">event-related potentials</a> or ERPs) What are the possible issues with those applications?</strong></p><p><strong>Farahany: </strong>This technique is being used pretty <a href="https://www.frontiersin.org/articles/10.3389/fnint.2019.00019/full" target="_blank">widely in neuromarketing already</a>, and has been for a while. For them it’s another marketing technique. People’s self-reported preferences have long been understood to be inaccurate and don’t reflect their buying behaviors. Using ERPs to try to decode emotional brain states of interest or attention when products are shown—this video elicited a weak response, while another elicited a stronger response, for example.</p><p>ERP techniques have also been used to try to infer people’s affinity with particular political viewpoints. When recording ERP signals from a person while presenting them with a series of statements and images about societal issues or political parties, researchers have tried to see positive or negative responses and then predict what a person’s political preferences or persuasions or likelihood of voting for a particular party or candidate is based on that information. That’s one of the potential uses and misuses, particularly when that’s done without consent, awareness, or transparency, or when used for the commodification of that brain data.</p><p>The same kind of signals are used in the criminal justice system through so-called brain fingerprinting technology. Scientifically, we should worry about the analytical validity of that quite a bit, but on top of concerns about validity we should also be deeply concerned about using interrogation techniques on a criminal defendant’s brain, as if that is a normalized or legitimate function of government, as if that is a permissible intrusion into their privacy. We should worry about whether people get it right, the pseudoscience of it, and then we should worry about the very fact that it is a technology that governments think is fine to use on human minds.</p><p><strong>Your book describes different companies developing “lie detector” devices based on <a href="https://en.wikipedia.org/wiki/Functional_magnetic_resonance_imaging" rel="noopener noreferrer" target="_blank">functional magnetic resonance imagining</a> (fMRI) signals. That sounds a lot like a shinier version of a polygraph, which is pretty widely understood to be inaccurate.</strong></p><p><strong>Farahany: </strong>And yet they drive a lot of confessions! It drives a lot of fear. Polygraphs already have a chilling effect on people. They already lead to false confessions and increased anxiety, but much less so, I think, than putting sensors on a person’s head and saying “it doesn’t matter what you say, because your brain is going to reveal the truth anyway.” That’s the future that has already arrived in countries already using this technology.</p><p><strong>You discuss companies like SmartCap, which makes <a href="https://www.smartcaptech.com/" target="_blank">an electroencephalogram (EEG) wakefulness monitor</a> and markets it to shipping companies as a means of avoiding accidents caused by sleep deprivation. At the corporate level, how else might employers or employees use neurotechnology?</strong></p><p><strong>Farahany: </strong>Fatigue management has become something used at a relatively wide scale across a number of companies internationally. When I presented this material at the World Economic Forum at Davos [Switzerland], I had a company that came up to me after my talk to say “we’re already using this technology. We plan on rolling it out at scale as one of the products we’re using.” I think in some ways, for things like fatigue monitoring and management, that’s not a bad use of it. If it improves safety, and the only data that’s used and extracted is quite limited, then I don’t find that to be a particularly troubling application. I worry when instead it’s used for productivity scoring or attention management or it’s integrated into wellness programs where the data being collected is not being disclosed to employees or being used to track people over time. We already talked about industries like neuromarketing, but other industries are already integrating this technology to gather brain heuristics into their workplaces at scale, and those uses are growing.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" data-rm-shortcode-id="fbf19aadf1f3d55765e7f0b573a7addd" data-rm-shortcode-name="rebelmouse-image" id="02726" loading="lazy" src="https://spectrum.ieee.org/media-library/image.jpg?id=33358784&width=980"/>
</p><p><strong>Do you think an interest in preserving cognitive liberty conflicts with the better interests of society?</strong></p><p><strong>Farahany: </strong>There are some aspects of cognitive liberty which are based on absolute rights, like freedom of thought, which protects a narrow category of our cognitive and effective functioning. And there are some aspects of cognitive liberty like mental privacy which is a relative right, where societal interests can in some instances be strong enough to justify intervention by the state and to limit the amount of liberty that a person can exercise.</p><p>It’s not that I think they’re in conflict, I think that it’s important to understand that individual liberties are always balanced against societal needs and interests. What I’m trying to do in the book is to show that cognitive liberty…isn’t always going to trump every interest that society has. There are going to be some instances in which we have to really find the right balance between the individual and the society at large.</p><p><strong>Are current national and international rights frameworks and laws sufficient to protect cognitive liberty?</strong></p><p><strong></strong><strong>Farahany: </strong>I believe that the existing set of rights—privacy, freedom of thought, and the collective right to self-determination—can be updated and expanded and interpreted. Human rights law is meant to evolve over time.…</p><p>A right, at the end of the day, has power on its own, but it’s really only as good as the enforcement of that right. What’s necessary is to enforce that right by looking at it in context-specific ways—in employment, in government use, in biometric use—and to understand what rules and regulations should be, how cognitive liberty translates into concrete rules and regulations worldwide.</p>]]></description>
      <pubDate>Tue, 28 Mar 2023 13:00:06 +0000</pubDate>
      <guid>https://spectrum.ieee.org/neurotechnology-battle-for-your-brain</guid>
      <category>Bci</category>
      <category>Fmri</category>
      <category>Neurotechnology</category>
      <dc:creator>Michael Nolan</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/colorful-outline-of-brain-on-black-background.jpg?id=33358750&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Rerouting Intention And Sensation In Paralyzed Patients</title>
      <link>https://spectrum.ieee.org/neural-bypass-for-paralysis</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.webp?id=33293244&width=980"/><br/><br/><iframe frameborder="no" height="180" scrolling="no" seamless="" src="https://share.transistor.fm/e/90e9de12" width="100%"></iframe><h3>Transcript</h3><p><strong>Eliza Strickland:</strong> Paralysis used to be thought of as a permanent condition, but over the past two decades, engineers have begun to find workarounds. They’re building on a new understanding of the electric code used by the nervous system. I’m Eliza Strickland, a guest host for <em>IEEE Spectrum’s</em> Fixing the Future podcast. Today I’m talking with <a href="https://www.linkedin.com/in/chad-bouton-ba0825a/" target="_blank">Chad Bouton</a>, who’s at the forefront of this electrifying field of research. Chad, welcome to the program, and can you please introduce yourself to our listeners?</p><p><strong>Chad Bouton: </strong>Yes, thanks so much, Eliza, for having me. And my name is Chad. I’m at the <a href="https://feinstein.northwell.edu/" target="_blank">Northwell Health Feinstein Institute for Medical research</a>.</p><p><strong>Strickland:</strong> And can you tell me a bit about the patient population that you’re working with? I believe these are people who had become paralyzed, and maybe you can tell us how that happened and the extent of their paralysis.</p><p><strong>Bouton:</strong> Absolutely. Absolutely. In fact, we work with folks that have been paralyzed either from a traumatic injury, stroke, or even a brain injury. And there’s over 100 million people worldwide that are living with paralysis. And so it’s a very devastating and important condition, and we are working to restore not only movement, but we’re making efforts to restore sensation as well, which is often not the focus and certainly should be.</p><p><strong>Strickland:</strong> So these are people who typically don’t have much movement below the head, below the neck?</p><p><strong>Bouton: </strong>So we have focused on tetraplegia or quadriplegia because, obviously, it’s extremely important and it is very difficult to achieve independence in our daily lives if you don’t have the use of your hands in addition to not being able to move around and walk. And it surprisingly accounts for about half of the cases of spinal cord injury, even slightly more than half. And it used to be thought of as something that was a more rare condition, but with car accidents and diving accidents, it’s a prominent and critical condition that we need to really address. And there’s no cure currently for paralysis. No easy solution. No simple fix at this point.</p><p><strong>Strickland: </strong>And from your experiences working with these people, what kind of capabilities would they like to get back if possible?</p><p><strong>Bouton: </strong>Well, individuals with paralysis would like to really regain independence. I’ve had patients and study participants comment on that and really ask for advances in technology that would give them that independence. I’ll speak to some of the things we’re doing in the lab, but folks often ask, “Could we take this home or take it outside the lab?” And we’re certainly working to do that as well. But the goal is to be more independent, ask for help less, be able to achieve functional abilities to do even things that we might consider just basic necessities, feeding, grooming, and even some of the personal aspects, being able to hold someone’s hand and to feel that person’s hand or a loved one’s hand. Those are the things that we’re really targeting and working hard to address.</p><p><strong>Strickland:</strong> Yeah, I thought it’s really interesting that your group is focused on hands. There are other groups that are working on letting people walk again, but the hands feel like a very obviously important target too.</p><p><strong>Bouton: </strong>Yeah, absolutely. And in fact, there’s been studies and widespread surveys on this topic, and folks that are living with tetraplegia or quadriplegia prioritize or say their top desire is to move their hands again. And if you step back and think about it for a second, it makes sense because we rely on our hands so much. And even losing one hand, say from a stroke, can be devastating and very disruptive to our lives.</p><p><strong>Strickland: </strong>Yeah, let’s go over the basics of electrophysiology for listeners who don’t have a background in that area. I love this field. It has such a long history that goes back to the 1780s when <a href="https://www.unibo.it/en/university/who-we-are/our-history/famous-people-and-students/luigi-galvani" target="_blank">Luigi Galvani </a>touched an exposed nerve of a dead frog with a scalpel that had an electric charge and saw the frog’s leg kick.</p><p><strong>Bouton:</strong> Yes.</p><p><strong>Strickland:</strong> Can you explain how the nervous system uses electricity?</p><p><strong>Bouton:</strong> Yes, absolutely. So it’s an electrochemical phenomenon. And of course, it involves neurotransmitters as well. When a neuron fires, as we say, that’s an electrical impulse. It only lasts a very brief moment, less than a thousandths of a second. But basically, there’s a polarization of the neuron itself and charges that are passing through ion channels. So what does this mean? Well, it’s kind of like in a computer where you have zeros and ones. For a brief moment, that cell has changed from, let’s say, a zero to a one, and it is firing or having this impulse that represents that binary one. And what’s so neat about it is that the firing rate, so basically how often those impulses are happening or how fast they’re happening, carries information. And then, of course, which neurons or nerve fibers carry the information or which ones are firing is what we call spatial encoding. So you have temporal encoding and spatial encoding. Those together can carry a tremendous amount of information or can mean different things, whether it’s a motor event where there’s a need to activate certain muscles in the hand or the fingers or the legs and any muscle throughout the body. And we also have sensory information that gets encoded by the same approach. And so information can pass from the brain to the body and from the body back to the brain, and we have these two-way information highways all throughout our central and peripheral nervous system. I call it often the most complex control system in nature, and we’re still trying to understand it.</p><p><strong>Strickland:</strong> Yeah, so for a person with tetraplegia, these electrical messages from the brain are essentially not getting through. The highway is blocked, right?</p><p><strong>Bouton: </strong>That’s right. Absolutely. And so let’s walk through that scenario. So now, someone who’s had a car accident or a diving accident, often the highest level of stress occurs at the base of the neck, and we call that C5, so it’s the cervical, a fifth vertebra there. Often that cord gets damaged because the vertebra itself, which normally would protect that cord, unfortunately, it gets fractured and can then slip or slide and can actually crush or damage the cord itself. So then what is often misunderstood is that you don’t get a simple complete shutdown. You get damage and certain levels of damage or amounts of damage. And what can happen is someone can become paralyzed but lose sensation as well along with motor capability. It’s not going to be the same for everyone. There’s different levels of it. But usually, there’s damage, and signals are able to get through but often very attenuated, very weak. And so I’ll talk through some of the approaches we’re taking now to boost, if you will, those signals and try to enhance those signals. The good news is that we’re finding more and more that those signals are there and can be boosted or enhanced, which is very, very exciting because it’s opening new doors to new therapies that we’re developing.</p><p><strong>Strickland:</strong> Yeah, I love that you call your system the neural bypass, which is very evocative. You can imagine picking up the signals in the brain, getting around the blockage, and sending the information onto the muscles. So maybe we can talk about the first part of that first. How do you get the information from the brain?</p><p><strong>Bouton: </strong>Well, yes, the neural bypass, so it’s funny because that phrase was used very briefly back in the ‘70s. And then it kind of went away and I think really because it wasn’t possible with technology at that time. But then in the early 2000s, we started to really explore this concept and use that phrase again and say, if we can put a microelectrode array in the brain, which we did back around 2005, 2006, and a number of colleagues and various team members kind of looked at that and said, yes, we can record from the brain. We can even stimulate the brain. But we said, why couldn’t we take that information, reroute it, as you say, around an injury or even a damaged part of the nervous system or the brain itself and create this neural bypass, and then reinsert the signals or link those signals directly to muscle stimulation? And that was what we called the one-way bypass, neural bypass. And why couldn’t we do that and restore movement? And so we attempted to do that and were thankfully successful in 2014. In fact, we had enrolled a young man named <a href="https://ianburkhart.com/" target="_blank">Ian Burkhart</a>. His name, of course, became public, and he was the first paralyzed individual to regain movement using a brain implant that formed this neural bypass, this one-way or unidirectional neural bypass. And it was very, very exciting, and he was able to do some pretty amazing things with this approach. And in fact, I still remember when he first drank from a glass on his own. He reached out, opened his fingers using the bypass, which he hadn’t been able to do for four years since his accident, and he was able to open his hand by himself without help, pick up a glass, bring it to his lips, and be able to just take a drink. It was really quite a moment, and the entire team and myself were very moved and we thought we’re really taking an important step forward here.</p><p><strong>Strickland:</strong> Ian Burkhart also played <em><a href="https://en.wikipedia.org/wiki/Guitar_Hero" target="_blank">Guitar Hero</a></em> if I remember right. Is that correct?</p><p><strong>Bouton:</strong> Yeah, so another very, very exciting moment was when we explored the idea of rhythmic movements in the hand. So I’ll do a little experiment here. We’ll do it even though this is a podcast, but we can all do this experiment. If you hold up one hand-- and you should try this, Eliza. Okay, so hold up, say, either left or right. Now take your other hand and drum your fingers against the palm of your hand and go very, very fast. Okay, now stop, and now try to reverse directions. Okay. And is it awkward and harder? Okay, so now pay attention to which way was the fastest, what we would call, quote, “natural” way for you. Was it pinky to index or index to pinky?</p><p><strong>Strickland: </strong>Pinky to index was really easy for me. The other way was almost impossible.</p><p><strong>Bouton:</strong> Okay, well, you’re what we call the normal group. So the 85 percent of population does the faster, more natural direction from pinky to index. Only 15 percent of the population goes from index to pinky. And the question is, why in the world is there a wiring, if you will, or a natural direction? And we looked at rhythmic movements. As we looked at the electrode array and the signals we were recording, we could see there was a group or an ensemble of neurons that were firing when we are thinking about rhythmic movements, say just wiggling a finger. And then the other, there’s a totally different group when you actually try to do a static movement of that finger. You’re trying to press it and hold that finger in a certain position. So we thought, let’s see if we can decipher these different groups. And then we linked those signals back to neuromuscular stimulators that we had developed, and we then asked the question, could Ian or others move the fingers in a more dynamic way? And we published another paper on this, but he was able to dynamically move his fingers and then also statically move those, and he could then play <em>Guitar Hero</em> just by thinking about different static or sustained movements and holding a note, let’s say, in the guitar or dynamically doing riffs. And we <a href="https://www.youtube.com/watch?v=60fAjaRfwnU" target="_blank">have videos</a> and whatnot online. But it was really amazing to deepen our understanding but also to allow, again, a little more independence, allow someone to do something fun, a little bit more recreational too.</p><p><strong>Strickland:</strong> Sure, sure. So Ian was using implanted electrodes to get his brain signals. Can you walk us through the different approaches in plants versus wearables?</p><p><strong>Bouton:</strong> Yes, actually, there are a number of ways of tapping into the nervous system and specifically into the brain. And a more recent approach we’ve been taking is to use a minimally invasive procedure to place a very thin electrode. It’s called a stereo electroencephalogram-type electrode, an SEEG. And these are used routinely at our location and a number of locations around the world for mapping the brain in epilepsy patients. But now we ask the question, well, could we use these electrodes to record and stimulate in the motor and sensory area? And we just recently this past year did both, and our findings were quite striking. We were able to not only decode individual finger movements with this different type of electrode and approach, but we were also able to stimulate in primary sensory cortex actually down in the central sulcus. That’s right between your motor and sensory area. And on the wall of the sulcus on the sensory side, we were able to stimulate and elicit highly focal percepts at the fingertips. And this has been a challenge with different electrodes, like the kind of electrodes that I was previously talking about, which were placed on the surface of the brain, not down into the sulcus. So this has allowed us to answer new questions and is also opening up a door to a minimally invasive approach that could be extremely effective in trying to restore even finer movements of the human hand and also sensations. You have to know that you can’t button your shirt without tactile feedback, and getting that feedback at the tips of the fingers is so important for fine motor tasks and dexterous hand movement, which is one of the goals of our lab and center.</p><p><strong>Strickland: </strong>Yeah, I wanted to ask about this idea of the two-way bypass. So in this idea, you have sensors on your fingers or on your hand, and those are sending information to electrodes that are conveying it to the brain?</p><p><strong>Bouton:</strong> That’s absolutely right. With the fingertips and the thin membrane sensors that we’ve developed, we can pick up not only the pressure level that the fingertips but also even directional information. So in other words, when we pick up, say, a cup, I have one here on my desk, and I’m picking this cup up. There’s a downward, what we call shear force that’s pushing the skin down towards the floor. And this is additional information the brain receives so that we know, oh, we’re picking something up that has some weight to it. And you don’t even realize you’re doing this, but there’s a circuit, a relatively complex circuit that involves interneurons in the spinal cord that tightens that grip naturally. You don’t, again, realize you’re doing it. Just a little subtle increase in your grasp. And so when we want to create a bidirectional or a two-way neural bypass, we have to use that information from the sensors, we have to route that back into our computer, we have to decode or decipher that information. That part is straightforward from the sensors, but then how do you encode that information so the brain will interpret that as, oh, I feel not only some kind of sensation at my fingertips, but what’s the level of that sensation?</p><p>And we just, again, last year, were able to show that we can encode the different levels of pressure or force felt, and the participants have reported very accurately what those levels are. And then once the computer understands and interprets that and then starts to send signals back to another set of what we call microstimulators that stimulates the brain, again, with the right firing rate or frequency, then the challenge still remains to make that feel natural. Right now, people still report it’s a bit of a slightly artificial sensation sometimes, or they feel like, I feel this pressure in different levels, but it’s a little bit electrical or even mechanical like a vibration. But it is still extremely useful, and we’re still refining that. But now what you’ve done is you’ve started to close the loop, right? Not only can signals from the brain be interpreted and sent to stimulation devices for muscle activation, we can also pick up the sensation, the tactile sensation, send it back into the brain, and now we have a fully closed loop or a bidirectional bypass.</p><p><strong>Strickland: </strong>So when you’re sending commands to muscles to have the hand do some movement, how much do we understand the neural code that makes one finger move versus another one?</p><p><strong>Bouton: </strong>Yeah, that’s a great question. So we surprisingly understand a fair amount on that after many years and many groups looking at this. We now understand that we can change the firing rate, and we can change how fast we’re stimulating or how fast we need to stimulate that muscle to get a certain contraction level. Recording this signal, understanding the signal from the motor cortex in the brain and how that translates to a different level of contraction, we also understand much better now. Even understanding if it should be a static movement or a dynamic movement, I spoke a little bit to that. I think what’s hard, that we’re still trying to understand, is synergistic movements, when you want to activate multiple fingers together and do a pinch grasp or you want to do something more intricate. There have been studies where people have tried to understand the signal when someone flips a quarter between the fingers, you’ve seen this trick, or a drum stick when you’re spinning it around and manipulating it and transferring it from one pair of fingers to another. Those super complex movements involve motor and sensory networks working together very, very, very closely. And so even if you’re, say, listening in or eavesdropping in on the motor cortex, you only really have half the picture. You only have half the story.</p><p>And so one of the things we’re going to be looking at, and we now have FDA clearance to do this, is to record in both motor and sensory and then to be able to stimulate in the sensory area of the brain. But by recording in both motor and sensory, we can start to look more deeply into this question of, well, how are those networks communicating with each other? How do we further decode or decipher that information? I have someone in my lab, <a href="https://www.linkedin.com/in/sadegh-ebrahimi-38207157/" target="_blank">Dr. Sadegh Ebrahimi</a>, who did his graduate work at Stanford and his postdoc work there, he looked at the question of how do different areas of the brain communicate and pass these massive amounts of information back and forth, and how are they connected, and how does this information flow? He is going to be looking at that question along with, can we use reinforcement learning techniques to further refine our decoding and more importantly our encoding and how we stimulate and how we even stimulate the muscles and get all of these networks working together?</p><p><strong>Strickland: </strong>And for the electrodes that are controlling movement, are those a wearable system that people can just have on their arm?</p><p><strong>Bouton:</strong> Yes, we’re very excited to announce that we’re now developing wearable versions of the neuromuscular stimulation technology, and our hopes are to make this available outside the lab in the next year or two. What we have done is we’ve developed very thin, flexible electrode arrays that have been ruggedized and encapsulated in a silicone material. And there are literally over 200 electrodes now that we have in these patches, and they’re able to precisely stimulate different muscles. But what’s so fascinating is that by using the right electrical waveforms, and we have been optimizing these for a number of years, but in the right electrode array design, turns out we can isolate individual finger movements very accurately. We can even get the pinky to move in very unique ways and the thumb in multiple directions. And with this approach and it also being wireless, people can, with this being lightweight and thin, they can actually wear it under their clothes and folks can use it out and about, outside the lab, in their homes. And so we’re really looking forward to accelerating this.</p><p>And you can link this wearable technology either to a brain-computer interface, which is what we’ve been talking a lot about, or there’s even a stand-alone mode where it uses the inertial sensing of what we call body language or basically body movements. These would be the residual movements that individuals are able to do even after their injury. It might be shoulder movement or lifting their arm. Often, in a C5-level injury, the biceps are spared, thankfully, and one can lift their arm and lift their shoulders. So folks can reach, but they can’t open and use their hand. But with this technology, we infer what they want to do. If they’re reaching for a cup of water, we can infer, ah, they’re reaching with a certain trajectory, and we use our machine learning or AI algorithms to detect, even before the hand gets to the target, we know, ah, they’re trying to reach and do what we call a power grasp or a cylindrical grasp. And we start to stimulate the muscles to help them finish that movement that they can’t otherwise do on their own. And this will not allow, say, playing <em>Guitar Hero</em>, but it is allowing folks to do very basic types of actions like picking up a cup or feeding themselves. We have a video of someone picking up a granola bar and a participant that fed himself for the first time. And that was also really an incredible moment because really achieving that independence is what we’re trying to do at the end of the day.</p><p><strong>Strickland:</strong> Yeah, let’s talk a little bit about commercialization. I imagine it’s a very different story when you’re talking about brain implants versus noninvasive devices. So where are you in that pathway?</p><p><strong>Bouton:</strong> Yeah, so you’re absolutely right. There’s a big difference between those two pathways. I spent many years commercializing technologies. And when you take them out of the lab and try to get through what we call the valley of death, it’s a tough road. And so what we decided to do is carve out the technology from the lab that was more mature and had a more direct regulatory path. We have been working closely with the FDA on this. We formed a company called Neuvotion, and Neuvotion is solely focused on taking the noninvasive versions of the technology and making those available to users and those that really can benefit from this technology. But the brain-computer interface itself is going to take a little bit longer in terms of the regulatory pathway. Thankfully, the FDA has now issued as of last year a guidance document, which is always a first step and a very important step, available. And this is a moment in time where it is no longer a question of whether we will have brain-computer interfaces for patients, but it’s now just a question of when.</p><p><strong>Strickland: </strong>Before we wrap up, I wanted to ask you about another very different approach to helping people with tetraplegia. So some researchers are using brain-computer interface technology to read out intentions from the brain, but then sending those messages to robotic limbs instead of the person’s own limbs. Can you talk about the tradeoffs, the challenges, and the advantages of each approach?</p><p><strong>Bouton: </strong>Absolutely. So the idea of using a brain-computer interface to interface with a robotic arm was and is an important step forward in understanding the nervous system and movement and even sensation. But the comment I heard from a number of participants through the years is that at the end of the day, they would like to be able to move their own arm and feel, of course, with their own hands. And so we have really been focused on that problem. However, it does bring in some additional challenges. Not only is a biological arm more complex and more difficult to control and you have fatigue, muscle fatigue, and things like this to deal with, but also, there’s another complication in the brain. So when we reach out for something, we pick up a cup, I talked earlier about the nervous system reacts to the weight of the cup and different things happen. Well, there’s another issue, too, when you stimulate in the sensory area and you cause a percept. Someone says, “Okay, I feel kind of pressure on my fingertips.” Well, the sensory cortex is right next door to the motor cortex primary, S1 and M1 as they’re called. And so you have all these interconnections, a huge number of interconnections.</p><p>And so we hypothesize and we have some evidence already on this is that when you stimulate and you start to encode and put information or you’re writing into the brain, if you will, well, guess what? When you’re on the read side and you’re reading from the motor cortex, because of all those interconnections, you’re going to cause changes in what we call modulation. You’re going to see changes in patterns. This is going to make the decoding algorithms more difficult to architect. We predicted this would happen when Ian became the first person to move their hand and to be able to pronate his arm. We predicted that during the transfer of objects, there might be difficulties and changes in the modulation and would affect the decoding algorithms. And indeed that did happen. So we believe as we close the loop on this bidirectional neural bypass, we’re going to run into similar challenges and changes in modulation, and we’re going to have to adapt to that. So we’re also working on adaptive decoding. And there’s been some great work in this area, but with actually reanimating or enabling movement and sensation in the human arm itself and the human hand itself, we believe we’re in for some additional challenges. But we’re up for it, and we are very excited to move into that space of this year.</p><p><strong>Strickland:</strong> Well, Chad, thank you so much for joining us on the Fixing the Future podcast. I really appreciate your time today.</p><p><strong>Bouton:</strong> Absolutely. Glad to do it, and thanks so much for talking with me.</p><p><strong>Strickland:</strong> Today on Fixing the Future, we were talking with Chad Bouton about a neural bypass to help people with paralysis move again. I’m Eliza Strickland for <em>IEEE Spectrum</em>, and I hope you’ll join us next time.</p>]]></description>
      <pubDate>Mon, 27 Mar 2023 18:09:25 +0000</pubDate>
      <guid>https://spectrum.ieee.org/neural-bypass-for-paralysis</guid>
      <category>Type:podcast</category>
      <category>Neurostimulation</category>
      <category>Fixing the future</category>
      <dc:creator>Eliza Strickland</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/33293244/origin.webp">
      </media:content>
    </item>
    <item>
      <title>Better Carbon Sequestration With AI</title>
      <link>https://spectrum.ieee.org/carbon-sequestration-with-ai</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.webp?id=33292594&width=980"/><br/><br/><iframe frameborder="no" height="180" scrolling="no" seamless="" src="https://share.transistor.fm/e/7ce47e23" width="100%">
</iframe><hr/><h3>Transcript</h3><p><strong>Eliza Strickland:</strong> Technology to combat climate change got a big boost this year when the US Congress passed the <a href="https://www.congress.gov/bill/117th-congress/house-bill/5376/text" target="_blank">Inflation Reduction Act</a>, which authorized more than 390 billion for spending on clean energy and climate change. One of the big winners was a technology called carbon capture and storage. I’m Eliza Strickland, a guest host for <em>IEEE Spectrum</em>‘s Fixing the Future podcast. Today, I’m speaking with <a href="https://www.microsoft.com/en-us/research/people/pwitte/" target="_blank">Philip Witte of Microsoft Research</a> who’s going to tell us about how artificial intelligence and machine learning are helping out this technology. Philip, thanks so much for joining us on the program.</p><p><strong>Philip Witte: </strong>Hi, Eliza, I’m glad to be here.</p><p><strong>Strickland:</strong> Can you just briefly tell us what you do at Microsoft Research, tell us a little bit about your position there?</p><p><strong>Witte:</strong> Sure. So I’m a researcher at Microsoft Research, and I’m working on scientific machine learning in a broader sense and high-performance computing in the cloud. And specifically, how do you apply recent advances in machine learning in the HPC to carbon capture? And I’m part of a group at Microsoft that’s called Research for Industry, and we’re overall part of Microsoft Research, but we’re specifically focusing on transferring technology and computer science to solving industry problems.</p><p><strong>Strickland:</strong> And how did you start working in this area? Why did you think there might be real benefits of applying artificial intelligence to this tricky technology?</p><p><strong>Witte:</strong> So I was actually pretty interested in this topic for a couple years now, and then really started diving deeper into it maybe a year-and-a-half ago when Microsoft had signed a memorandum of understanding with one of the big CCS projects that is called <a href="https://norlights.com/" target="_blank">Northern Lights</a>. So Microsoft and them signed a contract to explore possibilities of how Microsoft can support the Northern Lights project as a technology partner.</p><p><strong>Strickland:</strong> So we’ll get into some of these super tech details in a little bit. But before we get to those, let’s do a little basic tutorial on the climate science here. How and where can carbon dioxide be meaningfully captured, and how can it be stored, and where?</p><p><strong>Witte:</strong> So I think it’s worth pointing out that there are kind of two main technologies around carbon capture, and one is called direct air capture, where you capture CO<sub>2</sub> directly from ambient air. And the second one is what’s usually referred to as CCS carbon capture and storage, is more carbon capture in an industrial setting where you extract or capture CO<sub>2</sub> from industrial flue gases. And the big difference is that in direct air capture, where you’re capturing CO<sub>2</sub> directly from the air, the CO<sub>2</sub> content is very low in the ambient air. It’s about 0.04 percent overall. So the big challenge of direct air capture is that you have to process a lot of air to capture a given amount of CO<sub>2</sub>. But you are actively reducing the overall amount of CO<sub>2</sub> in the air, which is why it’s also referred to as a negative emission technology. And then on the other hand, if you have some CCS, where you extracting CO<sub>2</sub> from industrial flue gases, the advantage there is that the CO<sub>2</sub> content is much higher in these flue gases. It’s about a 3 to 20 percent. So by processing the same amount of air using CCS, you can extract, overall, much more CO<sub>2</sub> from the atmosphere, or more accurately, prevent CO<sub>2</sub> from entering the atmosphere in the first place. So this is basically to distinguish between direct air capture and CCS.</p><p>And then for the actual capture part of the CCS, there’s a bunch of different technologies so you can do that. And they are typically grouped into pre-combustion, post-combustion, and oxy-combustion. But the most popular one that’s mostly used in practice right now is a post-combustion process called the amine process, where essentially, we have your exhaust from factories that has very high CO<sub>2</sub> content, and you bring it in contact with a liquid that has this amine chemical that binds the CO<sub>2</sub>, that you basically suck the CO<sub>2</sub> out of the air. And now you have a liquid, this amine liquid with a high CO<sub>2</sub> concentration. And because you want to be able to reuse this chemical that binds the CO<sub>2</sub>, there has to be a second step in which you now separate the CO<sub>2</sub> from this amine. And this is actually where now you have to spend most of your energy because now you have to reheat this mixture to separate the CO<sub>2</sub> and get a very high content CO<sub>2</sub> stream out that you can then store, and then you can reuse the amine. So you have to invest a lot of energy and bring it up to temperature. I think it’s about 250 to 300 degrees Fahrenheit. And once you have extracted the CO<sub>2</sub>, you have to compress the CO<sub>2</sub> so that you can store it in the next step.</p><p>And then in between the capture and the storage, you have, of course, the transportation, because usually you have to transport it from wherever you captured it to where you can store it. The most common ways to transport the CO<sub>2</sub> is either in pipelines or in vessels. And then in the final step, when we actually want to store CO<sub>2</sub>, there’s different possibilities for a storage that has been explored in the past. So people that have looked even at storing CO<sub>2</sub> at the bottom of the ocean, which we kind of moved away from that idea now. I don’t think anybody’s really considering that anymore. People have also looked at storing CO2 in old mineshafts, and the approaches that are most seriously looked at now, or already used in practice, actually, is storing CO<sub>2</sub> in old oil and gas-depleted reservoirs or in deep saltwater aquifers that are a couple kilometers below the surface. The important factors when you look at storage sites and where should I source CO<sub>2</sub> is that, first of all, you have to have a large enough volume so that it’s very impactful that you can store enough CO<sub>2</sub> there. Obviously, it has to be safe. Once you store the CO<sub>2</sub> there, you’d want to make sure that it actually stays where you injected it. And then just as important as also the cost factor, if you can not store it cost-effectively, then it’s just not going to be used in practice. So like I said, this depleted oil and gas reservoirs in these deep-water saline aquifers are right now the storage sites that pretty much satisfy these three requirements.</p><p><strong>Strickland: </strong>And as I understand it, carbon capture and storage is looked on as a useful technology for this transition because it can help society move away from fossil fuels like power plants that run on gas and coal and factories that use fossil fuels. Those sort of entities can keep going for a little while, but if we can capture their emissions, then they’re not adding to our climate change problem. Is that how you think about it?</p><p><strong>Witte: </strong>I think so. There’s a few areas like, for example, the power grid, that we have a good understanding of how we can actually decarbonize it. Because a lot of it now is still using coal and natural gas, but we have kind of a path towards carbon-neutral energy using nuclear power plants, renewable energies, of course. But then there’s other areas where the answer is maybe not that obvious. For example, you release a lot of CO<sub>2</sub> and steel production or petrochemical production or cement, construction. So all these areas where we don’t really have a very good alternative at the moment, you could make that carbon neutral or carbon negative by using CCS technology. And then I guess also why CCS is considered one of the main options is just because it’s very mature in terms of technology because the underlying technology behind carbon capture itself and CCS dates back actually to the 1930s where they developed this process that I just described, but it captured the CO<sub>2</sub>. And then as part of other industrial processes, has been used extensively since the 1970s. That’s why we have this whole network of pipelines that you could use to transport CO<sub>2</sub>. So I mean, in terms of technology, we have a really good understanding of how CCS works. That’s why a lot of people are looking at this as one possible technology. But of course, it’s not going to solve all the problems. There’s no silver bullet, really. So eventually, it has to just be part of a whole bigger package for climate change mitigation.</p><p>And it’s going to have to be part of the package at pretty enormous scale, right? What volume of carbon could we be potentially storing below ground in decades to come?</p><p>I have some numbers that I got from listening to a talk from a Philip Ringrose, who is one of the leading CCS experts. Roughly, we are releasing about 40 gigatons of CO<sub>2</sub> into the atmosphere every year worldwide. And then one of the first commercial CCS projects that is currently being deployed is the Northern Lights project. And at the Northern Lights project, they’re looking at storing about 1.5 megatons initially, and then 3.5 tons at a later stage. So if you take these numbers and you look at the overall global release of CO<sub>2</sub>, you would have to have roughly 10,000-ish Northern Lights projects, 10,000 to 20,000 CO<sub>2</sub> injection wells. So if you hear that, you might think, “Wow, that’s really a lot. 10 to 20,000 projects. I mean, how would we ever be able to do that?” But I think you really need to put that into perspective as well. Just looking, for example, how many wells we have for oil and gas production just in the US alone, I think in 2014, it was roughly 1 million active wells for oil and gas exploration, and only in that year alone, they drilled an additional 33,000 new wells, only in 2014. So in that perspective, 10 to the 20,000 wells, only for CCS, doesn’t sound that bad, is actually quite doable. But you’re not going to be able to capture all the CO<sub>2</sub> emissions only with CCS. It’s just going to be part of it.</p><p><strong>Strickland: </strong>So how can artificial intelligence systems be helpful in this mammoth undertaking? Are you working on simulating how the carbon dioxide flows beneath the surface or trying to find the best spots to put it?</p><p><strong>Witte: </strong>Overall, you can apply AI to all the different three main components of CCS, the capture part, the transport part, whereas I’m focusing mainly on the storage part and the monitoring. So for that, there’s essentially three main questions that you have to answer before you can do anything. Where can I store the CO<sub>2</sub>? How much CO<sub>2</sub> can I store, and how much can it inject at a time? And then is it safe and can I do a cost-efficiently? In order to answer these questions, what you have to do is you have to run these so-called reservoir simulations, where you have a numerical simulator that predicts how the CO<sub>2</sub> behaves during injection and after injection. And the challenge of these reservoir simulations is that, first of all, it’s computationally very expensive. So it’s these big simulations that run on high-performance computing clusters for many hours or days, even. And then the second real big challenge is that you have to have a model of what the earth looks like so that you can simulate it. So specifically for reservoir simulation, you have to know what the permeability is like, what the porosity is like, how the different geological layers look like. And obviously, you can’t directly look into the subsurface. So the only information that we do have is from drilling wells, which usually in CCS projects, you don’t have very many wells, so that might only be one or two wells.</p><p>And then the second information comes from basically remote sensing, something like seismic imaging, where you get an image of the subsurface, but it’s not super accurate. But then using this very sporadic data from wells and seismic data and some additional ones, you build up this model of what this subsurface might look like, and then you can run your simulation. And the simulation is very accurate in the sense that if you give it a model, it’s going to give you a very accurate answer of what happens for that model. But like I said, the problem is that model is very inaccurate. So over time, you have to adjust that model and kind of tweak the different inputs so that it actually explains what’s really happening in practice. So one of the big challenges there is that you want to be able to run a lot of these simulations with always changing the input a little bit to see if you get the answer that you would expect.</p><p>So where we see the role of AI helping out is, on the one hand, providing a way to simulate much faster than with conventional methods, because like I said, the conventional methods, they’re very generic, but oftentimes, I sort of have an idea of what this subsurface looks like. I only want to tweak it a little bit here and there, which is where we think that AI might be helpful. Because you have a lot of data from just running the simulations, and now you can use that simulated data to train a surrogate model for that simulator. And you might be able to evaluate that surrogate model much, much faster, and then use it in downstream applications like optimization or uncertain quantification to eventually answer these three questions that I initially mentioned.</p><p><strong>Strickland: </strong>So you’re talking about using simulated data to train the model. How then do you check it against reality if you’re starting with simulated data?</p><p><strong>Witte:</strong> So the simulated data, you would still have to do the same process of matching the simulated data to the data that you measure when you’re out in the field. For example, in the CCS project, the CO<sub>2</sub> injection wells has all kinds of measurements at the bottom that measures, for example, pressure, temperature, and then you have these seismic surveys that you run during injection and after injection, and then you can get an image, for example, of where the CO<sub>2</sub> is after you inject it. So you have a rough idea of where the CO<sub>2</sub> plume is, and now you can run your simulations, and again, change the inputs that the CO<sub>2</sub> plume that you simulate actually matches the one that you observe in the seismic data or matches the information from your well logs. That’s something that’s often done by hand, which is very time-consuming. And the hope of machine learning is that you can not only make it faster, you can also maybe automate some of these things.</p><p><strong>Strickland: </strong>You’re using a type of neural network called Fourier Neural Operators in this work, which seem to be particularly useful in physics for modeling things like fluid flows. Can you tell us a little bit about what Fourier Neural Operators are, what kind of inputs they use, and what the benefit of using them is?</p><p><strong>Witte:</strong> Fourier Neural Operators is a kind of neural network that was designed for solving partial differential equations, and the original work was done by Anima Anandkumar, a PhD student, Zongyi Li, and I think Andrew Stuart from Caltech was also involved. And the idea is you simulate training data using a numerical simulator where you have a bunch of different inputs that could be, for example, the earth model, what does the earth look like? And then you simulator output would be how does the CO<sub>2</sub> behave over time? You have many different inputs, and then typically, you train this in a supervised fashion where I now have thousands of training pairs. And then you would train, for example, a Fourier Operator to simulate the CO<sub>2</sub> for a given input. And then you can use that in these downstream applications that require a lot of these simulations.</p><p><strong>Strickland: </strong>Okay. So to bring this back to the physical world, what happens if carbon dioxide that’s injected into a subsurface aquifer or something like that doesn’t stay put? Is there a safety problem? Could it potentially cause earth tremors, or is it just that it would negate the effect of putting CO<sub>2</sub> underground?</p><p><strong>Witte: </strong>There’s definitely a risk. It’s not risk-free, but I initially overestimated the risks because kind of the mental picture that I had is that there’s a big, empty space in the subsurface: You inject CO<sub>2</sub> as a gas, and then you only need the tiniest leak somewhere and the whole CO<sub>2</sub> is going to come back out. But when you actually inject the CO<sub>2</sub>, it’s not a gas anymore because you have it under very high pressure and very high temperature, so it’s more like a liquid. It’s not an actual liquid. It’s called a supercritical state, but essentially, it’s like a liquid. Philip Ringrose said, “Think of it as olive oil.” And then the second aspect is that in the subsurface where you store it, it’s not an empty space. It’s more like a sponge, like a very porous medium that absorbs the CO<sub>2</sub>. So overall, you have these different mechanisms, chemical, and mechanical mechanisms that trap the CO<sub>2</sub>, and they’re all additive. So the one mechanism is what’s called structural trapping, because if you inject CO<sub>2</sub>, for example, in these saltwater aquifers, the CO<sub>2</sub> rises up because it has a lower density than the salt water, and so you need a good geological seal that traps the CO<sub>2</sub>. You can kind of think of it maybe as an inverted bowl in the subsurface, where the CO<sub>2</sub> is now going to go up, but it’s going to be trapped by the seal. So that’s called structural trapping, and that’s very important, especially during the early project phases. But yes, you have these different trapping mechanisms that are additive, which generally, I mean, even if you would have a leak, the CO<sub>2</sub> would not all come out at the same time. It would be very, very slow. So in the CCS projects, they have measurements that measure the CO<sub>2</sub> content, for example, so that you could easily or very quickly detect that.</p><p><strong>Strickland:</strong> And can you talk a bit more about the Northern Lights project and tell us about its current status and what you’re working on next to help that project move forward?</p><p><strong>Witte:</strong> Yeah, so Northern Lights describes itself as the world’s first open-source CO<sub>2</sub> transport and storage project. It doesn’t mean open-source in the sense like in software. What it means in this case is that they essentially offer carbon capture and storage as a service so that if you’re a client, for example, you’re a steel factory and you install CCS technology to capture the carbon, you can now sell it to Northern Lights, and they will send a vessel, pick up the CO<sub>2</sub>, and then store it permanently using geological storage. So the idea is that Northern Lights builds the transportation and storage infrastructure, and then sells that as a service to companies like— I think the first client that they signed a contract with is a Dutch petrochemical company called Yara Sluiskil.</p><p><strong>Strickland: </strong>And to be sure I understand, you said that the companies that are generating the CO<sub>2</sub> are selling the CO<sub>2</sub> to the Northern Lights project, or is it the other way around?</p><p><strong>Witte:</strong> How I think about it more as they pay for the service that Northern Lights picks up the CO<sub>2</sub> and then stores it for them.</p><p><strong>Strickland:</strong> And one last question. If I remember right, Microsoft was really emphasizing open-source for this research. And what exactly is open-source here?</p><p><strong>Witte: </strong>So the training datasets that we create, we’re planning to make those open-source, the code to generate the datasets as well as the code to train the models. I’m actually currently working on open-sourcing that, and I think by the time this interview comes out, hopefully it will already be open-source, and you should be able to find that at the Microsoft Research industry website. But yeah, we really want to emphasize the open-sourceness of not just CCS itself, but the technology and the monitoring part, because I think in order for the public to accept CCS and have confidence that it works and that it’s safe, you have to have accountability and you have to be able to put that data, for example, the monitoring data out there, as well as the software. Traditionally, in oil and gas exploration, the data and also the codes to run simulations and to do monitoring are. I mean, the companies keep it very tight to the chest. There’s not a whole lot of open-source data or codes. And luckily, with CCS we already see that changing. Companies like Northern Lights are actually putting their data on the web as open-source material for people to use. But of course, the data is only part of the story. You also need to be able to do something with that data, process it in the cloud using HPC and AI. And so we work really hard on making some of these components accessible, and that does not only include the AI models, but also, for example, API suppresses data in the cloud using HPC. But eventually, we were really hoping to-- once we have all the data and the codes available, that it’s really helping the overall community to accelerate innovations and build on top of these tools and datasets.</p><p><strong>Strickland: </strong>And that’s a really good place to end. Philip, thank you so much for joining us today on Fixing the Future. I really appreciate it.</p><p><strong>Witte: </strong>Yeah, thanks, Eliza. I really enjoyed the conversation.</p><p><strong>Strickland: </strong>Today on fixing the future, we were talking with Philip Witte about using AI to help with carbon capture and storage. I’m Eliza Strickland for <em>IEEE Spectrum</em>, and I hope you’ll join us next time.</p><div></div><div>
</div>]]></description>
      <pubDate>Mon, 27 Mar 2023 18:08:06 +0000</pubDate>
      <guid>https://spectrum.ieee.org/carbon-sequestration-with-ai</guid>
      <category>Type:podcast</category>
      <category>Climate change</category>
      <category>Ai</category>
      <category>Microsoft</category>
      <category>Carbon sequestration</category>
      <category>Fixing the future</category>
      <dc:creator>Eliza Strickland</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://assets.rbl.ms/33292594/origin.webp">
      </media:content>
    </item>
    <item>
      <title>The Do-or-Die Moments That Determined the Fate of the Internet</title>
      <link>https://spectrum.ieee.org/computer-networking</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/this-photograph-of-the-exhibition-floor-shows-a-large-number-of-people-moving-about-and-visiting-booths-set-up-by-ibm-and-sony.jpg?id=33343083&width=1200&height=800&coordinates=0%2C209%2C0%2C0"/><br/><br/><p>
	New technologies often are introduced through spectacle: Think of the historic demonstrations carried out by Faraday, Edison, Morse, and Bell, or, more recently, by Steve Jobs onstage in his black turtleneck at Macworld 2007, holding the first iPhone. Indeed, hyped-up product announcements at industry events like the Consumer Electronics Show (now 
	<a href="https://www.ces.tech/" target="_blank">CES</a>) and the Game Developers Conference have become regular features of the digital world.
</p><p>
	There’s also a parallel tradition—less flashy but no less important—of industry events that focus attention on digital infrastructure. Several of these events, such as the first public demo of the 
	<a href="https://en.wikipedia.org/wiki/ARPANET" target="_blank">ARPANET</a> in 1972, or the mid-1980s conferences now known as Interop, alerted experts to new technologies, and, in some cases, altered the balance between competing approaches.
</p><p>
	Although many of these gatherings have escaped the attention of historians, our view is that these events should be recognized more fully as moments where experts could glimpse possible futures and judge for themselves what was most likely to happen. Here we describe a few of these do-or-die moments. You may not have heard of any of these events—but if you were there, you will never forget them.
</p><h2>Packet Switching Comes of Age</h2><p>
	The ARPANET was one of the first networks to apply 
	<a href="https://en.wikipedia.org/wiki/Packet_switching" target="_blank">packet switching</a>, an approach to communications that breaks messages into discrete chunks, or packets, of data. It was a major departure from circuit-switched networks, such as telephone networks, for which communication partners were linked through a dedicated circuit.
</p><p>
	The first node of the ARPANET was installed at the University of California, Los Angeles, in 1969. But the ARPANET didn’t take off immediately. And by mid-1971, program director 
	<a href="https://en.wikipedia.org/wiki/Lawrence_Roberts_(scientist)" target="_blank">Lawrence Roberts</a> of the Advanced Research Projects Agency (ARPA) was becoming impatient with the slow pace at which ARPA-funded researchers were getting connected. One of these researchers, <a href="https://en.wikipedia.org/wiki/Bob_Kahn" target="_blank">Bob Kahn</a>, suggested that Roberts organize a public demonstration of the ARPANET, both to educate other researchers about the network’s capabilities and to encourage new partners to support the initiative. Once Kahn found a venue for the demo—at the International Conference on Computer Communications (ICCC), to be held in Washington, D.C., in late October of 1972—he worked feverishly to get it organized.
</p><p>
	Kahn recruited about 50 people to act as facilitators, including the ARPA-funded researchers 
	<a href="https://en.wikipedia.org/wiki/Vint_Cerf" target="_blank">Vint Cerf</a>, <a href="https://en.wikipedia.org/wiki/Robert_Metcalfe" target="_blank">Robert Metcalfe</a>, and <a href="https://en.wikipedia.org/wiki/Jon_Postel" target="_blank">Jon Postel</a>, all of whom were destined for networking fame. Kahn’s plan called for a TIP—short for Terminal Interface Processor—to be installed at the Hilton Hotel, the site of the ICCC. From there, attendees could log on to one of the ARPANET hosts and run an application remotely.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="These three diagrams show schematically the makeup of the ARPANET, including the nodes (network-connection points) and the types of computers attached to those nodes" class="rm-shortcode" data-rm-shortcode-id="ea9dbdf50185901862181f7aa6cfab75" data-rm-shortcode-name="rebelmouse-image" id="6fd0d" loading="lazy" src="https://spectrum.ieee.org/media-library/these-three-diagrams-show-schematically-the-makeup-of-the-arpanet-including-the-nodes-network-connection-points-and-the-types.jpg?id=33350872&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">As these hand drawings from the time show, in December 1969 the ARPANET had just four nodes [top left]. That number grew to 15 by April 1971 [top right]. In an effort to speed the expansion further, network advocates organized a demonstration at the International Conference on Computer Communications in Washington, D.C., in October 1972. That meeting helped to grow the ARPANET, which by May of 1973 included some three dozen nodes [bottom].</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Computer History Museum</small>
</p><p>
	For this to work smoothly, Kahn arranged for various applications (called “scenarios”) to be created and tested. He also had to convince manufacturers to loan, install, and configure terminals. And he had to work with the hotel to prepare the room for the demo and arrange with 
	<a href="https://www.att.com/" target="_blank">AT&T</a> to run leased lines to the Hilton’s ballroom.
</p><p>
	The ICCC would prove to be for packet switching what the 
	<a href="https://www.nga.gov/research/library/imagecollections/photographs-of-international-expositions/philadelphia-1876.html" target="_blank">1876 Centennial Exposition</a> in Philadelphia <a href="https://www.loc.gov/ghe/cascade/index.html?appid=44f7c2bf113b4560af3c20cdc556ecaa" target="_blank">was for the telephone</a>: the public unveiling of what would eventually lead to a technological discontinuity.
</p><p>
	For the hundreds of computer-communications professionals, government employees, and academic researchers attending the ICCC, the demo permanently changed their perceptions of a computer as a single machine locked in an air-conditioned room. The TIP was on a raised floor in the middle of the ballroom, with dozens of connected computer terminals circled around it and dozens of ARPA scientists milling about, eager to show off their pride and joy.
</p><p>
	To sit at a terminal and with a few keystrokes be connected through the TIP, to the ARPANET, and then to applications running on computers at dozens of universities and research facilities must have felt like a visit to an alien world. And for the ARPA scientists involved, the bonds formed from staging the demonstration left them heady and optimistic about the future they were creating.
</p><h2>Think Globally. Act Locally</h2><p>
	Researchers in government, academia, and industry struggled over the next several years to realize the potential of what they had seen. How could they scale up and simplify the capabilities that Kahn and company spent a year bringing to the Hilton ballroom? One major problem was the cost and fragility of stringing a dedicated cable from every computer to every terminal. Several parties converged on a similar solution: a local area network, where one “local” cable could traverse an entire facility with all terminals and computers connected to it.
</p><p>
	Users in large organizations—including the U.S. Air Force, which had decades of experience and investments in computer communications—had the most to gain from solutions to these problems. To promote cooperation, 
	<a href="https://historyofcomputercommunications.info/interviews/robert-rosenthal/" target="_blank">Robert Rosenthal</a> at the <a href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank">U.S. National Bureau of Standards</a> and Norman Meisner at <a href="https://www.mitre.org/" target="_blank">Mitre</a>, a federally funded R&D organization, arranged a series of workshops in early 1979 to explore “Local Area Network Protocols.” Their goal was to provide a mechanism for sharing and obtaining results from the latest research—especially knowledge that was not available in the published literature. When Rosenthal and Meisner contacted potential participants, it became clear that while virtually everyone working on local area networking sensed its importance, they all expressed confusion over what to do about it.
</p><p>
	When it came to sorting out the solution, a meeting Rosenthal and Meisner organized in May 1979 proved to have enduring significance. The Local Area Communications Network Symposium, held at the Copley Plaza Hotel in Boston, featured five formal sessions, panel discussions, and twelve workshops. Rosenthal was astonished when about 400 people showed up. For most, it was a formative event, comparable in importance to the ARPANET demonstration in 1972. “There was electricity in the air,” Rosenthal recalled in a 
	<a href="https://archive.computerhistory.org/resources/access/text/2020/04/102792038-05-01-acc.pdf" target="_blank">1988 interview</a> with one of us (Pelkey). “You had leaders [like] Bob Metcalfe saying: ‘The world’s going to be a better place.’ ”
</p><p>
<a href="https://historyofcomputercommunications.info/interviews/bruce-hunt/" target="_blank">Bruce Hunt</a> of <a href="https://www.zilog.com/" target="_blank">Zilog</a> remembers “being amazed at how many people were really interested in local area networks,” and feeling satisfied that the instinct of the researchers involved—that they were onto something really important—was validated. And it wasn’t just hype by academics: Within a couple of months, three new companies were formed—<a href="https://en.wikipedia.org/wiki/Sytek" target="_blank">Sytek</a>, <a href="https://en.wikipedia.org/wiki/3Com" target="_blank">3Com</a>, and <a href="https://en.wikipedia.org/wiki/Ungermann-Bass" target="_blank">Ungermann-Bass</a>. Emboldened by the clear demand for commercial networking equipment, these startups raised millions from investors and immediately began selling products for local area networking.
</p><p>
	More and more professionals came to realize that networking technology would generate important benefits. But the engineers involved had not settled many technical details about how these networks would work. And a growing number of alternatives soon would be considered for standardization by the IEEE, including a now well-known technology called Ethernet, which will celebrate the 50th anniversary of its standardization in May of this year.
</p><h2>Birth of OSI</h2><p>
	In the meantime, work was underway on a broader approach to the challenge of creating standards for computer communication
	<strong>—</strong>one that could serve to link up different computer networks, a concept that began at this time to be called “internetworking.” In 1978, a few dozen experts from around the world held the first meeting for an ambitious project to create a comprehensive suite of standards and protocols for disparate networking technologies. This effort, known as <a href="https://spectrum.ieee.org/osi-the-internet-that-wasnt" target="_self">Open Systems Interconnection</a> (OSI), was hosted first by the <a href="https://www.iso.org/home.html" target="_blank">International Organization for Standardization</a> (ISO) and later, jointly, by ISO and the <a href="https://www.itu.int/en/Pages/default.aspx" target="_blank">International Telecommunication Union</a>. OSI’s founding premise was that a layered architecture would provide a way to pull together the standards, applications, and services that diverse groups around the world were developing.
</p><p>
	The lower layers of OSI concerned the formatting, encoding, and transmission of data over networks. The upper layers included advanced capabilities and applications, such as electronic mail and directory services.
</p><p>
	Several initiatives examined proposals for standards and applications within OSI’s seven-layer framework. One arose at 
	<a href="https://www.gm.com/" target="_blank">General Motors</a>, which had a strategic goal of <a href="https://ieeexplore.ieee.org/document/6370871" target="_blank">using computer-based automation</a> to combat growing competition from abroad. In 1981, GM held exploratory conversations with <a href="https://en.wikipedia.org/wiki/Digital_Equipment_Corporation" target="_blank">Digital Equipment Corp.</a>, <a href="https://www.hp.com/us-en/home.html" target="_blank">Hewlett-Packard</a>, and <a href="https://www.ibm.com/us-en?ar=1" target="_blank">IBM</a>. These discussions culminated in the release of GM’s Manufacturing Automation Protocol (MAP) version 1.0 in 1982.
</p><p>
<br/>
</p><div class="rm-embed embed-media"><div class="flourish-embed flourish-timeline" data-src="visualisation/12965537?607871">
<script src="https://public.flourish.studio/resources/embed.js"> </script>
</div></div><p>
<br/>
</p><p>
<a href="https://www.boeing.com/" target="_blank">Boeing</a>, with similar goals, announced that it would work with the National Bureau of Standards to lead the creation of an OSI protocol stack for technical and office environments, later to be named Technical and Office Protocols (TOP).
</p><p>
	Once again, potential users and customers sought out live demonstrations so that they could judge for themselves what was hype and what was reality. One highly anticipated demo took place at Autofact ’85, a conference whose name reflects the era’s deep preoccupation with factory automation.
</p><p>
	Autofact ’85 drew about 30,000 people to Detroit, with some 200 vendors exhibiting MAP-compatible and other kinds of automation products. In addition to data-processing equipment such as computers and terminals, a variety of factory-automation systems, including robots, vision systems, and engineering workstations, were on display. With them, attendees explored a custom-designed version of the Towers of Hanoi game, and an application for interactive file transfer, access, and management.
</p><p>
	Although Autofact ’85 was well attended and generally hailed in the trade press as a success, some were put off by its focus on things to come. As one press account put it, “On the show floor, there are plenty of demonstrations but few available products.” The lingering questions around actual commercial applications gave promoters reason to continue organizing public demonstrations, such as the Enterprise Networking Event (ENE) in Baltimore, in June 1988.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="This photo of the conference proceedings shows a large softcover book with a rather drab cover. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="9204e5a2f8a0f060d32741fb26af3d83" data-rm-shortcode-name="rebelmouse-image" id="9833e" loading="lazy" src="https://spectrum.ieee.org/media-library/this-photo-of-the-conference-proceedings-shows-a-large-softcover-book-with-a-rather-drab-cover.jpg?id=33343186&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Autofact ’85, a conference whose name reflects people’s keen interest in factory automation during that time, brought about 30,000 people and 200 vendors to Detroit in November of 1985. </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">SUNY Polytechnic Institute</small>
</p><p>
	The hope for ENE was to provide demos as well as showcase products that were actually available for purchase. All the U.S. computing giants—including IBM, HP, AT&T, 
	<a href="https://www.xerox.com/en-us" target="_blank">Xerox</a>, <a href="https://en.wikipedia.org/wiki/Data_General" target="_blank">Data General</a>, <a href="https://en.wikipedia.org/wiki/Wang_Laboratories" target="_blank">Wang Laboratories</a>, and <a href="https://www.honeywell.com/us/en" target="_blank">Honeywell</a>—would be there, as well as leading European manufacturers and some smaller and younger companies with OSI-compatible products, such as <a href="https://www.apple.com/" target="_blank">Apple</a>, <a href="https://en.wikipedia.org/wiki/Micom" target="_blank">Micom</a>, <a href="https://www.crunchbase.com/organization/retix" target="_blank">Retix</a>, <a href="https://en.wikipedia.org/wiki/Sun_Microsystems" target="_blank">Sun Microsystems</a>, 3Com, and Touch Communications. Keynote speakers from the upper levels of the U.S. Department of Defense, <a href="https://en.wikipedia.org/wiki/Arthur_Andersen" target="_blank">Arthur Andersen</a>, and the <a href="https://en.wikipedia.org/wiki/European_Communities" target="_blank">Commission of European Communities</a> reinforced the message that all major stakeholders were behind the global adoption of OSI.
</p><p>
	ENE confirmed both the hopes of OSI’s supporters and the fears of its critics. Vendors were able to demonstrate OSI standards for network management and electronic mail, but instead of products for sale, the 10,000 or so attendees saw mostly demonstrations of prototypes—a marginal improvement on Autofact ’85.
</p><h2>The Internet Cometh</h2><p>
	There was a painful reality to the computer networks of the mid-1980s: On the one hand, they held a vast potential to improve business practices and enhance productivity; on the other, actual products that could integrate the diversity of installed equipment and networks—and thus provide a robust means of internetworking—were very limited. The slow progress of MAP and TOP products left an opening for alternative approaches.
</p><p>
	And the most promising of those approaches was to rely on the core protocols then in use for the ARPANET: Transmission Control Protocol and Internet Protocol (known to insiders as 
	<a href="https://en.wikipedia.org/wiki/Internet_protocol_suite" target="_blank">TCP/IP</a>). A broad market for suitable equipment hadn’t yet developed, but the community of experts that had grown around the ARPANET was increasingly active in promoting the commercial adoption of such products.
</p><p>
	One of the chief promoters was 
	<a href="https://www.internethalloffame.org/inductee/dan-lynch/" target="_blank">Dan Lynch</a>, a consultant who was instrumental in managing the ARPANET’s transition to TCP in 1983. Lynch led the planning of a workshop in Monterey, Calif., in August 1986, where equipment vendors could learn about TCP/IP. Lynch wanted to get the apostles of TCP “to come out of their ivory towers” and provide some guidance for vendors implementing their protocols. And they did, as Lynch recalled in <a href="https://archive.computerhistory.org/resources/access/text/2016/02/102717120-05-01-acc.pdf" target="_blank">a 1988 interview</a>, where he called the workshop “outrageously successful.”
</p><p>
	This meeting, the first TCP/IP Vendors Workshop, featured a mix of leaders from the TCP/IP research community and representatives from 65 vendors, such as Ungermann-Bass and 
	<a href="https://en.wikipedia.org/wiki/Excelan" target="_blank">Excelan</a>. Lynch continued this trade-show-like approach with the TCP/IP Interoperability Conference in Monterey in March 1987 and the 2nd TCP/IP Interoperability Conference in Arlington, Va., in December of the same year.
</p><p>
	Lynch’s strategy for TCP/IP seemed to be gaining momentum, as evidenced by an article in 
	<em>Data Communications</em> in November 1987, which neatly summarized the state of affairs: “By the end of 1986, there were more than 100 vendor offerings of TCP/IP and its associated DARPA protocols. Moreover, major vendors, including IBM and Digital Equipment Corp. have recently begun to offer TCP/IP as part of their product lines…. While the long-term strategic direction taken by most companies is in the implementation of the OSI model and its protocols, TCP/IP appears to be solving the short-term problems of connections between networks.”
</p><p>
	The market-research firm 
	<a href="https://infonetx.com/" target="_blank">Infonetics</a> published a report in May 1988 that documented a “dramatic increase in the commercialization of TCP/IP” and noted that increasing numbers of users were seeking solutions to integrate diverse computer equipment and networks. “Every sector of the market is planning to purchase TCP/IP products in the next year,” the report stated. “There is no indication that OSI is affecting purchase intent.”
</p><h2>Get Ready for the World Wide Web</h2><p>
	At the time, Lynch was planning a new venue to promote the adoption of the protocols used for the ARPANET: the TCP/IP Interoperability Exhibition and Solutions Showcase, to be held in Santa Clara, Calif., in September 1988. And he gave the event a slick new title: Interop.
</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="The photo shows people milling around an exhibition hall, with the word \u201cInterop\u201d prominently displayed in the foreground, this name being embedded in the carpeting. " class="rm-shortcode" data-rm-shortcode-id="d43eb62846063489680fc6a45d112fa2" data-rm-shortcode-name="rebelmouse-image" id="d5df7" loading="lazy" src="https://spectrum.ieee.org/media-library/the-photo-shows-people-milling-around-an-exhibition-hall-with-the-word-u201cinterop-u201d-prominently-displayed-in-the-foregro.jpg?id=33343294&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">One of the key industry conferences that helped shape the Internet was the 1988 TCP/IP Interoperability Exhibition and Solutions Showcase in Santa Clara, Calif., which was given the shorter, catchier name “Interop.” </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Margot Simmons  </small>
</p><p>
	Interop featured lots of products: “every medium, every bridge box, every router you can imagine,” according to Peter de Vries of the Wollongong Group, which was responsible for putting together the network at Interop. That network provided connections among all vendors on display, including equipment available for purchase from 
	<a href="https://www.cisco.com/" target="_blank">Cisco Systems</a>, <a href="https://en.wikipedia.org/wiki/Proteon" target="_blank">Proteon</a>, and <a href="https://en.wikipedia.org/wiki/Wellfleet_Communications" target="_blank">Wellfleet Communications</a>.
</p><p>
	Using TCP/IP, attendees could traverse links to 
	<a href="https://www.nsf.gov/news/news_summ.jsp?cntn_id=103050" target="_blank">NSFNET</a>, the regional BARRNET in San Francisco, and a variety of other networks. Vendors could participate in TCP/IP “bake-offs,” where they could check to see whether their equipment interoperated with other vendors’ products. Self-appointed “net police” went so far as to hand out “tickets” to implementations that did not comply with the TCP/IP specifications.
</p><div class="ieee-sidebar-medium">
<h3>Editor’s Note:</h3>
<p>
		Much more information about the history of computer networking can be found in the authors’ recently published book
	</p>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="Cover of circuits, packets, and protocols" class="rm-shortcode" data-rm-shortcode-id="0b16af60e6f0bd868eb0e20ffb487298" data-rm-shortcode-name="rebelmouse-image" id="25755" loading="lazy" src="https://spectrum.ieee.org/media-library/cover-of-circuits-packets-and-protocols.jpg?id=33344050&width=980"/>
</p>
<p>
	Readers interested in purchasing 
		<em>Circuits, Packets, and Protocols: Entrepreneurs and Computer Communications, 1968–1988 </em>(ACM Books, 2022) will enjoy a 25% discount with the publisher by using the promo code “SPECTRUM25” <a href="https://bit.ly/3Lhj2NK" rel="noopener noreferrer" target="_blank"><br/>
<br/>
<strong>BUY HERE</strong></a>.
	</p>
</div><p>
	In many respects, Interop ’88 was far more successful than ENE. It featured working products from more vendors than did ENE. And whereas ENE carried the burden of people’s expectations that it would provide comprehensive solutions for large-scale manufacturing, office, and government procurement, Interop took on the immediate and narrower problems of network interconnection. In the “age of standards,” as an article in 
	<em>Data Communications </em>referred to that time, this focus on product compatibility, interoperability, and connectivity energized the estimated 5,000 attendees as well as the market for TCP/IP products.
</p><p>
	The stage was now set for innovations that would change global society: 
	<a href="https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web" target="_blank">the invention of the World Wide Web</a> the following year and the privatization of the NSFNET/Internet backbone in the mid-1990s. The advances in global computer networking that have come since then all rest on that initial foundation.
</p><p>
	Accounts of the beginnings of modern computing often include dramatic descriptions of a conference that has since become known as “the 
	<a href="https://www.youtube.com/watch?v=yJDv-zdhzMY" target="_blank">Mother of all Demos</a>”—a 1968 joint meeting of the Association for Computing Machinery and the IEEE Computer Society where ARPA-funded researcher <a href="https://en.wikipedia.org/wiki/Douglas_Engelbart" target="_blank">Douglas Engelbart</a> gave a 90-minute presentation that included the use of windows, hypertext, videoconferencing, and the computer mouse, among other innovations. His demo is rightly recognized as a turning point for expanding the realm of the possible in personal computing. But mind-expanding possibilities were also on display—and sometimes even for sale—at the five meetings we’ve described here. In our view, the contribution of these industry events to the development of today’s world of computing shouldn’t be forgotten, because the <em>connection</em> of different kinds of computers is the advance that has transformed our lives.
</p><p>
<em>Loring Robbins and Andrew Russell dedicate this article to their coauthor and longtime friend James Pelkey, who died shortly before it was published.</em> <br/>
</p>]]></description>
      <pubDate>Mon, 27 Mar 2023 15:00:00 +0000</pubDate>
      <guid>https://spectrum.ieee.org/computer-networking</guid>
      <category>Computer networking</category>
      <category>Internet</category>
      <category>Arpanet</category>
      <category>Local area network</category>
      <dc:creator>Loring Robbins</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/this-photograph-of-the-exhibition-floor-shows-a-large-number-of-people-moving-about-and-visiting-booths-set-up-by-ibm-and-sony.jpg?id=33343083&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>CubeSat Operators Launch an IoT Space Race</title>
      <link>https://spectrum.ieee.org/cubesat</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-rendering-of-the-earth-covered-in-blue-circles-and-criss-crossing-green-lines.jpg?id=33357721&width=1200&height=800&coordinates=0%2C63%2C0%2C63"/><br/><br/><p>A rocket carrying CubeSats launched into Earth orbit two years ago, on 22 March 2021. Two of those CubeSats represented competing approaches to bringing the Internet of Things (IoT) to space. One, operated by <a href="https://lacuna.space/about/" rel="noopener noreferrer" target="_blank">Lacuna Space</a>, uses a protocol called <a data-linked-post="2650278012" href="https://spectrum.ieee.org/loras-bid-to-rule-the-internet-of-things" target="_blank">LoRaWAN</a>, a long-range, low-power protocol owned by <a href="https://www.semtech.com/" target="_blank">Semtech</a>. The other, owned by <a href="https://sateliot.space/en/" target="_blank">Sateliot</a>, uses the <a href="https://en.wikipedia.org/wiki/Narrowband_IoT" target="_blank">narrowband IoT protocol</a>, following in the footsteps of <a href="https://www.oqtec.space/" rel="noopener noreferrer" target="_blank">OQ Technology</a>, which launched a similar IoT satellite demonstration in 2019. And separately, in late 2022, the cellular industry standard-setter <a href="https://www.3gpp.org/" target="_blank">3GPP</a> incorporated satellite-based 5G into standard cellular service with its <a href="https://www.3gpp.org/specifications-technologies/releases/release-17" rel="noopener noreferrer" target="_blank">release 17</a>.</p><p>In other words, there is now an IoT space race.</p><p>In addition to Lacuna and Sateliot, OQ Technology is also nipping at the heels of satellite telecom incumbents such as <a href="https://www.iridium.com/blog/what-is-satellite-iot-and-how-is-it-used/" rel="noopener noreferrer" target="_blank">Iridium</a>, <a href="https://www.orbcomm.com/en/partners/connectivity/iot-data-plans" rel="noopener noreferrer" target="_blank">Orbcomm</a>, and <a href="https://www.inmarsat.com/en/news/latest-news/enterprise/2022/new-iot-spectrum-leasing-growth-iot-solution-providers.html" rel="noopener noreferrer" target="_blank">Inmarsat</a> for a share of the growing satellite-IoT subscriber market. OQ Technology has three satellites in low Earth orbit and plans to launch seven more this year, says OQ Technology’s chief innovation officer, <a href="https://www.linkedin.com/in/prasannabnagarajan" rel="noopener noreferrer" target="_blank">Prasanna Nagarajan</a>. OQ has paying customers in the oil and gas, agriculture, and transport logistics industries.</p><p>Sateliot, based in Barcelona, has the satellite it launched in 2021 in orbit and plans to launch four more this year, says Sateliot’s business development manager, <a href="https://www.linkedin.com/in/paula-caudet-3b5704223/?originalSubdomain=es" target="_blank">Paula Caudet</a>. The company is inviting early adopters to sample its service for free this year while it builds more coverage. “Certain use cases are fine with flybys every few hours, such as agricultural sensors,” Caudet says.</p><p>OQ Technology claims it will launch enough satellites to offer at least hourly coverage by 2024 and near-real-time coverage later that year. Sateliot is also aiming for better-than-hourly coverage sometime in 2024 and near-real-time coverage in 2025.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt='a rendering of a small rectangular satellite in orbit. The satellite is covered with blue panels and the front says "sateliot"' class="rm-shortcode" data-rm-shortcode-id="ccfa565a6ab4a1f859168180432e0c09" data-rm-shortcode-name="rebelmouse-image" id="fafef" loading="lazy" src="https://spectrum.ieee.org/media-library/a-rendering-of-a-small-rectangular-satellite-in-orbit-the-satellite-is-covered-with-blue-panels-and-the-front-says-sateliot.jpg?id=33357724&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Sateliot</small></p><p>Incumbent satellite operators are already offering IoT coverage, but so far they require specific IoT hardware tuned to their spectrum bands and protocols. Insurgent companies that make use of the 3GPP release 17 standard will be able to offer satellite connectivity to devices originally designed to connect only to cellular towers.</p><p>New companies also see an opportunity to offer lower, more attractive pricing. “Legacy satellite providers were charging maybe [US] $100 for a few kilobits of data, and customers are not willing to pay so much for IoT,” says Nagarajan. “There seemed to be a huge market gap.” Another company, <a href="https://swarm.space/" target="_blank">Swarm</a>, which is a subsidiary of SpaceX, <a data-linked-post="2652903546" href="https://spectrum.ieee.org/swarm-takes-lora-skyhigh" target="_blank">offers low-bandwidth connectivity</a> via proprietary devices to its tiny satellites for $5 per month.</p><p>Thanks to shared launch infrastructure and cheaper IoT-compatible modules and satellites, new firms can compete with companies that have had satellites in orbit for decades. More and more hardware and services are available on an off-the-shelf basis. “An IoT-standard module is maybe 8 or 10 euros, versus 300 euros for satellite-specific modules,” says Caudet.</p><p>In fact, Sateliot contracted the construction of its first satellite to Open Cosmos. Open Cosmos mission manager <a href="https://www.linkedin.com/in/jordi-castellvi-esturi-21710aa" target="_blank">Jordi Castellví</a> says that CubeSat subsystems and certain specialized services are now available online from suppliers including <a href="https://alen.space/" rel="noopener noreferrer" target="_blank">AlénSpace</a>, <a href="https://www.cubesatshop.com/" rel="noopener noreferrer" target="_blank">CubeSatShop</a>, <a href="https://www.endurosat.com/" rel="noopener noreferrer" target="_blank">EnduroSat</a>, and <a href="https://www.isispace.nl/product-category/cubesat-structures/" rel="noopener noreferrer" target="_blank">Isispace</a>, among others.</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A rendering of a small satellite covered in black panels in orbit over Europe at night" class="rm-shortcode" data-rm-shortcode-id="8fe09b74d419abb1e6ef2d957f96f18c" data-rm-shortcode-name="rebelmouse-image" id="11e4a" loading="lazy" src="https://spectrum.ieee.org/media-library/a-rendering-of-a-small-satellite-covered-in-black-panels-in-orbit-over-europe-at-night.jpg?id=33357738&width=980"/>
<small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Open Cosmos</small></p><p>By building constellations of hundreds of satellites with IoT modules in low Earth orbit, IoT-satellite companies will be able to save money on hardware and still detect the faint signals from IoT gateways or even individual IoT sensors, such as those aboard shipping containers packed onto cargo ships at sea. They won’t move as much data as voice and broadband offerings in the works from AST SpaceMobile and Lynk Global’s <a href="https://spectrum.ieee.org/satellite-cellphone" target="_blank">larger and more complex satellites</a>, for example, but they may be able to meet growing demand for narrowband applications.</p><p>OQ Technology has its own licensed spectrum and can operate as an independent network operator for IoT users with the latest 3GPP release—although at first most users might not have direct contact with such providers; both Sateliot and OQ Technology have partnered with existing mobile-network operators to offer a sort of global IoT roaming package. For example, while a cargo ship is in port, a customer’s onboard IoT device will transmit via the local cellular network. Farther out at sea, the device will switch to transmitting to satellites overhead. “The next step is being able to integrate cellular and satellite services,” Caudet says.</p><p><em>This post was updated on 28 March to clarify the planned launch schedules and coverage schedules for OQ Technology and Sateliot.</em><br/></p>]]></description>
      <pubDate>Mon, 27 Mar 2023 14:00:00 +0000</pubDate>
      <guid>https://spectrum.ieee.org/cubesat</guid>
      <category>Internet of things</category>
      <category>Cubesats</category>
      <category>Lorawan</category>
      <category>Satellites</category>
      <dc:creator>Lucas Laursen</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-rendering-of-the-earth-covered-in-blue-circles-and-criss-crossing-green-lines.jpg?id=33357721&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Gallium Nitride and Silicon Carbide Fight for Green Tech Domination</title>
      <link>https://spectrum.ieee.org/silicon-carbide</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/photo-of-a-man-in-glasses-and-jacket-sitting-in-front-of-semiconductor-wafers.png?id=33329925&width=1200&height=800&coordinates=0%2C0%2C0%2C800"/><br/><br/><p>
<strong>Can advanced semiconductors</strong> cut emissions of greenhouse gases enough to make a difference in the struggle to halt climate change? The answer is a resounding yes. Such a change is actually well underway.
</p>
<p>
	Starting around 2001, the compound semiconductor gallium nitride fomented a revolution in lighting that has been, by some measures, the fastest technology shift in human history. In just two decades, the share of the global lighting market held by gallium-nitride-based light-emitting diodes has gone from zero to more than 50 percent, according to a study by the International Energy Agency. The research firm Mordor Intelligence recently predicted that, worldwide, LED lighting will be responsible for cutting the electricity used for lighting by 30 to 40 percent over the next seven years. Globally, lighting accounts for about 
	<a href="https://www.unep.org/resources/report/rapid-transition-energy-efficient-lighting-integrated-policy-approach" target="_blank">20 percent of electricity use and 6 percent</a> of carbon dioxide emissions, according to the United Nations Environment Program.
</p>
<hr/>
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="991b05bb18f90e15c800696050727730" data-rm-shortcode-name="rebelmouse-image" id="7920b" loading="lazy" src="https://spectrum.ieee.org/media-library/each-wafer-contains-hundreds-of-state-of-the-art-power-transistors.png?id=33329991&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Each wafer contains hundreds of state-of-the-art power transistors</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Peter Adams </small>
</p>
<p>
	This revolution is nowhere near done. Indeed, it is about to jump to a higher level. The very semiconductor technology that has transformed the lighting industry, gallium nitride (GaN), is also part of a revolution in power electronics that is now gathering steam. It is one of two semiconductors—the other being silicon carbide (SiC)—that have begun displacing silicon-based electronics in enormous and vital categories of power electronics.
</p>
<p>
	GaN and SiC devices perform better and are more efficient than the silicon components they are replacing. There are countless billions of these devices all over the world, and many of them operate for hours every day, so the energy savings are going to be substantial. The rise of GaN and SiC power electronics will ultimately have a greater positive impact on the planet’s climate than will the replacement of incandescent and other legacy lighting by GaN LEDs.
</p>
<p>
	Virtually everywhere that alternating current must be transformed to direct current or vice versa, there will be fewer wasted watts. This conversion happens in your phone’s or laptop’s wall charger, in the much larger chargers and inverters that power electric vehicles, and elsewhere. And there will be similar savings as other silicon strongholds fall to the new semiconductors, too. Wireless base-station amplifiers are among the growing applications for which these emerging semiconductors are clearly superior. In the effort to mitigate climate change, eliminating waste in power consumption is the low-hanging fruit, and these semiconductors are the way we’ll harvest it.
</p>
<p>
	This is a new instance of a familiar pattern in technology history: two competing innovations coming to fruition at the same time. How will it all shake out? In which applications will SiC dominate, and in which will GaN prevail? A hard look at the relative strengths of these two semiconductors gives us some solid clues.
</p>
<h2>Why Power Conversion Matters in Climate Calculations
</h2>
<p>
	Before we get to the semiconductors themselves, let’s first consider why we need them. To begin with: Power conversion is everywhere. And it goes far beyond the little wall chargers that sustain our smartphones, tablets, laptops, and countless other gadgets.
</p>
<p>
	Power conversion is the process that changes electricity from the form that’s available to the form required for a product to perform its function. Some energy is always lost in that conversion, and because some of these products run continuously, the energy savings can be enormous. Consider: Electricity consumption in the state of California remained essentially flat from 1980 even as the economic output of the state skyrocketed. One of the most important reasons why the demand remained flat is that the efficiency of refrigerators and air conditioners increased enormously over that period. The single-greatest factor in this improvement has been the use of variable-speed drives based on the insulated gate bipolar transistor (IGBT) and other power electronics, which greatly increased efficiency.
</p>
<div class="ieee-sidebar-large">
<h3>Gallium Nitride and Silicon Carbide: Where They Compete</h3>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A graph has fields of blue, green, and white to show applications in which GaN (blue) or SiC (white) dominate. A middle field of green shows where the two semiconductors are now competing, or will shortly." class="rm-shortcode" data-rm-shortcode-id="10af39afb2599b805233ce372a870198" data-rm-shortcode-name="rebelmouse-image" id="0fa6f" loading="lazy" src="https://spectrum.ieee.org/media-library/a-graph-has-fields-of-blue-green-and-white-to-show-applications-in-which-gan-blue-or-sic-white-dominate-a-middle-field-of.png?id=33331578&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">In the markets for high-voltage power transistors, gallium nitride devices dominate in applications below around 400 volts, while silicon carbide has the edge now for 800 V and above (the markets are relatively small above around 2,000 V). The landscape of the important battleground between 400 and 1,000 V will change as GaN devices improve. For example, with the introduction of 1,200-V GaN transistors—expected in 2025—the battle will be joined in the all-important market for electric-vehicle inverters.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chris Philpot</small>
</p>
</div>
<p>
	SiC and GaN are going to enable far greater reductions in emissions. GaN-based technologies alone could lead to a savings of over 1 billion tonnes of greenhouse gases in 2041 in just the 
	<a href="https://www.eia.gov/todayinenergy/detail.php?id=46676" target="_blank">United States</a> and <a href="https://www.iea.org/reports/india-energy-outlook-2021" target="_blank">India</a>, according to an analysis of publicly available data by Transphorm, a GaN-device company I cofounded in 2007. The data came from the International Energy Agency, Statista, and other sources. The same analysis indicates a 1,400-terawatt-hour energy savings—or 10 to 15 percent of the projected energy consumption by the two countries that year.
</p>
<h2>Wide-Bandgap’s Advantages 
</h2>
<p>
	Like an ordinary transistor, a power transistor can act as an amplifying device or as a switch. An important example of the amplifying role is in wireless base stations, which amplify signals for transmission to smartphones. All over the world, the semiconductor used to fabricate the transistors in these amplifiers is shifting from a silicon technology called laterally diffused metal-oxide semiconductor (LDMOS) to GaN. The newer technology has many advantages, including a power-efficiency improvement of 
	<a href="https://www.idtechex.com/en/research-article/5g-the-market-gan-needs/26288" target="_blank">10 percent</a> or more depending on frequencies. In power-conversion applications, on the other hand, the transistor acts as a switch rather than as an amplifier. The standard technique is called pulse-width modulation. In a common type of motor controller, for example, pulses of direct-current electricity are fed to coils mounted on the motor’s rotor. These pulses set up a magnetic field that interacts with that of the motor’s stator, which makes the rotor spin. The speed of this rotation is controlled by altering the length of the pulses: A graph of these pulses is a square wave, and the longer the pulses are “on” rather than “off,” the more rotational speed and torque the motor provides. Power transistors accomplish the on-and-off switching.
</p>
<div class="ieee-sidebar-medium">
<p class="shortcode-media shortcode-media-rebelmouse-image">
		This article was jointly produced by 
		<em>IEEE Spectrum</em> and <em>Proceedings of the IEEE </em>with similar versions published in both publications.
	</p>
</div>
<p>
	Pulse-width modulation is also used in switching power supplies, one of the most common examples of power conversion. Switching power supplies are the type used to power virtually all personal computers, mobile devices, and appliances that run on DC. Basically, the input AC voltage is converted to DC, and then that DC is “chopped” into a high-frequency alternating-current square wave. This chopping is done by power transistors, which create the square wave by switching the DC on and off. The square wave is applied to a transformer that changes the amplitude of the wave to produce the desired output voltage. To get a steady DC output, the voltage from the transformer is rectified and filtered.
</p>
<p>
	The important point here is that the characteristics of the power transistors determine, almost entirely, how well the circuits can perform pulse-width modulation—and therefore, how efficiently the controller regulates the voltage. An ideal power transistor would, when in the off state, completely block current flow even when the applied voltage is high. This characteristic is called high electric breakdown field strength, and it indicates how much voltage the semiconductor is able to withstand. On the other hand, when it is in the on state, this ideal transistor would have very low resistance to the flow of current. This feature results from very high mobility of the charges—electrons and holes—within the semiconductor’s crystalline lattice. Think of breakdown field strength and charge mobility as the yin and yang of a power semiconductor.
</p>
<p class="pull-quote">
	GaN transistors are very unusual because most of the current flowing through them is due to electron velocity rather than electron charge.
</p>
<p>
	GaN and SiC come much closer to this ideal than the silicon semiconductors they are replacing. First, consider breakdown field strength. Both GaN and SiC belong to a class called wide-bandgap semiconductors. The bandgap of a semiconductor is defined as the energy, in electron volts, needed for an electron in the semiconductor lattice to jump from the valence band to the conduction band. An electron in the valence band participates in the bonding of atoms within the crystal lattice, whereas in the conduction band electrons are free to move around in the lattice and conduct electricity.
</p>
<p>
	In a semiconductor with a wide bandgap, the bonds between atoms are strong and so the material is usually able to withstand relatively high voltages before the bonds break and the transistor is said to break down. The bandgap of silicon is 1.12 electron volts, as compared with 3.40 eV for GaN. For the most common type of SiC, the band gap is 3.26 eV. [See table below, “The Wide-Bandgap Menagerie”]
</p>
<div class="ieee-sidebar-large">
<h3>The Wide-Bandgap Menagerie</h3>
<div class="flourish-embed flourish-table" data-src="visualisation/12840392?672664">
<script src="https://public.flourish.studio/resources/embed.js">
</script>
</div>
<p class="caption">
	Speed of operation and the ability to block high voltage are two of the most important characteristics of a power transistor. These two qualities are in turn determined by key physical parameters of the semiconductor materials used to fabricate the transistor. Speed is determined by the mobility and velocity of charges in the semiconductor, while voltage blocking is established by the material’s bandgap and electric breakdown field.
Source: The Application of Third Generation Semiconductor in Power Industry, Yuqian Zhang,  E3S Web of Conferences, Volume 198, 2020
	</p>
</div>
<p>
	Now let’s look at mobility, which is given in units of centimeters squared per volt second (cm
	<sup>2</sup>/V·s). The product of mobility and electric field yields the velocity of the electron, and the higher the velocity the higher the current carried for a given amount of moving charge. For silicon this figure is 1,450; for SiC it is around 950; and for GaN, about 2,000. GaN’s unusually high value is the reason why it can be used not only in power-conversion applications but also in microwave amplifiers. GaN transistors can amplify signals with frequencies as high as 100 gigahertz—far above the 3 to 4 GHz generally regarded as the maximum for silicon LDMOS. For reference, 5G’s millimeter-wave frequencies top out at 52.6 GHz. This highest 5G band is not yet widely used, however, frequencies up to 75 GHz are being deployed in dish-to-dish communications, and researchers are now working with frequencies as high as 140 GHz for in-room communications. The appetite for bandwidth is insatiable.
</p>
<p>
	These performance figures are important, but they’re not the only criteria by which GaN and SiC should be compared for any particular application. Other critical factors include ease of use and cost, for both the devices and the systems into which they are integrated. Taken together, these factors explain where and why each of these semiconductors has begun displacing silicon—and how their future competition may shake out.
</p>
<h2>SiC Leads GaN in Power Conversion Today…
</h2>
<p>
	The first commercially viable SiC transistor that was superior to silicon was introduced by Cree (now Wolfspeed) in 2011. It could block 1,200 volts and had a respectably low resistance of 80 milliohms when conducting current. Today there are three different kinds of SiC transistors on the market. There’s a trench MOSFET (metal-oxide semiconductor field-effect transistor) from Rohm; DMOSs (double-diffused MOSs) from Infineon Technologies, ON Semiconductor Corp.,  STMicroelectronics, Wolfspeed, and others; and a vertical-junction field-effect transistor from Qorvo.
</p>
<p>
	One of the big advantages of SiC MOSFETs is their similarity to traditional silicon ones—even the packaging is identical. A SiC MOSFET operates in essentially the same way as an ordinary silicon MOSFET. There’s a source, a gate, and a drain. When the device is on, electrons flow from a heavily doped 
	<em>n</em>-type source across a lightly doped bulk region before being “drained” through a conductive substrate. This similarity means that there’s little learning curve for engineers making the switch to SiC.
</p>
<p>
	Compared to GaN, SiC has other advantages. SiC MOSFETs are inherently “fail-open” devices, meaning that if the control circuit fails for any reason the transistor stops conducting current. This is an important feature, because this characteristic largely eliminates the possibility that a failure could lead to a short circuit and a fire or explosion. (The price paid for this feature, however, is a lower electron mobility, which increases resistance when the device is on.)
</p>
<h2> …But GaN Is Gaining
</h2>
<p>
	GaN brings its own unique advantages. The semiconductor first established itself commercially in 2000 in the markets for light-emitting diodes and semiconductor lasers. It was the first semiconductor capable of reliably emitting bright green, blue, purple, and ultraviolet light. But long before this commercial breakthrough in optoelectronics, I and other researchers had already demonstrated the promise of GaN for high-power electronics. GaN LEDs caught on quickly because they filled a void for efficient lighting. But GaN for electronics had to prove itself superior to existing technologies: in particular, silicon CoolMOS transistors from Infineon for power electronics, and silicon-LDMOS and gallium-arsenide transistors for radio-frequency electronics.
</p>
<p>
	GaN’s main advantage is its extremely high electron mobility. Electric current, the flow of charge, equals the concentration of the charges multiplied by their velocity. So you can get high current because of high concentration or high velocity or some combination of the two. The GaN transistor is unusual because most of the current flowing through the device is due to electron velocity rather than charge concentration. What this means in practice is that, in comparison with Si or SiC, less charge has to flow into the device to switch it on or off. That, in turn, reduces the energy needed for each switching cycle and contributes to high efficiency.
</p>
<div class="ieee-sidebar-medium">
<h3>Enhancement-Mode GaN Transistor</h3>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A pair of illustrations shows the operation of an advanced gallium-nitride transistor." class="rm-shortcode" data-rm-shortcode-id="042e84b21115e0fb4b51889300990721" data-rm-shortcode-name="rebelmouse-image" id="d9604" loading="lazy" src="https://spectrum.ieee.org/media-library/a-pair-of-illustrations-shows-the-operation-of-an-advanced-gallium-nitride-transistor.gif?id=33331339&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">One of the two major types of gallium nitride transistor is called an enhancement-mode device. It uses a gate-control circuit operating at around 6 volts to control the main switching circuit, which can block 600 V or more when the control circuit is off. When the device is on (when 6 V are applied to the gate), electrons flow from the drain to the source in a flat region called a two-dimensional electron gas. In this region the electrons are extremely mobile—a factor that helps enable very high switching speeds—and confined beneath a barrier of aluminum gallium nitride. When the device is off, the region below the gate is depleted of electrons, breaking the circuit under the gate and stopping current flow.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chris Philpot</small>
</p>
</div>
<p>
	Meanwhile, GaN’s high electron mobility allows switching speeds on the order of 50 volts per nanosecond. That characteristic means power converters based on GaN transistors operate efficiently at frequencies in the multiple hundreds of kilohertz, as opposed to about 100 kilohertz for silicon or SiC.
</p>
<p>
	Taken together, the high efficiency and high frequency enables the power converter based on GaN devices to be quite small and lightweight: High efficiency means smaller heat sinks, and operation at high frequencies means that the inductors and capacitors can be very small, too.
</p>
<p>
	One disadvantage of GaN semiconductors is that they do not yet have a reliable insulator technology. This complicates the design of devices that are fail-safe—in other words, that fail open if the control circuit fails.
</p>
<p>
	There are two options to achieve this normally off characteristic. One is to equip the transistor with a type of gate that removes the charge in the channel when there’s no voltage applied to the gate and that conducts current only on application of a positive voltage to that gate. These are called enhancement-mode devices. They are offered by 
	<a href="https://epc-co.com/epc/" target="_blank">EPC</a>, <a href="https://gansystems.com/" target="_blank">GaN Systems</a>, <a href="https://www.infineon.com/cms/en/product/technology/gallium-nitride-gan/" target="_blank">Infineon</a>,<a href="https://www.innoscience.com/" target="_blank">Innoscience</a>, and <a href="https://navitassemi.com/" target="_blank">Navitas</a>, for example. [See illustration, "<span style="background-color: initial;">Enhancement</span>-<span style="background-color: initial;">Mode</span> <span style="background-color: initial;">GaN</span> <span style="background-color: initial;">Transistor</span>"]
</p>
<p>
	The other option is called the cascode solution. It uses a separate, low-loss silicon field-effect transistor to provide the fail-safe feature for the GaN transistor. This cascode solution is used by  
	<a href="https://www.power.com/company/our-innovations/powigan-technology" target="_blank">Power Integrations</a>, <a href="https://www.ti.com/power-management/gallium-nitride/overview.html" target="_blank">Texas Instruments</a>, and <a href="https://www.transphormusa.com/en/" target="_blank">Transphorm</a>. [See illustration, "Cascoded Depletion-Mode GaN Transistor"]
</p>
<div class="ieee-sidebar-medium">
<h3>Cascoded Depletion-Mode GaN Transistor</h3>
<p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="A pair of schematic illustrations shows the operation of an advanced gallium-nitride transistor." class="rm-shortcode" data-rm-shortcode-id="6fb2e3c8e4d3b4193c1f37485750033b" data-rm-shortcode-name="rebelmouse-image" id="4e068" loading="lazy" src="https://spectrum.ieee.org/media-library/a-pair-of-schematic-illustrations-shows-the-operation-of-an-advanced-gallium-nitride-transistor.gif?id=33331681&width=980"/>
<small class="image-media media-caption" placeholder="Add Photo Caption...">For safety, when a power transistor’s control circuit fails, it must fail into the open state, with no current flow. This is a challenge for gallium nitride devices because they lack a gate-insulator material that is reliable both in the high-voltage blocking state and in the current-carrying on state. One solution, called cascoded depletion-mode, uses a low-voltage signal on a silicon field-effect transistor (FET) to control the much larger voltage on a gallium nitride high electron mobility transistor [above right]. If the control circuit fails, the voltage on the gate of the FET drops to zero and it stops conducting current [above left]. With the FET no longer conducting current, the gallium nitride transistor also stops conducting, because there is no longer a closed circuit between the drain and the source of the combined device.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit...">Chris Philpot</small>
</p>
</div>
<p>
	No comparison of semiconductors is complete without a consideration of costs. A rough rule of thumb is—smaller die size means lower cost. Die size is the physical area of the integrated circuit containing the devices.
</p>
<p>
	SiC devices now generally have smaller dies than GaN ones. However, SiC’s substrate and fabrication costs are higher than those for GaN and, in general, the final device costs for applications at 5 kilowatts and higher are not much different today. Future trends, though, are likely to favor GaN. I base this belief on the relative simplicity of GaN devices, which will mean production costs low enough to overcome the larger die size.
</p>
<p>
	That said, for GaN to be viable for many high-power applications that also demand high voltages, it must have a cost-effective, high-performance device rated for 1,200 V. After all, there are already SiC transistors available at that voltage. Currently, the closest commercially available GaN transistors are rated for 900 V, produced by Transphorm, which I cofounded with 
	<a href="https://www.transphormchina.com/en/team/primit-parikh-ph-d/" target="_blank">Primit Parikh</a>. Lately, we have also demonstrated 1,200-V devices, fabricated on sapphire substrates, that have both electrical and thermal performance on a par with SiC devices.
</p>
<p>
	Projections from the research firm Omdia for 1,200-V SiC MOSFETs indicate a price of 16 cents per ampere in 2025. In my estimation, because of the lower cost of GaN substrates, the price of first-generation 1,200-V GaN transistors in 2025 will be less than that of their SiC counterparts. Of course, that’s just my opinion; we’ll all know for sure how this will shake out in a couple of years.
</p>
<h2>GaN vs. SiC: Handicapping the Contests
</h2>
<p>
	With these relative advantages and disadvantages in mind, let’s consider individual applications, one by one, and shed some light on how things might develop.
</p>
<p>
	• 
	<strong>Electric vehicle inverters and converters</strong>: Tesla’s adoption of SiC in 2017 for the onboard, or traction, inverters for its Model 3 was an early and major win for the semiconductor. In an EV, the traction inverter converts the DC from the batteries to AC for the motor. The inverter also controls the speed of the motor by varying the frequency of the alternating current. Today, Mercedes-Benz and Lucid Motors are also using SiC in their inverters and other EV makers are planning to use SiC in upcoming models, according to news reports. The SiC devices are being supplied by Infineon, OnSemi, Rohm, Wolfspeed, and others. EV traction inverters typically range from about 35 kW to 100 kW for a small EV to about 400 kW for a large vehicle.
</p>
<p>
	However, it’s too soon to call this contest for SiC. As I noted, to make inroads in this market, GaN suppliers will have to offer a 1,200-V device. EV electrical systems now typically operate at just 400 volts, but the Porsche Taycan has an 800-V system, as do EVs from Audi, Hyundai, and Kia. Other automakers are expected to follow their lead in coming years. (The Lucid Air has a 900-V system.) I expect to see the first commercial 1,200-V GaN transistors in 2025. These devices will be used not only in vehicles but also in high-speed public EV chargers.
</p>
<p>
	The higher switching speeds possible with GaN will be a powerful advantage in EV inverters, because these switches employ what are called hard-switched techniques. Here, the way to enhance performance is to switch very fast from on to off to minimize the time when the device is both holding high voltage 
	<em>and</em> passing high current.
</p>
<p>
	Besides an inverter, an EV also typically has an 
	<strong>onboard charger</strong>, which enables the vehicle to be charged from wall (mains) current by converting AC to DC. Here, again, GaN is very attractive, for the same reasons that make it a good choice for inverters.
</p>
<p>
	• 
	<strong>Electric-grid applications</strong>: Very-high-voltage power conversion for devices rated at 3 kV and higher will remain the domain of SiC for at least the next decade. These applications include systems to help stabilize the grid, convert AC to DC and back again at transmission-level voltages, and other uses.
</p>
<p>
	• 
	<strong>Phone, tablet, and laptop chargers</strong>: Starting in 2019, GaN-based wall chargers became available commercially from companies such as GaN Systems, Innoscience, Navitas, Power Integrations, and Transphorm. The high switching speeds of GaN coupled with its generally lower costs have made it the incumbent in lower-power markets (25 to 500 W), where these factors, along with small size and a robust supply chain, are paramount. These early GaN power converters had switching frequencies as high as 300 kHz and efficiencies above 92 percent. They set records for power density, with figures as high as 30 W per cubic inch (1.83 W/cm<sup>3</sup>)—roughly double the density of the silicon-based chargers they are replacing.
</p>
<p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Devices on an advanced semiconductor wafer are tested with probes. " class="rm-shortcode rm-resized-image" data-rm-shortcode-id="f151fdfe26b1e81abfca477d3a26a90b" data-rm-shortcode-name="rebelmouse-image" id="77654" loading="lazy" src="https://spectrum.ieee.org/media-library/devices-on-an-advanced-semiconductor-wafer-are-tested-with-probes.png?id=33330349&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">An automated system of probes applies a high voltage to stress test power transistors on a wafer. The automated system, at Transphorm, tests each one of some 500 die in minutes.  </small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Peter Adams</small>
</p>
<p>
	• 
	<strong>Solar-power microinverters</strong>: Solar-power generation has taken off in recent years, in both grid-scale and distributed (household) applications. For every installation, an inverter is needed to convert the DC from the solar panels to AC to power a home or release the electricity to the grid. Today, grid-scale photovoltaic inverters are the domain of silicon IGBTs and SiC MOSFETs. But GaN will begin making inroads in the distributed solar market, particularly.
</p>
<p>
	Traditionally, in these distributed installations, there was a single inverter box for all of the solar panels. But increasingly installers are favoring systems in which there is a separate microinverter for each panel, and the AC is combined before powering the house or feeding the grid. Such a setup means the system can monitor the operation of each panel in order to optimize the performance of the whole array.
</p>
<p>
	Microinverter or traditional inverter systems are critical to the modern data center. Coupled with batteries they create an 
	<strong>uninterruptible power supply</strong> to prevent outages. Also, all data centers use power-factor correction circuits, which adjust the power supply’s alternating-current waveforms to improve efficiency and remove characteristics that could damage equipment. And for these, GaN provides a low-loss and economical solution that is slowly displacing silicon.
</p>
<p>
	• 
	<strong>5G and 6G base stations</strong>: GaN’s superior speed and high power density will enable it to win and ultimately dominate applications in the microwave regimes, notably 5G and 6G wireless, and commercial and military radar. The main competition here are arrays of silicon LDMOS devices, which are cheaper but have lower performance. Indeed, GaN has no real competitor at frequencies of 4 GHz and above.
</p>
<p>
	For 5G and 6G wireless, the critical parameter is bandwidth, because it determines how much information the hardware can transmit efficiently. Next-generation 5G systems will have nearly 1 GHz of bandwidth, enabling blazingly fast video and other applications.
</p>
<p>
	Microwave-communication systems that use silicon-on-insulator technologies provide a 5G+ solution using high-frequency silicon devices where each device’s low output power is overcome with large arrays of them. GaN and silicon will coexist for a while in this space. The winner in a specific application will be determined by a trade-off among system architecture, cost, and performance.
</p>
<p>
	• 
	<strong>Radar</strong>: The U.S. military is deploying many ground-based radar systems based on GaN electronics. These include the Ground/Air Task Oriented Radar and the Active Electronically Scanned Array Radar built by Northrup-Grumman for the U.S. Marine Corps. Raytheon’s SPY6 radar was delivered to the U.S. Navy and tested for the first time at sea in December 2022. The system greatly extends the range and sensitivity of shipborne radar.
</p>
<h2>The Wide-Bandgap Battle Is Just Beginning
</h2>
<p>
	Today, SiC dominates in EV inverters, and generally wherever voltage-blocking capability and power handling are paramount and where the frequency is low. GaN is the preferred technology where high-frequency performance matters, such as in base stations for 5G and 6G, and for radar and high-frequency power-conversion applications such as wall-plug adapters, microinverters, and power supplies.
</p>
<p>
	But the tug-of-war between GaN and SiC is just beginning. Regardless of how the competition plays out, application by application and market by market, we can say for sure that the Earth’s environment will be a winner. Countless billions of tonnes of greenhouse gases will be avoided in coming years as this new cycle of technological replacement and rejuvenation wends its way inexorably forward. 
	<span class="ieee-end-mark"></span>
</p>]]></description>
      <pubDate>Sun, 26 Mar 2023 15:00:01 +0000</pubDate>
      <guid>https://spectrum.ieee.org/silicon-carbide</guid>
      <category>Wide bandgap semiconductors</category>
      <category>Silicon carbide</category>
      <category>Gallium nitride</category>
      <category>Power semiconductors</category>
      <category>Advanced semiconductors</category>
      <category>Power conversion</category>
      <category>Inverters</category>
      <dc:creator>Umesh K. Mishra</dc:creator>
      <media:content medium="image" type="image/png" url="https://spectrum.ieee.org/media-library/photo-of-a-man-in-glasses-and-jacket-sitting-in-front-of-semiconductor-wafers.png?id=33329925&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>We’re the New Renewables</title>
      <link>https://spectrum.ieee.org/thermoelectric-generators</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/image.png?id=33322337&width=980"/><br/><br/><p>The Big Picture features technology through the lens of photographers.</p><p>Every month, <em><a href="https://spectrum.ieee.org/" target="_self">IEEE Spectrum</a></em> selects the most stunning technology images recently captured by photographers around the world. We choose images that reflect an important advance, or a trend, or that are just mesmerizing to look at. We feature all images on our site, and one also appears on our monthly print edition.</p><p>Enjoy the latest images, and if you have suggestions, leave a comment below.</p><h3></h3><br/><img alt="A metal box sits on a rooftop under cloudy skies" class="rm-shortcode" data-rm-shortcode-id="278ed27048a2521d7442f3d57d767694" data-rm-shortcode-name="rebelmouse-image" id="2647e" loading="lazy" src="https://spectrum.ieee.org/media-library/a-metal-box-sits-on-a-rooftop-under-cloudy-skies.png?id=33322326&width=980"/><h2>Juice Box</h2><p><span style="background-color: initial;">For many years, environmentalists have looked forward to the coming of net-zero-energy buildings. Much effort was devoted to making lighting, heating, and cooling more efficient so buildings consumed less energy. But the net-zero target would never have been reachable without innovations in </span><span style="background-color: initial;">renewable-</span><span style="background-color: initial;">energy generation that let structures generate power </span><span style="background-color: initial;">on-</span><span style="background-color: initial;">site. Now residential and commercial buildings can be outfitted with roofing tiles that double as solar panels, or with rooftop boxes like this low-profile unit that transforms gusts of wind into electric current. This </span><span style="background-color: initial;">WindBox</span> turbine, installed on the roof of a building in Rouen, France, is 1.6 meters tall, and has a 4-square-meter footprint (leaving plenty of space for solar panels or tiles). The unit, which weighs130 kilograms, can generate up to 2,500 kilowatt-hours of electricity per year (enough to meet roughly one-quarter of the energy needs of a typical U.S. household).</p><p class="caption">Lou Benoist/AFP/Getty Images </p><h3></h3><br/><img alt="Photo of men standing on a antenna looking up at it." class="rm-shortcode" data-rm-shortcode-id="b27cb44e1ca272cb7382ebd529a46361" data-rm-shortcode-name="rebelmouse-image" id="07f19" loading="lazy" src="https://spectrum.ieee.org/media-library/photo-of-men-standing-on-a-antenna-looking-up-at-it.png?id=33323058&width=980"/><h2><a href="https://spectrum.ieee.org/cosmic-microwave-background" target="_blank">Nobel Horn Antenna Endangered</a></h2><p>
	This is the giant horn antenna that was used in physics research that led to the discovery of background cosmic radiation, which provided support for the big bang theory. Two Bell Labs researchers who were painstakingly attempting to eliminate noise from certain radio signals eventually realized that the noise didn’t arise from an antenna malfunction. It was, in fact, an artifact of the big bang, which created the cosmos. Now, this antenna, which was critical to their work, is under threat of being dismantled. The Holmdel, N.J., research site is now in private hands, and could be slated for rezoning and redevelopment, which might doom the instrument that made the Nobel Prize–winning discovery possible.
</p><p class="caption">
	Bettmann/Getty Images
</p><h3></h3><br/><img alt="Two hands in rubber gloves stretching a sheet with a zebra on it." class="rm-shortcode" data-rm-shortcode-id="3544b90dc0c9178699bcaccd295c6792" data-rm-shortcode-name="rebelmouse-image" id="db539" loading="lazy" src="https://spectrum.ieee.org/media-library/two-hands-in-rubber-gloves-stretching-a-sheet-with-a-zebra-on-it.png?id=33323056&width=980"/><h2><a href="https://spectrum.ieee.org/wearable-technology-biodegradable-power-source" target="_blank">You, the Power Source</a> </h2><p>
	The incandescent lightbulb, in addition to being a world-changing invention, is the prototypical example of something that wastes a lot of energy, giving it off as heat instead of light. Our bodies slough off a ton of heat too. Because generating heat is an inescapable part of our metabolic processes, researchers have been working to turn a lightbulb moment—the idea of harnessing body heat so all that thermal energy isn’t wasted—into a practical device that yields electric power. Thermoelectric generators, or TEGs, have, in fact, been around for a while. But a new generation of TEGs uses cheaper, less toxic materials that convert both heat and kinetic energy to electricity with greater efficiency than earlier versions. On the TEG pictured here, hot and cold regions represented by the zebra’s stripes produce a temperature gradient that creates a voltage difference. The result: electrical current. Its creators, researchers at Gwangju Institute of Science and Technology, in South Korea, envision weaving such devices into fabrics so that someday our garments will double as power outlets for our ubiquitous portable electronic gadgets.
</p><p class="caption">
	Korea University
</p><h3></h3><br/><img alt="Elve's nano-composite scandate tungsten cathode." class="rm-shortcode" data-rm-shortcode-id="e6c7bddeb212bcb0c8aa583d4caba4fc" data-rm-shortcode-name="rebelmouse-image" id="53fde" loading="lazy" src="https://spectrum.ieee.org/media-library/elve-s-nano-composite-scandate-tungsten-cathode.png?id=33323075&width=980"/><h2><a href="https://spectrum.ieee.org/millimeter-wave-power-amplifier-startup" target="_blank">3D Printed Amps for mm-Wave Wireless on the Cheap</a> </h2><p>Millimeter-wave power amplifiers are necessary for applications that require the highest data rates possible—mostly communications across vast distances. But with a price tag topping US $1 million each, it is easy to see why there is an urgent push for production innovation to bring the cost way down. Diana Gamzina, founder and CEO of Davis, Calif., startup Elve Speed, embodies the mission to bring ultrahigh-speed wireless connectivity to remote and urban areas. To accomplish this, her company has taken to 3D printing millimeter-wave amps to sidestep the manual, high-precision manufacturing processes that make the price of the amps so stratospheric.  </p><p class="caption">Gabriela Hasbun  </p>]]></description>
      <pubDate>Fri, 24 Mar 2023 20:00:05 +0000</pubDate>
      <guid>https://spectrum.ieee.org/thermoelectric-generators</guid>
      <category>The big picture</category>
      <category>Antennas</category>
      <category>Net zero energy</category>
      <category>Millimeter wave power amplifiers</category>
      <dc:creator>Willie Jones</dc:creator>
      <media:content medium="image" type="image/png" url="https://assets.rbl.ms/33322337/origin.png">
      </media:content>
    </item>
    <item>
      <title>Andrew Ng: Unbiggen AI</title>
      <link>https://spectrum.ieee.org/andrew-ng-data-centric-ai</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&width=1200&height=800&coordinates=0%2C0%2C0%2C210"/><br/><br/><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer nofollow" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer nofollow" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer nofollow" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer nofollow" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&A.</p><hr/><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer nofollow" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br/>
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer nofollow" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br/>
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer nofollow" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br/>
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer nofollow" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer nofollow" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p>]]></description>
      <pubDate>Wed, 09 Feb 2022 15:31:12 +0000</pubDate>
      <guid>https://spectrum.ieee.org/andrew-ng-data-centric-ai</guid>
      <category>Andrew ng</category>
      <category>Artificial intelligence</category>
      <category>Deep learning</category>
      <category>Type:cover</category>
      <dc:creator>Eliza Strickland</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>How AI Will Change Chip Design</title>
      <link>https://spectrum.ieee.org/ai-chip-design-matlab</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>The end of <a href="https://spectrum.ieee.org/on-beyond-moores-law-4-new-laws-of-computing" target="_self">Moore’s Law</a> is looming. Engineers and designers can do only so much to <a href="https://spectrum.ieee.org/ibm-introduces-the-worlds-first-2nm-node-chip" target="_self">miniaturize transistors</a> and <a href="https://spectrum.ieee.org/cerebras-giant-ai-chip-now-has-a-trillions-more-transistors" target="_self">pack as many of them as possible into chips</a>. So they’re turning to other approaches to chip design, incorporating technologies like AI into the process.</p><p>Samsung, for instance, is <a href="https://spectrum.ieee.org/processing-in-dram-accelerates-ai" target="_self">adding AI to its memory chips</a> to enable processing in memory, thereby saving energy and speeding up machine learning. Speaking of speed, Google’s TPU V4 AI chip has <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">doubled its processing power</a> compared with that of  its previous version.</p><p>But AI holds still more promise and potential for the semiconductor industry. To better understand how AI is set to revolutionize chip design, we spoke with <a href="https://www.linkedin.com/in/heather-gorr-phd" rel="noopener noreferrer" target="_blank">Heather Gorr</a>, senior product manager for <a href="https://www.mathworks.com/" rel="noopener noreferrer" target="_blank">MathWorks</a>’ MATLAB platform.</p><p><strong>How is AI currently being used to design the next generation of chips?</strong></p><p><strong>Heather Gorr:</strong> AI is such an important technology because it’s involved in most parts of the cycle, including the design and manufacturing process. There’s a lot of important applications here, even in the general process engineering where we want to optimize things. I think defect detection is a big one at all phases of the process, especially in manufacturing. But even thinking ahead in the design process, [AI now plays a significant role] when you’re designing the light and the sensors and all the different components. There’s a lot of anomaly detection and fault mitigation that you really want to consider.</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Portrait of a woman with blonde-red hair smiling at the camera" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="1f18a02ccaf51f5c766af2ebc4af18e1" data-rm-shortcode-name="rebelmouse-image" id="2dc00" loading="lazy" src="https://spectrum.ieee.org/media-library/portrait-of-a-woman-with-blonde-red-hair-smiling-at-the-camera.jpg?id=29288554&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Heather Gorr</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">MathWorks</small></p><p>Then, thinking about the logistical modeling that you see in any industry, there is always planned downtime that you want to mitigate; but you also end up having unplanned downtime. So, looking back at that historical data of when you’ve had those moments where maybe it took a bit longer than expected to manufacture something, you can take a look at all of that data and use AI to try to identify the proximate cause or to see  something that might jump out even in the processing and design phases. We think of AI oftentimes as a predictive tool, or as a robot doing something, but a lot of times you get a lot of insight from the data through AI.</p><p><strong>What are the benefits of using AI for chip design?</strong></p><p><strong>Gorr:</strong> Historically, we’ve seen a lot of physics-based modeling, which is a very intensive process. We want to do a <a href="https://en.wikipedia.org/wiki/Model_order_reduction" rel="noopener noreferrer" target="_blank">reduced order model</a>, where instead of solving such a computationally expensive and extensive model, we can do something a little cheaper. You could create a surrogate model, so to speak, of that physics-based model, use the data, and then do your <a href="https://institutefordiseasemodeling.github.io/idmtools/parameter-sweeps.html" rel="noopener noreferrer" target="_blank">parameter sweeps</a>, your optimizations, your <a href="https://www.ibm.com/cloud/learn/monte-carlo-simulation" rel="noopener noreferrer" target="_blank">Monte Carlo simulations</a> using the surrogate model. That takes a lot less time computationally than solving the physics-based equations directly. So, we’re seeing that benefit in many ways, including the efficiency and economy that are the results of iterating quickly on the experiments and the simulations that will really help in the design.</p><p><strong>So it’s like having a digital twin in a sense?</strong></p><p><strong>Gorr:</strong> Exactly. That’s pretty much what people are doing, where you have the physical system model and the experimental data. Then, in conjunction, you have this other model that you could tweak and tune and try different parameters and experiments that let sweep through all of those different situations and come up with a better design in the end.</p><p><strong>So, it’s going to be more efficient and, as you said, cheaper?</strong></p><p><strong>Gorr:</strong> Yeah, definitely. Especially in the experimentation and design phases, where you’re trying different things. That’s obviously going to yield dramatic cost savings if you’re actually manufacturing and producing [the chips]. You want to simulate, test, experiment as much as possible without making something using the actual process engineering.</p><p><strong>We’ve talked about the benefits. How about the drawbacks?</strong></p><p><strong>Gorr: </strong>The [AI-based experimental models] tend to not be as accurate as physics-based models. Of course, that’s why you do many simulations and parameter sweeps. But that’s also the benefit of having that digital twin, where you can keep that in mind—it's not going to be as accurate as that precise model that we’ve developed over the years.</p><p>Both chip design and manufacturing are system intensive; you have to consider every little part. And that can be really challenging. It's a case where you might have models to predict something and different parts of it, but you still need to bring it all together.</p><p>One of the other things to think about too is that you need the data to build the models. You have to incorporate data from all sorts of different sensors and different sorts of teams, and so that heightens the challenge.</p><p><strong>How can engineers use AI to better prepare and extract insights from hardware or sensor data?</strong></p><p><strong>Gorr: </strong>We always think about using AI to predict something or do some robot task, but you can use AI to come up with patterns and pick out things you might not have noticed before on your own. People will use AI when they have high-frequency data coming from many different sensors, and a lot of times it’s useful to explore the frequency domain and things like data synchronization or resampling. Those can be really challenging if you’re not sure where to start.</p><p>One of the things I would say is, use the tools that are available. There’s a vast community of people working on these things, and you can find lots of examples [of applications and techniques] on <a href="https://github.com/" rel="noopener noreferrer" target="_blank">GitHub</a> or <a href="https://www.mathworks.com/matlabcentral/" rel="noopener noreferrer" target="_blank">MATLAB Central</a>, where people have shared nice examples, even little apps they’ve created. I think many of us are buried in data and just not sure what to do with it, so definitely take advantage of what’s already out there in the community. You can explore and see what makes sense to you, and bring in that balance of domain knowledge and the insight you get from the tools and AI.</p><p><strong>What should engineers and designers consider wh</strong><strong>en using AI for chip design?</strong></p><p><strong>Gorr:</strong> Think through what problems you’re trying to solve or what insights you might hope to find, and try to be clear about that. Consider all of the different components, and document and test each of those different parts. Consider all of the people involved, and explain and hand off in a way that is sensible for the whole team.</p><p><strong>How do you think AI will affect chip designers’ jobs?</strong></p><p><strong>Gorr:</strong> It’s going to free up a lot of human capital for more advanced tasks. We can use AI to reduce waste, to optimize the materials, to optimize the design, but then you still have that human involved whenever it comes to decision-making. I think it’s a great example of people and technology working hand in hand. It’s also an industry where all people involved—even on the manufacturing floor—need to have some level of understanding of what’s happening, so this is a great industry for advancing AI because of how we test things and how we think about them before we put them on the chip.</p><p><strong>How do you envision the future of AI and chip design?</strong></p><p><strong>Gorr</strong><strong>:</strong> It's very much dependent on that human element—involving people in the process and having that interpretable model. We can do many things with the mathematical minutiae of modeling, but it comes down to how people are using it, how everybody in the process is understanding and applying it. Communication and involvement of people of all skill levels in the process are going to be really important. We’re going to see less of those superprecise predictions and more transparency of information, sharing, and that digital twin—not only using AI but also using our human knowledge and all of the work that many people have done over the years.</p>]]></description>
      <pubDate>Tue, 08 Feb 2022 14:00:01 +0000</pubDate>
      <guid>https://spectrum.ieee.org/ai-chip-design-matlab</guid>
      <category>Chip fabrication</category>
      <category>Moore’s law</category>
      <category>Chip design</category>
      <category>Ai</category>
      <category>Matlab</category>
      <category>Digital twins</category>
      <dc:creator>Rina Diane Caballar</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/layered-rendering-of-colorful-semiconductor-wafers-with-a-bright-white-light-sitting-on-one.jpg?id=29285079&amp;width=980">
      </media:content>
    </item>
    <item>
      <title>Atomically Thin Materials Significantly Shrink Qubits</title>
      <link>https://spectrum.ieee.org/2d-hbn-qubit</link>
      <description><![CDATA[
<img src="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&width=1200&height=800&coordinates=0%2C0%2C0%2C0"/><br/><br/><p>Quantum computing is a devilishly complex technology, with many technical hurdles impacting its development. Of these challenges two critical issues stand out: miniaturization and qubit quality.</p><p>IBM has adopted the superconducting qubit road map of <a href="https://spectrum.ieee.org/ibms-envisons-the-road-to-quantum-computing-like-an-apollo-mission" target="_self">reaching a 1,121-qubit processor by 2023</a>, leading to the expectation that 1,000 qubits with today’s qubit form factor is feasible. However, current approaches will require very large chips (50 millimeters on a side, or larger) at the scale of small wafers, or the use of chiplets on multichip modules. While this approach will work, the aim is to attain a better path toward scalability.</p><p>Now researchers at <a href="https://www.nature.com/articles/s41563-021-01187-w" rel="noopener noreferrer" target="_blank">MIT have been able to both reduce the size of the qubits</a> and done so in a way that reduces the interference that occurs between neighboring qubits. The MIT researchers have increased the number of superconducting qubits that can be added onto a device by a factor of 100.</p><p>“We are addressing both qubit miniaturization and quality,” said <a href="https://equs.mit.edu/william-d-oliver/" rel="noopener noreferrer" target="_blank">William Oliver</a>, the director for the <a href="https://cqe.mit.edu/" target="_blank">Center for Quantum Engineering</a> at MIT. “Unlike conventional transistor scaling, where only the number really matters, for qubits, large numbers are not sufficient, they must also be high-performance. Sacrificing performance for qubit number is not a useful trade in quantum computing. They must go hand in hand.”</p><p>The key to this big increase in qubit density and reduction of interference comes down to the use of two-dimensional materials, in particular the 2D insulator hexagonal boron nitride (hBN). The MIT researchers demonstrated that a few atomic monolayers of hBN can be stacked to form the insulator in the capacitors of a superconducting qubit.</p><p>Just like other capacitors, the capacitors in these superconducting circuits take the form of a sandwich in which an insulator material is sandwiched between two metal plates. The big difference for these capacitors is that the superconducting circuits can operate only at extremely low temperatures—less than 0.02 degrees above absolute zero (-273.15 °C).</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" data-rm-resized-container="25%" style="float: left;">
<img alt="Golden dilution refrigerator hanging vertically" class="rm-shortcode rm-resized-image" data-rm-shortcode-id="694399af8a1c345e51a695ff73909eda" data-rm-shortcode-name="rebelmouse-image" id="6c615" loading="lazy" src="https://spectrum.ieee.org/media-library/golden-dilution-refrigerator-hanging-vertically.jpg?id=29281593&width=980" style="max-width: 100%"/>
<small class="image-media media-caption" placeholder="Add Photo Caption..." style="max-width: 100%;">Superconducting qubits are measured at temperatures as low as 20 millikelvin in a dilution refrigerator.</small><small class="image-media media-photo-credit" placeholder="Add Photo Credit..." style="max-width: 100%;">Nathan Fiske/MIT</small></p><p>In that environment, insulating materials that are available for the job, such as PE-CVD silicon oxide or silicon nitride, have quite a few defects that are too lossy for quantum computing applications. To get around these material shortcomings, most superconducting circuits use what are called coplanar capacitors. In these capacitors, the plates are positioned laterally to one another, rather than on top of one another.</p><p>As a result, the intrinsic silicon substrate below the plates and to a smaller degree the vacuum above the plates serve as the capacitor dielectric. Intrinsic silicon is chemically pure and therefore has few defects, and the large size dilutes the electric field at the plate interfaces, all of which leads to a low-loss capacitor. The lateral size of each plate in this open-face design ends up being quite large (typically 100 by 100 micrometers) in order to achieve the required capacitance.</p><p>In an effort to move away from the large lateral configuration, the MIT researchers embarked on a search for an insulator that has very few defects and is compatible with superconducting capacitor plates.</p><p>“We chose to study hBN because it is the most widely used insulator in 2D material research due to its cleanliness and chemical inertness,” said colead author <a href="https://equs.mit.edu/joel-wang/" rel="noopener noreferrer" target="_blank">Joel Wang</a>, a research scientist in the Engineering Quantum Systems group of the MIT Research Laboratory for Electronics. </p><p>On either side of the hBN, the MIT researchers used the 2D superconducting material, niobium diselenide. One of the trickiest aspects of fabricating the capacitors was working with the niobium diselenide, which oxidizes in seconds when exposed to air, according to Wang. This necessitates that the assembly of the capacitor occur in a glove box filled with argon gas.</p><p>While this would seemingly complicate the scaling up of the production of these capacitors, Wang doesn’t regard this as a limiting factor.</p><p>“What determines the quality factor of the capacitor are the two interfaces between the two materials,” said Wang. “Once the sandwich is made, the two interfaces are “sealed” and we don’t see any noticeable degradation over time when exposed to the atmosphere.”</p><p>This lack of degradation is because around 90 percent of the electric field is contained within the sandwich structure, so the oxidation of the outer surface of the niobium diselenide does not play a significant role anymore. This ultimately makes the capacitor footprint much smaller, and it accounts for the reduction in cross talk between the neighboring qubits.</p><p>“The main challenge for scaling up the fabrication will be the wafer-scale growth of hBN and 2D superconductors like [niobium diselenide], and how one can do wafer-scale stacking of these films,” added Wang.</p><p>Wang believes that this research has shown 2D hBN to be a good insulator candidate for superconducting qubits. He says that the groundwork the MIT team has done will serve as a road map for using other hybrid 2D materials to build superconducting circuits.</p>]]></description>
      <pubDate>Mon, 07 Feb 2022 16:12:05 +0000</pubDate>
      <guid>https://spectrum.ieee.org/2d-hbn-qubit</guid>
      <category>Qubits</category>
      <category>Mit</category>
      <category>Ibm</category>
      <category>Superconducting qubits</category>
      <category>Hexagonal boron nitride</category>
      <category>2d materials</category>
      <category>Quantum computing</category>
      <dc:creator>Dexter Johnson</dc:creator>
      <media:content medium="image" type="image/jpeg" url="https://spectrum.ieee.org/media-library/a-golden-square-package-holds-a-small-processor-sitting-on-top-is-a-metal-square-with-mit-etched-into-it.jpg?id=29281587&amp;width=980">
      </media:content>
    </item>
  </channel>
</rss>