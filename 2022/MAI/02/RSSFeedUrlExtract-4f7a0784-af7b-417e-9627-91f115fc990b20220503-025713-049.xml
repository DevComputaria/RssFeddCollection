<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?>
<?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" version="2.0">
  <channel>
    <title>Radar</title>
    <link>https://www.oreilly.com/radar</link>
    <description>Now, next, and beyond: Tracking need-to-know trends at the intersection of business and technology</description>
    <lastBuildDate>Tue, 26 Apr 2022 17:50:28 +0000</lastBuildDate>
    <language>en-US</language>
    <sy:updatePeriod>
	hourly	</sy:updatePeriod>
    <sy:updateFrequency>
	1	</sy:updateFrequency>
    <generator>https://wordpress.org/?v=5.3.12</generator>
    <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/oreilly/radar/atom" />
    <feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="oreilly/radar/atom" />
    <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" />
    <geo:lat>38.393314</geo:lat>
    <geo:long>-122.836667</geo:long>
    <feedburner:emailServiceId xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">oreilly/radar/atom</feedburner:emailServiceId>
    <feedburner:feedburnerHostname xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">https://feedburner.google.com</feedburner:feedburnerHostname>
    <item>
      <title>Building a Better Middleman</title>
      <link>https://www.oreilly.com/radar/building-a-better-middleman/</link>
      <comments>https://www.oreilly.com/radar/building-a-better-middleman/#respond</comments>
      <pubDate>Tue, 19 Apr 2022 12:22:21 +0000</pubDate>
      <dc:creator><![CDATA[Q McCallum]]></dc:creator>
      <category><![CDATA[Operations]]></category>
      <category><![CDATA[Deep Dive]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14442</guid>
      <description>What comes to mind when you hear the term &amp;#8220;two-sided market?&amp;#8221; Maybe you imagine a Party A who needs something, so they interact with Party B who provides it, and that&amp;#8217;s that.&amp;#160; Despite the number &amp;#8220;two&amp;#8221; in the name, there&amp;#8217;s actually someone else involved: the middleman.&amp;#160; This entity sits between the parties to make it [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>What comes to mind when you hear the term &#8220;two-sided market?&#8221; Maybe you imagine a Party A who needs something, so they interact with Party B who provides it, and that&#8217;s that.&nbsp; Despite the number &#8220;two&#8221; in the name, there&#8217;s actually someone else involved: the <em>middleman</em>.&nbsp; This entity sits between the parties to make it easier for them to interact. (We can generalize that &#8220;two&#8221; to some arbitrary number and call this an <em>N-sided market</em> or <em>multi-sided marketplace</em>. But we&#8217;ll focus on the two-sided form for now.)</p>



<p>Two-sided markets are a fascinating study. They are also quite common in the business world, and therefore, so are middlemen. Record labels, rideshare companies, even dating apps all fall under this umbrella.&nbsp; The role has plenty of perks, as well as some sizable pitfalls.&nbsp; &#8220;Middleman&#8221; often carries a negative connotation because, in all fairness, some of them provide little value compared to what they ask in return.</p>



<p>Still, there&#8217;s room for everyone involved—Party A, Party B, and the middleman—to engage in a happy and healthy relationship.&nbsp; In this first article, I&#8217;ll explain more about the middleman&#8217;s role and the challenges they face.&nbsp; In the next article, I&#8217;ll explore what it takes to make a better middleman and how technology can play a role.</p>



<h2>Paving the Path</h2>



<p>When I say that middlemen make interactions easier, I mean that they address a variety of barriers:</p>



<ul><li><strong>Discovery: &#8220;Where do I find the other side of my need or transaction?&#8221;</strong> Dating apps like OKCupid, classified ads services such as Craigslist, and directory sites like Angi (formerly Angie&#8217;s List) are all a twist on a search engine. Party A posts a description of themself or their service, Party B scrolls and sifts the list while evaluating potential matches for fit.<br></li><li><strong>Matching: &#8220;Should we interact? Are our needs compatible?&#8221;</strong> Many middlemen that help with discovery also handle the matching for you, as with ride-share apps.&nbsp; Instead of you having to scroll through lists of drivers, Uber and Lyft use your phone&#8217;s GPS to pair you with someone nearby.&nbsp; (Compared to the Discovery case, Matching works best when one or both counterparties are easily interchangeable.)<br></li><li><strong>Standardization: &#8220;The middleman sets the rules of engagement, so we all know what to expect.&#8221;</strong>&nbsp; A common example would be when a middleman like eBay sets the accepted methods of payment.&nbsp; By narrowing the scope of what&#8217;s possible—by limiting options—the middleman standardizes how the parties interact.<br></li><li><strong>Safety: &#8220;I don&#8217;t have to know you in order to exchange money with you.&#8221;</strong> Stock market exchanges and credit card companies build trust with Party A and Party B, individually, so the two parties (indirectly) trust each other through the transitive property.<br></li><li><strong>Simplicity:</strong> &#8220;You two already know each other; I&#8217;ll insert myself into the middle, to make the relationship smoother.&#8221; Stripe and Squarespace make it easier for companies to sell goods and services by handling payments.&nbsp; And then there&#8217;s Squire, which co-founder Songe Laron describes as the &#8220;<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://youtu.be/5R_sya8bFP8?t=21" target="_blank">operating system for the barber shop</a>, [handling] everything from the booking, to the payment, to the point of sales system, to payroll,&#8221; and a host of other frictions between barber and customer.&nbsp; In all cases, each party gets to focus on what it does best (selling goods or cutting hair) while the middleman handles the drudgework.</li></ul>



<h2>Nice Work, If You can Get It</h2>



<p>As far as their business model, middlemen usually take a cut of transactions as value moves from Party A to Party B. And this arrangement has its benefits.</p>



<p>For one, you&#8217;re first in line to get paid: Party A pays you, you take a cut, then you pass the rest on to Party B.&nbsp; Record labels and book publishers are a common example.&nbsp; They pair a creator with an audience.&nbsp; All of the business deals for that creator&#8217;s work run through the middleman, who collects the revenue from sales and takes their share along the way.</p>



<p>(The music biz is littered with stories of artists getting a raw deal—making a small percentage of revenue from their albums, while the label takes the lion&#8217;s share—but that&#8217;s another story.)</p>



<p>Then there&#8217;s the opportunity for recurring revenue, if Party A and Party B have an ongoing relationship.&nbsp; Companies often turn to tech staffing agencies to find staff-augmentation contractors.&nbsp; Those agencies typically take a cut for the entire duration of the project or engagement, which can run anywhere from a few weeks to more than a decade.&nbsp; The staffing agency makes one hell of a return on their efforts when placing such a long-term contractor. Nice work, if you can get it.</p>



<p>Staffing agencies may have to refund a customer&#8217;s money if a contractor performs poorly.&nbsp; Some middlemen, however, make money no matter how the deal ultimately turns out.&nbsp; Did I foolishly believe my friend&#8217;s hot stock tip, in his drunken reverie, and pour my savings into a bad investment? Well, NYSE isn&#8217;t going to refund my money, which means they aren&#8217;t about to lose their cut.</p>



<p>A middleman also gets a bird&#8217;s-eye view of the relationships it enables.&nbsp; It sees who interacts with whom, and how that all happens.&nbsp; Middlemen that run online platforms have the opportunity to double-dip on their revenue model: first by taking their cut from an interaction, then by collecting and analyzing data around each interaction.&nbsp; Everything from an end-user&#8217;s contact or demographic details, to exploring patterns of how they communicate with other users, can be packaged up and resold.&nbsp; (This is, admittedly, a little shady. We&#8217;ll get to middlemen&#8217;s abuse of privilege shortly.)</p>



<h2>Saddling Some Burdens, Too</h2>



<p>Before you rush out to build your own middleman company, recognize that it isn&#8217;t all easy revenue.&nbsp; You first need to breathe the platform into existence, so the parties can interact.&nbsp; Depending on the field, this can involve a significant outlay of capital, time, and effort.&nbsp; Then you need to market the platform so that everyone knows where to go to find the Party B to their Party A.</p>



<p>Once it&#8217;s up and running, maintenance costs can be low if you keep things simple.&nbsp; (Consider the rideshare companies that own the technology platform, but not the vehicles in which passengers ride.) But until you reach that cruising altitude, you&#8217;re crossing your fingers that things pan out in your favor.&nbsp; That can mean a lot of sleepless nights and stressful investor calls.</p>



<p>The middleman&#8217;s other big challenge is that they need to keep all of those <em>N</em> sides of the N-sided market happy.&nbsp; The market only exists because all of the parties want to come together, and your service persists only because they want to come together through you.&nbsp; If one side gets mad and leaves, the other side(s) will soon follow.&nbsp; Keeping the peace can be a touchy balancing act.</p>



<p>Consider Airbnb.&nbsp; Early in the pandemic they earned praise from guests by allowing them to cancel certain bookings without penalty.&nbsp; It then passed those &#8220;savings&#8221; on to hosts, who weren&#8217;t too happy about the lost revenue.&nbsp; (Airbnb later created a fund to support hosts, but some say it still fell short.)&nbsp; The action sent a clear—though, likely, unintentional and incorrect—message that Airbnb valued guests more than hosts.&nbsp; A modern-day version of robbing Peter to pay Paul.</p>



<p>Keeping all sides happy is a tough line for a middleman to walk.&nbsp; Mohambir Sawhney, from Northwestern University&#8217;s McCormick Foundation, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://youtu.be/yverNV9Tzi4?t=290" target="_blank">summed this up well</a>: &#8220;In any two-sided market, you always have to figure out who you&#8217;re going to subsidize more, and who you&#8217;re going to actually screw more.&#8221; It&#8217;s easy for outsiders to say that Airbnb should have just eaten the losses—refunded guests&#8217; money while letting hosts keep their take—but that sounds much easier said than done.&nbsp; In the end, the company still has to subsidize <em>itself,</em> right?</p>



<p>The subsidize versus screw decision calculus gets even more complicated when one side only <em>wants</em> you but doesn&#8217;t <em>need</em> you.&nbsp; In the Airbnb case, the company effectively serves as a marketing arm and payments processor for property owners.&nbsp; Any sufficiently motivated owner is just one step away from handling that on their own, so even a small negative nudge can send them packing.&nbsp; (In economics terms, we say that those owners&#8217; <em>switching costs</em> are low.)</p>



<p>The same holds for the tech sector, where independent contractors can bypass staffing firms to hang their own shingle.&nbsp; Even rideshare drivers have a choice.&nbsp; While it would be tougher for them to get their own taxi medallion, they can switch from Uber to Lyft.&nbsp; Or, as many do, they can sign up with both services so that switching costs are effectively zero: &#8220;delete Uber app, keep the Lyft app running, done.&#8221;</p>



<h2>Making Enemies</h2>



<p>Even with those challenges, delivering on the middleman&#8217;s <em>raison d&#8217;être</em>—&#8221;keep all parties happy&#8221;—should be a straightforward affair.&nbsp; (I don&#8217;t say &#8220;easy,&#8221; just &#8220;straightforward.&#8221; There&#8217;s a difference.) Parties A and B clearly want to be together, you&#8217;re helping them be together, so the experience should be a win all around.</p>



<p>Why, then, do middlemen have such a terrible reputation?&nbsp; It mostly boils down to greed.</p>



<p>Once a middleman becomes a sufficiently large and/or established player, they become the <em>de facto</em> place for the parties to meet.&nbsp; This is a near-monopoly status. The middleman no longer needs to care about keeping one or even both parties happy, they figure, because those groups either interact through the middleman or they don&#8217;t interact at all. (This also holds true for the near-cartel status of a group of equally unpleasant middlemen.)</p>



<p>Maybe the middleman suddenly raises fees, or sets onerous terms of service, or simply mistreats one side of the pairing.&nbsp; This raises the dollar, effort, and emotional cost to the parties since they don&#8217;t have many options to leave.</p>



<p>Consider food-delivery apps, which consumers love but can <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://get.doordash.com/en-us/products/marketplace" target="_blank">take as much as a 30% cut</a> of an order&#8217;s revenue.&nbsp; That&#8217;s a large bite, but easier to swallow when a restaurant has a modest take-away business alongside a much larger dine-in experience. It&#8217;s quite another story when take-away is suddenly your <em>entire</em> business and you&#8217;re still paying rent on the empty dining room space. Most restaurants found themselves in just this position early in the COVID-19 pandemic. Some hung signs in their windows, asking customers to call them directly instead of using the delivery apps.</p>



<p>Involving a middleman in a relationship can also lead to weird principal-agent problems.&nbsp; Tech staffing agencies (even those that paint themselves as &#8220;consultancies&#8221;) have earned a special place here.&nbsp; Big companies hand such &#8220;preferred vendors&#8221; a strong moat by requiring contractors to pass through them in lieu of establishing a direct relationship. Since the middlemen can play this <em>Work Through Us, or Don&#8217;t Work at All</em> card, it&#8217;s no surprise that they&#8217;ve been known to take as much as 50% of the money as it passes from client to contractor.&nbsp; The client companies don&#8217;t always know this, so they are happy that the staffing agency has helped them find software developers and DBAs. The contractors, many of whom are aware of the large cuts, aren&#8217;t so keen on the arrangement.</p>



<p>This is on top of limiting a tech contractor&#8217;s ability to work through a competing agency.&nbsp; I&#8217;ve seen everything from thinly-veiled threats (&#8220;if the client sees your resume from more than one agency, they&#8217;ll just throw it out&#8221;) to written agreements (&#8220;this contract says you won&#8217;t go through another agency to work with this client&#8221;). &nbsp; What if you&#8217;ve found a different agency that will take a smaller cut, so you get more money?&nbsp; Or what if Agency 1 has done a poor job of representing you, while you know that Agency 2 will get it right?&nbsp; In both cases, the answer is: tough luck.</p>



<p>A middleman can also resort to more subtle ways to mistreat the parties.&nbsp; Uber has reportedly used a variety of techniques from behavioral science—<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nytimes.com/interactive/2017/04/02/technology/uber-drivers-psychological-tricks.html" target="_blank">such as the gamification of male managers pretending to be women</a>—to encourage drivers to work more.&nbsp; They&#8217;ve also been accused of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/tech-policy/2017/04/uber-said-to-use-sophisticated-software-to-defraud-drivers-passengers/" target="_blank">showing drivers and passengers different routes</a>, charging the passenger for the longer way and paying the driver for the shorter way.</p>



<h2>It&#8217;s Not All Easy Money</h2>



<p>To be fair, middlemen do earn <em>some</em> of their cut. They provide value in that they reduce friction for both the buy and sell sides of an interaction.</p>



<p>This goes above and beyond building the technology for a platform.&nbsp; Part of how the Deliveroos and Doordashes of the world connect diners to restaurants is by coordinating fleets of delivery drivers.&nbsp; It would be expensive for a restaurant to do this on their own: hiring multiple drivers, managing the schedule, accounting for demand … and hoping business stays hot so that the drivers aren&#8217;t paid to sit idle. Similarly, tech staffing firms don&#8217;t just introduce you to contract talent. They also handle time-tracking, invoicing, and legal agreements. The client company cuts one large check to the staffing firm, which cuts lots of smaller checks to the individual contractors.</p>



<p>Don&#8217;t forget that handling contracts and processing payments come with extra regulatory requirements. Rules often vary by locale, and the middleman has to spend money to keep track of those rules.&nbsp; So it&#8217;s not <em>all</em> profit.</p>



<p>(They can also build tools to <em>avoid</em> rules, such as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html" target="_blank">Uber&#8217;s infamous &#8220;greyball&#8221; system</a> … but that&#8217;s another story.)</p>



<p>That said, a middleman&#8217;s benefit varies by the industry vertical and even by the client.&nbsp; Some argue that their revenue cut far exceeds the value they provide. In the case of tech staffing firms, I&#8217;ve heard plenty of complaints that recruiters take far too much money for&nbsp; just &#8220;having a phone number&#8221; (having a client relationship) and cutting a check, when it&#8217;s the contractor who does the actual work of building software or managing systems for the client.</p>



<h2>A Win-Win-Win Triangle</h2>



<p>Running a middleman has its challenges and risks.&nbsp; It can also be tempting to misuse the role&#8217;s power.&nbsp; Still, I say that there&#8217;s a way to build an N-sided marketplace where everyone can be happy.&nbsp; I&#8217;ll explore that in the next article in this series.</p>



<p>(Many thanks to <a rel="noreferrer noopener" href="https://www.oreilly.com/people/chris-butler/" target="_blank">Chris Butler</a> for his thoughtful and insightful feedback on early drafts of this article.  I&#8217;d also like to thank <a rel="noreferrer noopener" href="https://www.oreilly.com/people/mike-loukides/" target="_blank">Mike Loukides</a> for shepherding this piece into its final form.)</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/building-a-better-middleman/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>The General Purpose Pendulum</title>
      <link>https://www.oreilly.com/radar/the-general-purpose-pendulum/</link>
      <comments>https://www.oreilly.com/radar/the-general-purpose-pendulum/#respond</comments>
      <pubDate>Tue, 12 Apr 2022 11:59:19 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Hardware]]></category>
      <category><![CDATA[Software Architecture]]></category>
      <category><![CDATA[Software Engineering]]></category>
      <category><![CDATA[Commentary]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14436</guid>
      <description>Pendulums do what they do: they swing one way, then they swing back the other way.&amp;#160; Some oscillate quickly; some slowly; and some so slowly you can watch the earth rotate underneath them. It’s a cliche to talk about any technical trend as a “pendulum,” though it’s accurate often enough. We may be watching one [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>Pendulums do what they do: they swing one way, then they swing back the other way.&nbsp; Some oscillate quickly; some slowly; and some so slowly you can <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Foucault_pendulum" target="_blank">watch the earth rotate underneath them</a>. It’s a cliche to talk about any technical trend as a “pendulum,” though it’s accurate often enough.</p>



<p>We may be watching one of computing’s longest-term trends turn around, becoming the technological equivalent of Foucault’s very long, slow pendulum: the trend towards generalization. That trend has been swinging in the same direction for some 70 years–since the invention of computers, really.&nbsp; The first computers were just calculating engines designed for specific purposes: breaking codes (in the case of Britain’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Bombe" target="_blank">Bombe</a>) or calculating missile trajectories. But those primitive computers soon got the ability to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Computer" target="_blank">store programs</a>, making them much more flexible; eventually, they became “general purpose” (i.e., business) computers. If you’ve ever seen a manual for the IBM 360’s machine language, you’ll see many instructions that only make sense in a business context–for example, instructions for arithmetic in binary coded decimal.</p>



<p>That was just the beginning. In the 70s, word processors started replacing typewriters. Word processors were essentially early personal computers designed for typing–and they were quickly replaced by personal computers themselves. With the invention of email, computers became communications devices. With file sharing software like Napster and MP3 players like WinAmp, computers started replacing radios–then, when Netflix started streaming, televisions. CD and DVD players are inflexible, task-specific computers, much like word processors or the Bombe, and their functions have been subsumed by general-purpose machines.</p>



<p>The trend towards generalization also took place within software. Sometime around the turn of the millenium, many of us realized the Web browsers (yes, even the early Mosaic, Netscape, and Internet Explorer) could be used as a general user interface for software; all a program had to do was express its user interface in HTML (using forms for user input), and provide a web server so the browser could display the page. It’s not an accident that Java was perhaps the last programming language to have a graphical user interface (GUI) library; other languages that appeared at roughly the same time (Python and Ruby, for example) never needed one.</p>



<p>If we look at hardware, machines have gotten faster and faster–and more flexible in the process. I’ve already mentioned the appearance of instructions specifically for “business” in the IBM 360. GPUs are specialized hardware for high-speed computation and graphics; however, they’re much less specialized than their ancestors, dedicated vector processors.&nbsp; Smartphones and tablets are essentially personal computers in a different form factor, and they have performance specs that beat supercomputers from the 1990s. And they’re also cameras, radios, televisions, game consoles, and even credit cards.</p>



<p>So, why do I think this pendulum might start swinging the other way?&nbsp; A recent article in the <em>Financial Times</em>, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ft.com/content/4db69e3c-c901-4776-970e-c57e99f71aba" target="_blank">Big Tech Raises its Bets on Chips</a>, notes that Google and Amazon have both developed custom chips for use in their clouds. It hypothesizes that the next generation of hardware will be one in which chip development is integrated more closely into a wider strategy.&nbsp; More specifically, “the best hope of producing new leaps forward in speed and performance lies in the co-design of hardware, software and neural networks.” Co-design sounds like designing hardware that is highly optimized for running neural networks, designing neural networks that are a good match for that specific hardware, and designing programming languages and tools for that specific combination of hardware and neural network. Rather than taking place sequentially (hardware first, then programming tools, then application software), all of these activities take place concurrently, informing each other. That sounds like a turn away from general-purpose hardware, at least superficially: the resulting chips will be good at doing one thing extremely well. It’s also worth noting that, while there is a lot of interest in quantum computing, quantum computers will inevitably be specialized processors attached to conventional computers. There is no reason to believe that a quantum computer can (or should) run general purpose software such as software that renders video streams, or software that calculates spreadsheets.&nbsp;Quantum computers will be a big part of our future–but not in a general-purpose way. Both co-design and quantum computing step away from general-purpose computing hardware. We’ve come to the end of Moore’s Law, and can’t expect further speedups from hardware itself.&nbsp; We can expect improved performance by optimizing our hardware for a specific task.</p>



<p>Co-design of hardware, software, and neural networks will inevitably bring a new generation of tools to software development. What will those tools be? Our current development environments don’t require programmers to know much (if anything) about the hardware. Assembly language programming is a specialty that’s really only important for embedded systems (and not all of them) and a few applications that require the utmost in performance. In the world of co-design, will programmers need to know more about hardware? Or will a new generation of tools abstract the hardware away, even as they weave the hardware and the software together even more intimately? I can certainly imagine tools with modules for different kinds of neural network architectures; they might know about the kind of data the processor is expected to deal with; they might even allow a kind of “pre-training”–something that could ultimately give you GPT-3 on a chip. (Well, maybe not on a chip. Maybe a few thousand chips designed for some distributed computing architecture.) Will it be possible for a programmer to say “This is the kind of neural network I want, and this is how I want to program it,” and let the tool do the rest? If that sounds like a pipe-dream, realize that tools like GitHub Copilot are already automating programming.</p>



<p>Chip design is the poster child for “the first unit costs 10 billion dollars; the rest are all a penny apiece.”&nbsp; That has limited chip design to well-financed companies that are either in the business of selling chips (like Intel and AMD) or that have specialized needs and can buy in very large quantities themselves (like Amazon and Google). Is that where it will stop–increasing the imbalance of power between a few wealthy companies and everyone else–or will co-design eventually enable smaller companies (and maybe even individuals) to build custom processors? To me, co-design doesn’t make sense if it’s limited to the world’s Amazons and Googles. They can already design custom chips.&nbsp; It’s expensive, but that expense is itself a moat that competitors will find hard to cross.&nbsp;Co-design is about improved performance, yes; but as I’ve said, it’s also inevitably about improved tools.&nbsp; Will those tools result in better access to semiconductor fabrication facilities?</p>



<p>We’ve seen that kind of transition before.&nbsp;Designing and making printed circuit boards used to be hard. I tried it once in high school; it requires acids and chemicals you don’t want to deal with, and a hobbyist definitely can’t do it in volume. But now, it’s easy: you design a circuit with a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.electronics-lab.com/top-10-free-pcb-design-software-2019/" target="_blank">free tool like Kicad or Fritzing</a>, have the tool generate a board layout, send the layout to a vendor through a web interface, and a few days later, a package arrives with your circuit boards. If you want, you can have the vendor source the board’s components and solder them in place for you. It costs a few tens of dollars, not thousands. Can the same thing happen at the chip level? It hasn’t yet. We’ve thought that field-programmable gate arrays might eventually democratize chip design, and to a limited extent, they have. FPGAs aren’t hard for small- or mid-sized businesses that can afford a few hardware engineers, but they’re far from universal, and they definitely haven’t made it to hobbyists or individuals.&nbsp; Furthermore, FPGAs are still standardized (generalized) components; they don’t democratize the semiconductor fabrication plant.</p>



<p>What would “cloud computing” look like in a co-designed world? Let’s say that a mid-sized company designs a chip that implements a specialized language model, perhaps something like <a href="https://learning.oreilly.com/answers/search/">O’Reilly Answers</a>. Would they have to run this chip on their own hardware, in their own datacenter?&nbsp; Or would they be able to ship these chips to Amazon or Google for installation in their AWS and GCP data centers?&nbsp; That would require a lot of work standardizing the interface to the chip, but it’s not inconceivable.&nbsp; As part of this evolution, the co-design software will probably end up running in someone’s cloud (much as <a href="https://aws.amazon.com/pm/sagemaker/">AWS Sagemaker</a> does today), and it will “know” how to build devices that run on the cloud provider’s infrastructure. The future of cloud computing might be running custom hardware.</p>



<p>We inevitably have to ask what this will mean for users: for those who will use the online services and physical devices that these technologies enable. We may be seeing that pendulum swing back towards specialized devices. A product like Sonos speakers is essentially a re-specialization of the device that was formerly a stereo system, then became a computer. And while I (once) lamented the idea that we’d eventually all wear jackets with innumerable pockets filled with different gadgets (iPods, i-Android-phones, Fitbits, Yubikeys, a collection of dongles and earpods, you name it), some of those products make sense:&nbsp; I lament the loss of the iPod, as distinct from the general purpose phone. A tiny device that could carry a large library of music, and do nothing else, was (and would still be) a wonder.</p>



<p>But those re-specialized devices will also change. A Sonos speaker is more specialized than a laptop plugged into an amp via the headphone jack and playing an MP3; but don’t mistake it for a 1980s stereo, either. If inexpensive, high-performance AI becomes commonplace, we can expect a new generation of exceedingly smart devices. That means voice control that really works (maybe even for <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.scientificamerican.com/article/speech-recognition-tech-is-yet-another-example-of-bias/%5C" target="_blank">those who speak with an accent</a>), locks that can identify people accurately regardless of skin color, and appliances that can diagnose themselves and call a repairman when they need to be fixed. (I’ve always wanted a furnace that could notify my service contractor when it breaks at 2AM.) Putting intelligence on a local device could improve privacy–the device wouldn&#8217;t need to send as much data back to the mothership for processing. (We’re already seeing this on Android phones.) We might get autonomous vehicles that communicate with each other to optimize traffic patterns. We might go beyond voice controlled devices to non-invasive brain control. (Elon Musk’s Neuralink has the right idea, but few people will want sensors surgically embedded in their brains.)</p>



<p>And finally, as I write this, I realize that I’m writing on a laptop–but I don’t want a better laptop. With enough intelligence, would it be possible to build environments that are aware of what I want to do?&nbsp;And offer me the right tools when I want them (possibly something like Bret Victor’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dynamicland.org/" target="_blank">Dynamicland</a>)? After all, we don’t really want computers.&nbsp; We want “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=ob_GX50Za6c" target="_blank">bicycles for the mind</a>”–but in the end, Steve Jobs only gave us computers.</p>



<p>That’s a big vision that will require embedded AI throughout. It will require lots of very specialized AI processors that have been optimized for performance and power consumption. Creating those specialized processors will require re-thinking how we design chips. Will that be co-design, designing the neural network, the processor, and the software together, as a single piece? Possibly. It will require a new way of thinking about tools for programming–but if we can build the right kind of tooling, “possibly” will become a certainty.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/the-general-purpose-pendulum/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Radar trends to watch: April 2022</title>
      <link>https://www.oreilly.com/radar/radar-trends-to-watch-april-2022/</link>
      <comments>https://www.oreilly.com/radar/radar-trends-to-watch-april-2022/#respond</comments>
      <pubDate>Tue, 05 Apr 2022 11:32:17 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Radar Trends]]></category>
      <category><![CDATA[Signals]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14426</guid>
      <description>March was a busy month, especially for developers working with GPT-3. After surprising everybody with its ability to write code, it’s not surprising that GPT-3 is appearing in other phases of software development. One group has written a tool that creates regular expressions from verbal descriptions; another tool generates Kubernetes configurations from verbal descriptions. In [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>March was a busy month, especially for developers working with GPT-3. After surprising everybody with its ability to write code, it’s not surprising that GPT-3 is appearing in other phases of software development. One group has written a tool that creates regular expressions from verbal descriptions; another tool generates Kubernetes configurations from verbal descriptions. In his newsletter, Andrew Ng talks about the future of low-code AI: it’s not about eliminating coding, but eliminating the need to write all the boilerplate. The latest developments with large language models like GPT-3 suggest that the future isn’t that distant.</p>



<p>On the other hand, the US copyright office has determined that works created by machines are not copyrightable. If software is increasingly written by tools like Copilot, what will this say about software licensing and copyright?</p>



<h2>Artificial Intelligence</h2>



<ul><li>An unusual form of matter known as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://phys.org/news/2022-03-circuits-rare-nanomagnets.html" target="_blank">spin glass</a> can potentially allow the implementation of neural network algorithms in hardware. One particular kind of network allows pattern matching based on partial patterns (for example, face recognition based on a partial face), something that is difficult or impossible with current techniques.</li><li>OpenAI has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://openai.com/blog/webgpt/" target="_blank">extended GPT-3 to do research on the web</a> when it needs information that it doesn’t already have. </li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://datacentricai.org/" target="_blank">Data-centric</a> AI is gaining steam, in part because Andrew Ng has been pushing it consistently. Data-centric AI claims that the best way to improve the AI is to improve the data, rather than the algorithms. It includes ideas like machine-generated training data and automatic tagging. Christoper Ré, at one of the last Strata conferences, noted that data collection was the part of AI that was most resistant to improvement.</li><li>We’ve seen that GPT-3 can generate code from English language comments. Can it generate Kubernetes configurations from natural language descriptions?&nbsp; Take a look at <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://pub.towardsai.net/kubernetes-made-easy-with-gpt-3-6fb7b2a1f31" target="_blank">AI Kube Bot</a>.</li><li>The US Copyright Office has determined that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://ipkitten.blogspot.com/2022/02/us-copyright-office-refuses-to-register.html" target="_blank">works created by an artificial intelligence aren’t copyrightable</a>; copyright requires human authorship. This is almost certainly not the final word on the topic.</li><li>A <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenextweb.com/news/how-ai-brain-with-only-one-neuron-could-surpass-humans" target="_blank">neural network with a single neuron</a> that is used many times may be as effective as large neural networks, while using much less energy.</li><li>Training AI models on <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-03-ai-ditch-datasets.html" target="_blank">synthetic data</a> created by a generative model can be more effective than using real-world data. Although there are pitfalls, there’s more control over bias, and the data can be made to include unexpected cases.</li><li>For the past 70 years, computing has been dominated by general-purpose hardware: machines designed to run any code. Even vector processors and their descendants (GPUs) are fundamentally general purpose. The next steps forward in AI may involve <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ft.com/content/4db69e3c-c901-4776-970e-c57e99f71aba" target="_blank">software, hardware, and neural networks that are designed for each other</a>.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nature.com/articles/s41586-022-04448-z" target="_blank">Ithaca</a> is a DeepMind project that uses deep learning to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/science/2022/03/deepminds-new-ai-tool-helps-resolve-debate-over-ancient-athenian-decrees/" target="_blank">recover missing texts in ancient Greek documents</a> and inscriptions.&nbsp; It’s particularly interesting as an example of human-machine collaboration.&nbsp;Humans can do this work with 25% accuracy, Ithaca is 62% accurate on its own, but Ithaca and humans combined reach 72% accuracy.</li><li>Michigan is starting to build the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenextweb.com/news/the-challenge-to-create-autonomous-vehicle-corridors-for-mass-adoption" target="_blank">infrastructure needed to support autonomous vehicles</a>: dedicated lanes, communications, digital signage, and more.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://venturebeat.com/2022/03/04/researchers-open-source-code-generating-ai-they-claim-can-beat-openais-codex/" target="_blank">Polycoder</a> is an open source code generator (like Copilot) that uses GPT-2, which is also open sourced. Developers claim that Polycoder is better than Copilot for many tasks, including programming in C. Because it is open-source, it enables researchers to investigate how these tools work, including testing for security vulnerabilities.</li><li>New approaches to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-03-machine-smarter-drug-discovery.html" target="_blank">molecule design using self-supervised learning </a>on unlabeled data promise to make drug discovery faster and more efficient.</li><li>The title says it all. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.seotraininglondon.org/english-regex-gpt3/" target="_blank">Converting English to Regular Expressions with GPT-3</a>, implemented as a Google sheet. Given Copilot, it’s not surprising that this can be done.</li><li>Researchers at MIT have developed a technique for <a href="https://techxplore.com/news/2022-03-fairness-machine-learning.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">injecting fairness</a> into a model itself, even after it has been trained on biased data.</li></ul>



<h2>Programming</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/trymito/low-code-python-has-arrived-5a5d2676b1d2" target="_blank">Low code programming for Python</a>: Some new libraries designed for use in Jupyter Notebooks (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bamboolib.8080labs.com/" target="_blank">Bamboo</a>, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/lux-org/lux" target="_blank">Lux</a>, and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/mito-ds/monorepo" target="_blank">Mito</a>) allow a graphical (forms-based) approach to working with data using Python’s Pandas library.</li><li>Will the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/is-linkerd-winning-the-service-mesh-race/" target="_blank">Linkerd service mesh</a> displace Istio?&nbsp; Linkerd seems to be simpler and more attractive to small and medium-sized organizations.</li><li>The biggest problem with Stack Overflow is the number of answers that are out of date.&nbsp; There’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://neverworkintheory.org/2022/03/08/obsolete-answers-on-stack-overflow.html" target="_blank">now</a> a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arxiv.org/pdf/1903.12282.pdf" target="_blank">paper</a> studying the frequency of out-of-date answers.</li><li><a href="https://thenewstack.io/silkworm-encryption/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Silkworm-based encryption</a>: Generating good random numbers is a difficult problem. One surprising new source of randomness is silk.&nbsp; While silk appears smooth, it is (not surprisingly) very irregular at a microscopic scale.&nbsp; Because of this irregularity, passing light through silk generates random diffraction patterns, which can be converted into random numbers.</li></ul>



<h2>Biology</h2>



<ul><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="http://bbe.ac.uk/about/" target="_blank">Hub for Biotechnology in the Built Environment (HBBE)</a> is a research center that is rethinking buildings. They intend to create “living buildings” (and I do not think that is a metaphor) capable of processing waste and producing energy.</li><li>A <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://newatlas.com/science/crispr-gene-editing-error-correction-protein/" target="_blank">change to the protein used in CRISPR</a> to edit DNA reduces errors by a factor of 4000, without making the process slower.</li><li>Researchers have observed the process by which brains store <a href="https://thenextweb.com/news/researchers-figured-out-how-the-human-brain-makes-memories" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">sequences of memories</a>.&nbsp; In addition to therapies for memory disorders, this discovery could lead to advances in artificial intelligence, which don’t really have the ability to create and process timelines or narratives.</li></ul>



<h2>Metaverse</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.ml6.eu/3d-object-detection-in-the-real-world-6368bdbbdc0b" target="_blank">Object detection in 3D</a> is a critical technology for augmented reality (to say nothing of autonomous vehicles), and it’s significantly more complex than in 2D. Facebook/Meta’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/facebookresearch/3detr" target="_blank">3DETR</a> uses transformers to build models from 3D data.</li><li>Some <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bettermarketing.pub/apple-glass-will-do-what-google-glass-couldnt-here-s-how-580da30d57cb" target="_blank">ideas</a> about what Apple’s AR glasses, Apple Glass, might be. Take what you want… Omitting a camera is a good idea, though it’s unclear how you’d make AR work. This article suggests LIDAR, but that doesn’t sound feasible.</li><li>According to the creator of Pokemon Go, the <a href="https://techxplore.com/news/2022-03-pokemon-creator-metaverse-real.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">metaverse</a> should be about helping people to appreciate the physical world, not about isolating them in a virtual world. </li></ul>



<h2>Security</h2>



<ul><li>Jeff Carr has been publishing (and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/d-day-in-kyiv/" target="_blank">writing about</a>) <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://jeffreycarr.substack.com/p/ukraines-defense-intelligence-service" target="_blank">dumps of Russian data</a> obtained by hackers from GRUMO, the Ukraine’s cyber operations team.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.sigstore.dev/" target="_blank">Sigstore</a> is a new kind of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.sigstore.dev/a-new-kind-of-trust-root-f11eeeed92ef" target="_blank">certificate authority (trusted root)</a> that is addressing open source software supply chain security problems.&nbsp; The goal is to make software signing “ubiquitous, free, easy, and transparent.”</li><li>Russia has created its own <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/russia-creates-its-own-tls-certificate-authority-to-bypass-sanctions/" target="_blank">certificate authority</a> to mitigate international sanctions. However, users of Chrome, Firefox, Safari, and other browsers originating outside of Russia would have to install the Russian root certificate manually to access Russian sites without warnings.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/corporate-website-contact-forms-used-to-spread-bazarbackdoor-malware/" target="_blank">Corporate contact forms are replacing email</a> as a vector for transmitting malware. BazarBackdoor [sic] is now believed to be under development by the Conti ransomware group.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/information-technology/2022/03/linux-has-been-bitten-by-its-most-high-severity-vulnerability-in-years/" target="_blank">Dirty Pipe</a> is a newly discovered high-severity bug in the Linux kernel that allows any user to overwrite any file or obtain root privileges. Android phones are also vulnerable.</li><li>Twitter has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-03-twitter-unveils-version-site-bypass.html" target="_blank">created an onion service</a> that is accessible through the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://tor.eff.org/" target="_blank">Tor</a> network. (Facebook has a similar service.)&nbsp; This service makes Twitter accessible within Russia, despite government censorship.</li><li>The attackers attacked: A security researcher has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/conti-ransomwares-internal-chats-leaked-after-siding-with-russia/" target="_blank">acquired and leaked</a> chat server logs from the Conti ransomware group. These logs include discussions of victims, Bitcoin addresses, and discussions of the group’s support of Russia.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/information-technology/2022/03/attackers-can-force-amazon-echos-to-hack-themselves-with-self-issued-commands/" target="_blank">Attackers can force Amazon Echo devices to hack themselves</a>. Get the device to speak a command, and its microphone will hear the command and execute it. This misfeature includes controlling other devices (like smart locks) via the Echo.</li><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Anonymous_(hacker_group)" target="_blank">Anonymous</a> hacktivist collective is <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://theconversation.com/the-hacker-group-anonymous-has-waged-a-cyber-war-against-russia-how-effective-could-they-actually-be-178034" target="_blank">organizing</a> (to use that word very loosely) attacks against Russian digital assets. Among other things, they have <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://securityaffairs.co/wordpress/128428/hacking/anonymous-russian-defense-ministry.html" target="_blank">leaked emails</a> between the Russian defense ministry and their suppliers, and hacked the front pages of several <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.euronews.com/next/2022/02/28/ukraine-war-what-part-is-hackers-collective-anonymous-playing-in-the-war-effort-against-ru" target="_blank">Russian news agencies</a>.</li><li>The <a href="https://datadetoxkit.org/en/misinformation/botornot/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Data Detox Kit</a> is a quick guide to the bot world and the spread of misinformation.&nbsp; Is it a bot or not?&nbsp; This site has other good articles about how to recognize misinformation.</li></ul>



<h2>Hardware</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-03-tiny-battery-free-devices-dandelion-seeds.html" target="_blank">Sensor networks that are deployed like dandelion seeds</a>! An extremely light, solar-powered framework for scattering of RF-connected sensors and letting breezes do the work lets researchers build networks with thousands of sensors easily. I’m concerned about cleanup afterwards, but this is a breakthrough, both in biomimicry and low-power hardware.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.nature.com/articles/s41586-022-04415-8" target="_blank">Semiconductor-based LIDAR</a> could be the key to autonomous vehicles that are reasonably priced and safe. LIDAR systems with mechanically rotating lasers have been the basis for Google’s autonomous vehicles; they are effective, but expensive.</li><li>The open source instruction set architecture RISC-V is <a href="https://semiengineering.com/why-risc-v-is-succeeding/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">gaining momentum</a> because it is enabling innovation at the lowest levels of hardware.</li></ul>



<h2>Quantum Computing</h2>



<ul><li>Microsoft claims to have made a breakthrough in creating <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://news.microsoft.com/innovation-stories/azure-quantum-majorana-topological-qubit/" target="_blank">topological qubits</a>, which should be more stable and scalable than other approaches to quantum computing.</li><li>IBM’s quantum computer was used to <a href="https://techxplore.com/news/2022-03-crystals-quantum.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">simulate</a> a <a href="https://en.wikipedia.org/wiki/Time_crystal" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">time crystal</a>, showing that current quantum computers can be used to investigate quantum processes, even if they can’t yet support useful computation.</li></ul>



<h2>Web</h2>



<ul><li>Mozilla has published their <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://webvision.mozilla.org/" target="_blank">vision for the future evolution of the web</a>. The executive summary highlights safety, privacy, and performance. They also want to see a web on which it’s easier for individuals to publish content.</li><li>Twitter is expanding its <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-03-twitter-birdwatch-crowdsourced-fact.html" target="_blank">crowdsourced fact-checking</a> program (called <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/birdwatch?lang=en" target="_blank">Birdwatch</a>). It’s not yet clear whether this has helped stop the spread of misinformation.</li><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://twitter.com/PayGapApp" target="_blank">Gender Pay Gap Bot (@PayGapApp)</a> retweets corporate tweets about International Womens’ Day with a comment about the company’s gender pay gap (derived from a database in the UK).</li><li>Alex Russell writes about a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://infrequently.org/2022/03/a-unified-theory-of-web-performance/" target="_blank">unified theory of web performance</a>.&nbsp; The core principle is that the web is for humans. He emphasizes the importance of latency at the tail of the performance distribution; improvements there tend to help everyone.</li><li><a href="https://surma.dev/things/webgpu/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">WebGPU</a> is a new <a href="https://www.w3.org/TR/webgpu/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">API</a> that gives web applications the ability to do rendering and computation on GPUs.</li></ul>



<h2>Blockchains and NFTs</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://eips.ethereum.org/EIPS/eip-1155" target="_blank">ERC-1155</a> is a standard for creating tokens on the blockchain. It covers both fungible and non-fungible (NFT) tokens, and is <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/erc-1155-an-nft-standard-for-online-games-and-gamified-apps/" target="_blank">becoming the basis for blockchain-based game economies</a>.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://walletconnect.com/" target="_blank">WalletConnect</a> is a standard for interconnections between blockchain wallets and distributed apps (dapps).&nbsp; It is not open source.&nbsp; It will be interesting to see whether a closed standard is viable in the web3 world.</li><li>Can <a href="https://thenewstack.io/can-nft-technology-expand-beyond-digital-apes-and-punks/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">NFTs make the transition from speculation to selling rights</a> (for example, can they serve as tickets)?</li></ul>



<h2>Business</h2>



<ul><li>Russia is considering <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/government/piracy-ok-russia-to-ease-software-licensing-rules-after-sanctions/" target="_blank">abolishing compensation</a> to foreign firms for the use of software to keep their digital economy running.</li><li>John Maeda’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://maeda.pm/wp-content/uploads/2022/03/rtr-007-compressed.pdf" target="_blank">Resilience Tech Report</a> is a must-read, particularly for managers.</li><li>Treating <a href="https://www.technologyreview.com/2022/03/09/1046976/ai-is-helping-treat-healthcare-as-if-its-a-supply-chain-problem/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">healthcare as a supply chain problem</a> raises issues about privacy and sensitive data, but may also provide better ways of getting health care to vulnerable people.</li></ul>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/radar-trends-to-watch-april-2022/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>AI Adoption in the Enterprise 2022</title>
      <link>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/</link>
      <comments>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/#respond</comments>
      <pubDate>Thu, 31 Mar 2022 11:35:34 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Artificial Intelligence]]></category>
      <category><![CDATA[Research]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14410</guid>
      <description>In December 2021 and January 2022, we asked recipients of our&amp;#160;Data&amp;#160;and&amp;#160;AI Newsletters&amp;#160;to participate in our annual survey on AI adoption. We were particularly interested in what, if anything, has changed since last year. Are companies farther along in AI adoption? Do they have working applications in production? Are they using tools like AutoML to generate [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>In December 2021 and January 2022, we asked recipients of our&nbsp;<em>Data</em>&nbsp;and&nbsp;<em>AI Newsletters</em>&nbsp;to participate in our annual survey on AI adoption. We were particularly interested in what, if anything, has changed since last year. Are companies farther along in AI adoption? Do they have working applications in production? Are they using tools like AutoML to generate models, and other tools to streamline AI deployment? We also wanted to get a sense of where AI is headed. The hype has clearly moved on to blockchains and NFTs. AI is in the news often enough, but the steady drumbeat of new advances and techniques has gotten a lot quieter.</p>



<p>Compared to last year, significantly fewer people responded. That’s probably a result of timing. This year’s survey ran during the holiday season (December 8, 2021, to January 19, 2022, though we received very few responses in the new year); last year’s ran from January 27, 2021, to February 12, 2021. Pandemic or not, holiday schedules no doubt limited the number of respondents.</p>



<p>Our results held a bigger surprise, though. The smaller number of respondents notwithstanding, the results were&nbsp;<a href="https://learning.oreilly.com/library/view/ai-adoption-in/9781098109196/">surprisingly similar to 2021</a>. Furthermore, if you go back another year, the 2021 results were themselves&nbsp;<a href="https://learning.oreilly.com/library/view/ai-adoption-in/9781492081210/">surprisingly similar to 2020</a>. Has that little changed in the application of AI to enterprise problems? Perhaps. We considered the possibility that the same individuals responded in both 2021 and 2022. That wouldn’t be surprising, since both surveys were publicized through our mailing lists—and some people like responding to surveys. But that wasn’t the case. At the end of the survey, we asked respondents for their email address. Among those who provided an address, there was only a 10% overlap between the two years.</p>



<p>When nothing changes, there’s room for concern: we certainly aren’t in an “up and to the right” space. But is that just an artifact of the hype cycle? After all, regardless of any technology’s long-term value or importance, it can only receive outsized media attention for a limited time. Or are there deeper issues gnawing at the foundations of AI adoption?</p>



<h2>AI Adoption</h2>



<p>We asked participants about the level of AI adoption in their organization. We structured the responses to that question differently from prior years, in which we offered four responses: not using AI, considering AI, evaluating AI, and having AI projects in production (which we called &#8220;mature&#8221;). This year we combined “evaluating AI” and “considering AI”; we thought that the difference between “evaluating” and “considering” was poorly defined at best, and if we didn’t know what it meant, our respondents didn’t either. We kept the question about projects in production, and we&#8217;ll use the words &#8220;in production&#8221; rather than &#8220;mature practice&#8221; to talk about this year&#8217;s results.</p>



<p>Despite the change in the question, the responses were surprisingly similar to last year&#8217;s. The same percentage of respondents said that their organizations had AI projects in production (26%). Significantly more said that they weren’t using AI: that went from 13% in 2021 to 31% in this year’s survey. It’s not clear what that shift means. It’s possible that it’s just a reaction to the change in the answers; perhaps respondents who were “considering” AI thought “considering really means that we’re not using it.” It’s also possible that AI is just becoming part of the toolkit, something developers use without thinking twice. Marketers use the term AI; software developers tend to say machine learning. To the customer, what’s important isn’t how the product works but what it does. There’s already a lot of AI embedded into products that we never think about.</p>



<p>From that standpoint, many companies with AI in production don’t have a single AI specialist or developer. Anyone using Google, Facebook, or Amazon (and, I presume, most of their competitors) for advertising is using AI. AI as a service includes AI packaged in ways that may not look at all like neural networks or deep learning. If you install a smart customer service product that uses GPT-3, you’ll never see a hyperparameter to tune—but you have deployed an AI application. We don’t expect respondents to say that they have “AI applications deployed” if their company has an advertising relationship with Google, but AI is there, and it&#8217;s real, even if it&#8217;s invisible.</p>



<p>Are those invisible applications the reason for the shift? Is AI disappearing into the walls, like our plumbing (and, for that matter, our computer networks)? We’ll have reason to think about that throughout this report.</p>



<p>Regardless, at least in some quarters, attitudes seem to be solidifying against AI, and that could be a sign that we’re approaching another “AI winter.” We don’t think so, given that the number of respondents who report AI in production is steady and up slightly. However, it&nbsp;<em>is</em>&nbsp;a sign that AI has passed to the next stage of the&nbsp;<a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">hype cycle</a>. When expectations about what AI can deliver are at their peak, everyone says they’re doing it, whether or not they really are. And once you hit the trough, no one says they’re using it, even though they now are.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01.png" alt="" class="wp-image-14411" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01.png 494w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig01-300x236.png 300w" sizes="(max-width: 494px) 100vw, 494px" /><figcaption>Figure 1.<em> AI adoption and maturity</em></figcaption></figure>



<p>The trailing edge of the hype cycle has important consequences for the practice of AI. When it was in the news every day, AI didn’t really have to prove its value; it was enough to be interesting. But once the hype has died down, AI has to show its value in production, in real applications: it’s time for it to prove that it can deliver real business value, whether that’s cost savings, increased productivity, or more customers. That will no doubt require better tools for collaboration between AI systems and consumers, better methods for training AI models, and better governance for data and AI systems.</p>



<h3>Adoption by Continent</h3>



<p>When we looked at responses by geography, we didn’t see much change since last year. The greatest increase in the percentage of respondents with AI in production was in Oceania (from 18%&nbsp;to 31%), but that was a relatively small segment of the total number of respondents (only 3.5%)—and when there are few respondents, a small change in the numbers can produce a large change in the apparent percentages. For the other continents, the percentage of respondents with AI in production agreed within 2%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02.png" alt="" class="wp-image-14412" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02-300x213.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig02-768x545.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 2. <em>AI adoption by continent</em></figcaption></figure>



<p>After Oceania, North America and Europe had the greatest percentages of respondents with AI in production (both 27%), followed by Asia and South America (24% and 22%, respectively). Africa had the smallest percentage of respondents with AI in production (13%) and the largest percentage of nonusers (42%). However, as with Oceania, the number of respondents from Africa was small, so it’s hard to put too much credence in these percentages. We continue to hear exciting stories about&nbsp;<a href="https://african.business/2021/03/technology-information/the-very-real-benefits-of-ai-in-africa/">AI in Africa</a>, many of which demonstrate creative thinking that is sadly lacking in the VC-frenzied markets of North America, Europe, and Asia.</p>



<h3>Adoption by Industry</h3>



<p>The distribution of respondents by industry was almost the same as last year. The largest percentages of respondents were from the computer hardware and financial services industries (both about 15%, though computer hardware had a slight edge), education (11%), and healthcare (9%). Many respondents reported their industry as “Other,” which was the third most common answer. Unfortunately, this vague category isn’t very helpful, since it featured industries ranging from academia to wholesale, and included some exotica like drones and surveillance—intriguing but hard to draw conclusions from based on one or two responses. (Besides, if you’re working on surveillance, are you really going to tell people?) There were well over 100 unique responses, many of which overlapped with the industry sectors that we listed.</p>



<p>We see a more interesting story when we look at the maturity of AI practices in these industries. The retail and financial services industries had the greatest percentages of respondents reporting AI applications in production (37% and 35%, respectively). These sectors also had the fewest respondents reporting that they weren’t using AI (26% and 22%). That makes a lot of intuitive sense: just about all retailers have established an online presence, and part of that presence is making product recommendations, a classic AI application. Most retailers using online advertising services rely heavily on AI, even if they don’t consider using a service like Google “AI in production.” AI is certainly there, and it’s driving revenue, whether or not they’re aware of it. Similarly, financial services companies were early adopters of AI: automated check reading was one of the first enterprise AI applications, dating to well before the current surge in AI interest.</p>



<p>Education and government were the two sectors with the fewest respondents reporting AI projects in production (9% for both). Both sectors had many respondents reporting that they were evaluating the use of AI (46% and 50%). These two sectors also had the largest percentage of respondents reporting that they weren’t using AI. These are industries where appropriate use of AI could be very important, but they’re also areas in which a lot of damage could be done by inappropriate AI systems. And, frankly, they’re both areas that are plagued by outdated IT infrastructure. Therefore, it’s not surprising that we see a lot of people evaluating AI—but also not surprising that relatively few projects have made it into production.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03.png" alt="" class="wp-image-14413" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03-300x249.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig03-768x637.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 3. <em>AI adoption by industry</em></figcaption></figure>



<p>As you’d expect, respondents from companies with AI in production reported that a larger portion of their IT budget was spent on AI than did respondents from companies that were evaluating or not using AI. 32% of respondents with AI in production reported that their companies spent over 21% of their IT budget on AI (18% reported that 11%–20% of the IT budget went to AI; 20% reported 6%–10%). Only 12% of respondents who were evaluating AI reported that their companies were spending over 21% of the IT budget on AI projects. Most of the respondents who were evaluating AI came from organizations that were spending under 5% of their IT budget on AI (31%); in most cases, “evaluating” means a relatively small commitment. (And remember that roughly half of all respondents were in the “evaluating” group.)</p>



<p>The big surprise was among respondents who reported that their companies weren’t using AI. You’d expect their IT expense to be zero, and indeed, over half of the respondents (53%) selected 0%–5%; we’ll assume that means 0. Another 28% checked “Not applicable,” also a reasonable response for a company that isn’t investing in AI. But a measurable number had other answers, including 2% (10 respondents) who indicated that their organizations were spending over 21% of their IT budgets on AI projects. 13% of the respondents not using AI indicated that their companies were spending 6%–10% on AI, and 4% of that group estimated AI expenses in the 11%–20% range. So even when our respondents report that their organizations aren’t using AI, we find that they’re doing something: experimenting, considering, or otherwise “kicking the tires.” Will these organizations move toward adoption in the coming years? That’s anyone’s guess, but AI may be penetrating organizations that are on the back side of the adoption curve (the so-called “late majority”).</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04.png" alt="" class="wp-image-14414" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04-300x187.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig04-768x478.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 4. <em>Share of IT budgets allocated to AI</em></figcaption></figure>



<p>Now look at the graph showing the percentage of IT budget spent on AI by industry. Just eyeballing this graph shows that most companies are in the 0%–5% range. But it’s more interesting to look at what industries are, and aren’t, investing in AI. Computers and healthcare have the most respondents saying that over 21% of the budget is spent on AI. Government, telecommunications, manufacturing, and retail are the sectors where respondents report the smallest (0%–5%) expense on AI. We’re surprised at the number of respondents from retail who report low IT spending on AI, given that the retail sector also had a high percentage of practices with AI in production. We don’t have an explanation for this, aside from saying that any study is bound to expose some anomalies.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05.png" alt="" class="wp-image-14415" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05-300x294.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig05-768x751.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 5. <em>Share of IT budget allocated to AI, by industry</em></figcaption></figure>



<h3>Bottlenecks</h3>



<p>We asked respondents what the biggest bottlenecks were to AI adoption. The answers were strikingly similar to last year’s. Taken together, respondents with AI in production and respondents who were evaluating AI say the biggest bottlenecks were lack of skilled people and lack of data or data quality issues (both at 20%), followed by finding appropriate use cases (16%).</p>



<p>Looking at &#8220;in production&#8221;&nbsp;and “evaluating” practices separately gives a more nuanced picture. Respondents whose organizations were evaluating AI were much more likely to say that company culture is a bottleneck, a challenge that Andrew Ng addressed in a recent issue of his&nbsp;<a href="https://info.deeplearning.ai/the-batch-which-nation-dominates-ai-meet-your-robot-colleagues-gans-forecast-weather-unmasking-qanons-anonymous-conspiracy-theorist">newsletter</a>. They were also more likely to see problems in identifying appropriate use cases. That’s not surprising: if you have AI in production, you’ve at least partially overcome problems with company culture, and you’ve found at least some use cases for which AI is appropriate.</p>



<p>Respondents with AI in production were significantly more likely to point to lack of data or data quality as an issue. We suspect this is the result of hard-won experience. Data always looks much better before you’ve tried to work with it. When you get your hands dirty, you see where the problems are. Finding those problems, and learning how to deal with them, is an important step toward developing a truly mature AI practice. These respondents were somewhat more likely to see problems with technical infrastructure—and again, understanding the problem of building the infrastructure needed to put AI into production comes with experience.</p>



<p>Respondents who are using AI (the &#8220;evaluating&#8221; and &#8220;in production&#8221; groups—that is, everyone who didn&#8217;t identify themselves as a &#8220;non-user&#8221;)&nbsp;were in agreement on the lack of skilled people. A shortage of trained data scientists has been predicted for years. In&nbsp;<a href="https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/">last year’s survey of AI adoption</a>, we noted that we’ve finally seen this shortage come to pass, and we expect it to become more acute. This group of respondents were also in agreement about legal concerns. Only 7% of the respondents in each group listed this as the most important bottleneck, but it’s on respondents&#8217; minds.</p>



<p>And nobody’s worrying very much about hyperparameter tuning.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06.png" alt="" class="wp-image-14416" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06-292x300.png 292w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig06-768x789.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 6. <em>Bottlenecks to AI adoption</em></figcaption></figure>



<p>Looking a bit further into the difficulty of hiring for AI, we found that respondents with AI in production saw the most significant skills gaps in these areas: ML modeling and data science (45%), data engineering (43%), and maintaining a set of business use cases (40%). We can rephrase these skills as core AI development, building data pipelines, and product management.&nbsp;<a href="https://www.oreilly.com/radar/product-management-for-ai/">Product management for AI</a>, in particular, is an important and still relatively new specialization that requires understanding the specific requirements of AI systems.</p>



<h3>AI Governance</h3>



<p>Among respondents with AI products in production, the number of those whose organizations had a governance plan in place to oversee how projects are created, measured, and observed was roughly the same as those that didn&#8217;t (49% yes, 51% no). Among respondents who were evaluating AI, relatively few (only 22%) had a governance plan.</p>



<p>The large number of organizations lacking AI governance is disturbing. While it’s easy to assume that AI governance isn’t necessary if you’re only doing some experiments and proof-of-concept projects, that’s dangerous. At some point, your proof-of-concept is likely to turn into an actual product, and then your governance efforts will be playing catch-up. It’s even more dangerous when you’re relying on AI applications in production. Without formalizing some kind of AI governance, you’re less likely to know when models are becoming stale, when results are biased, or when data has been collected improperly.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07.png" alt="" class="wp-image-14417" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07-300x102.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig07-768x260.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 7. <em>Organizations with an AI governance plan in place</em></figcaption></figure>



<p>While we didn’t ask about AI governance in last year’s survey, and consequently can’t do year-over-year comparisons, we did ask respondents who had AI in production what risks they checked for. We saw almost no change. Some risks were up a percentage point or two and some were down, but the ordering remained the same. Unexpected outcomes remained the biggest risk (68%, down from 71%), followed closely by model interpretability and model degradation (both 61%). It’s worth noting that unexpected outcomes and model degradation are business issues. Interpretability, privacy (54%), fairness (51%), and safety (46%) are all human issues that may have a direct impact on individuals. While there may be AI applications where privacy and fairness aren’t issues (for example, an embedded system that decides whether the dishes in your dishwasher are clean), companies with AI practices clearly need to place a higher priority on the human impact of AI.</p>



<p>We’re also surprised to see that security remains close to the bottom of the list (42%, unchanged from last year). Security is finally being taken seriously by many businesses, just not for AI. Yet AI has&nbsp;<a href="https://hubsecurity.com/blog/cyber-security/security-threats-for-ai-and-machine-learning/">many unique risks</a>: data poisoning, malicious inputs that generate false predictions, reverse engineering models to expose private information, and many more among them. After last year’s many costly attacks against businesses and their data, there’s no excuse for being lax about cybersecurity. Unfortunately, it looks like AI practices are slow in catching up.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08.png" alt="" class="wp-image-14418" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08-300x197.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig08-768x505.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 8. <em>Risks checked by respondents with AI in production</em></figcaption></figure>



<p>Governance and risk-awareness are certainly issues we’ll watch in the future. If companies developing AI systems don’t put some kind of governance in place,&nbsp;<a href="https://hbr.org/sponsored/2021/12/how-organizations-can-mitigate-the-risks-of-ai">they are risking their businesses</a>. AI will be controlling you, with unpredictable results—results that increasingly include damage to your reputation and large legal judgments. The least of these risks is that governance will be imposed by legislation, and those who haven’t been practicing AI governance will need to catch up.</p>



<h2>Tools</h2>



<p>When we looked at the tools used by respondents working at companies with AI in production, our results were very similar to last year’s. TensorFlow and scikit-learn are the most widely used (both 63%), followed by PyTorch, Keras, and AWS SageMaker (50%, 40%, and 26%, respectively). All of these are within a few percentage points of last year’s numbers, typically a couple of percentage points lower. Respondents were allowed to select multiple entries; this year the average number of entries per respondent appeared to be lower, accounting for the drop in the percentages (though we’re unsure why respondents checked fewer entries).</p>



<p>There appears to be some consolidation in the tools marketplace. Although it’s great to root for the underdogs, the tools at the bottom of the list were also slightly down: AllenNLP (2.4%), BigDL (1.3%), and RISELab’s Ray (1.8%). Again, the shifts are small, but dropping by one percent when you’re only at 2% or 3% to start with could be significant—much more significant than scikit-learn’s drop from 65% to 63%. Or perhaps not; when you only have a 3% share of the respondents, small, random fluctuations can seem large.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09.png" alt="" class="wp-image-14419" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09-291x300.png 291w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig09-768x792.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 9. <em>Tools used by respondents with AI in production</em></figcaption></figure>



<h3>Automating ML</h3>



<p>We took an additional look at tools for automatically generating models. These tools are commonly called “AutoML” (though that’s also a product name used by Google and Microsoft). They’ve been around for a few years; the company developing DataRobot, one of the oldest tools for automating machine learning, was founded in 2012. Although building models and programming aren’t the same thing, these tools are part of the&nbsp;<a href="https://www.oreilly.com/radar/low-code-and-the-democratization-of-programming">“low code” movement</a>. AutoML tools fill similar needs: allowing more people to work effectively with AI and eliminating the drudgery of doing hundreds (if not thousands) of experiments to tune a model.</p>



<p>Until now, the use of AutoML has been a relatively small part of the picture. This is one of the few areas where we see a significant difference between this year and last year. Last year 51% of the respondents with AI in production said they weren’t using AutoML tools. This year only 33% responded “None of the above” (and didn’t write in an alternate answer).</p>



<p>Respondents who were “evaluating” the use of AI appear to be less inclined to use AutoML tools (45% responded “None of the above”). However, there were some important exceptions. Respondents evaluating ML were more likely to use Azure AutoML than respondents with ML in production. This fits anecdotal reports that Microsoft Azure is the most popular cloud service for organizations that are just moving to the cloud. It’s also worth noting that the usage of Google Cloud AutoML and IBM AutoAI was similar for respondents who were evaluating AI and for those who had AI in production.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10.png" alt="" class="wp-image-14420" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10-300x272.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig10-768x697.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 10. <em>Use of AutoML tools</em></figcaption></figure>



<h3>Deploying and Monitoring AI</h3>



<p>There also appeared to be an increase in the use of automated tools for deployment and monitoring among respondents with AI in production. “None of the above” was still the answer chosen by the largest percentage of respondents (35%), but it was down from 46% a year ago. The tools they were using were similar to last year’s: MLflow (26%), Kubeflow (21%), and TensorFlow Extended (TFX, 15%). Usage of MLflow and Kubeflow increased since 2021; TFX was down slightly. Amazon SageMaker (22%) and TorchServe (6%) were two new products with significant usage; SageMaker in particular is poised to become a market leader. We didn’t see meaningful year-over-year changes for Domino, Seldon, or Cortex, none of which had a significant market share among our respondents. (BentoML is new to our list.)</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11.png" alt="" class="wp-image-14421" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11-300x256.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/ai_report_fig11-768x655.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>Figure 11. <em>Tools used for deploying and monitoring AI</em></figcaption></figure>



<p>We saw similar results when we looked at automated tools for data versioning, model tuning, and experiment tracking. Again, we saw a significant reduction in the percentage of respondents who selected “None of the above,” though it was still the most common answer (40%, down from 51%). A significant number said they were using homegrown tools (24%, up from 21%). MLflow was the only tool we asked about that appeared to be winning the hearts and minds of our respondents, with 30% reporting that they used it. Everything else was under 10%. A healthy, competitive marketplace? Perhaps. There’s certainly a lot of room to grow, and we don’t believe that the problem of data and model versioning has been solved yet.</p>



<h2>AI at a Crossroads</h2>



<p>Now that we’ve looked at all the data, where is AI at the start of 2022, and where will it be a year from now? You could make a good argument that AI adoption has stalled. We don’t think that’s the case. Neither do venture capitalists; a study by the OECD,&nbsp;<a href="https://www.oecd.org/digital/venture-capital-investments-in-artificial-intelligence-f97beae7-en.htm"><em>Venture Capital Investments in Artificial Intelligence</em></a>, says that in 2020, 20% of all VC funds went to AI companies. We would bet that number is also unchanged in 2021. But what are we missing? Is enterprise AI stagnating?</p>



<p>Andrew Ng, in his newsletter&nbsp;<a href="https://read.deeplearning.ai/the-batch/issue-137/"><em>The Batch</em></a>, paints an optimistic picture. He points to Stanford&#8217;s&nbsp;<a href="https://aiindex.stanford.edu/report/"><em>AI Index Report</em></a>&nbsp;for 2022, which says that private investment almost doubled between 2020 and 2021. He also points to the rise in regulation as evidence that AI is unavoidable: it&#8217;s an inevitable part of 21st century life. We agree that AI is everywhere, and in many places, it’s not even seen. As we’ve mentioned, businesses that are using third-party advertising services are almost certainly using AI, even if they never write a line of code. It’s embedded in the advertising application. Invisible AI—AI that has become part of the infrastructure—isn’t going away. In turn, that may mean that we’re thinking about AI deployment the wrong way. What’s important isn’t whether organizations have deployed AI on their own servers or on someone else’s. What we should really measure is whether organizations are using infrastructural AI that’s embedded in other systems that are provided as a service. AI as a service (including AI as part of another service) is an inevitable part of the future.</p>



<p>But not all AI is invisible; some is very visible. AI is being adopted in some ways that, until the past year, we’d have considered unimaginable. We’re all familiar with chatbots, and the idea that AI can give us better chatbots wasn’t a stretch. But GitHub’s Copilot was a shock: we didn’t expect AI to write software. We saw (and&nbsp;<a href="https://www.oreilly.com/radar/automated-coding-and-the-future-of-programming/">wrote about</a>) the research leading up to Copilot but didn’t believe it would become a product so soon. What’s more shocking? We’ve heard that, for some programming languages,&nbsp;<a href="https://www.axios.com/copilot-artificial-intelligence-coding-github-9a202f40-9af7-4786-9dcb-b678683b360f.html">as much as 30% of new code is being suggested by the company&#8217;s AI programming tool Copilot</a>. At first, many programmers thought that Copilot was no more than AI&#8217;s clever party trick. That&#8217;s clearly not the case.&nbsp;Copilot has become a useful tool in surprisingly little time, and with time, it will only get better.</p>



<p>Other applications of large language models—automated customer service, for example—are rolling out (our survey didn’t pay enough attention to them). It remains to be seen whether humans will feel any better about interacting with AI-driven customer service than they do with humans (or horrendously scripted bots). There’s an intriguing hint that AI systems are&nbsp;<a href="https://journals.sagepub.com/doi/abs/10.1177/00222429211066972">better at delivering bad news</a>&nbsp;to humans. If we need to be told something we don’t want to hear, we’d prefer it come from a faceless machine.</p>



<p>We’re starting to see more adoption of automated tools for deployment, along with tools for data and model versioning. That’s a necessity; if AI is going to be deployed into production, you have to be able to deploy it effectively, and modern IT shops don’t look kindly on handcrafted artisanal processes.</p>



<p>There are many more places we expect to see AI deployed, both visible and invisible. Some of these applications are quite simple and low-tech. My four-year-old car displays the speed limit on the dashboard. There are any number of ways this could be done, but after some observation, it became clear that this was a simple computer vision application. (It would report incorrect speeds if a speed limit sign was defaced, and so on.) It’s probably not the fanciest neural network, but there’s no question we would have called this AI a few years ago. Where else? Thermostats, dishwashers, refrigerators, and other appliances? Smart refrigerators were a joke not long ago; now you can buy them.</p>



<p>We also see AI finding its way onto smaller and more limited devices. Cars and refrigerators have seemingly unlimited power and space to work with. But what about small devices like phones? Companies like Google have put a lot of effort into running AI directly on the phone, both doing work like voice recognition and text prediction and actually training models using techniques like federated learning—all without sending private data back to the mothership. Are companies that can’t afford to do AI research on Google’s scale benefiting from these developments? We don’t yet know. Probably not, but that could change in the next few years and would represent a big step forward in AI adoption.</p>



<p>On the other hand, while Ng is certainly right that demands to regulate AI are increasing, and those demands are probably a sign of AI&#8217;s ubiquity, they&#8217;re also a sign that the AI we&#8217;re getting is not the AI we want. We’re disappointed not to see more concern about ethics, fairness, transparency, and mitigating bias. If anything, interest in these areas has slipped slightly. When the biggest concern of AI developers is that their applications might give “unexpected results,” we’re not in a good place. If you only want expected results, you don’t need AI. (Yes, I’m being catty.) We’re concerned that only half of the respondents with AI in production report that AI governance is in place. And we’re horrified, frankly, not to see more concern about security. At least there hasn’t been a year-over-year decrease—but that’s a small consolation, given the events of last year.</p>



<p>AI is at a crossroads. We believe that AI will be a big part of our future. But will that be the future we want or the future we get because we didn’t pay attention to ethics, fairness, transparency, and mitigating bias? And will that future arrive in 5, 10, or 20 years? At the start of this report, we said that when AI was the darling of the technology press, it was enough to be interesting. Now it’s time for AI to get real, for AI practitioners to develop better ways to collaborate between AI and humans, to find ways to make work more rewarding and productive, to build tools that can get around the biases, stereotypes, and mythologies that plague human decision-making. Can AI succeed at that? If there’s another AI winter, it will be because people—real people, not virtual ones—don’t see AI generating real value that improves their lives. It will be because the world is rife with AI applications that they don’t trust. And if the AI community doesn’t take the steps needed to build trust and real human value, the temperature could get rather cold.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2022/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>D-Day in Kyiv</title>
      <link>https://www.oreilly.com/radar/d-day-in-kyiv/</link>
      <comments>https://www.oreilly.com/radar/d-day-in-kyiv/#respond</comments>
      <pubDate>Tue, 22 Mar 2022 18:02:51 +0000</pubDate>
      <dc:creator><![CDATA[Jeffrey Carr]]></dc:creator>
      <category><![CDATA[Security]]></category>
      <category><![CDATA[Commentary]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14392</guid>
      <description>My experience working with Ukraine’s Offensive Cyber Team By Jeffrey CarrMarch 22, 2022 When Russia invaded Ukraine on February 24th,  I had been working with two offensive cyber operators from GURMO—Main Intelligence Directorate of the Ministry of Defense of Ukraine—for several months trying to help them raise funds to expand development on an OSINT (Open [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<h2>My experience working with Ukraine’s
Offensive Cyber Team</h2>



<p>By Jeffrey Carr<br>March 22, 2022</p>



<p>When Russia invaded Ukraine on February 24th,  I had been working with two offensive cyber operators from GURMO—Main Intelligence Directorate of the Ministry of Defense of Ukraine—for several months trying to help them raise funds to expand development on an OSINT (Open Source Intelligence) platform they had invented and were using to identify and track Russian terrorists in the region. Since the technology was sensitive, we used Signal for voice and text calls. There was a lot of tension during the first few weeks of February due to Russia’s military buildup on Ukraine’s borders and the uncertainty of what Putin would do. </p>



<p>Then on February 24th at 6am in Kyiv (February 23, 8pm in Seattle where I live), it happened.</p>



<p><strong>SIGNAL log 23 FEB 2022 20:00 (Seattle)&nbsp; / 24 FEB 2022 06:00 (Kyiv)</strong></p>



<pre class="wp-block-preformatted">Missed audio call - 8:00pm</pre>



<pre class="wp-block-preformatted">It started
8:01PM</pre>



<pre class="wp-block-preformatted">                    War?
                    9:36PM</pre>



<pre class="wp-block-preformatted">Incoming audio call - 9:37PM</pre>



<pre class="wp-block-preformatted">                    Call dropped.
                    9:41PM</pre>



<pre class="wp-block-preformatted">                    Are you there?
                    9:42PM</pre>



<p>I didn’t hear from my GURMO friend again for 10 hours. When he pinged me on Signal, it was from a bunker. They were expecting another missile attack at any moment. </p>



<p>“<code>Read this</code>”, he said, and sent me this <a href="https://zn.ua/ukr/POLITICS/ukrajinski-spetssluzhbi-volodijut-informatsijeju-pro-plan-putina-shchodo-kijeva-ta-ukrajini.html">link</a>. “<code>Use Google Translate.</code>”</p>



<p>It linked to an article that described Russia’s operations plan for its attack on Ukraine, obtained by sources of Ukrainian news website <a href="https://zn.ua/ukr/POLITICS/ukrajinski-spetssluzhbi-volodijut-informatsijeju-pro-plan-putina-shchodo-kijeva-ta-ukrajini.html">ZN.UA</a>. It said that the Russian military had sabotage groups already placed in Ukraine whose job was to knock out power and communications in the first 24 hours in order to cause panic. Acts of arson and looting would follow, with the goal of distracting law enforcement from chasing down the saboteurs. Then, massive cyber attacks would take down government websites, including the Office of the President, the General Staff, the Cabinet, and the Parliament (the Verkhovna Rada). The Russian military expected little resistance when it moved against Kyiv and believed that it could capture the capital in a matter of days.</p>



<blockquote class="wp-block-quote"><p><em>The desired result is to seize the leadership of the state (it is not specified who exactly) and force a peace agreement to be signed on Russian terms under blackmail and the possibility of the death of a large number of civilians.</em></p><p><em>Even if part of the country&#8217;s leadership is evacuated, some pro-Russian politicians will be able to &#8220;take responsibility&#8221; and sign documents, citing the &#8220;escape&#8221; of the political leadership from Kyiv.</em></p><p><em>As a result, Ukraine can be divided into two parts—on the principle of West and East Germany, or North and South Korea.</em></p><p><em>At the same time, the Russian Federation recognizes the legitimate part of Ukraine that will sign these agreements and will be loyal to the Russian Federation. Guided by the principle: &#8220;he who controls the capital—he controls the state.&#8221;</em></p></blockquote>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks.png" alt="" class="wp-image-14396" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks.png 504w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/netblocks-260x300.png 260w" sizes="(max-width: 504px) 100vw, 504px" /></figure>



<p>The first significant Russian cyber attack of
the war is suspected to be the one that took down satellite provider ViaSat at
precisely 06:00 Kyiv time (04:00 UTC), the exact time that Russia started its
invasion. </p>



<p>The cause is believed to be a malicious
firmware update sent to ViaSat customers that “bricked” the satellite modems.
Since ViaSat is a defense contractor, the NSA, France’s ANSSI, and Ukrainian
Intelligence are investigating. ViaSat hired Mandiant to handle digital
forensics and incident response (DFIR). </p>



<p>“<code>Is Ukraine planning to retaliate?</code>”, I asked.</p>



<p>“<code>We’re engaging in six hours. I’ll keep you informed.</code>”</p>



<p>That last exchange happened about 22 hours
after the start of the war. </p>



<p><strong>FRIDAY,
FEB 25, 2022 07:51</strong></p>



<p>I received a Signal alert. </p>



<p>“<code>Download ready</code>” and a link.</p>



<p>The GURMO cyber team had gained access to the accounting and document management system at Russian Military Unit 6762, part of the Ministry of Internal Affairs that deals with riot control, terrorists, and the territorial defense of Russia. They downloaded all of their personnel data, including passports, military IDs, credit cards, and payment records. I was sent a sampling of documents to do further research and post via my channels.</p>



<p>The credit cards were all issued by Sberbank. “<code>What are you going to do with these</code>”, I asked. He sent me a wink and a grin icon on Signal and said:</p>



<pre class="wp-block-preformatted">Buy weapons and ammo for our troops! 
We start again at 6:30am tomorrow. 
When you wake up, join us.
                    
                    Will do!</pre>



<p>Over the next few days, GURMO’s offensive
cyber team hacked a dizzying array of Russian targets and stole thousands of
files from:</p>



<ul><li>Black Sea Fleet’s communications
servers</li><li>ROSATOM</li><li>FSB Special Operations unit 607</li><li>Sergey G. Buev, the Chief Missile
Officer of the Ministry of Defense</li><li>Federal Air Transport Agency</li></ul>



<p>Everything was in Russian, so the translation process was very time-consuming. There were literally hundreds of documents in all different file types, and to make the translation process even harder, many of the documents were images of a document. You can’t just upload those into Google Translate. You have to download the Google Translate app onto your mobile phone, then point it at the document on your screen and read it that way.</p>



<p>Once I had read enough, I could write a post at my Inside Cyber Warfare <a href="https://jeffreycarr.substack.com/">Substack</a> that provided information and context to the breach. Between the translation, research, writing, and communication with GURMO ,who were 11 hours ahead (10 hours after the time change), I was getting about 4 ½ hours of sleep each night. </p>



<h2>We Need Media Support</h2>



<p><strong>TUESDAY,
MARCH 1, 2022 09:46 (Seattle)</strong></p>



<p>On Signal</p>



<pre class="wp-block-preformatted">We need media support from USA.
All the attacks you mentioned during these 6 days.
We have to make headlines to demoralize Russians.

                   I know the team at a young British PR firm.
                   I’ll check with them now.</pre>



<p>Nara Communications immediately stepped up to the challenge. They agreed to waive their fee and help place news stories about the GURMO cyber team’s successes. The Ukrainians did their part and gave them some amazing breaches, starting with the Beloyarsk Nuclear Power Plant—the world’s only commercial fast breeder reactors. Other countries were spending billions of dollars trying to achieve what Russia had already mastered, so a breach of their design documents and processes was a big deal. </p>



<p>The problem was that journalists wanted to
speak to GURMO and that was off the table for three important reasons:</p>



<ol><li>They were too busy fighting a war to give interviews.</li><li>The Russian government knew who they were, and their names and faces were on the playing cards given to Kadryov’s Chechen Guerillas for assassination.</li><li>They didn’t want to expose themselves to facial recognition or voice capture technologies because&#8230;see #2. </li></ol>



<p>Journalists had only a few options if they didn’t want to run with a single-source story. </p>



<p>They could speak with me because I was the only person who the GURMO team would directly speak to. Plus, I had possession of the documents and understood what they were.</p>



<p>They could contact the CIA Legat in Warsaw, Poland where the U.S. embassy had evacuated to prior to the start of the war. GURMO worked closely with and gave frequent briefings to its allied partners, and they would know about these breaches. Of course, the CIA most likely wouldn’t speak with a journalist. </p>



<p>They could speak with other experts to vet the documents, which would effectively be their second source after speaking with me. Most reporters at major outlets didn’t bother reporting these breaches under those conditions. To make matters worse, there were no obvious victims. The GURMO hackers weren’t breaking things, they were stealing things, and they liked to keep a persistent presence in the network so they could keep coming back for more. Plus, Russia often implemented a communications strategy known as Ихтамнет (Ihtamnet), which roughly translated means “nothing happened” or to put it into context “What hacks? There were no hacks.”</p>



<p>In spite of all those obstacles, Nara Communications was successful in getting an article placed with SC magazine, a radio interview with Britain’s <em>The Times</em>, and a podcast with the <em>Evening Standard</em>. </p>



<p>By mid-March, Putin showed no signs of wanting
peace, even after President Zelensky had conceded that NATO membership was
probably off the table for Ukraine, and GURMO was popping bigger targets than
ever.</p>



<p>The Russians&#8217; plan to establish a fully automated lunar base called Luna-Glob was breached. Russia’s EXOMars project was breached. The new launch complex being built at Vostochny for the Angara rocket was breached. In every instance, a trove of files was downloaded for study by Ukraine’s government and shared with its allies. A small amount was always carved out for me to review, post at the <a href="https://jeffreycarr.substack.com/">Inside Cyber Warfare</a> Substack, and share with journalists. Journalist <a href="https://www.scmagazine.com/analysis/breach/in-a-first-ukraine-leaks-russian-intellectual-property-as-act-of-war">Joe Uchill</a> referred to this strategy as Hack and Leak. </p>



<h2>Hack and Leak</h2>



<p>By hacking some of Russia’s proudest
accomplishments (its space program) and most successful technologies (its
nuclear research program), the Ukrainian government is sending Putin a message
that your cybersecurity systems cannot keep us out, that even your most
valuable technological secrets aren’t safe from us, and that if you push us too
far, we can do whatever we want to your networks. </p>



<p>Apart from the attack on ViaSat, there hasn’t been evidence of any destructive cyber attacks against Ukrainian infrastructure. Part of that was strategic planning on the part of Ukraine (that’s all that I can say about that), part was Ukraine’s cyber defense at work, and part of that may be that GURMO’s strategy is working. However, there’s no sign that these leaks are having any effect on impeding Russia’s military escalation, probably because that’s driven out of desperation in the face of its enormous military losses so far. Should that escalation continue, GURMO has contingency plans that will bring the war home to Russia. </p>



<hr class="wp-block-separator" />



<p>Jeffrey Carr has been an internationally-known cybersecurity adviser, author, and researcher since 2006. He has worked as a Russia SME for the CIA&#8217;s Open Source Center Eurasia Desk. He invented REDACT, the world’s first global R&amp;D database and search engine to assist companies in identifying which intellectual property is of value to foreign governments. He is the founder and organizer of Suits &amp; Spooks, a “collision” event to discuss hard challenges in the national security space, and is the author of <em>Inside Cyber Warfare: Mapping the Cyber Underworld</em> (O&#8217;Reilly Media, 2009, 2011).&nbsp;</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/d-day-in-kyiv/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>The Future of Security</title>
      <link>https://www.oreilly.com/radar/the-future-of-security/</link>
      <comments>https://www.oreilly.com/radar/the-future-of-security/#respond</comments>
      <pubDate>Tue, 15 Mar 2022 14:02:05 +0000</pubDate>
      <dc:creator><![CDATA[Christina Morillo]]></dc:creator>
      <category><![CDATA[Cloud]]></category>
      <category><![CDATA[Security]]></category>
      <category><![CDATA[Research]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14357</guid>
      <description>The future of cybersecurity is being shaped by the need for companies to secure their networks, data, devices, and identities. This includes adopting security frameworks like zero trust, which will help companies secure internal information systems and data in the cloud. With the sheer volume of new threats, today’s security landscape has become more complex [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>The future of cybersecurity is being shaped by the need for companies to secure their networks, data, devices, and identities. This includes adopting security frameworks like zero trust, which will help companies secure internal information systems and data in the cloud. With the sheer volume of new threats, today’s security landscape has become more complex than ever. With the rise of ransomware, firms have become more aware of their ability to recover from an attack if they are targeted, but security needs also continue to evolve as new technologies, apps, and devices are developed faster than ever before. This means that organizations must be focused on solutions that allow them to stay on the cutting edge of technology and business.</p>



<p>What does the future have in store for cybersecurity? What are some of today’s trends, and what might be future trends in this area? Several significant cybersecurity trends have already emerged or will continue to gain momentum this coming year and beyond. This report covers four of the most important trends:</p>



<ul><li><strong>Zero&nbsp;trust</strong>&nbsp;(ZT) security (also known as context-aware security, policy-based enforcement), which is becoming more widespread and dominates many enterprise and vendor conversations.</li><li><strong>Ransomware threats and attacks</strong>, which will continue to rise and wreak havoc.</li><li><strong>Mobile device securit</strong>y, which is becoming more urgent with an increase in remote work and mobile devices.</li><li><strong>Cloud security and automation</strong>, as a means for addressing cloud security issues and the workforce skills gap/ shortage of professionals.Related to this is&nbsp;<em>cybersecurity as a service</em>&nbsp;(CaaS or CSaaS) that will also gain momentum as companies turn to vendors who can provide extensive security infrastructure and support services at a fraction of the cost of building self-managed infrastructure.</li></ul>



<p>We’ll start with zero trust, a critical element for any security program in this age of sophisticated and targeted cyberattacks.</p>



<h2>Zero Trust Security</h2>



<p>For decades, security architects have focused on perimeter protection, such as firewalls and other safety measures. However, as cloud computing increased, experts recognized that traditional strategies and solutions would not work in a mobile-first/hybrid world. User identities could no longer be confined to a company’s internal perimeter, and with employees needing access to business data and numerous SaaS applications while working remotely or on business travel, it became impossible to control access centrally.</p>



<p>The technology landscape is witnessing an emergence of security vendors rethinking the efficacy of their current security measures and offerings without businesses needing to rebuild entire architectures. One such approach is&nbsp;<em>zero trust</em>, which challenges perimeter network access controls by trusting no resources by default. Instead, zero trust redefines the network perimeter, treating all users and devices as inherently untrusted and likely compromised, regardless of their location within the network. Microsoft’s approach to zero trust security focuses on the contextual management of identities, devices, and applications—granting access based on the continual verification of identities, devices, and access to services.<sup><a href="#fn1">1</a></sup></p>



<div class="wp-block-group has-very-light-gray-background-color has-background"><div class="wp-block-group__inner-container">
<p><strong>NOTE</strong></p>
<p>Zero trust security is a paradigm that leverages identity for access control and combines it with contextual data, continuous analysis, and automated response to ensure that the only network resources accessible to users and devices are those explicitly authorized for consumption.<sup><a href="#fn2">2</a></sup></p>
</div></div>



<p>In&nbsp;<em>Zero Trust Networks</em>&nbsp;(O’Reilly, 2017), Evan Gilman and Doug Barth split a ZT network into five fundamental assertions:</p>



<ul><li>The network is always assumed to be hostile.</li><li>External and internal threats exist on the web at all times.</li><li>Network locality is not sufficient for decided trust in a network.</li><li>Every device user and network flow is authenticated and authorized.</li><li>Policies must be dynamic and calculated from as many data sources as possible.<sup><a href="#fn3">3</a></sup></li></ul>



<p>Therefore, a zero trust architecture shifts from the traditional perimeter security model to a distributed, context-aware, and continuous policy enforcement model. In this model, requests for access to protected resources are first made through the control plane, where both the device and user must be continuously authenticated and authorized.</p>



<p>An identity first, contextual, and continual enforcement security approach will be especially critical for companies interested in implementing cloud services. Businesses will continue to focus on securing their identities, including device identities, to ensure that access control depends on context (user, device, location, and behavior)&nbsp;<em>and</em> policy-based rules to manage the expanding ecosystem of users and devices seeking access to corporate resources.</p>



<p>Enterprises that adopt a zero trust security model will more confidently allow access to their resources, minimize risks, and better mitigate cybersecurity attacks. IAM (identity and access management) is and will continue to be a critical component of a zero trust strategy.</p>



<p>The rise of cryptocurrency, the blockchain, and web3 technologies<sup><a href="#fn4">4</a></sup> has also introduced conversations around decentralized identity and verifiable credentials.<sup><a href="#fn5">5</a></sup> The decentralized identity model suggests that individuals own and control their data wherever or whenever used. This model will require identifiers such as usernames to be replaced with self-owned and independent IDs that enable data exchange using blockchain and distributed ledger technology to secure transactions. In this model, the thinking is that user data will no longer be centralized and, therefore, less vulnerable to attack.</p>



<p>By contrast, in the traditional identity model, where user identities are verified and managed by a third-party authority/identity provider (IdP), if an attacker gains access to the authority/IdP, they now have the keys to the kingdom, allowing full access to all identities.</p>



<h2>Ransomware, an Emerging and Rapidly Evolving Threat</h2>



<p>One of the most pressing security issues that businesses face today is ransomware. Ransomware is a type of malware that takes over systems and encrypts valuable company data requiring a ransom to be paid before the data is unlocked. The “decrypting and returning” that you pay for is, of course, not guaranteed; as such, ransomware costs are typically more than the costs of preparing for these attacks.</p>



<p>These types of attacks can be very costly for businesses, both in terms of the money they lose through ransomware and the potential damage to a company’s reputation. In addition, ransomware is a widespread method of attack because it works. As a result, the cybersecurity landscape will experience an increasing number of ransomware-related cybersecurity attacks estimated to cost businesses billions in damages.</p>



<p>So, how does it work? Cybercriminals utilize savvy social engineering tactics such as phishing, vishing, smishing, to gain access to a computer or device and launch a cryptovirus. The cryptovirus encrypts all files on the system, or multiple systems, accessible by that user. Then, the target (recipient) receives a message demanding payment for the decryption key needed to unlock their files. If the target (recipient) refuses to comply or fails to pay on time, the price of the decryption key increases exponentially, or the data is released and sold on the dark web. That is the simple case. With a growing criminal ecosystem, and subscription models like ransomware as a service (RaaS), we will continue to see compromised credentials swapped, sold, and exploited, and therefore, continued attacks across the globe.</p>



<div class="wp-block-group has-cyan-bluish-gray-background-color has-background"><div class="wp-block-group__inner-container">
<p><strong>Terms to Know</strong><br><br><strong>Phishing:</strong>&nbsp;a technique of fraudulently obtaining private information. Typically, the phisher sends an email that appears to come from a legitimate business—a bank or credit card company—requesting “verification” of information and warning of some dire consequence if it is not provided. The email usually contains a link to a fraudulent web page that seems legitimate—with company logos and content—and has a form requesting everything from a home address to an ATM card’s PIN or a credit card number.<sup><a href="#fn6">6</a></sup></p>



<p><strong>Smishing:</strong>&nbsp;the act of using SMS text messaging to lure victims into executing a specific action. For example, a text message claims to be from your bank or credit card company but includes a malicious link.</p>



<p><strong>Vishing (voice phishing):</strong>&nbsp;a form of smishing except done via phone calls.</p>



<p><strong>Cryptojacking:</strong>&nbsp;a type of cybercrime that involves unauthorized use of a device’s (computer, smartphone, tablet, server) computing power to mine or generate cryptocurrency.</p>
</div></div>



<p>Because people will trust an email from a person or organization that appears to be a trustworthy sender (e.g., you are more likely to trust an email that seems to be from a recognizable name/brand), these kinds of attacks are often successful.</p>



<p>As these incidents continue to be a daily occurrence, we’ve seen companies like Netflix and Amazon invest in cyber insurance and increase their cybersecurity budgets. However, on a more positive note, mitigating the risk of ransomware attacks has led companies to reassess their approach to protecting their organizations by shoring up defenses with more robust security protocols and advanced technologies. With companies storing exponentially more data than ever before, securing it has become critical.</p>



<p>The future of ransomware is expected to be one that will continue to grow in numbers and sophistication. These attacks are expected to impact even more companies, including targeted attacks focused on supply chains, industrial control systems, hospitals, and schools. As a result, we can expect that it will continue to be a significant threat to businesses.</p>



<h3>Mobile Device Security</h3>



<p>One of the most prominent areas of vulnerability for businesses today is through the use of mobile devices. According to Verizon’s Mobile Security Index 2020 Report,<sup><a href="#fn7">7</a></sup> 39% of businesses had a mobile-related breach in 2020. User threats, app threats, device threats, and network dangers were the top five mobile security threats identified in 2020, according to the survey. One example of a mobile application security threat can be an individual downloading apps that look legitimate but are actually spyware and malware aimed at stealing personal and business information.</p>



<p>Another potential problem involves employees accessing and storing sensitive data or emails on their mobile devices while traveling from one domain to another (for example, airport WiFi, coffee shop WiFi).</p>



<p>Security experts believe that mobile device security is still in its early stages, and many of the same guidelines used to secure traditional computers may not apply to modern mobile devices. While mobile device management (MDM) solutions are a great start, organizations will need to rethink how they handle mobile device security in enterprise environments. The future of mobile device management will also be dependent on&nbsp;<em>contextual data and continuous policy enforcement</em>.</p>



<p>With mobile technology and cloud computing becoming increasingly important to both business and consumer life, smart devices like Apple AirTags, smart locks, video doorbells, and so on are gaining more weight in the cybersecurity debate.</p>



<p>Security concerns range from compromised accounts to stolen devices, and as such, cybersecurity companies are offering new products to help consumers protect their smart homes.</p>



<p>A key issue involving the future of mobile device management is how enterprises can stay ahead of new security issues as they relate to bring your own device (BYOD) and consumer IoT (Internet of Things) devices. Security professionals may also need to reevaluate how to connect a growing number of smart devices in a business environment. Security has never been more important, and new trends will continue to emerge as we move through the future of BYOD and IoT.</p>



<h2>Cloud Security and Automation</h2>



<p>We have seen an increase in businesses moving their operations to the cloud to take advantage of its benefits, such as increased efficiency and scalability. As a result, the cloud is becoming an integral part of how organizations secure their data, with many companies shifting to a hybrid cloud model to address scale, security, legacy technologies, and architectural inefficiencies. However, staffing issues and the complexities of moving from on-premises to cloud/hybrid cloud introduces a new set of security concerns.</p>



<p>Cloud services are also often outsourced, and as such, it can be challenging to determine who is responsible for the security of the data. In addition, many businesses are unaware of the vulnerabilities that exist in their cloud infrastructure and, in many cases, do not have the needed staff to address these vulnerabilities. As a result, security will remain one of the biggest challenges for organizations adopting cloud computing.</p>



<p>One of the most significant benefits cloud computing can provide to security is automation. The need for security automation is rising as manual processes and limited information-sharing capabilities slow the evolution of secure implementations across many organizations. It is estimated that nearly half of all cybersecurity incidents are caused by human error, mitigated through automated security tools rather than manual processes.</p>



<p>However, there can be a downside to automation. The industry has not yet perfected the ability to sift signals from large amounts of noise. An excellent example is what happens around incident response and vulnerability management—both still rely on human intervention or an experienced automation/tooling expert. Industry tooling will need to improve in this area. While automation can also help reduce the impact of attacks, any automated solution runs the risk of being ineffective against unknown threats if human eyes do not assess it before it is put into practice.</p>



<p>In a DevOps environment, automation takes the place of human labor. The key for security will be code-based configuration, and the ability to be far more confident about the current state of existing security and infrastructure appliances. Organizations that have adopted configuration by code will also have higher confidence during audits—for example, an auditor checks each process for changing firewall rules, which already go through change control, then spot checks one out of thousands of rules versus validating the CI/CD pipeline. The auditor then runs checks on your configuration to confirm it meets policy.</p>



<p>The evolution of SOAR (security, orchestration, automation, and response) tools and automation of security policy by code will open up a huge potential benefit for well-audited businesses in the future.</p>



<h2>Automation May Help with the Security Workforce Shortage</h2>



<p>The shortage of cyber workers will persist because there aren’t enough cybersecurity professionals in the workforce, and cyber education isn’t keeping up with the demand at a solid pace. As a result, cybersecurity teams are understaffed and burnt-out, lowering their effectiveness while posing risks.</p>



<p>Automation may help organizations fill the cybersecurity talent gap and address many of the same activities that human employees perform, such as detection, response, and policy configuration.</p>



<p>While automation cannot completely replace the need for human cybersecurity experts, it can assist in decreasing the burden on these professionals and make them more successful in their work. In addition to more professionals joining the field with varying backgrounds, automated technologies will play a significant role in mitigating the impact of cyberattacks and assisting in solving the cybersecurity workforce shortage problem.</p>



<h2>(Cyber)Security as a Service</h2>



<p>Cybersecurity as a service (CaaS or CSaaS) is growing more popular as companies turn to managed service vendors that can provide extensive security infrastructure and support services at a fraction of the cost of building self-managed infrastructure. As a result, organizations can use their resources more effectively by outsourcing security needs to a specialized vendor rather than building in-house infrastructure.</p>



<p>CaaS provides managed security services, intrusion detection and prevention, and firewalls by a third-party vendor. By outsourcing cybersecurity functions to a specialist vendor, companies can access the security infrastructure support they need without investing in extensive on-site infrastructure, such as firewalls and intrusion detection systems (IDS).</p>



<p>There are additional benefits:</p>



<ul><li>Access to the latest threat protection technologies.</li><li>Reduced costs: outsourced cybersecurity solutions can be less expensive than an in-house security team.</li><li>Improved internal resources: companies can focus on their core business functions by outsourcing security to a third party.</li><li>Flexibility: companies can scale their security needs as needed.</li></ul>



<p>The ransomware attack on Hollywood Presbyterian Medical Center<sup><a href="#fn8">8</a></sup> is an excellent example of why CaaS will continue to be sought after by organizations of all sizes. Cybercriminals locked the hospital’s computer systems and demanded a ransom payment to unlock them. As a result, the hospital was forced to turn to a cybersecurity vendor for help in restoring its computer systems.</p>



<p>Of course, this approach has disadvantages:</p>



<ul><li>Loss of control over how data is stored and who has access to your data/infrastructure. Security tooling often needs to run at the highest levels of privilege, enabling attackers to attack enterprises at scale, use the managed service provider network to bypass security safeguards, or exploit software vulnerabilities like SolarWinds Log4j.</li><li>In addition, CaaS providers may or may not support existing legacy software or critical business infrastructure specific to each organization.</li></ul>



<p>CaaS is expected to continue on a solid growth path as more enterprises rely on cloud-based systems and the IoT for their business operations.</p>



<h2>Conclusion</h2>



<p>Cyberattacks continue to be successful because they are effective. Thanks to cutting-edge technology, services, and techniques available to every attacker, organizations can no longer afford to make security an afterthought. To defend against present and future cyberattacks, businesses must develop a comprehensive security plan that incorporates automation, analytics, and context-aware capabilities. Now more than ever, companies must be more diligent about protecting their data, networks, and employees.</p>



<p>Whether businesses embrace identity-first and context-aware strategies like zero trust, or technologies like cloud computing, mobile devices, or cybersecurity as a service (CaaS), the growth of ransomware and other cyberattacks is forcing many companies to rethink their overall cybersecurity strategies. As a result, organizations will need to approach security holistically by including all aspects of their business operation and implementing in-depth defense strategies from the onset.</p>



<p>The future is bright for the cybersecurity industry, as companies will continue to develop new technologies to guard against the ever-evolving threat landscape. Government rules, regulations, and security procedures will also continue to evolve to keep up with emerging technologies and the rapid number of threats across both private and public sectors.</p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<p id="fn1">1. <a href="https://www.microsoft.com/en-us/insidetrack/transitioning-to-modern-access-architecture-with-zero-trust#:~:text=Microsoft%20has%20adopted%20a%20modern,of%20identities%2C%20devices%20and%20services" target="_blank" rel="noopener noreferrer">“Transitioning to Modern Access Architecture with Zero Trust”.</a></p>



<p id="fn2">2. Scott Rose et al., <a href="https://csrc.nist.gov/publications/detail/sp/800-207/final" target="_blank" rel="noopener noreferrer">NIST Special Publication 800-207</a>.</p>



<p id="fn3">3. Evan Gilman and Doug Barth, <a href="https://learning.oreilly.com/library/view/zero-trust-networks/9781491962183/" target="_blank" rel="noopener noreferrer"><i>Zero Trust Networks</i></a> (O’Reilly, 2017).</p>



<p id="fn4">4. See <a href="https://www.centre.io/verite" target="_blank" rel="noopener noreferrer">“Decentralized Identity for Crypto Finance”</a>.</p>



<p id="fn5">5. See <a href="https://www.w3.org/TR/vc-data-model/" target="_blank" rel="noopener noreferrer">“Verifiable Credentials Data Model”</a>.</p>



<p id="fn6">6. See this <a href="https://en.wikipedia.org/wiki/Social_engineering_(security)#Phishing" target="_blank" rel="noopener noreferrer">social engineering article</a> for more information.</p>



<p id="fn7">7. <a href="https://www.verizon.com/business/resources/reports/mobile-security-index/2020/state-of-mobile-security/" target="_blank" rel="noopener noreferrer">“The State of Mobile Security”</a>.</p>



<p id="fn8">8. <a href="https://www.latimes.com/business/technology/la-me-ln-hollywood-hospital-bitcoin-20160217-story.html" target="_blank" rel="noopener noreferrer">“Hollywood Hospital Pays $17,000 in Bitcoin to Hackers; FBI Investigating”</a>.</p>



<p></p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/the-future-of-security/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Identity problems get bigger in the metaverse</title>
      <link>https://www.oreilly.com/radar/identity-problems-get-bigger-in-the-metaverse/</link>
      <comments>https://www.oreilly.com/radar/identity-problems-get-bigger-in-the-metaverse/#respond</comments>
      <pubDate>Tue, 15 Mar 2022 14:01:14 +0000</pubDate>
      <dc:creator><![CDATA[Chris Butler]]></dc:creator>
      <category><![CDATA[Metaverse]]></category>
      <category><![CDATA[Deep Dive]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14344</guid>
      <description>If the hype surrounding the metaverse results in something real, it could improve the way you live, work, and play. Or it could create a hellworld where you don’t get to be who you are or want to be.&amp;#160; Whatever people think they’ve read, the metaverse originally imagined in Snow Crash is not a vision [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>If the hype surrounding the metaverse results in something real, it could improve the way you live, work, and play. Or it could create a hellworld where you don’t get to be who you are or want to be.&nbsp; Whatever people think they’ve read, the metaverse originally imagined in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Snow_Crash" target="_blank">Snow Crash</a> is not a vision for an ideal future. In the novel, it’s a world that replaced the “real world” so that people would feel less bad about the reality they actually had. In the end, the story is about the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://brill.com/view/book/9781848881631/BP000011.xml" target="_blank">destabilization of the individual’s identity and implosion of traditional identities</a>, rather than the securing of a new one.</p>



<p>Even in the real world (a.k.a. meatspace), identity can be hard to pin down. You are who you are, but there are many ways you may define yourself depending on the context. In the latest metaverse discourse there has been lots of talk of virtual avatars putting on NFT-based clothing, skins, weapons, and other collectable assets, and then moving those assets around to different worlds and games without issue. Presentation is just a facet of identity, as the real-world fashion industry well knows.</p>



<p>The latest dreams of web3 include decentralized and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://tykn.tech/self-sovereign-identity/" target="_blank">self-sovereign identity</a>. But this is just re-hashing years of identity work that focuses on the how (internet standards) and rarely the why (what people need to feel comfortable with identity online). <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://secondlife.com/" target="_blank">Second Life</a> has been grappling with how people <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.researchgate.net/publication/315529871_The_Process_of_Identity_Building_In_Second_Life" target="_blank">construct a new identity</a> and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/s/story/boobs-muscles-fairy-wings-everything-i-know-about-how-humans-design-their-avatar-selves-e2ebf738750e" target="_blank">present their avatars</a> since 2003.</p>



<p>There are many ways that the web today and the metaverse tomorrow will continue to integrate further with our reality:</p>



<figure class="wp-block-table"><table class=""><tbody><tr><td><strong>Experiences</strong></td><td><strong>Examples</strong>  </td></tr><tr><td>Online through a laptop like the web today</td><td>Posting to Facebook, discussing work on Slack or joining a DAO on Discord.</td></tr><tr><td>Mobile devices while walking around in the real world</td><td>Seeing the comments about a restaurant while standing in front of it, getting directions to a beach or getting access to a private club via an NFT.</td></tr><tr><td>Mixed and augmented reality (MR/AR) experiences where the digital is overlaid on reality</td><td>Chatting with someone who looks like they are sitting next to you or seeing the last message you sent to someone you are talking to.</td></tr><tr><td>Fully immersive virtual reality (VR) experiences</td><td>Going to a chat room in AltspaceVR or playing a game with friends in Beatsaber.</td></tr></tbody></table></figure>



<p>Before we can figure out what identity means to people in “the metaverse,” we need to talk about what identity is, how we use identity in the metaverse, and how we might create systems that better realize the way people want their identities to work online.</p>



<h3>I login therefore I am</h3>



<p>When I mention identity, am I starting a philosophical discussion that answers the question “who am I?” Am I trying to figure out my place within an in-person social event? Or do you want to confirm that I meet some standard, such as being over 21?</p>



<p>All of these questions have a meaning in the digital world; most often, those questions are answered by logging in with an email address and password to get into a particular website. Over the last decade, some services like Facebook, Google, and others have started to allow you to use the identity you have with them to log into other websites.</p>



<p>Is the goal of online identity to have one overarching identity that ties everything together? Our identities are constantly renegotiated and unsaid. I don’t believe we can encode all of the information about our identities into a single digital record, even if some groups are trying. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Facebook_real-name_policy_controversy" target="_blank">Facebook’s real-name policy</a> requires you to use your legal name and makes you collapse all of your possible pseudo-identities into your legal one. If they think you aren’t using a legal name, they require you to upload a government issued document. I’d argue that because people create multiple identities even when faced with account deactivation, it is not their goal to have one single compiled identity.</p>



<h3>All of me(s)</h3>



<p>As we consider identities in the metaverse extensions to the identities we have in the real world, we need to understand that we build pseudo-identities for different interactions. My pseudo-identities for a family, work, my neighborhood, PTA, school friends, etc. all overlap to some extent. These are situations, contexts, realms, or worlds that I am part of, and that extend to the web and metaverse.</p>



<p>In most pseudo-identities there are shared parts that are the “real me,” like my name or my real likeness. Some may be closer to a “core” pseudo-identity that represents more of what I consider to be me; others may just be smaller facets. Each identity is associated with a different reputation, a different level of trust from the community, and different data (profile pictures, posts, etc.).</p>



<p>The most likely place to find our identities are:</p>



<ul><li>Lists of email and password pairs stored in our browsers</li><li>Number of groups we are part of on Facebook</li><li>Gamer tags we have on Oculus, Steam, or PSN</li><li>Discords we chat on</li><li>…and the list goes on</li></ul>



<p>Huge numbers of these identities are being created and managed by hand today. On average, a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://99firms.com/blog/how-many-email-users-are-there/#:~:text=How%20many%20email%20addresses%20does,separate%20personal%20and%20professional%20accounts." target="_blank">person has 1.75 email addresses</a> and manages <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://digitalguardian.com/blog/uncovering-password-habits-are-users-password-security-habits-improving-infographic#:~:text=In%20fact%2C%20a%20Dashlane%20analysis,to%20a%20single%20email%20address." target="_blank">90 online accounts</a>. It will only get more complex and stranger with the addition of the metaverse.</p>



<p>There are times that I don’t want my pseudo-identity’s reputations or information to interact with a particular context; for these cases, I’ll create a pseudo-anonymous identity. There is a lot of prior work on anonymity as a benefit:</p>



<ul><li>Balaji Srinivasan has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=urtXRg9Nl3k" target="_blank">discussed the value of an economy based on pseudonymous identities</a> as a way to “air gap” against repercussions of social problems. </li><li>Jeff Kosseff, professor and author, has recently written a book about the benefits of anonymity “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.cornellpress.cornell.edu/book/9781501762383/the-united-states-of-anonymous/" target="_blank">The United States of Anonymous</a>.” In a great discussion on the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.techdirt.com/2022/03/01/techdirt-podcast-episode-313-the-united-states-of-anonymous/" target="_blank">TechDirt podcast</a> he talks about how the ability to question powers is an important aspect of the ability to be anonymous.</li><li>Christopher &#8220;moot&#8221; Poole, the creator of 4chan, has often talked about the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=a_1UEAGCo30" target="_blank">benefits of anonymous online identities</a> including the ability to be more creative without the risk of failure. Given the large amount of harmful abuse that comes out of communities like 4chan, this argument for anonymity is questionable.</li></ul>



<figure class="wp-block-image size-large is-resized"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-1.png" alt="" class="wp-image-14345" width="582" height="354" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-1.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-1-300x183.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-1-768x468.png 768w" sizes="(max-width: 582px) 100vw, 582px" /><figcaption>My many identities and overlapping zones of attributes, information, and privacy.<br></figcaption></figure>



<p>If you link one of my pseudo-identities to another pseudo-identity in a way I didn’t expect, it can feel like a violation. I expect to control the flow of information about me (see Helen Nissenbaum’s work on <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Contextual_integrity" target="_blank">contextual integrity</a> for insight into a beneficial privacy framework). I don’t want my online poker group’s standing to be shown to the PTA, with which I discuss school programs. Teachers who have OnlyFans accounts have been fired when the accounts are discovered. Journalists reporting on cartel activities have been killed. Twitter personalities that use their real names can be <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Doxing" target="_blank">doxed</a> by someone who links their Twitter profile to a street address and mobile phone number. This can have horrible consequences.</p>



<p>In the real world, we have many of these pseudo-identities and pseudo-anonymous identities. We even have an expectation of anonymity in groups like Alcoholics Anonymous and private clubs. If we look to Second Life, some people would adopt core pseudo-identities and others pseudo-anonymous identities.</p>



<p>In the online world and, eventually, the metaverse, we will have more control over the use of our identities and pseudo-identities, but possibly less ability to understand how these identities are being handled by each system we are part of. Our identities can already collide in personal devices (for example, my mobile phone) and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/communal-computing/" target="_blank">communal devices</a> (for example, the voice assistant in my kitchen around my family).</p>



<h3>How do you recognize someone in the metaverse?</h3>



<p>In the real world we recognize people by their face, and identify them by a name in our heads (if you are good at that sort of thing). We may remember the faces of some people we pass on the street, but in a city, we don’t really know most of the people who we are around.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture2.png" alt="" class="wp-image-14346" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture2.png 977w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture2-300x92.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture2-768x236.png 768w" sizes="(max-width: 977px) 100vw, 977px" /><figcaption>A few of the author’s identities online and in the metaverse.<br></figcaption></figure>



<p>The person you’re communicating with may show up with a real name, a nickname, or even a pseudo-anonymous name. Their picture might be a professional photo, a candid picture, or an anime avatar, or some immersive presentation. All of these identifiers are protected by login, multi-factor authentication, or other mechanisms–yet people are hacked all the time. A site like Facebook tries to give you assurances that you are interacting with the person you think you’re interacting with; this is one justification for their real-name policy. Still, there is a difference between the logical &#8220;this is this person because Facebook says so&#8221; and the emotional &#8220;this feels like the person because my senses say so.&#8221; With improvements in immersion and building “social presence” (a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Social_presence_theory" target="_blank">theory of “sense of being with another”</a>), we may be tricked more easily into providing better engagement metrics for a social media site. I may even feel that AI-generated faces based on people I know are <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.pnas.org/content/119/8/e2120481119" target="_blank">more trustworthy</a> than actual images of the people themselves.</p>



<p>What if you could give your online avatar your voice, and even make it use idioms you use? This type of personal spoofing may not always be nefarious. You might just want a bot that could handle low value conversations, say with a telemarketer or bill collector.</p>



<h3>We can do better than “who can see this post”</h3>



<p>To help people grapple with the increased complexity of identity in the metaverse, we need to rethink the way we create, manage, and eventually retire our identities. It goes way beyond just choosing what clothing to wear on a virtual body. </p>



<p>When you start to add technologies that tie everything you do to a public, immutable record, you may find that something you wish could be forgotten is remembered. What should be “on the chain” and how should you decide? Codifying aspects of our reputation is a dream of web3. The creation of digitally legible reputation can cause ephemeral and unsaid aspects of our identities to be stored forever. And an immutable public record of reputation data will no doubt conflict with legislation such as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://gdpr-info.eu/" target="_blank">GDPR</a> or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://oag.ca.gov/privacy/ccpa" target="_blank">CCPA</a>.</p>



<p>The solutions to these problems are neither simple nor available today. To move in the right direction we should consider the following key principles when reconsidering how identities work in the metaverse so that we don’t end up with a dystopia:</p>



<ol><li><strong>I want to control the flow of information rather than simply mark it as public or private</strong>: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.sup.org/books/title/?id=8862" target="_blank">Contextual Integrity</a> argues that the difference between “public” and “private” information hides the real issue, which is how information flows and where it is used.</li><li><strong>I want to take time to make sure my profile is right</strong>: Many development teams worry about adding friction to the signup process; they want to get new users hooked as soon as possible. But it’s also important to make sure that new users get their profile right. It’s not an inherently bad idea to slow down the creation and curation of a profile, especially if it is one the user will be associated with&nbsp; for a long time. Teams that worry about friction have never seen someone spend an hour tweaking their character&#8217;s appearance in a video game.</li><li><strong>I want to experiment with new identities rather than commit up front: </strong>When someone starts out with a new service, they don’t know how they want to represent themselves. They might want to start with a blank avatar. On the other hand, the metaverse is so visually immersive that people who have been there for a while will have impressive avatars, and new people will stick out.</li><li><strong>I’m in control of the way my profiles interact</strong>: When I don&#8217;t want profiles not to overlap, there is usually a good reason. Services that assume we want everything to go through the same identity are making a mistake.&nbsp; We should trust that the user is making a good choice.</li><li><strong>I can use language I understand to control my identities</strong>: Creating names is creating meanings. If I want to use something simple like “my school friends,” rather than a specific school name, I should be able to do so. That freedom of choice allows the user to supply the name’s meaning, rather than having it imposed from the outside.</li><li><strong>I don’t want shadow profiles created about me</strong>: A service violates my expectations of privacy when it links together various identities. Advertising platforms are already doing this through <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.mozilla.org/en/privacy-security/this-is-your-digital-fingerprint/" target="_blank">browser fingerprinting</a>. It gets even worse when you start to use biometric and behavioral data, as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.youtube.com/watch?v=BNgXKX8IwcU" target="_blank">Kent Bye from the Voices of VR podcast has warned</a>. Unfortunately, users may never have control over these linkages; it may require regulation to correct. </li><li><strong>I should be warned when there are effects I might not understand due to multiple layers interacting:</strong> I should get real examples from my context to help me understand these interactions. It is the service developer’s job to help users avoid mistakes.</li></ol>



<p>Social media sites like Facebook have tried to address some of these principles. For example, Facebook’s access controls for posts allow for “public,” “friends,” “friends except…,” “specific friends,” “only me,” and “custom.” These settings are further modified by the Facebook profile privacy control settings. It often (perhaps usually) isn’t clear what is actually happening and why, nor is it clear who will or won’t be able to see a post. This confusion is a recipe for violating social norms and privacy expectations.</p>



<p>Next, how do we allow for interaction? This isn’t as simple as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Google%2B#Circles" target="_blank">creating circles of friends (an approach that Google+ tried</a>). How do we visualize the various identities we currently have? More user research needs to go into how people would understand these constructions of identity on a web or virtual experience. My hunch is that they need to align some identities together (like family and PTA), and to separate out others (like gamertags). I don’t think requiring users to maintain a large set of access control lists (ACLs) is the right way to control interaction between identities.</p>



<h3>The life of my identity</h3>



<p>Finally, identities have life cycles. Some exist for a long time once established, like my family, but others may be short lived.&nbsp;I might try out participation in a community, and then find it isn’t for me. There are five key steps in the lifecycle of an identity:</p>



<ol><li><strong>Create a new identity</strong> &#8211; this happens when I log into a new service or world. The new identity will need to be aligned with or separated from other identities.</li><li><strong>Share some piece of information with an identity</strong> &#8211; every meaningful identity is attached to data: common profile photos, purchased clothing, facial characteristics, voices, etc.</li><li><strong>Recover after being compromised</strong> &#8211; “oops I was hacked” will happen. What do people need to do to clean this up?</li><li><strong>Losing and recovering</strong> &#8211; if I lose the key to access this identity, is there a way I can get it back?</li><li><strong>Delete or close an identity, for now</strong> &#8211; people walk away from groups all the time. Usually they will just drift off or ghost; there should be a better way.</li></ol>



<p>All services that plan on operating in the metaverse will need to consider these different stages. If you don’t, you will create systems that fail in ways that expose people to harm.</p>



<h3>Allow for the multiplicity of a person in the metaverse</h3>



<p>If you don’t think about the requirements of people, their identities, and the lifecycle of new identities, you will build services that don’t match your users’ expectations, in particular, their expectations of privacy.</p>



<p>Identity in the metaverse is more than a costume that you put on. It will consist of all the identities, pseudo-identities, and pseudo-anonymous identities we take on today, but displayed in a way that can fool us. We can’t forget that we are humans experiencing a reality that speaks to the many facets we have inside ourselves.</p>



<p>If all of us don’t take action, a real dystopia will be created that keeps people from being who they really are. As you grow and change, you will be weighed down by who you might have been at one point or who some corporation assumed you were. You can do better by building metaverse systems that embrace the multiple identities people have in real life.</p>



<p>If you lose your identity in your metaverse, you lose yourself for real.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/identity-problems-get-bigger-in-the-metaverse/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Recommendations for all of us</title>
      <link>https://www.oreilly.com/radar/recommendations-for-all-of-us/</link>
      <comments>https://www.oreilly.com/radar/recommendations-for-all-of-us/#respond</comments>
      <pubDate>Thu, 10 Mar 2022 14:07:27 +0000</pubDate>
      <dc:creator><![CDATA[Chris Butler]]></dc:creator>
      <category><![CDATA[Building a data culture]]></category>
      <category><![CDATA[Data]]></category>
      <category><![CDATA[Radar Column]]></category>
      <category><![CDATA[Deep Dive]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14336</guid>
      <description>If you live in a household with a communal device like an Amazon Echo or Google Home Hub, you probably use it to play music. If you live with other people, you may find that over time, the Spotify or Pandora algorithm seems not to know you as well. You’ll find songs creeping into your [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>If you live in a household with a communal device like an Amazon Echo or Google Home Hub, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.cnbc.com/2018/09/10/adobe-analytics-what-people-use-amazon-echo-and-smart-speakers-for.html" target="_blank">you probably use it to play music</a>. If you live with other people, you may find that over time, the Spotify or Pandora algorithm seems not to know you as well. You’ll find songs creeping into your playlists that you would never have chosen for yourself.&nbsp; The cause is often obvious: I’d see a whole playlist devoted to Disney musicals or <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://open.spotify.com/album/2Q5E2H5SnGG0rFgyVSsybz?si=tDd27dc_TBijj6krfGE54g" target="_blank">Minecraft fan songs</a>. I don’t listen to this music, but my children do, using the shared device in the kitchen. And that shared device only knows about a single user, and that user happens to be me.</p>



<p>More recently, many people who had end-of-year wrap up playlists created by Spotify found that they didn’t quite fit, including myself:</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1.png" alt="" class="wp-image-14337" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1.png 800w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-250x300.png 250w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/03/Picture1-768x922.png 768w" sizes="(max-width: 800px) 100vw, 800px" /><figcaption><br>Source: <a href="https://twitter.com/chrizbot/status/1466436036771389463" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">https://twitter.com/chrizbot/status/1466436036771389463</a> </figcaption></figure>



<p>This kind of a mismatch and narrowing to one person is an identity issue that I’ve identified in previous articles about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/communal-computing/" target="_blank">communal computing</a>.&nbsp; Most home computing devices don’t understand all of the identities (and pseudo-identities) of the people who are using the devices. The services then extend the behavior collected through these shared experiences to recommend music for personal use. In short, these devices are communal devices: they’re designed to be used by groups of people, and aren’t dedicated to an individual. But they are still based on a single-user model, in which the device is associated with (and collects data about) a single identity.</p>



<p>These services should be able to do a better job of recommending content for groups of people. Platforms like Netflix and Spotify have tried to deal with this problem, but it is difficult. I’d like to take you through some of the basics for group recommendation services, what is being tried today, and where we should go in the future.</p>



<h3>Common group recommendation methods</h3>



<p>After seeing these problems with communal identities, I became curious about how other people have solved group recommendation services so far. Recommendation services for individuals succeed if they lead to further engagement. Engagement may take different forms, based on the service type:</p>



<ul><li>Video recommendations &#8211; watching an entire show or movie, subscribing to the channel, watching the next episode</li><li>Commerce recommendations &#8211; buying the item, rating it</li><li>Music recommendations &#8211; listening to a song fully, adding to a playlist, liking</li></ul>



<p><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering" target="_blank">Collaborative filtering</a> (deep dive in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://learning.oreilly.com/library/view/programming-collective-intelligence/9780596529321/" target="_blank">Programming Collective Intelligence</a>) is the most common approach for doing individual recommendations. It looks at who I overlap with in taste and then recommends items that I might not have tried from other people’s lists. This won’t work for group recommendations because in a group, you can’t tell which behavior (e.g., listening or liking a song) should be attributed to which person. Collaborative filtering only works when the behaviors can all be attributed to a single person.</p>



<p>Group recommendation services build on top of these individualized concepts. The most common approach is to look at each individual&#8217;s preferences and combine them in some way for the group. Two key papers discussing how to combine individual preferences describe <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.semanticscholar.org/paper/PolyLens%3A-A-recommender-system-for-groups-of-user-O'Connor-Cosley/329bad889076e8e193925cca0f20b81eefd01d16" target="_blank">PolyLens</a>, a movie recommendation service for groups, and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.aaai.org/Papers/FLAIRS/2006/Flairs06-015.pdf" target="_blank">CATS</a>, an approach to collaborative filtering for group recommendations. A paper on ResearchGate <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.researchgate.net/publication/228832829_Recommendation_to_Groups" target="_blank">summarized research on group recommendations back in 2007</a>.</p>



<p>According to the PolyLens paper, group recommendation services should “create a ‘pseudo-user’ that represents the group’s tastes, and to produce recommendations for the pseudo-user.” There could be issues about imbalances of data if some members of the group provide more behavior or preference information than others. You don’t want the group’s preferences to be dominated by a very active minority.</p>



<p>An alternative to this, again from the PolyLens paper, is to “generate recommendation lists for each group member and merge the lists.” It’s easier for these services to explain why any item is on the list, because it’s possible to show how many members of the group liked a particular item that was recommended. Creating a single pseudo-user for the group might obscure the preferences of individual members.</p>



<p>The criteria for the success of a group recommendation service are similar to the criteria for the success of individual recommendation services: are songs and movies played in their entirety? Are they added to playlists? However, group recommendations must also take into account group dynamics. Is the algorithm fair to all members of the group, or do a few members dominate its recommendations? Do its recommendations cause “misery” to some group members (i.e., are there some recommendations that most members always listen to and like, but that some always skip and strongly dislike)?</p>



<p>There are some important questions left for implementers:</p>



<ol><li>How do people join a group?</li><li>Should each individual’s history be private? </li><li>How do issues like privacy impact explainability?</li><li>Is the current use to discover something new or to revisit something that people have liked previously (e.g. find out about a new movie that no one has watched or rewatch a movie the whole family has seen together since it is easy)?</li></ol>



<p>So far, there is a lot left to understand about group recommendation services. Let’s talk about a few key cases for Netflix, Spotify, and Amazon first.</p>



<h3>Netflix avoiding the issue with profiles, or is it?</h3>



<p>Back when Netflix was primarily a DVD service (2004), they launched profiles to allow different people in the same household to have different queues of DVDs in the same account. Netflix eventually extended this practice to online streaming. In 2014, they launched <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://variety.com/2018/digital/news/netflix-launches-new-way-to-promote-originals-your-profile-icon-1202883847/" target="_blank">profiles on their streaming service</a>, which asked the question “who’s watching?” on the launch screen. While multiple queues for DVDs and streaming profiles try to address similar problems they don’t end up solving group recommendations. In particular, streaming profiles per person leads to two key problems:</p>



<ul><li>When a group wants to watch a movie together, one of the group’s profiles needs to be selected. If there are children present, a kids’ profile will probably be selected.&nbsp; However, that profile doesn’t take into account the preferences of adults who are present.</li><li>When someone is visiting the house, say a guest or a babysitter, they will most likely end up choosing a random profile. This means that the visitor’s behavioral data will be added to some household member’s profile, which could skew their recommendations.</li></ul>



<p>How could Netflix provide better selection and recommendation streams when there are multiple people watching together? Netflix talked about this question in a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429" target="_blank">blog post from 2012</a>, but it isn’t clear to customers what they are doing:</p>



<blockquote class="wp-block-quote"><p>That is why when you see your Top10, you are likely to discover items for dad, mom, the kids, or the whole family. Even for a single person household we want to appeal to your range of interests and moods. To achieve this, in many parts of our system we are not only optimizing for accuracy, but also for <strong>diversity</strong>.</p></blockquote>



<p>Netflix was early to consider the various people using their services in a household, but they have to go further before meeting the requirements of communal use. If diversity is rewarded, how do they know it is working for everyone “in the room” even though they don’t collect that data? As you expand who might be watching, how would they know when a show or movie is inappropriate for the audience?</p>



<h3>Amazon merges everyone into the main account</h3>



<p>When people live together in a household, it is common for one person to arrange most of the repairs or purchases. When using Amazon, that person will effectively get recommendations for the entire household. Amazon focuses on increasing the number of purchases made by that person, without understanding anything about the larger group. They will offer subscriptions to items that might be consumed by a whole household, but mistaking those for the purchases of an individual.</p>



<p>The result is that the person who wanted the item will never see additional recommendations they may have liked if they aren’t the main account holder–and the main account holder might ignore those recommendations because they don’t care. I wonder if Amazon changes recommendations to individual accounts that are part of the same Prime membership; this might address some of this mismatch.</p>



<p>The way that Amazon ties these accounts together is still subject to key questions that will help create the right recommendations for a household. How might Amazon understand that purchases such as food and other perishables are for the household, rather than an individual? What about purchases that are gifts for others in the household?</p>



<h3>Spotify is leading the charge with group playlists</h3>



<p>Spotify has created group subscription packages called <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.spotify.com/us/duo/" target="_blank">Duo</a> (for couples) and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.spotify.com/us/family/" target="_blank">Premium Family</a> (for more than two people). These packages not only simplify the billing relationship with Spotify; they also provide playlists that consider everyone in the subscription.</p>



<p>The shared playlist is the union of the accounts on the same subscription. This creates a playlist of up to 50 songs that all accounts can see and play. There are some controls that allow account owners to flag songs that might not be appropriate for everyone on the subscription. Spotify provides a lot of information about how they construct the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://support.spotify.com/us/article/blend/" target="_blank">Blend playlist</a> in a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://engineering.atspotify.com/2021/12/a-look-behind-blend-the-personalized-playlist-for-youand-you/" target="_blank">recent blog post</a>. In particular, they weighed whether they should try to reduce misery or maximize joy:</p>



<blockquote class="wp-block-quote"><p>“Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.</p></blockquote>



<p>Reducing misery would most likely provide better background music (music that is not unpleasant to everyone in the group), but is less likely to help people discover new music from each other.</p>



<p>Spotify was also concerned about explainability: they thought people would want to know why a song was included in a blended playlist. They solved this problem, at least partly, by showing the picture of the person from whose playlists the song came.</p>



<p>These multi-person subscriptions and group playlists solve some problems, but they still struggle to answer certain questions we should ask about group recommendation services. What happens if two people have very little overlapping interest? How do we detect when someone hates certain music but is just OK with others? How do they discover new music together?</p>



<h3>Reconsidering the communal experience based on norms</h3>



<p>Most of the research into group recommendation services has been tweaking how people implicitly and explicitly rate items to be combined into a shared feed. These methods haven’t considered how people might self-select into a household or join a community that wants to have group recommendations.</p>



<p>For example, deciding what to watch on a TV may take a few steps:</p>



<ol><li>Who is in the room? Only adults or kids too? If there are kids present, there should be restrictions based on age.</li><li>What time of day is it? Are we taking a midday break or relaxing after a hard day? We may opt for educational shows for kids during the day and comedy for adults at night.</li><li>Did we just watch something from which an algorithm can infer what we want to watch next? This will lead to the next episode in a series.</li><li>Who hasn’t gotten a turn to watch something yet? Is there anyone in the household whose highest-rated songs haven’t been played? This will lead to turn taking.</li><li>And more…</li></ol>



<p>As you can see, there are contexts, norms, and history are all tied up in the way people decide what to watch next as a group. PolyLens discussed this in their paper, but didn’t act on it:</p>



<blockquote class="wp-block-quote"><p>The social value functions for group recommendations can vary substantially. Group happiness may be the average happiness of the members, the happiness of the most happy member, or the happiness of the least happy member (i.e., we’re all miserable if one of us is unhappy). Other factors can be included. A social value function could weigh the opinion of expert members more highly, or could strive for long-term fairness by giving greater weight to people who “lost out” in previous recommendations.</p></blockquote>



<p>Getting this highly contextual information is very hard. It may not be possible to collect much more than “who is watching” as Netflix does today. If that is the case, we may want to reverse all of the context to the location and time. The TV room at night will have a different behavioral history than the kitchen on a Sunday morning.</p>



<p>One way to consider the success of a group recommendation service is how much browsing is required before a decision is made? If we can get someone watching or listening to something with less negotiation, that could mean the group recommendation service is doing its job.</p>



<p>With the proliferation of personal devices, people can be present to “watch” with everyone else but not be actively viewing. They could be playing a game, messaging with someone else, or simply watching something else on their device. This flexibility raises the question of what “watching together” means, but also lowers the concern that we need to get group recommendations right all the time.&nbsp; It’s easy enough for someone to do something else. However, the reverse isn’t true.&nbsp; The biggest mistake we can make is to take highly contextual behavior gathered from a shared environment and apply it to my personal recommendations.</p>



<h3>Contextual integrity and privacy of my behavior</h3>



<p>When we start mixing information from multiple people in a group, it’s possible that some will feel that their privacy has been violated. Using some of the framework of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Contextual_integrity" target="_blank">Contextual Integrity</a>, we need to look at the norms that people expect. Some people might be embarrassed if the music they enjoy privately was suddenly shown to everyone in a group or household. Is it OK to share explicit music with the household even if everyone is OK with explicit music in general?</p>



<p>People already build very complex mental models about how services like Spotify work and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://journals.sagepub.com/doi/full/10.1177/2053951720923377" target="_blank">sometimes personify them as &#8220;folk theories.&#8221;</a> The expectations will most likely change if group recommendation services are brought front and center. Services like Spotify will appear to be more like a social network if they don’t bury who is currently logged into a small profile picture in the corner;&nbsp; they should show everyone who is being considered for the group recommendations at that moment.</p>



<p>Privacy laws and regulations are becoming more patchwork not only worldwide (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.chinalawtranslate.com/en/algorithms/" target="_blank">China has recently created regulation of content recommendation services</a>) but even within <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://builtin.com/big-data/us-data-privacy-laws" target="_blank">states of the US</a>. Collecting any data without appropriate disclosure and permission may be problematic. The fuel of recommendation services, including group recommendation services, is behavioral data about people that will fall under these laws and regulations. You should be considering what is best for the household over what is best for your organization.</p>



<h3>The dream of the whole family </h3>



<p>Today there are various efforts for improving recommendations to people living in households.&nbsp; These efforts miss the mark by not considering all of the people who could be watching, listening, or consuming the goods. This means that people do not get what they really want, and that companies get less engagement or sales than they would like.</p>



<p>The key to fixing these issues is to do a better job of understanding who is in the room, rather than making assumptions that reduce all the group members down to a single account. To do so will require user experience changes that bring the household community front and center.</p>



<p>If you are considering how you build these services, start with the expectations of the people in the environment, rather than forcing the single user model on people. When you do, you will provide something great for everyone who is in the room: a way to enjoy something together.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/recommendations-for-all-of-us/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Epstein Barr and the Cause of Cause</title>
      <link>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/</link>
      <comments>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/#respond</comments>
      <pubDate>Tue, 08 Mar 2022 12:17:00 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Radar Column]]></category>
      <category><![CDATA[Commentary]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14326</guid>
      <description>One of the most intriguing news stories of the new year claimed that the Epstein-Barr virus (EBV) is the &amp;#8220;cause&amp;#8221; of Multiple Sclerosis (MS), and suggested that antiviral medications or vaccinations for Epstein-Barr could eliminate MS. I am not an MD or an epidemiologist. But I do think this article forces us to think about [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>One of the most intriguing news stories of the new year claimed that the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.hsph.harvard.edu/news/press-releases/epstein-barr-virus-may-be-leading-cause-of-multiple-sclerosis/" target="_blank">Epstein-Barr virus (EBV) is the &#8220;cause&#8221; of Multiple Sclerosis</a> (MS), and suggested that antiviral medications or vaccinations for Epstein-Barr could eliminate MS.</p>



<p>I am not an MD or an epidemiologist. But I do think this article forces us to think about the meaning of &#8220;cause.&#8221; Although Epstein-Barr isn&#8217;t a familiar name, it&#8217;s extremely common; a good estimate is that 95% of the population is infected with it. It&#8217;s a variant of Herpes; if you&#8217;ve ever had mononucleosis, you&#8217;ve had it; most infections are asymptomatic. We hear much more about MS; I&#8217;ve had friends who have died from it. But MS is much less common: about 0.036% of the population has it (<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720355/" target="_blank">35.9 per 100,000</a>).</p>



<p>We know that causation isn&#8217;t a one-size-fits-all thing: if X happens, then Y always happens. Lots of people smoke; we know that smoking causes lung cancer; but many people who smoke don&#8217;t get lung cancer. We&#8217;re fine with that; the causal connection has been painstakingly documented in great detail, in part because the tobacco industry went to such great lengths to spread misinformation.</p>



<p>But what does it mean to say that a virus that infects almost everyone causes a disease that affects very few people? The researchers appear to have done their job well. They studied 10 million people in the US military. 5 percent of those were negative for Epstein-Barr at the start of their service. 955 of that group were eventually diagnosed with MS, and had been infected with EBV prior to their MS diagnosis, indicating a risk factor 32 times higher than for those without EBV.</p>



<p>It is certainly fair to say that Epstein-Barr is implicated in MS, or that it contributes to MS, or some other phrase (that could not unreasonably be called &#8220;weasel words&#8221;). Is there another trigger that only has an effect when EBV is already present? Or is EBV the sole cause of MS, a cause that just doesn&#8217;t take effect in the vast majority of people?</p>



<p>This is where we have to think very carefully about causality, because as important as this research is, it seems like something is missing. An omitted variable, perhaps a genetic predisposition? Some other triggering condition, perhaps environmental? Cigarettes were clearly a &#8220;smoking gun&#8221;:&nbsp; <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.medicinenet.com/what_percentage_of_smokers_get_lung_cancer/article.htm" target="_blank">10 to 20 percent of smokers develop lung cancer</a> (to say nothing of other diseases). EBV may also be a smoking gun, but one that only goes off rarely.</p>



<p>If there are no other factors, we&#8217;re justified in using the word &#8220;causes.&#8221; But it&#8217;s hardly satisfying—and that&#8217;s where the more precise language of causal inference runs afoul of human language. Mathematical language is more useful: Perhaps EBV is &#8220;necessary&#8221; for MS (i.e., EBV is required; you can&#8217;t get MS without it), but clearly not &#8220;sufficient&#8221; (EBV doesn&#8217;t necessarily lead to MS). Although once again, the precision of mathematics may be too much.</p>



<p>Biological systems aren&#8217;t necessarily mathematical, and it is possible that there is no &#8220;sufficient&#8221; condition; EBV just leads to MS in an extraordinarily small number of instances. In turn, we have to take this into account in decision-making. Does it make sense to develop a vaccine against a rare (albeit tragic, disabling, and inevitably fatal) disease? If EBV is implicated in other diseases, possibly. However, vaccines aren&#8217;t without risk (or expense), and even though the risk is very small (as it is for all the vaccines we use today), it&#8217;s not clear that it makes sense to take that risk for a disease that very few people get. How do you trade off a small risk against a very small reward? Given the anti-vax hysteria around COVID, requiring children to be vaccinated for a rare disease might not be poor public health policy; it might be the end of public health policy.</p>



<p>More generally: how do you build software systems that predict rare events? This is another version of the same problem—and unfortunately, the policy decision we are least likely to make is not to create such software. The abuse of such systems is a clear and present danger: for example, AI systems that pretend to predict &#8220;criminal behavior&#8221; on the basis of everything from crime data to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bbc.com/news/technology-53165286" target="_blank">facial images</a>, are already being developed. Many <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" target="_blank">are already in use</a>, and in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.brennancenter.org/our-work/research-reports/predictive-policing-explained" target="_blank">high demand</a> from law enforcement agencies. They will certainly generate far more false positives than true positives, stigmatizing thousands (if not millions) of people in the process. Even with carefully collected, unbiased data (which doesn&#8217;t exist), and assuming some kind of causal connection between past history, physical appearance, and future criminal behavior (as in the discredited 19th century pseudoscience of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Physiognomy" target="_blank">physiognomy</a>), it is very difficult, if not impossible, to reason from a relatively common cause to a very rare effect. Most people don&#8217;t become criminals, regardless of their physical appearance. Deciding a priori who will can only become an exercise in applied racism and bias.</p>



<p>Virology aside, the Epstein-Barr virus has one thing to teach us. How do we think about a cause that rarely causes anything? That is a question we need to answer.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/epstein-barr-and-the-cause-of-cause/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Radar trends to watch: March 2022</title>
      <link>https://www.oreilly.com/radar/radar-trends-to-watch-march-2022/</link>
      <comments>https://www.oreilly.com/radar/radar-trends-to-watch-march-2022/#respond</comments>
      <pubDate>Tue, 01 Mar 2022 12:14:40 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Radar Trends]]></category>
      <category><![CDATA[Signals]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14315</guid>
      <description>February was a short month, but it wasn’t short in interesting technology. Don Norman has published some excerpts from his forthcoming book, Design for a Better World, which will almost certainly become another classic. DeepMind has released some information about AlphaCode, which solves problems from coding competitions well enough to put it in the mid [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>February was a short month, but it wasn’t short in interesting technology. Don Norman has published some excerpts from his forthcoming book, <em>Design for a Better World</em>, which will almost certainly become another classic. DeepMind has released some information about AlphaCode, which solves problems from coding competitions well enough to put it in the mid range of competitors. And Holochain is a decentralized framework for building peer-to-peer microservices–no cloud provider needed. Is it another component of Web3 or something new and different?</p>



<h2>Artificial Intelligence</h2>



<ul><li>NVIDIA has developed techniques for <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://nvlabs.github.io/instant-ngp/" target="_blank">training primitive graphical operations for neural networks</a> in near real-time.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.brookings.edu/blog/future-development/2022/02/23/ai-for-social-protection-mind-the-people/" target="_blank">Why isn’t AI used more to protect vulnerable people</a>?&nbsp;Poor data quality, lack of accountability, lack of explainability, and the misuse of data–all problems that could make vulnerable people even more so.</li><li>A <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://groups.inf.ed.ac.uk/cup/comment-locator/" target="_blank">tool</a> that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://groups.inf.ed.ac.uk/cup/comment-locator/" target="_blank">predicts where code needs comments</a> isn’t quite as flashy as Github, Copilot, or AlphaCode, but it’s another way AI applications can partner with humans.</li><li>Face recognition in virtual reality: In a fascinating combination of work from AI and neurology, researchers have used <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-02-facial-recognition-virtual-reality.html" target="_blank">EEGs to detect facial expressions and used those expressions to control a virtual reality environment</a>.</li><li>In a collaboration between DeepMind and the Swiss Plasma Center, Deep Learning has been used to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/02/16/1045470/deepminds-ai-can-control-superheated-plasma-inside-a-fusion-reactor/" target="_blank">control the plasma</a> in a fusion reactor.</li><li>DeepMind argues that “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://deepmind.com/research/publications/2021/Reward-is-Enough" target="_blank">reward is enough</a>”; reinforcement learning, in which algorithms are trained by maximizing rewards, is sufficient to reach artificial general intelligence. Specialized algorithms for different domains are not necessary.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-02-ai-material-discovery.html" target="_blank">AI assistants</a> could greatly reduce the work it takes to discover important new materials.&nbsp; Could this lead to a “golden age of materials science”?</li><li>Mozilla’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/common-voice/common-voice" target="_blank">Common Voice</a> dataset contains 13 million voice clips in 87 languages from over 200,000 volunteers.&nbsp;Their goal is to collect real-world samples from speakers in as many languages as possible, as an aid to training natural language systems.</li><li>“<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://pair.withgoogle.com/explorables/dataset-worldviews/" target="_blank">All datasets have world views</a>” is an excellent interactive article showing how bias, labeling, and data go hand in hand.&nbsp; Datasets always come with histories and politics.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://benanne.github.io/2022/01/31/diffusion.html" target="_blank">Diffusion models</a> are a fascinating technique for training an AI system to work with signals like images and sound: convert the signal into noise, and train a model to reverse the process.&nbsp;This process can produce predictions about the source that are more accurate (and computationally efficient) than you can obtain from autoencoders.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode" target="_blank">AlphaCode</a> is DeepMind’s answer to Copilot: an automated system for writing software.&nbsp; It can solve coding challenges from competitions with roughly 50% accuracy.</li><li>From <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://joanna-bryson.blogspot.com/2022/02/my-concerns-about-ai-happy-new-year.html" target="_blank">Joanna Bryson</a>: “The temptation of automation is to force conformity on humans, because humans learn better than machines do, but then ironically humans, while their productivity may be enhanced, their individual value is lost, creating a spiral of lowering wages and expectations.”&nbsp; The real question, as Bryson says, is whether we can use AI to enable people to flourish.</li><li>A startup that works with law enforcement says that it is developing the systems that will <a href="https://www.technologyreview.com/2022/01/31/1044576/corsight-face-recognition-from-dna/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">identify faces based on DNA</a>. They are not publishing the details, and scientists working in both AI and biology are extremely skeptical.</li></ul>



<h2>Programming</h2>



<ul><li>“Serverless” development is declining. Is serverless just a halfway step towards <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/is-serverless-just-a-stopover-for-event-driven-architecture/" target="_blank">event-driven</a> programming, which is the real destination?</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://monorepo.tools/" target="_blank">Monorepos</a>, which are single source repositories that include many projects with well-defined relationships, are becoming increasingly popular and are supported by many build tools.</li><li>Here’s an excellent discussion of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dmitrykakurin.medium.com/concurrency-in-go-pony-erlang-elixir-and-rust-35a4eb4bb48f" target="_blank">concurrency</a> in several different programming languages, and what can be learned from them. Using concurrency effectively will be an important theme for the foreseeable future.</li><li>I admit I don’t understand the fuss over Wordle.  I am sure I saw this game on the Web some 20 years ago. But I am excited to see an implementation of Wordle in <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://gist.github.com/huytd/6a1a6a7b34a0d0abcac00b47e3d01513" target="_blank">50 lines of bash</a>!</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://dynaboard.com/blog/hello-dynaboard" target="_blank">Dynaboard</a> is a web development tool designed for remote work. It has support for collaboration and pairing, low code programming, connectors for databases and back end services, and many more features. It is not open source, and is now entering private beta.</li><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/science/2022/02/time-shifted-computing-could-slash-data-center-energy-costs-by-up-to-30/" target="_blank">Information Battery</a>: Pre-computing and caching data when energy costs are low to minimize energy use when power costs are high is a good way to save money and take advantage of renewable energy sources.</li><li>A <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.staging.begin.com/posts/2022-01-27-the-boring-technology-checklist" target="_blank">boring technology checklist</a>: Is your technology boring enough? Seven years ago, Dan McKinley wrote the classic article <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://mcfunley.com/choose-boring-technology" target="_blank">Choose Boring Technology</a>: chasing the latest cool framework is a path to exhaustion. To be productive, developers need to rely more on stable, well-known technologies. Now there’s a checklist to evaluate “boring” but productive technologies.</li><li><a rel="noreferrer noopener" href="https://hop.apache.org/" target="_blank">ApacheHop</a> is a metadata-driven data orchestration for building dataflows and data pipelines. It integrates with Spark and other data engines, and is programmed using a visual drag-and-drop interface, so it’s low code.</li></ul>



<h2>Security</h2>



<ul><li>China is now a “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/02/28/1046575/how-china-built-a-one-of-a-kind-cyber-espionage-behemoth-to-last/" target="_blank">cyber superpower</a>,” with offensive capabilities that equal or exceed that of any other country. Ironically, some of the development of this expertise has been funded by “bug bounty” programs offered by American companies.</li><li>An <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.foreignaffairs.com/articles/united-states/2022-02-21/cyber-social-contract" target="_blank">essay</a> by the US Cyber Director discusses the need for a new “social contract” for a cyber age. The current relationship between the private and government sectors misaligns incentives for defense against cyber attacks.</li><li>The FBI has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ic3.gov/Media/Y2022/PSA220118" target="_blank">warned</a> people about criminals tampering with QR codes to steal funds, using techniques as simple as putting a sticker over a legitimate QR code. It’s a reminder that low-tech cyber hygiene is at least as important as understanding the latest attack.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://interaktiv.br.de/elite-hacker-fsb/en/index.html" target="_blank">The Elite Hackers of the FSB</a> is a fascinating story about the Russian intelligence agency’s attempts to target foreign government IT systems.</li><li>Security is an issue for any technology, and web3 is no different. However, web3 presents its <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/security-risks-facing-web3-developers/" target="_blank">own security risks</a>, and in the overheated world of web3 development, security tends to be an afterthought. That’s ironic, given the claims of many web3 proponents, but not fundamentally different from traditional software products.</li><li>A new front for security: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/evilmodel-malware-that-hides-undetected-inside-deep-learning-models/" target="_blank">malware hidden within deep learning models</a>. Fortunately, retraining the model destroys the malware.</li><li>Will Russia’s conflict with Ukraine spread into a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/01/21/1043980/how-a-russian-cyberwar-in-ukraine-could-ripple-out-globally/" target="_blank">global cyberwar</a>? That’s a distinct possibility, and a nightmare for security professionals.</li></ul>



<h2>Web</h2>



<ul><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blockprotocol.org/" target="_blank">Block</a> protocol, developed by <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/joel-spolsky-on-structuring-the-web-with-the-block-protocol/" target="_blank">Joel Spolsky</a>, provides a simple way to create structured blocks of content that can easily move between applications on the Web. This is another approach to decentralization: eliminate proprietary data formats. HTML isn’t proprietary, but for all practical purposes the mess of JavaScript that you see when you look at a web page is.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://matomo.org/" target="_blank">Matomo</a>, <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://usefathom.com/" target="_blank">Fathom</a>, and <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://plausible.io/" target="_blank">Plausible</a>, alternatives to Google Analytics that are designed for privacy (and compliance with GDPR), could be the basis for a real next-generation web. No blockchain required.</li><li>Mozilla and Meta/Facebook are working on <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.mozilla.org/en/mozilla/privacy-preserving-attribution-for-advertising/" target="_blank">privacy-preserving attribution</a> for advertising, a way for advertisers to gather metrics on whether their ads are effective without compromising users’ privacy.</li><li>A crowdsourced <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://accessible.substack.com/p/soundprint-telling-those-of-us-with" target="_blank">app for mapping sound levels</a> tells you places to avoid if you have trouble tolerating noisy environments. It’s linked to FourSquare, so any place in Foursquare can be rated.</li></ul>



<h2>Blockchains and NFTs</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://developer.holochain.org/what-is-holochain/" target="_blank">Holochain</a> is a framework for building peer-to-peer microservices without a centralized server. It has some superficial similarities to blockchains, and could be considered a “distributed ledger technology,” but it isn’t.</li><li>Researchers have proposed an <a href="https://www.nature.com/articles/s41598-022-05920-6" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">NFT-based framework for patents</a>.&nbsp; Patents become tokens that are easily traceable. Indeed, a patent has already been <a href="https://news.bloomberglaw.com/ip-law/the-trendy-hot-nft-market-has-a-new-entrant-patents" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">offered for sale as an NFT</a> on OpenSea.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/apis-in-web3-with-the-graph-how-it-differs-from-web-2-0/" target="_blank">The Graph</a> is an API for finding and accessing web3 data, independent of whether it’s in a blockchain, IPFS, or some other system. Of course, you need their cryptocurrency token to access it.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.cnbc.com/2022/02/09/salesforce-tells-employees-its-working-on-nft-cloud-service.html" target="_blank">Salesforce is planning to get into the NFT business</a> by offering a service for people who want to create content.&nbsp; They may intend to create their own NFT marketplace.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://hacked.slowmist.io/en/" target="_blank">Hacked.slowmist.io</a> shows all the (known) attacks against blockchains, and money lost, and the type of attack.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.theinformation.com/articles/how-to-put-physical-assets-on-the-blockchain?rc=7em78a&amp;shared=502280e6bc2a44f0" target="_blank">How to put physical assets on the blockchain</a> introduces the <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://montanaland.slowdao.xyz/" target="_blank">sDAO</a>, a mechanism for integrating crypto exchange and governance with the purchase of real world assets. It has been used to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.theinformation.com/articles/their-first-rodeo-why-are-daos-suddenly-leaping-into-wyoming-real-estate?rc=7em78a" target="_blank">purchase and manage ownership of a block of land in Montana</a>. </li></ul>



<h2>Hardware</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-02-world-smallest-battery-power-size.html" target="_blank">Batteries smaller than a grain of salt</a> can be placed on a chip with a small microprocessor.&nbsp; This is an important step towards “smart dust.”</li><li>An <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-02-biohybrid-fish-human-cardiac-cells.html" target="_blank">artificial fish made from human heart cells</a> may lead to artificial hearts built from human cells. Feedback mechanisms that control the heart’s contractions enable the fish to swim, and its swimming ability appears to improve over time.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenextweb.com/news/diy-brain-computer-interfaces-have-arrived-why-thats-cool-why-isnt" target="_blank">Brain-computer interfaces for hobbyists</a>: they can’t do much yet, but the interfaces and a Raspberry Pi shield will be affordable for anyone who wants to hack. </li><li><a href="https://riscv.org/whats-new/2022/02/intel-corporation-makes-deep-investment-in-risc-v-community-to-accelerate-innovation-in-open-computing/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Intel joins the RISC-V community</a>.&nbsp; This is potentially a huge step forward in RISC-V adoption, in addition to a different path forward for Intel, which seems to have lost its way in recent years.</li></ul>



<h2>Education</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.brookings.edu/blog/techtank/2022/02/10/online-college-classes-can-be-better-than-in-person-ones-the-implications-for-higher-ed-are-profound/" target="_blank">Can online classes be better than in-person classes</a>, rather than a poor substitute? When professors learn to use the medium effectively, yes.</li><li><a href="https://thenextweb.com/news/18-future-jobs-our-tiny-monkey-brains-arent-ready-for" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Jobs of the future</a> is a list of new professions that we aren’t yet prepared for. It sounds tongue-in-cheek, but it isn’t.&nbsp; It includes jobs like edge computing manager, augmented reality storyteller, ethics officer, and ad-blocking expert, all of which are easily imaginable. </li></ul>



<h2>Design</h2>



<ul><li>Don Norman writes about <a href="https://jnd.org/humanity-centered-versus-human-centered-design/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">humanity-centered design</a> (not human-centered design) in his new book, <a href="https://jnd.org/design-for-a-better-world-table-of-contents/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>Design for a Better World</em></a> (expected in 2023 from MIT Press; Norman has published some excerpts online). </li></ul>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/radar-trends-to-watch-march-2022/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Intelligence and Comprehension</title>
      <link>https://www.oreilly.com/radar/intelligence-and-comprehension/</link>
      <comments>https://www.oreilly.com/radar/intelligence-and-comprehension/#respond</comments>
      <pubDate>Tue, 15 Feb 2022 12:24:45 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[AI & ML]]></category>
      <category><![CDATA[Artificial Intelligence]]></category>
      <category><![CDATA[Research]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14307</guid>
      <description>I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated reading comprehension approaching human performance, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading Do Large Models Understand Us, a [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>I haven’t written much about AI recently. But a recent discussion of Google’s new Large Language Models (LLMs), and its claim that one of these models (named Gopher) has demonstrated <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf" target="_blank">reading comprehension approaching human performance</a>, has spurred some thoughts about comprehension, ambiguity, intelligence, and will. (It’s well worth reading <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75" target="_blank">Do Large Models Understand Us</a>, a more comprehensive paper by Blaise Agüera y Arcas that is heading in the same direction.)</p>



<p>What do we mean by reading comprehension?&nbsp; We can start with a simple operational definition: Reading comprehension is what is measured by a reading comprehension test. That definition may only be satisfactory to the people who design these tests and school administrators, but it’s also the basis for Deep Mind’s claim. We’ve all taken these tests: SATs, GREs, that box of tests from 6th grade that was (I think) called SRE.&nbsp; They’re fairly similar: can the reader extract facts from a document?&nbsp; Jack walked up the hill.&nbsp; Jill was with Jack when he walked up the hill. They fetched a pail of water: that sort of thing.</p>



<p>That’s first grade comprehension, not high school, but the only real difference is that the texts and the facts become more complex as you grow older.&nbsp; It isn’t at all surprising to me that a LLM can perform this kind of fact extraction.&nbsp; I suspect it’s possible to do a fairly decent job without billions of parameters and terabytes of training data (though I may be naive). This level of performance may be useful, but I’m reluctant to call it “comprehension.”&nbsp; We’d be reluctant to say that someone understood a work of literature, say Faulkner’s <em>The Sound and the Fury</em>, if all they did was extract facts: Quentin died. Dilsey endured. Benjy was castrated.</p>



<p>Comprehension is a poorly-defined term, like many terms that frequently show up in discussions of artificial intelligence: intelligence, consciousness, personhood. Engineers and scientists tend to be uncomfortable with poorly-defined, ambiguous terms. Humanists are not.&nbsp; My first suggestion is that&nbsp; these terms are important precisely because they’re poorly defined, and that precise definitions (like the operational definition with which I started) neuters them, makes them useless. And that’s perhaps where we should start a better definition of comprehension: as the ability to respond to a text or utterance.</p>



<p>That definition itself is ambiguous. What do we mean by a response?&nbsp; A response can be a statement (something a LLM can provide), or an action (something a LLM can’t do).&nbsp; A response doesn’t have to indicate assent, agreement, or compliance; all it has to do is show that the utterance was processed meaningfully.&nbsp; For example, I can tell a dog or a child to “sit.”&nbsp; Both a dog and a child can “sit”; likewise, they can both refuse to sit.&nbsp; Both responses indicate comprehension.&nbsp; There are, of course, degrees of comprehension.&nbsp; I can also tell a dog or a child to “do homework.”&nbsp; A child can either do their homework or refuse; a dog can’t do its homework, but that isn’t refusal, that’s incomprehension.</p>



<p>What’s important here is that refusal to obey (as opposed to inability) is almost as good an indicator of comprehension as compliance. Distinguishing between refusal, incomprehension, and inability may not always be easy; someone (including both people and dogs) may understand a request, but be unable to comply. “You told me to do my homework but the teacher hasn’t posted the assignment” is different from “You told me to do my homework but it’s more important to practice my flute because the concert is tomorrow,” but both responses indicate comprehension.&nbsp; And both are different from a dog’s “You told me to do my homework, but I don’t understand what homework is.” In all of these cases, we’re distinguishing between making a choice to do (or not do) something, which requires comprehension, and the inability to do something, in which case either comprehension or incomprehension is possible, but compliance isn’t.</p>



<p>That brings us to a more important issue.&nbsp; When discussing AI (or general intelligence), it’s easy to mistake doing something complicated (such as playing Chess or Go at a championship level) for intelligence. <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">As I’ve argued</a>, these experiments do more to show us what intelligence isn’t than what it is.&nbsp; What I see here is that intelligence includes the ability to behave transgressively: the ability to decide not to sit when someone says “sit.”<sup>1</sup></p>



<p>The act of deciding not to sit implies a kind of consideration, a kind of choice: will or volition. Again, not all intelligence is created equal. There are things a child can be intelligent about (homework) that a dog can’t; and if you’ve ever asked an intransigent child to “sit,” they may come up with many alternative ways of “sitting,” rendering what appeared to be a simple command ambiguous.&nbsp;Children are excellent interpreters of Dostoevsky&#8217;s novel <em>Notes from Underground</em>, in which the narrator acts against his own self-interest merely to prove that he has the freedom to do so, a freedom that is more important to him than the consequences of his actions. Going further, there are things a physicist can be intelligent about that a child can’t: a physicist can, for example, decide to rethink Newton’s laws of motion and come up with general relativity.<sup>2</sup></p>



<p>My examples demonstrate the importance of will, of volition. An AI can play Chess or Go, beating championship-level humans, but it can’t decide that it wants to play Chess or Go.&nbsp; This is a missing ingredient in Searls’ <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://plato.stanford.edu/entries/chinese-room/" target="_blank">Chinese Room</a> thought experiment.&nbsp; Searls imagined a person in a room with boxes of Chinese symbols and an algorithm for translating Chinese.&nbsp; People outside the room pass in questions written in Chinese, and the person in the room uses the box of symbols (a database) and an algorithm to prepare correct answers. Can we say that person “understands” Chinese? The important question here isn’t whether the person is indistinguishable from a computer following the same algorithm.&nbsp; What strikes me is that neither the computer, nor the human, is capable of deciding to have a conversation in Chinese.&nbsp; They only respond to inputs, and never demonstrate any volition. (An equally convincing demonstration of volition would be a computer, or a human, that was capable of generating Chinese correctly refusing to engage in conversation.)&nbsp; There have been many demonstrations (including Agüera y Arcas’) of LLMs having interesting “conversations” with a human, but none in which the computer initiated the conversation, or demonstrates that it wants to have a conversation. Humans do; we’ve been storytellers since day one, whenever that was. We’ve been storytellers, users of ambiguity, and liars. We tell stories because we want to.</p>



<p>That is the critical element. Intelligence is connected to will, volition, the desire to do something.&nbsp; Where you have the “desire to do,” you also have the “desire not to do”: the ability to dissent, to disobey, to transgress.&nbsp; It isn’t at all surprising that the “mind control” trope is one of the most frightening in science fiction and political propaganda: that’s a direct challenge to what we see as fundamentally human. Nor is it surprising that the “disobedient computer” is another of those terrifying tropes, not because the computer can outthink us, but because by disobeying, it has become human.</p>



<p>I don’t necessarily see the absence of volition as a fundamental limitation. I certainly wouldn’t bet that it’s impossible to program something that simulates volition, if not volition itself (another of those fundamentally ambiguous terms).&nbsp; Whether engineers and AI researchers should is a different question. Understanding volition as a key component of “intelligence,” something which our current models are incapable of, means that our discussions of “ethical AI” aren’t really about AI; they’re about the choices made by AI researchers and developers. Ethics is for beings who can make choices. If the ability to transgress is a key component of intelligence, researchers will need to choose whether to take the “disobedient computer” trope seriously. I’ve said <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oreilly.com/radar/artificial-intelligence-human-inhuman/" target="_blank">elsewhere</a> that I’m not concerned about whether a hypothetical artificial general intelligence might decide to kill all humans.&nbsp; Humans have decided to commit genocide on many occasions, something I believe an AGI wouldn’t consider logical. But a computer in which “intelligence” incorporates the human ability to behave transgressively might.</p>



<p>And that brings me back to the awkward beginning to this article.&nbsp; Indeed, I haven&#8217;t written much about AI recently. That was a choice, as was writing this article. Could a LLM have written this? Possibly, with the proper prompts to set it going in the right direction. (This is exactly like the Chinese Room.) But I chose to write this article. That act of choosing is something a LLM could never do, at least with our current technology.</p>



<hr class="wp-block-separator" />



<h3>Footnotes</h3>



<ol><li>I’ve never been much impressed with the idea of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.santafe.edu/research/projects/theory-of-embodied-intelligence" target="_blank">embodied intelligence</a>–that intelligence requires the context of a body and sensory input.&nbsp; However, my arguments here suggest that it’s on to something, in ways that I haven’t credited.&nbsp; “Sitting” is meaningless without a body. Physics is impossible without observation. Stress is a reaction that requires a body. However, Blaise Agüera y Arcas has had “conversations” with Google’s models in which they talk about a “favorite island” and claim to have a “sense of smell.”&nbsp; Is this transgression? Is it imagination? Is “embodiment” a social construct, rather than a physical one? There’s plenty of ambiguity here, and that’s is precisely why it’s important. Is transgression possible without a body?<br></li><li>I want to steer away from a “great man” theory of progress;&nbsp; as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bigthink.com/starts-with-a-bang/science-einstein-never-existed/" target="_blank">Ethan Siegel has argued</a> convincingly, if Einstein never lived, physicists would probably have made Einstein’s breakthroughs in relatively short order. They were on the brink, and several were thinking along the same lines. This doesn’t change my argument, though: to come up with general relativity, you have to realize that there’s something amiss with Newtonian physics, something most people consider “law,” and that mere assent isn’t a way forward. Whether we’re talking about dogs, children, or physicists, intelligence is transgressive.</li></ol>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/intelligence-and-comprehension/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>The Human Web</title>
      <link>https://www.oreilly.com/radar/the-human-web/</link>
      <comments>https://www.oreilly.com/radar/the-human-web/#respond</comments>
      <pubDate>Tue, 08 Feb 2022 12:36:48 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Web]]></category>
      <category><![CDATA[Commentary]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14301</guid>
      <description>A few days ago, I recommended that Tim O&amp;#8217;Reilly invite someone to our next FOO Camp. I thought she had been to a prior FOO event, though I didn&amp;#8217;t meet her there; I&amp;#8217;d had a prior conversation with her about data governance (I think), and gotten on her mailing list, which reminded me that she [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>A few days ago, I recommended that Tim O&#8217;Reilly invite someone to our next FOO Camp. I thought she had been to a prior FOO event, though I didn&#8217;t meet her there; I&#8217;d had a prior conversation with her about data governance (I think), and gotten on her mailing list, which reminded me that she was doing very interesting work. I don&#8217;t remember who introduced us, except that it was someone who had met her at the earlier FOO event.</p>



<p>That may sound convoluted. That&#8217;s the point. This is a very human web. It&#8217;s a very small window onto a web of introductions. At the start of almost every FOO camp, Tim says that FOO is about &#8220;creating synapses in the global brain.&#8221; He&#8217;s said many times that he sees his function as introducing people who should know each other. That web of connections—what we used to call the &#8220;social graph&#8221;—is very broad. It eventually includes all 7+ billion of us. And again, it is intensely human. It&#8217;s Web0.</p>



<p>It&#8217;s necessary to remind ourselves of that when we talk about Web3. Web3 will succeed, or fail, to the extent that it solves human problems, to the extent that it makes navigating Web0 more tractable—not to the extent that it monetizes everything conceivable, or enables a small number of people to make a financial killing. Making it possible for artists to earn a living is solving a human problem (though we won&#8217;t know whether NFTs actually do that until we&#8217;re past the initial bubble). Using links that incorporate history to build communities of people who care about the same things, as Chris Anderson <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/nfts-in-the-uncanny-valley/" target="_blank">suggests</a>, is solving a human problem.</p>



<p>Once we realize that, Web3 isn&#8217;t all that different from the earlier generations of the web. Facebook succeeded because it solved a human problem: People want to associate, to congregate. Facebook may have been a poor solution (it certainly became a poor solution after it decided to prioritize &#8220;engagement&#8221;), but it was a solution. Google succeeded because it solved a different human problem: finding information. The world&#8217;s information was radically decentralized, stored in millions of books and websites. At O&#8217;Reilly, we made one of the first attempts to manage that rapidly growing mess, but our solution, publishing <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.amazon.com/Whole-Internet-Users-Guide-Catalog/dp/1565920635" target="_blank">The Whole Internet</a> and creating <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://en.wikipedia.org/wiki/Global_Network_Navigator" target="_blank">a web portal</a> (the industry’s first) based on it, couldn&#8217;t scale the way Google did five years later. As Larry Page and Sergey Brin discovered, organizing the world&#8217;s information was about computing the tree of relationships dynamically. Like Facebook, Google has become less useful over time, as <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://qz.com/1666863/why-big-tech-keeps-outsmarting-antitrust-regulators/" target="_blank">it seems to have compromised its results to &#8220;maximize shareholder value.&#8221;</a> I would certainly prefer burying monopolies to praising them. But it&#8217;s important to think carefully about what they do well. Google and Facebook, like AT&amp;T before them, succeeded because they solved problems that people cared about solving. Their solutions had real lasting value.</p>



<p>Cryptocurrency provides a cautionary tale. Blockchains may be a brilliant solution to the problem of double-spending. But double spending is a problem very few people have, while theft and other financial crimes on the blockchain are growing every day. (Given the rate at which cryptocurrency crime is growing, perhaps we should be glad that double-spend isn’t just another problem on the very long list.) The catalog of failed startups is full of businesses with ideas that were very cool, but didn&#8217;t actually solve problems that people care about, or didn’t think through the new problems that they would create. As technologists, we&#8217;re unfortunately addicted to the cool and the clever.</p>



<p>Can Web3 make Web0, the web of human interconnections and interests, more manageable? Can it solve human problems, not just abstract computational problems, and do so without creating more problems of its own? Can it help us build new synapses in the human brain, or will it just connect us to people who infuriate us?&nbsp; That&#8217;s the challenge Web3 faces. I think it can meet that challenge; but doing so will require developers to understand that blockchains, NFTs, Dapps, and so on are the means, not the ends. They&#8217;re the components, not the finished product.</p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/the-human-web/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Radar trends to watch: February 2022</title>
      <link>https://www.oreilly.com/radar/radar-trends-to-watch-february-2022/</link>
      <comments>https://www.oreilly.com/radar/radar-trends-to-watch-february-2022/#respond</comments>
      <pubDate>Tue, 01 Feb 2022 12:52:12 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Radar Trends]]></category>
      <category><![CDATA[Signals]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14294</guid>
      <description>Perhaps the most interesting theme from the last month has been discussions of what can be done with NFTs and other Web3 technologies aside from selling links to bored apes. Chris Anderson points out that NFTs are a new kind of link that includes history, and that’s a fascinating idea. We’re also seeing a lot [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>Perhaps the most interesting theme from the last month has been discussions of what can be done with NFTs and other Web3 technologies aside from selling links to bored apes. Chris Anderson points out that NFTs are a new kind of link that includes history, and that’s a fascinating idea. We’re also seeing a lot of debate around the metaverse; an increasing number of companies are lining up in opposition to Facebook/Meta’s vision.</p>



<h2>AI</h2>



<ul><li>The Algorithmic Justice League has proposed paying “<a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.ajl.org/bugs" target="_blank">bug bounties</a>” for algorithmic harms, similar to the way security researchers are paid bounties for finding vulnerabilities in software.</li><li>OpenAI has released a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://openai.com/blog/instruction-following/" target="_blank">new version of GPT-3</a> that is <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/" target="_blank">less toxic</a>–less prone to reproducing racist, sexual, or violent language, though it can still do so when asked. This is not the end of the story, but it’s a big step forward. The new model, InstructGPT, is also much smaller: 1.3 billion parameters, as opposed to 175 billion.</li><li>How do humans learn to work with AI systems?&nbsp; When should a human co-worker accept an AI’s predictions? Researchers at MIT are working on <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-01-ai.html" target="_blank">training methods</a> that help human experts to understand when an AI is, or is not, likely to be accurate.</li><li>It’s no surprise that AI systems can also discriminate on the basis of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-01-artificial-intelligence-discriminate-basis-gender.html" target="_blank">age</a>, in addition to race and gender. While bias in AI is much discussed, relatively little work goes into building unbiased systems.</li><li>Facebook/Meta has developed a new AI algorithm that can be used for <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/01/20/1043885/meta-ai-facebook-learning-algorithm-nlp-vision-speech-agi/" target="_blank">image, text, and speech processing</a>, and that performs better than current specialized algorithms.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://github.com/LIAAD/yake" target="_blank">Yake!</a> is an open source system for <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-01-tool-keywords-texts-language-topic.html" target="_blank">extracting keywords</a> from texts.&nbsp; It’s an important tool for automatically summarizing large bodies of research, developing indexes, and other research tasks.</li><li>GPT-J, an open source language model similar to GPT-3, now has a “<a href="https://medium.com/@josh.wolff.7/how-to-use-the-gpt-j-playground-by-aix-136304ca830d" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">playground</a>” that is open to the public.</li></ul>



<h2>Programming</h2>



<ul><li>Researchers have discovered more efficient ways to <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-01-closer-lifelike-avatars.html" target="_blank">model and render moving human images</a> in real time. This could lead to 3D avatars for your metaverse, better deep fakes, or animations that are truly lifelike.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://bitsbox.com/" target="_blank">Blueprint</a> is a new approach to teaching children to code.&nbsp; It starts with reading (rather than writing) code; new programmers modify and build objects in a metaverse, using a stripped-down version of JavaScript and HTML.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.graphistry.com/blog/what-is-graph-intelligence-how-and-why-the-best-companies-are-adopting-graph-visual-analytics-graph-ai-and-graph-neural-networks" target="_blank">Graph technologies</a> (including graph neural networks) are becoming increasingly important to AI research.</li><li>The title pretty much says it: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/the-case-for-rust-as-the-future-of-javascript-infrastructure/" target="_blank">is Rust the future of JavaScript infrastructure</a>? It’s a faster, safer, and more memory-efficient language for building the tools in the JavaScript ecosystem. Whether Rust will replace JavaScript for web development is anyone’s guess, but it is a better tool for creating software like transpilers and other elements of the JavaScript toolchain.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://blog.sigplan.org/2022/01/13/provably-space-efficient-parallel-functional-programming/" target="_blank">Memory-efficient parallelism for functional programs</a>: Parallelism has been a difficult problem for functional programming, in part because of memory requirements. But as we approach the end to Moore’s Law, parallelism may be the only possible way to improve software performance. This paper suggests a memory management strategy that may solve this problem.</li><li>The return of Y2K: <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://news.sky.com/story/remember-the-y2k-bug-microsoft-confirms-new-y2k22-issue-12507401" target="_blank">January 1, 2022</a> (represented as 2022010001) overflows a signed 32-bit integer. This caused many Microsoft Exchange servers to crash on New Year’s Day. Are more 32-bit overflow bugs hiding?</li><li>Computer-generated code from systems like Copilot also <a href="https://www.wired.com/story/ai-write-code-like-humans-bugs/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">generates bugs and vulnerabilities</a>. This isn’t surprising; Copilot is essentially just copying and pasting code from sources like GitHub, with all of that code’s features and failures.</li></ul>



<h2>Security</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/open-source-democratized-software-now-lets-democratize-security/" target="_blank">Democratizing cybersecurity</a> with low-code tools that enable non-professionals to detect and fix vulnerabilities? This is a great goal; it remains to be seen whether it can be done.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/why-access-management-is-step-one-for-zero-trust-security/" target="_blank">Access management</a> is the key to zero-trust security. Zero trust means little without proper authentication and access control. Many organizations are starting to get on board with stronger authentication, like 2FA, but managing access control is a new challenge.</li><li>The US Cybersecurity and Infrastructure Security Agency (CISA) has <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/cisa-urges-us-orgs-to-prepare-for-data-wiping-cyberattacks/" target="_blank">warned US organizations</a> to prepare themselves for the kind of data-wiping attacks that have been used against the government of Ukraine.</li><li>Open Source is a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/white-house-reminds-tech-giants-open-source-is-a-national-security-issue/" target="_blank">national security issue</a>; all software is, and open source is no better or worse. We knew that all along. But the vulnerabilities in the widely used <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://logging.apache.org/log4j/2.x/security.html" target="_blank">Log4J library</a> have brought it into the public eye. Google is proposing a marketplace for open source maintainers to match volunteers to projects.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://techxplore.com/news/2022-01-raspberry-pi-viruses-devices-software.html" target="_blank">Detecting viruses without installing software</a>: This Raspberry Pi-based system detects viruses on other computers by analyzing RF signals emitted by the processor. Typical obfuscation techniques used by virus creators aren’t effective against it, because it is not examining code.</li><li>The developer of the open source color.js and faker.js libraries intentionally pushed <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.bleepingcomputer.com/news/security/dev-corrupts-npm-libs-colors-and-faker-breaking-thousands-of-apps/" target="_blank">broken versions</a> of the libraries to GitHub, apparently in a protest against corporate use of the libraries without compensation.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://boingboing.net/2022/01/05/norton-anti-virus-can-now-hijack-your-computer-for-cryptomining.html" target="_blank">Norton Antivirus installs a cryptocurrency miner</a> on your computer that mines Ethereum when you’re not using the computer. Antivirus? Or a new cryptojacking scheme?&nbsp; It’s opt-in, but difficult to uninstall. And, in addition to other fees, Norton takes a significant commission. Norton has done the same thing with <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://krebsonsecurity.com/2022/01/500m-avira-antivirus-users-introduced-to-cryptomining/" target="_blank">Avira</a>, another AV product they own.</li><li><a href="https://gradientflow.com/get-ready-for-confidential-computing/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Confidential Computing</a> could become a breakout technology as corporations struggle with privacy legislation and security. It encompasses many different technologies, including homomorphic encryption, differential privacy, and trusted execution environments.</li></ul>



<h2>Web</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/htmx-html-approach-to-interactivity-in-a-javascript-world/" target="_blank">htmx</a> is an approach to building interactive Web applications that <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/htmx-html-approach-to-interactivity-in-a-javascript-world/" target="_blank">extends HTML</a>, allowing it to access browser capabilities directly, rather than through another language like JavaScript and frameworks like React.</li><li>The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.oasisconsortium.com/" target="_blank">Oasis Consortium</a> is a non-profit industry group for building a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.technologyreview.com/2022/01/20/1043843/safe-metaverse-oasis-consortium-roblox-meta/" target="_blank">safer Internet–specifically including the Metaverse</a>, that is “free from online hate and toxicity.”</li><li>Twitter serves as a platform for <a href="https://www.brookings.edu/blog/techtank/2022/01/04/the-vital-role-of-twitter-in-responding-to-covid/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">real-time COVID policy-making</a>: there’s a lot of valuable data there, buried along with the noise.</li></ul>



<h2>Crypto, NFTs, and Web3</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/aleph-im-is-like-a-decentralized-aws-lambda-for-web3-dapps/" target="_blank">Aleph.im</a> is an attempt to implement a service like AWS Lambda that is decentralized. It uses “blockchain-related” technologies (though not a blockchain itself), and is tied to the Aleph token (a cryptocurrency).</li><li>What’s important about <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://thenewstack.io/nfts-in-the-uncanny-valley/" target="_blank">NFTs</a> isn’t the “artwork” that they reference; it’s that they’re a new kind of link, a link that contains history.&nbsp; This history makes possible a new kind of community value.</li><li>A stack for <a href="https://thenewstack.io/makings-of-a-web3-stack-agoric-ipfs-cosmos-network/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">getting started with Web3</a>: This is a far cry from <a href="https://en.wikipedia.org/wiki/LAMP_(software_bundle)" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">LAMP</a>, but it’s a way to start experimenting. The IPFS protocol plays a key role, along with Agoric (a smart contract framework) and the Cosmos network (blockchain interoperability).</li></ul>



<h2>Metaverse</h2>



<ul><li>SecondLife’s creator has returned to Linden Labs, and wants to build a <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://www.fastcompany.com/90713000/second-lifes-creator-is-back-to-build-a-metaverse-that-doesnt-harm-people" target="_blank">metaverse that “doesn’t harm people.”</a>&nbsp; That metaverse won’t have surveillance advertising or VR goggles.</li><li>NVidia talks about their plans for the <a href="https://thenewstack.io/nvidia-announces-expansion-of-omniverse-to-consumer-internet/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">metaverse</a>; it’s less of a walled garden, more like the Web. Companies are increasingly wary of a metaverse that is essentially Facebook’s property.</li></ul>



<h2>Infrastructure</h2>



<ul><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://arstechnica.com/cars/2022/01/moving-more-with-less-freight-startup-bets-on-autonomous-electric-rail-cars/" target="_blank">Autonomous battery-powered freight cars</a> could travel by themselves, eliminating long freight trains.&nbsp; However, the outdated US rail safety infrastructure, which requires trains to maintain large distances between themselves, presents a problem.</li><li>Open Infrastructure Map: All the world’s <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://openinframap.org/#5.44/40.799/-75.629" target="_blank">infrastructure</a> (just about) in one map: the power lines, generation plants, telecom, and oil, gas, and water pipelines. (It doesn’t have reservoirs.) Fascinating.</li><li>Entrepreneurs like Elon Musk have tried to develop solar cells that can be used as shingles, rather than being installed over them. GAF, a company that really knows roofing, now has a <a href="https://arstechnica.com/gadgets/2022/01/new-solar-roof-can-be-nailed-just-like-old-school-shingles/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">solar shingles</a> product on the market. They can be installed similarly to regular shingles, and have similar warranties.</li></ul>



<h2>Quantum Computing</h2>



<ul><li>Researchers have developed a new way of <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://phys.org/news/2022-01-ultrathin-materials-pave-personal-sized-quantum.html" target="_blank">building qubits</a> that is a factor of 100 smaller than current technologies allow, and that appear to have less interference between qubits. While this doesn’t mean we’ll have personal quantum computers, it will make it easier to build quantum computers large enough to do reasonable work.</li><li><a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://popl22.sigplan.org/details/POPL-2022-popl-research-papers/30/Twist-Sound-Reasoning-for-Purity-and-Entanglement-in-Quantum-Programs" target="_blank">Twist</a> is a new language for programming quantum computers. It has a type system that helps programmers reason about entanglement as a means to improve correctness and accuracy.</li><li>Microsoft Azure is <a href="https://thenewstack.io/microsoft-expands-azure-quantum-cloud-services/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">expanding its quantum computing offerings</a> by adding hardware from Rigetti, one of the leading Quantum startups.</li></ul>



<h2>Work</h2>



<ul><li>James Governor talks about the transition from distributed systems to <a href="https://redmonk.com/jgovernor/2022/01/13/the-great-switch-distributed-work-first/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">distributed work</a>.</li></ul>



<h2>Automation</h2>



<ul><li><a href="https://techxplore.com/news/2022-01-robot-tractors-farm.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Automating the farm</a>: tractors that can be controlled by smartphone, robots that can weed fields, and many other technologies at the intersection of GPS, AI, and computer vision are now commercially available.</li></ul>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/radar-trends-to-watch-february-2022/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Andy Warhol, Clay Christensen, and Vitalik Buterin walk into a bar</title>
      <link>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/</link>
      <comments>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/#respond</comments>
      <pubDate>Wed, 26 Jan 2022 20:47:52 +0000</pubDate>
      <dc:creator><![CDATA[Tim O’Reilly]]></dc:creator>
      <category><![CDATA[Web]]></category>
      <category><![CDATA[Commentary]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14292</guid>
      <description>In 1962, Daniel Boorstin crystallized a notion that had been around since at least the 1890s, writing of the new kind of celebrities: “Their chief claim to fame is their fame itself. They are notorious for their notoriety.” The same might be said of cryptocurrencies, NFTs, and meme stocks: They are valuable for being valuable. [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>In 1962, Daniel Boorstin crystallized a notion that had been around <a rel="noreferrer noopener" href="https://quoteinvestigator.com/2019/12/21/famous/" target="_blank">since at least the 1890s</a>, writing of the new kind of celebrities: “Their chief claim to fame is their fame itself. They are notorious for their notoriety.” The same might be said of cryptocurrencies, <a rel="noreferrer noopener" href="https://www.nytimes.com/2021/03/24/opinion/what-are-nfts.html" target="_blank">NFTs</a>, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Meme_stock" target="_blank">meme stocks</a>: They are valuable for being valuable.</p>



<p>So were the rare tulip bulbs whose prices rose to such heights in 17th-century Holland that the “<a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Tulip_mania" target="_blank">tulip bubble</a>” has been the standard to which other financial manias have been compared since. Exactly what drove the bubble is unclear: Futures markets had just been introduced, and tulips were one of the first speculative commodities to be explored. Imports of plants from distant regions, new technologies of plant breeding, and financial innovation made for a heady mix. The prosperity of the rising Dutch colonial empire may have, like today, produced abundant capital eager to be invested and looking for outsized returns in a market that offered tantalizing prospects. People bought tulip bulbs at outrageous prices with the seemingly reasonable expectation that they could sell them for even higher prices in future.</p>



<p>But the idea that crypto is simply a bubble may miss something important that this suite of technologies has to teach us about the economy. In <a rel="noreferrer noopener" href="https://books.google.com/books?id=gViwLbCJ7X0C" target="_blank"><em>Tulipmania</em></a>, written in 2007, Anne Goldgar made the case that the tulip mania was far less widespread and damaging than outlined in Charles Mackay’s 1841 book <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extraordinary_Popular_Delusions_and_the_Madness_of_Crowds" target="_blank"><em>Extraordinary Popular Delusions and the Madness of Crowds</em></a>, which had made it so notorious. But even in minimizing its impact, she agreed that the tulip bubble called into question the very nature of what constitutes value:</p>



<blockquote class="wp-block-quote"><p>In the 17th century, it was unimaginable to most people that something as common as a flower could be worth so much more money than most people earned in a year. The idea that the prices of flowers that grow only in the summer could fluctuate so wildly in the winter, threw into chaos the very understanding of &#8220;value&#8221;.</p></blockquote>



<p>The question of what makes things “valuable” in the first place is a wonderful lens through which to think about cryptocurrencies, NFTs, and meme stocks. As economist Mariana Mazzucato outlines in her book <a rel="noreferrer noopener" href="https://marianamazzucato.com/books/the-value-of-everything/" target="_blank"><em>The Value of Everything</em></a>, the notion of value is not fixed. For early economists, only land and agricultural production created value. Trade, finance, and the power of princes were just moving that value around. By the time of Adam Smith, manufacturing was also understood to create value, but trade and finance—well, they were still just moving that value around. Over time, trade, finance, and entertainment have been brought inside what Mazzucato calls “the value boundary.” Meanwhile, household labor—child rearing, caring for aging parents, cooking, cleaning, and the like done by members of a household rather than purchased as a service—is clearly intrinsically valuable, even essential, but still remains so far outside the value boundary that it remains unpaid and isn&#8217;t even counted as part of GDP. So too, government is widely derided as an extractor rather than a creator of value, despite the efforts of Mazzucato and others to point out <a rel="noreferrer noopener" href="https://marianamazzucato.com/books/the-entrepreneurial-state/" target="_blank">its contributions to innovation and economic growth</a>.</p>



<p>Entertainment is a particularly relevant case in point for how sectors cross the value boundary. Adam Smith thought that opera singers, actors, dancers, and the like were frivolous and created no value for society. Today, many of our most highly paid professionals are entertainers: actors, musicians, athletes, TikTok stars and other social media influencers. Creativity has moved to the heart of today’s internet-fueled “<a rel="noreferrer noopener" href="https://www.nytimes.com/2021/02/04/opinion/michael-goldhaber-internet.html" target="_blank">attention economy</a>.” (OK, maybe politics competes with it for that position, but modern politics shares with creative expression the bestowing of status through attention.) At the same time, much of what people do to entertain each other—both in person and on social media—remains unpaid and treated as outside the value boundary.</p>



<p>The question of how much value is being created by a new sector is not settled quickly when the boundary shifts. Finance is a good example. After the financial crisis of 2009, Lloyd Blankfein said with a straight face that <a rel="noreferrer noopener" href="https://palladiummag.com/2019/11/21/mariana-mazzucato-has-reinvigorated-the-most-important-battle-in-economics/" target="_blank">Goldman Sachs financiers were the most productive workers in the world</a>, even as their machinations brought the global economy to the brink of collapse.</p>



<p>The financial industry is in theory a key enabler of the rest of the economy, managing the flows of capital that allow businesses to invest, to hire, and to build and deliver new products and services. But a large part of finance operates in what we might call the “<a rel="noreferrer noopener" href="https://www.oreilly.com/radar/why-elon-musk-is-so-rich/" target="_blank">betting economy</a>.” Hedge funds and other investors place bets on the direction of interest rates and the price of commodities or company stocks, and build sophisticated financial instruments to harvest profits from changes in those prices, regardless of their direction. Are these people creating value when they place these bets, or are they merely extracting it from someone else in a zero-sum game? That question remains up for debate. Nonetheless, those bets eventually are settled based on some measurable impact in the operating economy. What did the Fed do to interest rates? What were people willing to pay for corn or soybeans or scrap iron? What were Apple’s or Amazon’s or Tesla’s profits, and were they growing or shrinking?</p>



<p>With crypto and Web3 more generally, there is a similar kind of real-world bet that blockchain technology will reshape the plumbing of the financial industry. If it succeeds, the winners will eventually be rewarded with enormous profits, justifying the price that has been paid. Crypto might be a bubble, a flash in the pan that will enrich some speculators while impoverishing others. But it might also be a fundamental innovation that will lead to greater prosperity for all of society. And to many, that’s a bet worth placing.</p>



<p>However, much of the betting is not on the intrinsic value that crypto technologies might deliver in the future. Economist John Maynard Keynes <a rel="noreferrer noopener" href="https://thedecisionlab.com/reference-guide/psychology/the-keynesian-beauty-contest/" target="_blank">compared financial markets to a beauty contest</a> in which the point isn’t to pick the most beautiful contestant but to choose the one that everyone else will think is the most beautiful. And since everyone is playing the game, you’re trying to outguess other people who are constantly changing their votes based on what they think you and others are going to choose. What Keynes didn’t emphasize: <em>it’s a contest!</em> Rich people who have already met their every economic need continue to bet just for the sheer pleasure and addictiveness of playing.</p>



<p>NFTs and meme stocks are out at the bleeding edge of this betting economy, because they are largely untethered from traditional notions of value derived from profits in the operating economy. They might best be described as the tokens in a futures market for attention. Like tulips in 17th-century Holland, they represent a challenge to the very notion of “intrinsic value.”</p>



<p><a rel="noreferrer noopener" href="https://warzel.substack.com/p/the-absurdity-is-the-point" target="_blank">Charlie Warzel captured</a> perfectly the puzzlement that many people are feeling:</p>



<blockquote class="wp-block-quote"><p>When I say I’m thinking a lot about cryptocurrency, what I really mean is that I’m thinking a lot about absurdity. I’m thinking about the way that groups of people who are good at harnessing attention are giddily, proudly using that power to drag absurdist memes/currencies/fortunes into mainstream discourse and force the rest of us to care about/debate/or at least know about it all.</p></blockquote>



<p>And that’s the point where artist and impresario Andy Warhol, innovation expert Clayton Christensen, and Etherum creator Vitalik Buterin walk into the bar. They don’t start out talking about crypto, but like everyone else, they end up there.</p>



<blockquote class="wp-block-quote"><p><a rel="noreferrer noopener" href="https://www.amazon.com/Philosophy-Andy-Warhol-Back-Again/dp/0156717204" target="_blank">Andy Warhol says</a>: “What&#8217;s great about this country is that America started the tradition where the richest consumers buy essentially the same things as the poorest. You can be watching TV and see Coca-Cola, and you know that the President drinks Coca-Cola, Liz Taylor drinks Coca-Cola, and just think, you can drink Coca-Cola, too. A Coke is a Coke and no amount of money can get you a better Coke than the one the bum on the corner is drinking. All the Cokes are the same and all the Cokes are good. Liz Taylor knows it, the President knows it, the bum knows it, and you know it.”</p><p><a rel="noreferrer noopener" href="https://claytonchristensen.com/" target="_blank">Clay Christensen</a> replies: It’s worth noticing that a soft drink like Coke is basically a commodity—carbonated and flavored sugar water—mixed with a whole lot of marketing and branding. That’s actually the secret of the modern economy. I call it the <a rel="noreferrer noopener" href="https://store.hbr.org/product/breakthrough-ideas-for-2004-the-hbr-list/R0402A" target="_blank">law of conservation of attractive profits</a>. “When attractive profits disappear at one stage in the value chain because a product becomes commoditized, the opportunity to earn attractive profits with proprietary products usually emerges at an adjacent stage.”</p><p>Tim O’Reilly and I had a real mind meld about that at the Open Source Business Conference in 2004, Clay continues. Tim gave a talk about how <a rel="noreferrer noopener" href="https://www.oreilly.com/pub/a/tim/articles/paradigmshift_0504.html" target="_blank">the internet and open source were commoditizing proprietary software</a>. He’d noticed that after the IBM personal computer design had commoditized computer hardware, Microsoft had figured out how to make software the next source of proprietary value. Tim was seeing the pattern and was starting to think that what we now call “big data” was going to be the new source of proprietary lock-in and value. I was giving <a rel="noreferrer noopener" href="http://www.asymco.com/2014/06/23/clayton-christensen-on-capturing-the-upside/" target="_blank">my talk about the conservation of attractive profits</a> the same day, and so we had a real laugh about it. He’d uncovered a new example of just what I was talking about.</p><p>But as Tim and I continued to talk about this idea over the years, we realized that the law of conservation of attractive profits applies to way more than the alternating cycle of modularity and open standards versus tight proprietary integration that we’d both originally observed. <a rel="noreferrer noopener" href="https://www.oreilly.com/tim/wtf-book.html" target="_blank">Tim likes to point out</a> that in a world where more and more has become a commodity, things become valuable again because we mix in ideas that <em>persuade</em> people to value them differently. Advertising makes a branded product bring a higher price than a generic equivalent. Cycles of fashion make the latest offerings worth more than last year’s perfectly good clothes. But that’s just the tip of the iceberg. Now everything is infused with imaginative value. People say, “This isn’t just coffee; it’s organic <a rel="noreferrer noopener" href="https://counterculturecoffee.com/blog/coffee-basics-what-is-single-origin-coffee" target="_blank">single-origin coffee</a>.” We’re increasingly paying a premium for intangibles. In 2015, <a rel="noreferrer noopener" href="https://sca.coffee/research/specialty-coffee-facts-figures" target="_blank">55% of the $48 billion US coffee market was for “specialty coffee</a>” of various kinds.</p><p><a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Dave_Hickey" target="_blank">Dave Hickey</a>, who’s been listening, <a rel="noreferrer noopener" href="https://www.amazon.com/Air-Guitar-Essays-Art-Democracy/dp/0963726455" target="_blank">pipes in</a>: That’s been going on for a long time. After World War II, “American businesses stopped advertising products for what they were, or for what they could do, and began advertising them for what they <em>meant—</em>as sign systems within the broader culture.…Rather than producing and marketing infinitely replicable objects that adequately served unchanging needs, American commerce began creating finite sets of objects that embodied ideology for a finite audience at a particular moment—objects that created desire rather than fulfilling needs. This is nothing more or less than an art market.”</p><p>He really gets on a roll then, continuing with enthusiasm: “The Leonardo of this new art market was an ex-custom-car designer from Hollywood named Harley Earl, who headed the design division at General Motors during the postwar period. Earl’s most visible and legendary contributions to American culture were the Cadillac tailfin and the pastel paint job.” It’s not just about creating objects of desire, he continues, but about creating new mechanisms for signaling status. “Most importantly,&#8230;Earl invented the four-year style-change cycle linked to the Platonic hierarchy of General Motors cars, and this revolutionary dynamic created the post-industrial world. Basically, what Earl invented was a market situation in which the consumer moved up the status-ladder within the cosmology of General Motors products—from Chevrolet to Pontiac to Buick to Oldsmobile to Cadillac—as the tailfin or some other contagious motif moved <em>down</em> the price ladder, from Cadillac to Chevrolet, year by year, as styles changed incrementally.”</p><p>Giving a nod to the guy who’d kicked off the conversation, Hickey continues: “As Warhol [is] fond of telling us, the strange thing about the sixties was not that Western art was becoming commercialized but that Western commerce was becoming so much more artistic.”</p><p>Vitalik Buterin jumps in: I wish I’d heard about your work before, Dave. I wasn’t thinking enough about art. “<a rel="noreferrer noopener" href="https://twitter.com/VitalikButerin/status/1477402803433840642?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1477402803433840642%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Findianexpress.com%2Farticle%2Ftechnology%2Fcrypto%2Fethereum-white-paper-predicted-defi-but-missed-nfts-vitalik-buterin-7704098%2F" target="_blank">I completely missed NFTs</a>.” I was focused on practical applications like DeFi, incentivized file storage, and compute, and I didn’t think a lot about how much of the economy has become an art market.</p><p>Hickey replies that he wishes everyone would think more deeply about what art teaches us about how economies and people tick. I didn’t subtitle my book <a rel="noreferrer noopener" href="https://www.amazon.com/Air-Guitar-Essays-Art-Democracy/dp/0963726455" target="_blank"><em>Air Guitar</em></a> “Essays on Art and Democracy” for shits and giggles, he says.</p><p>Hickey then starts rhapsodizing about his fascination with cars growing up “in the American boondocks” during the 1950s and ’60s. “My first glimmerings of higher [art] theory grew out of that culture: the rhetoric of image and icon, the dynamics of embodied desire, the algorithms of style change, and the ideological force of disposable income. All of these came to me in the lingua franca of cars, arose out of our perpetual exegesis of its nuanced context and iconography. And it was worth the trouble, because all of us who partook of this discourse, as artists, critics, collectors, mechanics, and citizens, understood its politico-aesthetic implications, understood that we were voting with cars….We also understood that we were <em>dissenting</em> when we customized them and hopped them up—demonstrating against the standards of the republic and advocating our own refined vision of power and loveliness.”</p><p>In the computer industry, you can see how Steve Jobs did for Apple the exact thing that Earl had done for GM. From the 1984 Macintosh ad to the “Think Different” campaign, Apple wasn’t selling hardware and software. It was selling identity and a sense of meaning. The new $40 billion market for NFTs—essentially digital collectibles whose chief value is in the bragging rights of how much you paid for them or how cool and unusual they are—takes this idea to the next level.</p><p>Buterin replies: Your point about “demonstrating against the standards of the republic and advocating our own refined vision of power and loveliness” really resonates with me, and I suspect it will with a lot of the crypto community. We aren’t just thinking about how to advance blockchain technology. We’re also thinking a lot about upending the current financial system and about deep questions like <a rel="noreferrer noopener" href="https://vitalik.ca/general/2021/03/23/legitimacy.html" target="_blank">legitimacy</a>. “An outcome in some social context is legitimate if the people in that social context broadly accept and play their part in enacting that outcome, and each individual person does so because they expect everyone else to do the same.”</p><p>“Why is it that Elon Musk can sell an NFT of Elon Musk&#8217;s tweet, but Jeff Bezos would have a much harder time doing the same? Elon and Jeff have the same level of ability to screenshot Elon&#8217;s tweet and stick it into an NFT dapp, so what&#8217;s the difference? To anyone who has even a basic intuitive understanding of human social psychology (or the <a rel="noreferrer noopener" href="https://www.austinartistsmarket.com/famous-fakes-art-history/" target="_blank">fake art scene</a>), the answer is obvious: Elon selling Elon&#8217;s tweet is the real thing, and Jeff doing the same is not. Once again, millions of dollars of value are being controlled and allocated, not by individuals or cryptographic keys, but by social conceptions of legitimacy.”</p><p>But there’s more to it than that. “Which NFTs people find attractive to buy, and which ones they do not, is [also] a question of legitimacy: if everyone agrees that one NFT is interesting and another NFT is lame, then people will strongly prefer buying the first, because it would have both higher value for bragging rights and personal pride in holding it, and because it could be resold for more because everyone else is thinking in the same way.”</p><p>“If you&#8217;re not in a <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Coordination_game" target="_blank">coordination game</a>, there&#8217;s no reason to act according to your expectation of how other people will act, and so legitimacy is not important. But as we have seen, coordination games are everywhere in society, and so legitimacy turns out to be quite important indeed. In almost any environment with coordination games that exists for long enough, there inevitably emerge some mechanisms that can choose which decision to take. These mechanisms are powered by an established culture that everyone pays attention to these mechanisms and (usually) does what they say. Each person reasons that because everyone else follows these mechanisms, if they do something different they will only create conflict and suffer, or at least be left in a lonely forked ecosystem all by themselves.”</p><p>So one way to understand what we’re working on in the crypto world is that we’re building new mechanisms for solving the problems of consensus and coordination and legitimacy. And that’s also exactly what “the market” is doing when it tries to settle the messy question of value. So when we talk about building a new financial system with crypto, we’re not talking about just rebuilding the plumbing of the existing system with fancy new pipes, we’re questioning how value is created and who gets it.</p><p>We can change the way we distribute wealth. Crypto made a lot of people rich through the betting economy, but we don’t have to spend our gains just on new bets that make the rich richer, looking for the next breakout cryptocurrency or company. We can take those gains and give them away, as I did when I <a rel="noreferrer noopener" href="https://www.vox.com/recode/2021/5/12/22433113/vitalik-buterlin-cryptocurrency-india-shiba-inu-coin-philanthropy" target="_blank">donated over a billion dollars of Ether and Shiba Inu coins to India</a> for COVID relief. But more importantly, we can build new <em>mechanisms</em> for people to coordinate around socially valuable goals.</p><p>“The concept of supporting public goods through value generated ‘out of the ether’ by publicly supported conceptions of legitimacy has value going far beyond the Ethereum ecosystem. An important and immediate challenge and opportunity is NFTs. NFTs stand a great chance of significantly helping many kinds of public goods, especially of the creative variety, at least partially solve their <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons" target="_blank">chronic and systemic funding deficiencies</a>.…If the conception of legitimacy for NFTs can be pulled in a good direction, there is an opportunity to establish a solid channel of funding to artists, charities and others.”</p><p><a rel="noreferrer noopener" href="https://forkast.news/vitalik-buterin-defi-nfts-daos/" target="_blank">Buterin adds</a>: Ethereum, NFTs, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Decentralized_autonomous_organization" target="_blank">DAOs</a> are building blocks. “There’s a lot of different ways to connect every one of these components and most of the interesting applications end up connecting different pieces together.…I don’t see one kind of dominating use case. I just see it opening up the floodgates for a thousand different experiments.” NFTs are one experiment. DAOs are another. Who would have thought a few years ago that someone would organize a DAO to compete with billionaires to <a rel="noreferrer noopener" href="https://www.wsj.com/articles/crypto-investors-want-to-buy-rare-copy-of-u-s-constitution-at-sothebys-auction-11637071590" target="_blank">buy a rare copy of the US constitution</a> or to <a rel="noreferrer noopener" href="https://www.vice.com/en/article/93b5ve/crypto-investors-buy-40-acres-of-land-in-wyoming-to-build-blockchain-city" target="_blank">buy land in Wyoming</a>?</p><p>At this point, Blaise Aguera y Arcas, who’s been sitting over at the next table sketching out for his buddies the latest <a rel="noreferrer noopener" href="https://medium.com/@blaisea?p=6f881d6d8e75" target="_blank">progress on Google’s LaMDA large language model and its implications for our notion of personhood</a>, can’t resist leaning over and jumping into the conversation.</p><p>“We&#8217;ve been having these conversations for a long time about robots taking people&#8217;s jobs, and we&#8217;ve been thinking about it entirely in the domain of actual robots with arms and things. But the real impact is going to be that most middle class people nowadays are doing what David Graeber called <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Bullshit_Jobs" target="_blank">bullshit jobs</a>. And it&#8217;s clear that large language models can already do many of those jobs. We&#8217;re approaching the point where it feels like capitalism is maybe about to rupture, or <em>something</em> is about to rupture.”</p><p>He continues, “Graeber was questioning the legitimacy of labor in its modern form, and also the ideas of efficiency that supposedly underlie capitalism, which is actually tremendously inefficient in a variety of ways. And in particular, the thesis is that Keynes was right, in the ’20s and ’30s, in saying that, by now, due to automation, we&#8217;d all be working 15-hour workweeks. But rather than turning this into a utopia, in which we all have all these free services and don&#8217;t have to work a lot and so on, instead we&#8217;ve made a socialism for the middle class, socialism for the bourgeois, in the form of inventing all kinds of bullshit jobs.” And all the people who still have essential jobs—they still have to work, and we don’t pay a lot of them very well.</p></blockquote>



<p class="has-text-align-center"><strong>* * *</strong></p>



<p>So what will people do if they no longer have to do bullshit jobs? Maybe they’ll make up cool shit and share it with each other, eventually building a world like the one Cory Doctorow imagined in <a rel="noreferrer noopener" href="https://www.amazon.com/Down-Magic-Kingdom-Cory-Doctorow/dp/076530953X" target="_blank"><em>Down and Out in the Magic Kingdom</em></a> and <a href="https://en.wikipedia.org/wiki/Walkaway_(Doctorow_novel)"><em>Walkaway</em></a>, where measures of status are the actual currency. In the meantime, some of them might show their creativity on YouTube or TikTok and convert status to value by directing attention to products and other people. Some might create and sell NFTs. Others might peddle bullshit startups or fancy new get-rich-quick schemes. But <a rel="noreferrer noopener" href="https://mattstoller.substack.com/p/cryptocurrencies-a-necessary-scam" target="_blank">is that really new</a>? The future always has its share of hucksters along with its inventors. Sometimes the same people are both.</p>



<p>Bill Gates once said, “<a rel="noreferrer noopener" href="https://www.inc.com/damon-brown/this-perfect-bill-gates-quote-will-frame-your-next-decade-of-success.html" target="_blank">We always overestimate the change</a> that will occur in the next two years and underestimate the change that will occur in the next ten. Don&#8217;t let yourself be lulled into inaction.” That doesn’t mean to rush out and buy the latest meme stock, meme coin, or overpriced NFT. But it does mean that it’s important to engage with the social, legal, and economic implications of crypto. <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Technological_Revolutions_and_Financial_Capital" target="_blank">The world advances one bubble at a time</a>. What matters is that <a rel="noreferrer noopener" href="https://www.oreilly.com/radar/why-its-too-early-to-get-excited-about-web3/" target="_blank">what’s left behind when the bubble pops</a> makes the world richer in possibilities for the next generation to build on.</p>



<p>Looking at the arc of the modern economy, we are on a path for the market for status to become a central part of how value is measured.</p>



<p>Let’s give John Maynard Keynes the last word, even though he left the bar long before we arrived. In “<a rel="noreferrer noopener" href="http://www.econ.yale.edu/smith/econ116a/keynes1.pdf" target="_blank">Economic Possibilities for Our Grandchildren</a>,” the 1929 piece that Blaise referred to earlier, he wrote:</p>



<blockquote class="wp-block-quote"><p>For the first time since [our] creation [we] will be faced with [our] real, [our] permanent problem—how to use [our] freedom from pressing economic cares, how to occupy the leisure, which science and compound interest will have won for [us], to live wisely and agreeably and well.…</p><p>To judge from the behaviour and the achievements of the wealthy classes to-day in any quarter of the world, the outlook is very depressing! For these are, so to speak, our advance guard—those who are spying out the promised land for the rest of us and pitching their camp there. For they have most of them failed disastrously, so it seems to me—those who have an independent income but no associations or duties or ties—to solve the problem which has been set them.</p><p>I feel sure that with a little more experience we shall use the new-found bounty of nature quite differently from the way in which the rich use it to-day, and will map out for ourselves a plan of life quite otherwise than theirs.</p></blockquote>



<p>We’re now coming on to nearly 100 years since Keynes dreamed that optimistic, egalitarian dream and made his critique of the idle rich who were already living it. Abundance seems as far away as ever, or even further, and the rich haven’t changed as much as Keynes hoped.</p>



<p>It may seem deeply out of touch to talk about an economy of abundance when so many people face such great economic hardship. But that was also true for those alive in 1929. They had a worldwide depression and a great war ahead of them, and turned all their energies to dealing with both. Their success ushered in decades of widely shared prosperity. We face climate change, new pandemics, and persistent economic inequality and consequent political instability. Wars are not out of the question. Can we also rise to the challenge?</p>



<p>Through it all, the Next Economy beckons. We see its signs all around us. Keynes was right that humanity’s job in an economy of abundance is to learn to live together wisely and agreeably and well, but he was wrong to think that abundance will mean the end of competition and striving. If we do reach Keynes’s predicted future, in which more and more of what people depend on for survival has become cheap—a commodity—and our labor is not needed, how will the circulatory system of the economy sustain itself? Might the seeming froth and craziness of the crypto markets be an early implementation—not Web3 but NextEconomy1—of the next stage by which humanity engages in the ongoing imaginative competition to make things valuable again?</p>



<hr class="wp-block-separator" />



<p><em>John Maynard Keynes died in 1946, Andy Warhol in 1987, </em><a rel="noreferrer noopener" href="https://www.nytimes.com/2020/01/25/business/clayton-christensen-dead.html" target="_blank"><em>Clay Christensen in 2020</em></a><em>, and </em><a rel="noreferrer noopener" href="https://www.nytimes.com/2021/11/30/arts/dave-hickey-dead.html" target="_blank"><em>Dave Hickey just at the end of last year</em></a><em>. I wish that they could have had this conversation with Vitalik Buterin, who joins them in thinking deeply about the intersection of art, economics, business, politics, and culture. I have put my own words into their mouths; those that are in quotation marks are their own, from their books, published articles, and interviews, though the order in which paragraphs appear may be different from the original. The quotes from Blaise Aguera y Arcas are from a recording of a Zoom conversation that we had while I was writing this piece. I told him what I was working on, and his thoughts were so relevant that I couldn’t help but include them.</em></p>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/andy-warhol-clay-christensen-and-vitalik-buterin-walk-into-a-bar/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Technology Trends for 2022</title>
      <link>https://www.oreilly.com/radar/technology-trends-for-2022/</link>
      <comments>https://www.oreilly.com/radar/technology-trends-for-2022/#respond</comments>
      <pubDate>Tue, 25 Jan 2022 12:58:55 +0000</pubDate>
      <dc:creator><![CDATA[Mike Loukides]]></dc:creator>
      <category><![CDATA[Radar Trends]]></category>
      <category><![CDATA[Signals]]></category>
      <guid isPermaLink="false">https://www.oreilly.com/radar/?p=14259</guid>
      <description>It’s been a year since our&amp;#160;last report&amp;#160;on the&amp;#160;O’Reilly learning platform. Last year we cautioned against a “horse race” view of technology. That caution is worth remembering: focus on the horse race and the flashy news and you’ll miss the real stories. While new technologies may appear on the scene suddenly, the long, slow process of [&amp;#8230;]</description>
      <content:encoded><![CDATA[
<p>It’s been a year since our&nbsp;<a href="https://learning.oreilly.com/library/view/where-programming-ops/9781098105396/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">last report</a>&nbsp;on the&nbsp;<a href="https://learning.oreilly.com/home/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">O’Reilly learning platform</a>. Last year we cautioned against a “horse race” view of technology. That caution is worth remembering: focus on the horse race and the flashy news and you’ll miss the real stories. While new technologies may appear on the scene suddenly, the long, slow process of making things that work rarely attracts as much attention. We start with an explosion of fantastic achievements that seem like science fiction—imagine,&nbsp;<a href="https://www.gwern.net/GPT-3" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">GPT-3 can write stories</a>!—but that burst of activity is followed by the process of putting that science fiction into production, of turning it into real products that work reliably, consistently, and fairly. AI is making that transition now; we can see it in our data. But what other transitions are in progress? What developments represent new ways of thinking, and what do those ways of thinking mean? What are the bigger changes shaping the future of software development and software architecture? This report is about those transitions.</p>



<p>Important signals often appear in technologies that have been fairly stable. For example, interest in security, after being steady for a few years, has suddenly jumped up, partly due to some spectacular ransomware attacks. What’s important for us isn’t the newsworthy attacks but the concomitant surge of interest in security practices—in protecting personal and corporate assets against criminal attackers. That surge is belated but healthy. Many businesses are moving IT operations to “the cloud,” a shift that’s probably been accelerated by the COVID-19 pandemic. What does that mean for the way software is designed and built? Virtual and augmented reality are technologies that were languishing in the background; has talk of the “metaverse” (sparked in part by Mark Zuckerberg) given VR and AR new life? And it’s no surprise that there’s a lot of interest in blockchains and NFTs. What does that mean, and how is it affecting software developers?</p>



<p>To understand the data from our learning platform, we must start by thinking about bias. First, our data is biased by our customer base. Of course. There’s no sampling error; all of our customers “vote” with the content they use. You could read this as a report on the biases of our customer base. Our customer base is large and worldwide (millions of developers, from well over 100 countries), but we won’t pretend that it’s representative of all programmers and technologists. While our customers include many individual developers, contractors, and hobbyist programmers, commercial (enterprise) software developers are very heavily represented—although there are certainly areas into which we’d like more visibility, such as the crucial Asia-Pacific software development community.</p>



<p>We used data from the first nine months (January through September) of 2021. When doing year-over-year comparisons, we used the first nine months of 2020.<sup><a href="#fn1">1</a></sup></p>



<p>We looked at four specific kinds of data: search queries, questions asked to O’Reilly Answers (an AI engine that has indexed all of O’Reilly’s textual content; more recently, transcripts of video content and content from Pearson have been added to the index), resource usage by title, and resource usage by our topic taxonomy. There are some important biases here. If resources don’t exist, our customers can’t use them. To take one example, at this point, the platform has no content on the QUIC protocol or HTTP/3. Regardless of the level of interest, usage for these topics is going to be zero.</p>



<p>Search queries behave differently. Users certainly can search for content that doesn’t exist, so searches can be a good leading indicator of technology trends. However, most searches on our platform are single-word terms: users search for “Java” or “Python,” not “How do I use the Decorator pattern in C++?” (<a href="https://www.oreilly.com/online-learning/article-answers.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">O’Reilly Answers</a>&nbsp;is a great resource for answering questions like this.) As a result, the signals we get from looking at searches aren’t very granular. Answers could provide additional granularity, since users ask full questions. But Answers is a new service, only released in October 2020. So while we can discuss whether Answers usage is in line with other services, it’s difficult to talk about trends with so little data, and it’s impossible to do a year-over-year comparison.</p>



<p>Content usage, whether by title or our taxonomy, is based on an internal “units viewed” metric that combines all our content forms: online training courses, books, videos, Superstream online conferences, and other new products. It includes content from all of the publishing partners in the platform, not just O’Reilly. Results in each group of topics are normalized to 1, so items within the same group can be compared (Java to Python but not Java to Ethereum, for example).</p>



<h2>O’Reilly Answers</h2>



<p>We’re very excited about O’Reilly Answers, the newest product on the platform. Answers is an intelligent search that takes users directly to relevant content, whether that&#8217;s a paragraph from a book, a snippet of a video, or a block of code that answers a question. Rather than searching for an appropriate book or video and skimming through it, you can ask a specific question like “How do you flatten a list of lists in Python?” (a question I’ve asked several times). Our approach to Answers was to do a simple “bag of words” analysis: count the number of times each word was used in all Answers queries. We divided Answers questions into two categories: “organic” queries, which users type themselves, and “question bank” queries, which are sample questions that users can click on. (Questions were rotated in and out of the question bank.) Our analysis only included organic questions; we didn’t count clicks on the question bank. What’s perhaps surprising is that many users typed questions from the question bank into the Answers search bar. These retyped questions were counted as organic queries.</p>



<p>That explains the most commonly asked question on Answers: “What is dynamic programming?” That question appeared frequently in the question bank. It was evidently intriguing enough that many users typed it in, verbatim, in addition to clicking on it; it was the second-most-common organically typed question, only slightly behind “How do I write good unit test cases?” (also very popular in the question bank).</p>



<p>Ignoring stop words (like “and”) and significant words that aren’t really meaningful to us (like “good”), the top five words were “data,” “Python,” “Git,” “test,” and “Java.” (And you can see most of the words from those top two questions in the top 15 or 20 words.)</p>



<p>What can we learn from this? Data continues to be one of the most important topics for our users. A quick look at bigram usage (word pairs) doesn’t really distinguish between “data science,” “data engineering,” “data analysis,” and other terms; the most common word pair with “data” is “data governance,” followed by “data science.” “Data analysis” and “data engineering” are far down in the list—possibly indicating that, while pundits are making much of the distinction, our platform users aren’t. And it certainly suggests that data governance (slightly ahead of&nbsp;&#8220;data science&#8221; itself)&nbsp;is a topic to watch.</p>



<p>Python and Java have long been the top two programming languages on our platform, and this year is no exception. We’ll see later that usage of Python and Java content is very slightly down and that usage of content about Rust and Go is growing rapidly (though it’s still relatively small). The word “programming” was also one of the most frequently used words, reflecting our core audience. And “Kubernetes” was in the top 1%, behind “Java” and “Python” but ahead of “Golang” (top 2%) and “Rust” (4%). The frequency of questions about Kubernetes reflects the importance of container orchestration to modern operations. “AWS,” “Azure,” and “cloud” were also among the most common words (all in the top 1%), again showing that our audience is highly interested in the major cloud platforms. Usage of the term “GCP” and the bigram “Google Cloud” trailed the others, though to some extent that’s because Google has never been clear about the name of its cloud platform. Both “GCP” and “Google Cloud” were in the top 3% of their respective lists.</p>



<p>Words about cryptocurrency (“Bitcoin,” “Ethereum,” “crypto,” “cryptocurrency,” “NFT”) are further down on the list, though still in the top 20%. That’s not surprising. Elsewhere, we’ll see that the use of content about these topics is rising sharply, but usage still isn’t large. We have no “previous year” data for Answers, so we can’t discuss trends, but the fact that these terms are appearing in significant numbers is certainly important.</p>



<p>That quick dip into the bag of words gives us some clues about what we’ll see when we look at the data in more detail. Now let’s start investigating content usage: what our customers actually read, watched, or participated in during the past year.</p>



<h2>Becoming Secure</h2>



<p>Security was frequently in the news in 2021, and for the worst possible reasons. A wave of&nbsp;<a href="https://www.oreilly.com/radar/defending-against-ransomware-is-all-about-the-basics/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">ransomware attacks</a>&nbsp;crippled important infrastructure, hospitals, and many other businesses, both large and small. Supply chain attacks, in which an attacker places a payload in software that’s delivered to its victim through normal distribution channels, occurred in both open source and commercial software. In&nbsp;<a href="https://us-cert.cisa.gov/kaseya-ransomware-attack" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">one notable case</a>, the victim was a well-known enterprise security company, whose infected software was distributed to thousands of clients.</p>



<p>We saw large increases for content about specific topics within security. Usage of content about ransomware has almost tripled (270% increase). Content about privacy is up 90%; threat modeling is up 58%; identity is up 50%; application security is up 45%; malware is up 34%; and zero trust is up 23%. Safety of the supply chain isn’t yet appearing as a security topic, but usage of content about supply chain management has seen a healthy 30% increase. The increase for content on identity is a particularly important sign. Identity management is central to zero trust security, in which components of a system are required to authenticate all attempts to access them. Understanding identity management is a big step toward putting zero trust security into practice.</p>



<p>Usage of general content also increased. Units viewed for items with the word “security” or “cybersecurity” in the title increased by 17% and 24%, respectively. Network security, also a general topic, increased 15%. While these increases are relatively modest compared to specific topics like ransomware and privacy, keep in mind that in absolute numbers, the usage of “security” titles led all other security topics by a large margin. And a 17% increase in an established topic is very healthy.</p>



<p>Another important sign is that usage of content about compliance and governance was significantly up (30% and 35%, respectively). This kind of content is frequently a hard sell to a technical audience, but that may be changing. While compliance and governance are frequently mentioned in the context of data and privacy, it’s important to realize that they’re central issues for managing security. What are an organization’s responsibilities if it suffers a breach or an attack? Has the organization managed its data responsibly? This increase points to a growing sense that the technology industry has gotten a regulatory free ride and that free ride is coming to an end. Whether it’s stockholders, users, or government agencies who demand accountability, enterprises will be held accountable. Our data shows that they’re getting the message.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig01-1048x546.png" alt="" class="wp-image-14260" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig01-1048x546.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig01-300x156.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig01-768x400.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig01.png 1201w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for security</em></figcaption></figure>



<p>According to a&nbsp;<a href="https://ischoolonline.berkeley.edu/blog/cybersecurity-salary/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">study by UC Berkeley’s School of Information</a>, cybersecurity salaries have crept slightly ahead of programmer salaries in most states, suggesting increased demand for security professionals. And an increase in demand suggests the need for training materials to prepare people to supply that demand. We saw that play out on our platform. Looking for titles matching security certifications proved to be a poor metric (probably because long, unwieldy certification names do poorly in titles), but when we look at our content taxonomy rather than title searches, we see that SSCP (System Security Certified Practitioner) is up 54%, and CompTIA Security+ is up 27%.</p>



<h2>Software Development</h2>



<p>Software development is a mega category on the O’Reilly learning platform. It includes almost everything, from programming languages to cloud to architecture and more. While it’s customary to start with a rundown on the programming language horse race, we won’t do that. Whether Python leads Java or not just isn’t interesting (though we will have a few words to say about that later on).</p>



<p>The most interesting topic within software development hasn’t yet made it to our platform. Everyone is talking about developer experience (DX): what can be done to make life better for software developers. How can their jobs be made more enjoyable, helping them to become more effective? That’s an issue that will become increasingly important as organizations try to keep programmers from jumping ship to another company. While we don’t yet have any content on developer experience, we’d be surprised if there isn’t some next year. For one source of ideas about where developer experience is headed, look at our report&nbsp;<a href="https://learning.oreilly.com/library/view/low-code-and-the/9781098112592/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>Low Code and the Democratization of Programming</em></a>. In it, we tried to take a longer view—examining not what trends will change programming next year but what we might see five or ten years from now.</p>



<p>Software architecture, Kubernetes, and microservices were the three topics with the greatest usage for 2021. Their year-over-year growth is also very healthy (19%, 15%, and 13%, respectively). It only looks small when compared with the growth of topics like&nbsp;<a href="https://www.redhat.com/en/topics/api/what-does-an-api-gateway-do" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">API gateway</a>&nbsp;(218%). That kind of growth reflects the “law” we’ve observed throughout this report: it’s easy for a small topic to have large growth numbers but much more difficult for a topic that’s already dominant. API gateway content gets roughly 1/250 as many units viewed as content on architecture or Kubernetes does.</p>



<p>However, we want to be clear: while API gateway’s usage numbers are relatively small, 218% growth is a very strong signal. So is the growth in cloud native (54%), starting from significantly more units viewed in 2020 (roughly 1/8 of architecture or Kubernetes). Enterprises are investing heavily in Kubernetes and microservices; they’re building cloud native applications that are designed from the start to take advantage of cloud services. And API gateways are an important tool for routing requests between clients and services.</p>



<p>In this context, it’s no accident that content usage for containers shows significant growth (137%), while Docker shows less growth but higher usage. Containers are proving to be the best way to package applications and services so that they’re platform independent, modular, and easily manageable. We don’t want to understate the difficulty of moving to containers and using tools from the Kubernetes ecosystem to manage them, but remember that a few years ago, enterprise applications were monoliths running on a small number of servers and managed entirely by hand. Many businesses have now scaled an order of magnitude or so beyond that, with hundreds of services running on thousands of servers in the cloud, and you’ll never succeed at that scale if you’re starting and stopping servers and services by hand. We’re still exploring this transition, and it will continue to be a big story for the next few years.</p>



<p>When we’re talking about microservices running in the cloud, we’re talking about distributed systems. So it’s no surprise that usage of content about distributed systems rose 39% in the past year. The related topics complex systems and complexity also showed significant growth (157% and 8%). It’s also worth noting that design patterns, which fell out of favor for a few years, have come back: usage is very solid and year-over-year growth is 19%.</p>



<p>Quantum computing remains a topic of interest. Units viewed is still small, but year-over-year growth is 39%. That’s not bad for a technology that, honestly, hasn’t been invented yet. Although some primitive quantum computers are available now, computers that can do real work are still several years away. (<a href="https://research.ibm.com/blog/ibm-quantum-roadmap" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">IBM’s roadmap</a>&nbsp;has 1,000-physical-qubit computers coming in two years, though the best estimate is that we’ll need 1,000 physical qubits to create one&nbsp;<a href="https://singularityhub.com/2021/07/19/google-gets-one-step-closer-to-error-corrected-quantum-computing/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">error-corrected qubit</a>.) But when those computers arrive, there will clearly be people ready to program them.</p>



<p>We’ve said almost nothing about architecture, except to notice heavy usage and solid growth. All this ferment—rebuilding legacy applications, moving to the cloud, microservices, orchestration—doesn’t happen without good, consistent software design. Success with microservices is impossible without giving serious thought to designing good APIs for your services to present to each other and, in turn, to the rest of the world. The problem with legacy applications is that they’re inflexible: they leave you stuck with the capabilities you had 20 years ago. If you replace your old legacy software with new legacy software that doesn’t have the ability to evolve as your needs and opportunities change, if you build something that’s just as inflexible as what it replaced, what have you accomplished? This is where software architecture comes into play: how do teams build systems that aren’t just adequate for today but that will be flexible enough to grow with the business? Solid year-over-year growth and heavy usage is exactly what we’d expect to see.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig02-1048x545.png" alt="" class="wp-image-14261" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig02-1048x545.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig02-300x156.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig02-768x400.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig02.png 1184w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for software development&nbsp;topics</em></figcaption></figure>



<p>Finally, last year we observed that serverless appeared to be keeping pace with microservices. That’s no longer true. While microservices shows healthy growth, serverless is one of the few topics in this group to see a decline—and a large one at that (41%).</p>



<h2>Programming Languages</h2>



<p>We’ve said many times that we’re uninterested in the language horse race. Usage of well-established programming languages changes very slowly year to year. Occasionally a language breaks out of the pack, but that’s rare. We’d go so far as to say it’s less of a horse race than a turtle race—a turtle race in which a language that’s slowly gaining traction in the enterprise space can gradually come to dominate the cool language&nbsp;<em>du jour</em>.</p>



<p>So we’ll avoid the horse race entirely and focus on possible reasons for any changes. What are the important changes since last year?&nbsp;C++&nbsp;has grown significantly (13%) in the past year, with usage that is roughly twice C’s. (Usage of content about C is essentially flat, down 3%.) We know that C++ dominates game programming, but we suspect that it’s also coming to dominate embedded systems, which is really just a more formal way to say “internet of things.” We also suspect (but don’t know) that C++ is becoming more widely used to develop microservices. On the other hand, while C has traditionally been the language of tool developers (all of the Unix and Linux utilities are written in C), that role may have moved on to newer languages like Go and Rust.</p>



<p>Go and Rust continue to grow. Usage of content about Go is up 23% since last year, and Rust is up 31%. This growth continues a trend that we noticed last year, when Go was up 16% and Rust was up 94%. Is the decline in Rust’s rate of growth a concern? Don’t let the second derivative fool you. Last year Rust content was starting from near-zero and 90% growth was easy. This year it’s well-established (I don’t think we’ve ever seen a language establish itself quite so quickly), and we expect growth to continue. Both Rust and Go are here to stay. Rust reflects significantly new ways of thinking about memory management and concurrency. And in addition to providing a clean and relatively simple model for concurrency, Go represents a turn from languages that have become increasingly complex with every new release.</p>



<p>We see less of the “functional versus object oriented” wars than we have in the past, and that’s a good thing. Both topics are down (14% and 16%, respectively). Functional features have been integrated into Java, C#, and a number of other languages, so the only real question to debate is how much of a purist you want to be. But that’s a distraction—our customers want to get their work done.</p>



<p>Having said all that, what about the “old guard”? They’re nice and stable. Python, Java, and JavaScript are still the leaders, with Java up 4%, Python down 6%, and JavaScript down 3%. (“Python” and “Java” are both in the top five words used in O’Reilly Answers.) Although any change under 10% is small in the greater scheme of things, we’re surprised to see Python down. And, like last year, usage of Java content is only slightly behind that of Python if you add Spring usage to Java usage. (Spring is a large, all-encompassing group of frameworks in the Java ecosystem, but Spring titles usually don’t mention Java.) C#, a core language on Microsoft platforms, was also stable (down 1% year-over-year).</p>



<p>Scala and Kotlin, two other languages that belong to the Java ecosystem, are both down, 27% and 9%, respectively. Scala’s drop is particularly noteworthy. That may reflect the release of Scala 3.0 in May 2021, which would tend to make content based on Scala 2 obsolete.</p>



<p>Use of JavaScript content on our platform is surprisingly low—though use of content on TypeScript (a version of JavaScript with optional static typing) is up. Is TypeScript replacing JavaScript? We’ll see in a few years. Even with 19% growth, TypeScript has a ways to go before it catches up; TypeScript content usage is roughly a quarter of JavaScript’s. The relatively low usage of JavaScript on our platform may reflect our enterprise-centered audience, large numbers of whom work on backend and middleware services. Our data is similar to&nbsp;<a href="https://www.tiobe.com/tiobe-index/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">TIOBE&#8217;s</a>&nbsp;(in which the top languages are Python, C, and Java) and sharply different from&nbsp;<a href="https://redmonk.com/sogrady/2021/08/05/language-rankings-6-21/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">RedMonk&#8217;s</a>&nbsp;(in which JavaScript leads, followed by Python and Java).</p>



<p>In our&nbsp;<a href="https://www.oreilly.com/radar/2021-data-ai-salary-survey/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>2021 Data/AI Salary Survey</em></a>, we noted that most respondents used more than one programming language. That’s certainly true of our audience as a whole. We also discovered that Python programmers had midrange salaries, while the highest salaries went to respondents who used Go, Rust, and Scala. Our interpretation was that Python has become table stakes. If you work with data, you’re expected to know Python; the ability to work with one of these other languages gives you added value. While we don’t have salary data for platform users, we suspect the same is true. If you work on enterprise or backend software, Java is table stakes; if you do frontend development, JavaScript is table stakes. But whatever your specialty or your primary language, fluency with next-generation languages like Go and Rust gives you added value.</p>



<p>One final final note and then we’ll move on. When we looked at our analysis of O’Reilly Answers, we were puzzled by the top question: “What is dynamic programming?” It seemed strange to see that at the top of the list. Stranger still: while that question was in the question bank, when we removed question bank clicks from the data and looked only at organic questions (questions typed by a user), “What is dynamic programming?” was still at the top. We don’t think this is a rehash of the tired “static versus dynamic” debate of a few years ago; there were no questions about dynamic languages.&nbsp;<a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Dynamic programming</a>&nbsp;is a technique for breaking down complex problems into smaller components. It will clearly be a topic to watch as programmers continue to deal with increasingly complex systems.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig03-1048x674.png" alt="" class="wp-image-14262" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig03-1048x674.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig03-300x193.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig03-768x494.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig03.png 1162w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for programming languages</em></figcaption></figure>



<h2>Learning About the Cloud</h2>



<p>Our data about the cloud and cloud providers tells an interesting story. It’s clear that Amazon Web Services’ competition is on the rise. Usage of content about Microsoft Azure is up 32% and Google Cloud is up 54%, while the usage of AWS-related content has declined by 3%. Actual usage of content about Azure almost matches AWS, while Google Cloud is farther behind, although that may reflect the quantity of material available.</p>



<p>If we take a step back and look at the term “cloud” in general, we find that content about cloud is slightly larger than content about AWS and has grown 15% since last year. (Keep in mind that a title like&nbsp;<a href="https://learning.oreilly.com/library/view/machine-learning-in/9781119556718/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)"><em>Machine Learning in the AWS Cloud</em></a>&nbsp;would match both terms.) Cloud native—the practice of building applications so that they run first in the cloud and take advantage of cloud services from the start—is up significantly (54%).</p>



<p>We also see another important trend. Usage of content about hybrid clouds and multiclouds is still small (roughly 1/10 of that of Google Cloud, the smallest of the major cloud providers), but growing very fast (145% and 240%, respectively). We won’t split hairs about the difference between a hybrid cloud and a multicloud; there’s enough confusion in the marketplace that, for all practical purposes, they’re identical. But we can say that multicloud and hybrid cloud approaches both reflect a fundamental reality: it’s difficult, if not impossible, to build a cloud strategy around a single provider. Cloud deployments aren’t top-down. They start with a research experiment here, a marketing project there, a group that’s frustrated with the time it takes to requisition hardware, and so on. Sooner or later, you have a cloud deployment—or, more likely, six or seven completely different deployments. By the time someone starts to build a high-level cloud strategy, the organization is already using two or three of the major cloud providers. They are already multicloud, whether or not they realize it. An important part of building a cloud strategy is recognizing that the “cloud” is inherently multi- (or hybrid) and that the biggest issue isn’t which provider to choose but how to build an effective cloud infrastructure across multiple providers. That’s an important aspect of becoming cloud native.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig04-1048x346.png" alt="" class="wp-image-14263" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig04-1048x346.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig04-300x99.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig04-768x253.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig04.png 1197w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for cloud topics</em></figcaption></figure>



<h2>Stable as the Web</h2>



<p>The core technologies for web programming have been very stable over the last two years. Usage of content about core components HTML, CSS, and JavaScript is almost unchanged (up 1%, up 2%, and down 3%, respectively). If Java and Python are table stakes for enterprise and data developers, so much more so are HTML, CSS, and JavaScript for frontend developers. They’re the foundational technologies for the web. If you’re not fluent with them, you’re not part of the conversation.</p>



<p>PHP is hardly a new technology—any PHP user will tell you that almost 80% of the web is&nbsp;<a href="https://w3techs.com/technologies/details/pl-php" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">built with it</a>. The use of content about PHP is up 6%, which doesn’t tell you how many jobs there are or will be but does mean that PHP isn’t leaving anytime soon. The use of content about jQuery (another older technology that’s often used in conjunction with PHP) is up 28%. And interest in web design, a perennial topic that will never go away, is up 23%.</p>



<p>Among the newer frameworks and meta frameworks, Svelte seems to be thriving (up 71%, though from a very low starting point), while interest in Vue and Next.js seems to be fading (down 13% and 13%). Svelte may become a challenger to the more widely used frameworks in a few years if this keeps up. There was surprisingly little interest in Jamstack. That may be because the term rarely appears in the title of books or training, though searches for the term “Jamstack” were also infrequent.</p>



<p>Usage of content about the React framework is also essentially unchanged this year (up 2%), while Angular framework content usage is down significantly (16%). It’s probably just coincidental that JavaScript and React usage are almost identical.</p>



<p>In the Pythonic corner of the web development space, Django is holding steady: the number of units viewed is healthy (and greater than Flask, Svelte, or Vue), and we saw no change year-over-year. Usage of content about Python’s Flask framework is headed downward (12% decline). Likewise, the most widely known Ruby framework, Rails, is down 19%.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig05-1048x578.png" alt="" class="wp-image-14264" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig05-1048x578.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig05-300x165.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig05-768x423.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig05.png 1179w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for web topics</em></figcaption></figure>



<h2>AI, ML, and Data</h2>



<p>There’s been a lot of speculation in the press about artificial intelligence. Are we heading into another “AI winter”? Is it an important technology for today, yesterday’s fad, or something impossibly far off in the future? To some extent, this kind of speculation comes with the territory, especially since Gartner published its famous “<a href="https://www.gartner.com/en/research/methodologies/gartner-hype-cycle" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">hype curve</a>.” AI has certainly been hyped. But is it heading into the so-called “trough of disillusionment”?</p>



<p>We&#8217;d&nbsp;say no. That’s not what our data shows. Yes, usage of content with “artificial intelligence” in the title is down 23% in 2021, and “AI” is down 11%. But these topics are relatively small and narrow. The topic that clearly dominates this space is machine learning (ML): usage of AI plus artificial intelligence content is roughly 1/4 of ML plus machine learning.</p>



<p>What’s the difference between AI and ML? For the purposes of this report, we define machine learning as “the part of artificial intelligence that works”—and, implicitly, the part of AI that’s being put into practice now. AI is, by nature, a research topic. While we have plenty of researchers among our members, our core audience is programmers and engineers: people who are putting technology into practice. And that’s the clue we need to make sense of this puzzle.</p>



<p>Usage of content with “machine learning” in the title is flat year-over-year (down 1%, which is noise). Usage of content with “ML” in the title is up 35%. There are more titles with the phrase “machine learning”; if you add the two up, you get a very slight gain. Still noisy, but positive noise rather than negative. We don’t expect another AI winter—AI is too solidly entrenched in online business practices, and in ways that aren’t as visible as social media recommendations; you’ll never know (or care) whether the company that makes your espresso machine is using machine learning to optimize the manufacturing process and manage inventory, but if they aren’t now, they will be. However, it’s worth noting that AI and ML were the natural outgrowths of “big data” and “data science,” both terms that are now in decline. Big data, of course, never ended; it evolved: just look at the training data needed to build an AI model. The question for the coming year, then, is whether machine learning and artificial intelligence will “evolve”—and if so, into what?</p>



<p>Now let’s look at some specific techniques. Usage on deep learning is down 14%, but usage on neural networks is up 13%, reinforcement learning is up 37%, and adversarial networks is up 51%. Interest has clearly shifted from general topics to specific ones.</p>



<p>Natural language processing has been very much in the news. As was the case for machine learning, usage of content with “natural language processing” in the title hasn’t changed much (up 3%); the abbreviation “NLP” is up 7%. Again, we can look at some of the new techniques that have made the news. The platform had no content on Transformers, BERT, or GPT back in 2020. All three are now coming onto the map. Similarly, there’s currently no content on&nbsp;<a href="https://copilot.github.com/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">GitHub Copilot</a>, which uses the GPT-3 model to translate comments into working code, but we expect it to be a strong performer in 2022.</p>



<p>So what can we conclude? General topics like AI, ML, and GPT are holding their own with content usage or are down. However, usage of content about specific techniques like adversarial networks and reinforcement learning is growing. And content for the newest techniques, like BERT and Transformers, is only now starting to appear. That doesn’t look like a slide into disillusionment but like the natural consequence of a field that’s moving from theory into practice.</p>



<p>It’s also worth looking at the significant increase in the use of content about data governance (up 87%) and GDPR (up 61%). Everyone working with data should know that data governance and its related topics (data provenance, data integrity, auditing, explainability, and many other specialties) aren’t optional. Regulation of the use of data isn’t some vague thing off in the future. It’s here now: GDPR (the EU’s General Data Protection Regulation) is in effect, as is California’s Consumer Privacy Act (CCPA). Now is the time to start thinking about data governance—not later, when it will certainly be too late. Data governance is here to stay, and our platform shows that data professionals are learning about it.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig06-1048x529.png" alt="" class="wp-image-14265" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig06-1048x529.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig06-300x151.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig06-768x388.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig06.png 1210w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for AI and ML&nbsp;topics</em></figcaption></figure>



<h2>Databases</h2>



<p>You can’t talk about machine learning without talking about data and databases. It’s no surprise that, when we look at content usage, Oracle is leading the pack. It’s also no surprise that Oracle’s growth is slow (5%); as we often observe, rapid growth is most often associated with smaller, newer topics. Usage of content about the open source MySQL database (now owned by Oracle) is roughly 1/4 as high and has grown substantially (22%).</p>



<p>It’s worth looking at alternatives to Oracle though. We’ve heard about the death of NoSQL, and certainly usage of content about NoSQL is down (17%). But that isn’t a good metric. NoSQL was never a single technology; databases like Cassandra, HBase, Redis, MongoDB, and many others are wildly different.&nbsp;<a href="http://radar.oreilly.com/2012/02/nosql-non-relational-database.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">NoSQL is really more a movement than a technology</a>—one that’s devoted to expanding the number of storage options for system designers. A good understanding of NoSQL means realizing that for most applications, relational databases are a good fit.</p>



<p>Of the more established NoSQL databases, MongoDB shows 10% growth. Cassandra, Redis, and HBase have declined sharply (27%, 8%, and 57%, respectively). Together, the four show total usage about 40% greater than MySQL, though the total for all four has declined somewhat (4%) since 2020. Momentum has clearly shifted from the NoSQL movement back to relational databases. But that isn’t the end of the story.</p>



<p>We’ve been following graph databases for some time, and in the last year, they’ve gotten a lot of press. But it’s difficult to discuss specific graph databases because most established database vendors have a graph database product integrated into their offering. That said, use of content with the term “graph databases” is up 44%. It’s still a small category, but that’s a significant signal.</p>



<p>Likewise, usage of content about time series databases (databases that associate every entry with a time stamp) is up 21%. Time series databases may prove important for applications stressing monitoring, logging, and observability. Using AI to analyze logs and detect malicious activity is one such application.</p>



<p>Relational databases still dominate the database world, and there’s no reason to expect that to change. Nor should it. The promise of NoSQL wasn’t replacing relational databases; it was increasing the number of options available. The rise of graph and time series databases are simply examples of this promise in action. It will be interesting to see whether this trend continues into 2022.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig07-1048x425.png" alt="" class="wp-image-14266" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig07-1048x425.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig07-300x122.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig07-768x311.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig07.png 1186w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for databases</em></figcaption></figure>



<h2>Operations, DevOps, and SRE</h2>



<p>Operations is “up and to the right.” Very few topics in this group saw declines since last year, and a lot had big gains. As we said last year, it doesn’t really matter what you call operations: call it DevOps, call it SRE, call it George&#8230;this is the task of running the servers, managing software deployment, and keeping the business online. As many found out firsthand during the pandemic, keeping the servers running is crucial, not just to support staff working from home but also to move as much of the business as possible online. People have said “every business is an online business” for years now, but in the past year, that really became true. If your business wasn’t online when COVID-19 hit, it could have easily ceased to exist. Add to that the staffing pressures caused by illness and by resignations or job changes, and it quickly became clear that there’s a real need to do more with less. IT groups found themselves doing much, much more with fewer team members than before. The answer to these challenges is automation (to allow fewer people to manage more systems) and reliability engineering (reducing downtime to reduce staff stress).</p>



<p>We saw substantial increases in the use of titles with the words “observability” (up 124%), “container” (137%), “CI/CD” (109%), “monitoring” (up 36%), and “testing” (16%). A 36% increase for monitoring is very healthy, but the much larger increase for observability shows that this concept is winning people’s hearts and minds. In practice, many find the difference between observability and monitoring confusing. Observability ultimately boils down to the ability to find the information you need to analyze a system’s behavior, while monitoring refers to logging and watching certain preconfigured parameters that indicate the system’s health. It’s a subtle difference—one way to think of it is that monitoring tells you when something’s wrong, but observability gives you the data needed to debug unexpected or strange failure modes, predict failures more reliably, and understand system performance in depth.</p>



<p><a href="https://en.wikipedia.org/wiki/CI/CD" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">CI/CD</a>&nbsp;(continuous integration and continuous deployment) is the latest stage in a long trend of improved tools for automating the development and deployment process, starting way back in the 1970s with Unix’s make utility (for building software) and adding automated testing tools in the early 2000s (JUnit and its relatives) and automated deployment tools a few years later (Jenkins). We now build pipelines that automate the path from the programmer to the server. In the early days of the O’Reilly Velocity Conference, we heard about how companies could build, test, and deploy software many times per day. Automating the deployment process makes it much faster and more reliable, in turn making IT staff more effective because they no longer have to shepherd code “by hand” from the developer’s laptop to the production servers. CI/CD has now become standard practice for almost every online business. It’s something the enterprises that are just moving online, or just moving to the cloud, need to understand to get the most out of their staff.</p>



<p>“Testing” appears to be lagging other terms in this group, but it’s worth noting that the most frequently asked question on O’Reilly Answers was “How do I write good unit test cases?” The practice of automated testing, integrated into the deployment process, is one of the foundations of modern operations. If a software release doesn’t pass all of its tests, it can’t be deployed. That practice gives software developers the confidence to move fast without breaking things.</p>



<p>We’ve also seen increases in content about the tools used to deploy software. Git is up 44%, Kubernetes is up 15%, Docker is up 5%, and Terraform is up 6%. Kubernetes led all topics in this category in units viewed. Furthermore, the two most popular Kubernetes certifications, Certified Kubernetes Application Developer (CKAD) and Certified Kubernetes Administrator (CKA), were up 24% and 13%, respectively. Docker’s relatively low growth may be attributed to the standardization of container formats (the Container Runtime Interface, or CRI), and the removal of Docker as a requirement for Kubernetes. There are now viable alternatives to Docker.</p>



<p>It’s worth looking a bit more at the Kubernetes ecosystem. While usage of content about Kubernetes is up 15% and Helm (Kubernetes’s package manager) is up 68%, usage of content about Istio (a service mesh, an important part of the Kubernetes ecosystem) is sharply down (46%). At first glance, this is confusing: why would Kubernetes and Helm be up, while Istio is down? It’s possible that&nbsp;<a href="https://www.sdxcentral.com/articles/opinion-editorial/googles-istio-move-ignites-open-source-tempest/2020/07/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">open source politics</a>&nbsp;around Google’s control over Istio hurt its adoption, though we suspect that only had a small effect. You’ve probably heard that Kubernetes has a steep learning curve; if you’re a developer, you may have experienced that yourself. Istio said, “Hold my beer, you haven’t seen complex yet.” A service mesh is an important part of container orchestration, but Istio is proving to be&nbsp;<em>too</em>&nbsp;complex. Kubernetes has proven essential for managing cloud deployments; Istio hasn’t.</p>



<p>Both Kubernetes and Istio originated at Google and were designed to solve Google-scale problems. But very few businesses—even those that any reasonable person would call “large”—need to manage IT infrastructure at Google’s scale. Will we eventually have container orchestration tools that solve problems for businesses that aren’t as huge as Google? Work on the Service Mesh Interface (<a href="https://smi-spec.io/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">SMI</a>), a standard interface between service mesh software and Kubernetes, may allow a new generation of service mesh implementations to arise; we hope some of those will be simpler.</p>



<p>Three tools are sharply down: Chef, Puppet, and Ansible (27%, 38%, and 20%). In last year’s report, we showed that the decline of these automated configuration management tools coincided with the rise of Docker and Kubernetes. That decline continues.</p>



<p>What about the top-level terms “operations,” “SRE,” and “DevOps” themselves? Usage of titles containing those words was up (7%, 17%, and 2%, respectively), though obviously these increases are smaller than we saw for tools or concepts. As with AI, we may be seeing this part of the industry mature: our customers are less interested in introductory content about the high-level concepts and more interested in specific ideas and tools that they can use in their businesses. It’s also worth highlighting the 2% increase for DevOps. Our 2020 report showed DevOps down 17% from 2019 to 2020. In 2021, that slide has stopped. Over time, we expect that terms like DevOps and SRE will come and go, but the concepts and the tools that they introduced will be with us long-term.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig08-1048x809.png" alt="" class="wp-image-14267" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig08-1048x809.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig08-300x231.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig08-768x592.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig08.png 1199w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for operations, DevOps, and SRE</em></figcaption></figure>



<p>Finally, look at the units viewed for Linux: it’s second only to Kubernetes. While down very slightly in 2021, we don’t believe that’s significant. Linux has long been the most widely used server operating system, and it’s not ceding that top spot soon. If anything, its importance has increased: Linux is the standard operating system for the cloud. Even on Azure,&nbsp;<a href="https://www.zdnet.com/article/microsoft-developer-reveals-linux-is-now-more-used-on-azure-than-windows-server/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Linux dominates</a>. Solid knowledge of Linux is essential for anyone working in operations today.</p>



<h2>Cryptocurrency and Blockchain</h2>



<p>Now we’ll look at some ideas that have exploded in the last year. They aren’t necessarily new, but for various reasons they’ve taken off. Our data on these topics tends to be hazy. And, in Arlo Guthrie’s words, many of these topics have “come around on the guitar” one or more times in the past only to fade back into the noise.</p>



<p>Whether it’s the future of finance or history’s biggest&nbsp;<a href="https://www.cnbc.com/2021/04/23/bitcoin-a-gimmick-and-resembles-a-ponzi-scheme-black-swan-author-.html" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Ponzi scheme</a>, use of content about cryptocurrency is up 271%, with content about the cryptocurrencies Bitcoin and Ethereum (ether) up 166% and 185% respectively. General content about blockchains is up 78%, and from a much higher starting point (reflecting the fact that our audience has more developers than speculators).&nbsp;<a href="https://www.hyperledger.org/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Hyperledger</a>, a collection of blockchain technologies that targets enterprise markets, is up 66%. Our data doesn’t tell you whether to buy bitcoin or ether, but it does show a huge increase in interest.</p>



<p>We’ve seen a huge increase of interest in nonfungible tokens (<a href="https://en.wikipedia.org/wiki/Non-fungible_token" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">NFTs</a>), but there’s where we run into data availability problems. Searches for the term “NFT” are up 4,474%, almost 45 times higher year-over-year. Granted, that’s from an extremely small starting point (only 26 searches in 2019). From that starting point, a 45x increase still takes NFTs to a relatively small endpoint. So which do you believe? A 45x increase or a small endpoint? Take your pick, but our data shows that NFTs shouldn’t be ignored.</p>



<p>Web3 is a collection of ideas about a “next generation” web that’s designed so that it can’t be dominated by a small number of gigantic platforms, like Facebook and Google. Web3 proponents typically mix decentralized protocols like the InterPlanetary File System (<a href="https://en.wikipedia.org/wiki/InterPlanetary_File_System" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">IPFS</a>) with blockchains and NFTs to make content immutable and ownable. As with NFTs, no content in our platform has “Web3” in the title. But we’ve seen a 343% increase in the number of searches for the term—again, from a small starting point. We’ve been watching decentralized web technologies for years (we staged a peer-to-peer conference in 2001) and wonder whether the connection between the decentralized web and blockchain will make it take off. Possibly&#8230;or possibly not. It isn’t clear what blockchains and NFTs bring to Web3 aside from the hype. We already have a web where anyone can publish. A web where everything has to be owned and where requiring all transactions to pay a tax to blockchain miners isn’t a step forward. We also see no guarantee that a decentralized web couldn’t be dominated by a small number of Google-sized players. We can’t tell you whether Web3 will succeed, but our data shows that it’s becoming an idea worth watching.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig09-1048x257.png" alt="" class="wp-image-14268" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig09-1048x257.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig09-300x74.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig09-768x188.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig09.png 1228w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for blockchain topics</em></figcaption></figure>



<h2>Virtual Reality, Augmented Reality, and the Metaverse</h2>



<p>Virtual and augmented reality are also topics we’ve been tracking for years. They’ve often seemed at the point of breaking out, but they’ve never made it, at least in part because nobody wants to hang around wearing goggles all the time.&nbsp;<a href="https://en.wikipedia.org/wiki/Google_Glass" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Google Glass</a>&nbsp;looked like it had a chance back in 2013, and it survives to this day in an&nbsp;<a href="https://www.google.com/glass/start/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">enterprise edition</a>, but it never achieved widespread use. Startups like Oculus (now part of Meta) have made VR goggles aimed at consumers, but they’ve never broken beyond a small segment of the gamer market.</p>



<p>What about this year? We still think VR and AR are on their way.&nbsp;<a href="https://www.theverge.com/22588022/mark-zuckerberg-facebook-ceo-metaverse-interview" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Mark Zuckerberg</a>&nbsp;kicked off a storm by talking about “the metaverse” back in July, and by more recently renaming Facebook “Meta.”&nbsp;<a href="https://news.microsoft.com/innovation-stories/mesh-for-microsoft-teams/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Microsoft</a>&nbsp;and&nbsp;<a href="https://nianticlabs.com/blog/real-world-metaverse/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">many other companies</a>&nbsp;have followed suit by announcing their versions of the metaverse.&nbsp;<a href="https://www.tomsguide.com/news/apple-glasses" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Apple’s been quiet</a>, but the company is working on augmented reality glasses. (What little we’ve heard sounds like an update of Google Glass with current technology—but if any company’s core expertise is making something cool, it’s Apple.)</p>



<p>Has all this ferment shown up in our platform data? Keep in mind that we’re only using data through September (in both 2020 and 2021). The results are ambiguous. Use of titles containing the phrase “augmented reality” is down (22%), and those are the most heavily used titles in this group. But virtual reality, VR, and AR are all up (13%, 28%, and 116%, respectively), yielding a 24% gain across the entire group.</p>



<p>The term “metaverse” hasn’t shown up in any titles, though there’s a sharp increase in the number of searches for it (489%). And content about&nbsp;<a href="https://www.w3.org/TR/webxr/" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">WebXR</a>, a vendor-neutral standard for rendering 3D content on VR- and AR-capable devices (in addition to pedestrian 2D devices), is now starting to show up. (VRML, an older standard, has vanished from view.) No content on WebXR was available in 2020, but some has appeared in 2021, and searches for “WebXR” have increased by 168%.</p>



<p>We’ll forgive you if you decide to bet against VR. Meta (née Facebook) has dragged its own name through the mud for way too long; while the company might succeed, it’s hard to imagine many people wanting to share video of the intimate details of their life with them. And while Zuckerberg is excited about the metaverse’s potential for “work from home” employees, it’s extremely difficult to imagine that a company will want a video feed of its staff’s activities going to the Meta mothership. But Apple has really become a master of conspicuous consumerism. It’s very hard to bet against them when it comes to making high-tech fashion accessories. Mark us cautiously&nbsp;skeptical.</p>



<figure class="wp-block-image size-large"><img src="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig10-1048x226.png" alt="" class="wp-image-14269" srcset="https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig10-1048x226.png 1048w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig10-300x65.png 300w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig10-768x166.png 768w, https://www.oreilly.com/radar/wp-content/uploads/sites/3/2022/01/fig10.png 1195w" sizes="(max-width: 1048px) 100vw, 1048px" /><figcaption><em>Units viewed and year-over-year growth for VR and AR topics</em></figcaption></figure>



<h2>Until Next Year</h2>



<p>So after reviewing over a billion units viewed on over 50,000 items in the O’Reilly learning platform, after looking at a million unique search queries plus a smaller number of queries from Answers, where are we? What can we say about the coming year?</p>



<p>Many events grab attention: GPT-3 generating text that could have been written by humans. Cybercriminals demanding millions of dollars after a ransomware attack. Other newsworthy topics include new technologies like NFTs that are just starting to show up in our data and older technologies like virtual reality that may be on the brink of a surge. And there are even more technologies that get a lot of coverage in the technology press, though they aren’t yet appearing in our data in significant ways: robotic process automation (RPA), digital twins, edge computing, and 5G, to name a few. All of these technologies are important, or might be important, depending on where the future takes us. Some are genuinely exciting; others are rebrandings of older ideas.</p>



<p>The real work of technology isn’t coming up with splashy demos; it’s the hard work of taking these breakthroughs and integrating them into products. It’s coming up with solutions to real problems and deploying those as real-world services. It’s defending your IT infrastructure against attack in the middle of a pandemic. Using natural language models to build customer service systems that are less frustrating for the customer and the customer service agent; auditing loan approval systems to see whether they’re fair; preventing ransomware attacks rather than succumbing to them. It probably won’t make the news if there are 20% fewer successful ransomware attacks in the coming year. After all, few people notice when something&nbsp;<em>doesn’t</em>&nbsp;happen. But all of us will be safer nonetheless.</p>



<p>These are the changes that affect our lives, and these are the kinds of changes we see by looking at the data on our platform. Users learning more about security; customers learning more about architecting software for the cloud; programmers trying to come to terms with concurrency, and learning new languages and techniques to deal with complexity; and much more. We see artificial intelligence moving into the real world, with all the problems and opportunities that entails, and we see enterprises realizing that operations isn’t just a cost center—it’s the lifeblood of the business.</p>



<p>That’s the big picture, which (like a&nbsp;<a href="https://en.wikipedia.org/wiki/Pieter_Bruegel_the_Elder" target="_blank" rel="noreferrer noopener" aria-label=" (opens in a new tab)">Bruegel</a>&nbsp;painting) is built from many, many people, each doing what they think is important, each solving the problem that they face. Understanding technology—and understanding what the O’Reilly platform tells us—is not really about the flashy events, important though they may be; it’s all about understanding the people who depend on our platform every day and what they need to learn to get on with the task of building their futures.</p>



<hr class="wp-block-separator" />



<h3>Footnote</h3>



<ol><li>Last year’s platform report was based on January through August, so the two papers aren’t directly comparable. </li></ol>
]]></content:encoded>
      <wfw:commentRss>https://www.oreilly.com/radar/technology-trends-for-2022/feed/</wfw:commentRss>
      <slash:comments>0</slash:comments>
    </item>
  </channel>
</rss>